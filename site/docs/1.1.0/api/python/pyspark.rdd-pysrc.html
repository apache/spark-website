<?xml version="1.0" encoding="ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
          "DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <title>pyspark.rdd</title>
  <link rel="stylesheet" href="epydoc.css" type="text/css" />
  <script type="text/javascript" src="epydoc.js"></script>
</head>

<body bgcolor="white" text="black" link="blue" vlink="#204080"
      alink="#204080">
<!-- ==================== NAVIGATION BAR ==================== -->
<table class="navbar" border="0" width="100%" cellpadding="0"
       bgcolor="#a0c0ff" cellspacing="0">
  <tr valign="middle">
  <!-- Home link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="pyspark-module.html">Home</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Tree link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="module-tree.html">Trees</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Index link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="identifier-index.html">Indices</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Help link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="help.html">Help</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Project homepage -->
      <th class="navbar" align="right" width="100%">
        <table border="0" cellpadding="0" cellspacing="0">
          <tr><th class="navbar" align="center"
            ><a class="navbar" target="_top" href="http://spark.apache.org">Spark 1.1.0 Python API Docs</a></th>
          </tr></table></th>
  </tr>
</table>
<table width="100%" cellpadding="0" cellspacing="0">
  <tr valign="top">
    <td width="100%">
      <span class="breadcrumbs">
        <a href="pyspark-module.html">Package&nbsp;pyspark</a> ::
        Module&nbsp;rdd
      </span>
    </td>
    <td>
      <table cellpadding="0" cellspacing="0">
        <!-- hide/show private -->
        <tr><td align="right"><span class="options"
            >[<a href="frames.html" target="_top">frames</a
            >]&nbsp;|&nbsp;<a href="pyspark.rdd-pysrc.html"
            target="_top">no&nbsp;frames</a>]</span></td></tr>
      </table>
    </td>
  </tr>
</table>
<h1 class="epydoc">Source Code for <a href="pyspark.rdd-module.html">Module pyspark.rdd</a></h1>
<pre class="py-src">
<a name="L1"></a><tt class="py-lineno">   1</tt>  <tt class="py-line"><tt class="py-comment">#</tt> </tt>
<a name="L2"></a><tt class="py-lineno">   2</tt>  <tt class="py-line"><tt class="py-comment"># Licensed to the Apache Software Foundation (ASF) under one or more</tt> </tt>
<a name="L3"></a><tt class="py-lineno">   3</tt>  <tt class="py-line"><tt class="py-comment"># contributor license agreements.  See the NOTICE file distributed with</tt> </tt>
<a name="L4"></a><tt class="py-lineno">   4</tt>  <tt class="py-line"><tt class="py-comment"># this work for additional information regarding copyright ownership.</tt> </tt>
<a name="L5"></a><tt class="py-lineno">   5</tt>  <tt class="py-line"><tt class="py-comment"># The ASF licenses this file to You under the Apache License, Version 2.0</tt> </tt>
<a name="L6"></a><tt class="py-lineno">   6</tt>  <tt class="py-line"><tt class="py-comment"># (the "License"); you may not use this file except in compliance with</tt> </tt>
<a name="L7"></a><tt class="py-lineno">   7</tt>  <tt class="py-line"><tt class="py-comment"># the License.  You may obtain a copy of the License at</tt> </tt>
<a name="L8"></a><tt class="py-lineno">   8</tt>  <tt class="py-line"><tt class="py-comment">#</tt> </tt>
<a name="L9"></a><tt class="py-lineno">   9</tt>  <tt class="py-line"><tt class="py-comment">#    http://www.apache.org/licenses/LICENSE-2.0</tt> </tt>
<a name="L10"></a><tt class="py-lineno">  10</tt>  <tt class="py-line"><tt class="py-comment">#</tt> </tt>
<a name="L11"></a><tt class="py-lineno">  11</tt>  <tt class="py-line"><tt class="py-comment"># Unless required by applicable law or agreed to in writing, software</tt> </tt>
<a name="L12"></a><tt class="py-lineno">  12</tt>  <tt class="py-line"><tt class="py-comment"># distributed under the License is distributed on an "AS IS" BASIS,</tt> </tt>
<a name="L13"></a><tt class="py-lineno">  13</tt>  <tt class="py-line"><tt class="py-comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</tt> </tt>
<a name="L14"></a><tt class="py-lineno">  14</tt>  <tt class="py-line"><tt class="py-comment"># See the License for the specific language governing permissions and</tt> </tt>
<a name="L15"></a><tt class="py-lineno">  15</tt>  <tt class="py-line"><tt class="py-comment"># limitations under the License.</tt> </tt>
<a name="L16"></a><tt class="py-lineno">  16</tt>  <tt class="py-line"><tt class="py-comment">#</tt> </tt>
<a name="L17"></a><tt class="py-lineno">  17</tt>  <tt class="py-line"> </tt>
<a name="L18"></a><tt class="py-lineno">  18</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt class="py-name">base64</tt> <tt class="py-keyword">import</tt> <tt class="py-name">standard_b64encode</tt> <tt class="py-keyword">as</tt> <tt class="py-name">b64enc</tt> </tt>
<a name="L19"></a><tt class="py-lineno">  19</tt>  <tt class="py-line"><tt class="py-keyword">import</tt> <tt id="link-0" class="py-name" targets="Method pyspark.statcounter.StatCounter.copy()=pyspark.statcounter.StatCounter-class.html#copy"><a title="pyspark.statcounter.StatCounter.copy" class="py-name" href="#" onclick="return doclink('link-0', 'copy', 'link-0');">copy</a></tt> </tt>
<a name="L20"></a><tt class="py-lineno">  20</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt class="py-name">collections</tt> <tt class="py-keyword">import</tt> <tt class="py-name">defaultdict</tt> </tt>
<a name="L21"></a><tt class="py-lineno">  21</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt class="py-name">collections</tt> <tt class="py-keyword">import</tt> <tt class="py-name">namedtuple</tt> </tt>
<a name="L22"></a><tt class="py-lineno">  22</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt class="py-name">itertools</tt> <tt class="py-keyword">import</tt> <tt class="py-name">chain</tt><tt class="py-op">,</tt> <tt class="py-name">ifilter</tt><tt class="py-op">,</tt> <tt class="py-name">imap</tt> </tt>
<a name="L23"></a><tt class="py-lineno">  23</tt>  <tt class="py-line"><tt class="py-keyword">import</tt> <tt class="py-name">operator</tt> </tt>
<a name="L24"></a><tt class="py-lineno">  24</tt>  <tt class="py-line"><tt class="py-keyword">import</tt> <tt class="py-name">os</tt> </tt>
<a name="L25"></a><tt class="py-lineno">  25</tt>  <tt class="py-line"><tt class="py-keyword">import</tt> <tt class="py-name">sys</tt> </tt>
<a name="L26"></a><tt class="py-lineno">  26</tt>  <tt class="py-line"><tt class="py-keyword">import</tt> <tt class="py-name">shlex</tt> </tt>
<a name="L27"></a><tt class="py-lineno">  27</tt>  <tt class="py-line"><tt class="py-keyword">import</tt> <tt class="py-name">traceback</tt> </tt>
<a name="L28"></a><tt class="py-lineno">  28</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt class="py-name">subprocess</tt> <tt class="py-keyword">import</tt> <tt class="py-name">Popen</tt><tt class="py-op">,</tt> <tt class="py-name">PIPE</tt> </tt>
<a name="L29"></a><tt class="py-lineno">  29</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt class="py-name">tempfile</tt> <tt class="py-keyword">import</tt> <tt class="py-name">NamedTemporaryFile</tt> </tt>
<a name="L30"></a><tt class="py-lineno">  30</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt class="py-name">threading</tt> <tt class="py-keyword">import</tt> <tt class="py-name">Thread</tt> </tt>
<a name="L31"></a><tt class="py-lineno">  31</tt>  <tt class="py-line"><tt class="py-keyword">import</tt> <tt class="py-name">warnings</tt> </tt>
<a name="L32"></a><tt class="py-lineno">  32</tt>  <tt class="py-line"><tt class="py-keyword">import</tt> <tt class="py-name">heapq</tt> </tt>
<a name="L33"></a><tt class="py-lineno">  33</tt>  <tt class="py-line"><tt class="py-keyword">import</tt> <tt class="py-name">bisect</tt> </tt>
<a name="L34"></a><tt class="py-lineno">  34</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt id="link-1" class="py-name" targets="Module pyspark.mllib.random=pyspark.mllib.random-module.html"><a title="pyspark.mllib.random" class="py-name" href="#" onclick="return doclink('link-1', 'random', 'link-1');">random</a></tt> <tt class="py-keyword">import</tt> <tt class="py-name">Random</tt> </tt>
<a name="L35"></a><tt class="py-lineno">  35</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt class="py-name">math</tt> <tt class="py-keyword">import</tt> <tt id="link-2" class="py-name" targets="Variable pyspark.statcounter.sqrt=pyspark.statcounter-module.html#sqrt"><a title="pyspark.statcounter.sqrt" class="py-name" href="#" onclick="return doclink('link-2', 'sqrt', 'link-2');">sqrt</a></tt><tt class="py-op">,</tt> <tt class="py-name">log</tt><tt class="py-op">,</tt> <tt class="py-name">isinf</tt><tt class="py-op">,</tt> <tt class="py-name">isnan</tt> </tt>
<a name="L36"></a><tt class="py-lineno">  36</tt>  <tt class="py-line"> </tt>
<a name="L37"></a><tt class="py-lineno">  37</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt id="link-3" class="py-name" targets="Package pyspark=pyspark-module.html"><a title="pyspark" class="py-name" href="#" onclick="return doclink('link-3', 'pyspark', 'link-3');">pyspark</a></tt><tt class="py-op">.</tt><tt id="link-4" class="py-name" targets="Module pyspark.serializers=pyspark.serializers-module.html"><a title="pyspark.serializers" class="py-name" href="#" onclick="return doclink('link-4', 'serializers', 'link-4');">serializers</a></tt> <tt class="py-keyword">import</tt> <tt class="py-name">NoOpSerializer</tt><tt class="py-op">,</tt> <tt class="py-name">CartesianDeserializer</tt><tt class="py-op">,</tt> \ </tt>
<a name="L38"></a><tt class="py-lineno">  38</tt>  <tt class="py-line">    <tt class="py-name">BatchedSerializer</tt><tt class="py-op">,</tt> <tt class="py-name">CloudPickleSerializer</tt><tt class="py-op">,</tt> <tt class="py-name">PairDeserializer</tt><tt class="py-op">,</tt> \ </tt>
<a name="L39"></a><tt class="py-lineno">  39</tt>  <tt class="py-line">    <tt id="link-5" class="py-name" targets="Class pyspark.serializers.PickleSerializer=pyspark.serializers.PickleSerializer-class.html"><a title="pyspark.serializers.PickleSerializer" class="py-name" href="#" onclick="return doclink('link-5', 'PickleSerializer', 'link-5');">PickleSerializer</a></tt><tt class="py-op">,</tt> <tt class="py-name">pack_long</tt><tt class="py-op">,</tt> <tt class="py-name">CompressedSerializer</tt> </tt>
<a name="L40"></a><tt class="py-lineno">  40</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt id="link-6" class="py-name"><a title="pyspark" class="py-name" href="#" onclick="return doclink('link-6', 'pyspark', 'link-3');">pyspark</a></tt><tt class="py-op">.</tt><tt id="link-7" class="py-name" targets="Method pyspark.rdd.RDD.join()=pyspark.rdd.RDD-class.html#join"><a title="pyspark.rdd.RDD.join" class="py-name" href="#" onclick="return doclink('link-7', 'join', 'link-7');">join</a></tt> <tt class="py-keyword">import</tt> <tt class="py-name">python_join</tt><tt class="py-op">,</tt> <tt class="py-name">python_left_outer_join</tt><tt class="py-op">,</tt> \ </tt>
<a name="L41"></a><tt class="py-lineno">  41</tt>  <tt class="py-line">    <tt class="py-name">python_right_outer_join</tt><tt class="py-op">,</tt> <tt class="py-name">python_cogroup</tt> </tt>
<a name="L42"></a><tt class="py-lineno">  42</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt id="link-8" class="py-name"><a title="pyspark" class="py-name" href="#" onclick="return doclink('link-8', 'pyspark', 'link-3');">pyspark</a></tt><tt class="py-op">.</tt><tt id="link-9" class="py-name" targets="Module pyspark.statcounter=pyspark.statcounter-module.html"><a title="pyspark.statcounter" class="py-name" href="#" onclick="return doclink('link-9', 'statcounter', 'link-9');">statcounter</a></tt> <tt class="py-keyword">import</tt> <tt id="link-10" class="py-name" targets="Class pyspark.statcounter.StatCounter=pyspark.statcounter.StatCounter-class.html"><a title="pyspark.statcounter.StatCounter" class="py-name" href="#" onclick="return doclink('link-10', 'StatCounter', 'link-10');">StatCounter</a></tt> </tt>
<a name="L43"></a><tt class="py-lineno">  43</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt id="link-11" class="py-name"><a title="pyspark" class="py-name" href="#" onclick="return doclink('link-11', 'pyspark', 'link-3');">pyspark</a></tt><tt class="py-op">.</tt><tt class="py-name">rddsampler</tt> <tt class="py-keyword">import</tt> <tt class="py-name">RDDSampler</tt><tt class="py-op">,</tt> <tt class="py-name">RDDStratifiedSampler</tt> </tt>
<a name="L44"></a><tt class="py-lineno">  44</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt id="link-12" class="py-name"><a title="pyspark" class="py-name" href="#" onclick="return doclink('link-12', 'pyspark', 'link-3');">pyspark</a></tt><tt class="py-op">.</tt><tt id="link-13" class="py-name" targets="Module pyspark.storagelevel=pyspark.storagelevel-module.html"><a title="pyspark.storagelevel" class="py-name" href="#" onclick="return doclink('link-13', 'storagelevel', 'link-13');">storagelevel</a></tt> <tt class="py-keyword">import</tt> <tt id="link-14" class="py-name" targets="Class pyspark.storagelevel.StorageLevel=pyspark.storagelevel.StorageLevel-class.html"><a title="pyspark.storagelevel.StorageLevel" class="py-name" href="#" onclick="return doclink('link-14', 'StorageLevel', 'link-14');">StorageLevel</a></tt> </tt>
<a name="L45"></a><tt class="py-lineno">  45</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt id="link-15" class="py-name"><a title="pyspark" class="py-name" href="#" onclick="return doclink('link-15', 'pyspark', 'link-3');">pyspark</a></tt><tt class="py-op">.</tt><tt id="link-16" class="py-name" targets="Module pyspark.resultiterable=pyspark.resultiterable-module.html"><a title="pyspark.resultiterable" class="py-name" href="#" onclick="return doclink('link-16', 'resultiterable', 'link-16');">resultiterable</a></tt> <tt class="py-keyword">import</tt> <tt id="link-17" class="py-name" targets="Class pyspark.resultiterable.ResultIterable=pyspark.resultiterable.ResultIterable-class.html"><a title="pyspark.resultiterable.ResultIterable" class="py-name" href="#" onclick="return doclink('link-17', 'ResultIterable', 'link-17');">ResultIterable</a></tt> </tt>
<a name="L46"></a><tt class="py-lineno">  46</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt id="link-18" class="py-name"><a title="pyspark" class="py-name" href="#" onclick="return doclink('link-18', 'pyspark', 'link-3');">pyspark</a></tt><tt class="py-op">.</tt><tt class="py-name">shuffle</tt> <tt class="py-keyword">import</tt> <tt class="py-name">Aggregator</tt><tt class="py-op">,</tt> <tt class="py-name">InMemoryMerger</tt><tt class="py-op">,</tt> <tt class="py-name">ExternalMerger</tt><tt class="py-op">,</tt> \ </tt>
<a name="L47"></a><tt class="py-lineno">  47</tt>  <tt class="py-line">    <tt class="py-name">get_used_memory</tt> </tt>
<a name="L48"></a><tt class="py-lineno">  48</tt>  <tt class="py-line"> </tt>
<a name="L49"></a><tt class="py-lineno">  49</tt>  <tt class="py-line"><tt class="py-keyword">from</tt> <tt class="py-name">py4j</tt><tt class="py-op">.</tt><tt class="py-name">java_collections</tt> <tt class="py-keyword">import</tt> <tt class="py-name">ListConverter</tt><tt class="py-op">,</tt> <tt class="py-name">MapConverter</tt> </tt>
<a name="L50"></a><tt class="py-lineno">  50</tt>  <tt class="py-line"> </tt>
<a name="L51"></a><tt class="py-lineno">  51</tt>  <tt class="py-line"><tt class="py-name">__all__</tt> <tt class="py-op">=</tt> <tt class="py-op">[</tt><tt class="py-string">"RDD"</tt><tt class="py-op">]</tt> </tt>
<a name="portable_hash"></a><div id="portable_hash-def"><a name="L52"></a><tt class="py-lineno">  52</tt>  <tt class="py-line"> </tt>
<a name="L53"></a><tt class="py-lineno">  53</tt>  <tt class="py-line"> </tt>
<a name="L54"></a><tt class="py-lineno">  54</tt>  <tt class="py-line"><tt class="py-comment"># TODO: for Python 3.3+, PYTHONHASHSEED should be reset to disable randomized</tt> </tt>
<a name="L55"></a><tt class="py-lineno">  55</tt>  <tt class="py-line"><tt class="py-comment"># hash for string</tt> </tt>
<a name="L56"></a><tt class="py-lineno">  56</tt> <a class="py-toggle" href="#" id="portable_hash-toggle" onclick="return toggle('portable_hash');">-</a><tt class="py-line"><tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd-module.html#portable_hash">portable_hash</a><tt class="py-op">(</tt><tt class="py-param">x</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="portable_hash-collapsed" style="display:none;" pad="++++" indent="++++"></div><div id="portable_hash-expanded"><a name="L57"></a><tt class="py-lineno">  57</tt>  <tt class="py-line">    <tt class="py-docstring">"""</tt> </tt>
<a name="L58"></a><tt class="py-lineno">  58</tt>  <tt class="py-line"><tt class="py-docstring">    This function returns consistent hash code for builtin types, especially</tt> </tt>
<a name="L59"></a><tt class="py-lineno">  59</tt>  <tt class="py-line"><tt class="py-docstring">    for None and tuple with None.</tt> </tt>
<a name="L60"></a><tt class="py-lineno">  60</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L61"></a><tt class="py-lineno">  61</tt>  <tt class="py-line"><tt class="py-docstring">    The algrithm is similar to that one used by CPython 2.7</tt> </tt>
<a name="L62"></a><tt class="py-lineno">  62</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L63"></a><tt class="py-lineno">  63</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; portable_hash(None)</tt> </tt>
<a name="L64"></a><tt class="py-lineno">  64</tt>  <tt class="py-line"><tt class="py-docstring">    0</tt> </tt>
<a name="L65"></a><tt class="py-lineno">  65</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; portable_hash((None, 1))</tt> </tt>
<a name="L66"></a><tt class="py-lineno">  66</tt>  <tt class="py-line"><tt class="py-docstring">    219750521</tt> </tt>
<a name="L67"></a><tt class="py-lineno">  67</tt>  <tt class="py-line"><tt class="py-docstring">    """</tt> </tt>
<a name="L68"></a><tt class="py-lineno">  68</tt>  <tt class="py-line">    <tt class="py-keyword">if</tt> <tt class="py-name">x</tt> <tt class="py-keyword">is</tt> <tt class="py-name">None</tt><tt class="py-op">:</tt> </tt>
<a name="L69"></a><tt class="py-lineno">  69</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-number">0</tt> </tt>
<a name="L70"></a><tt class="py-lineno">  70</tt>  <tt class="py-line">    <tt class="py-keyword">if</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">tuple</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L71"></a><tt class="py-lineno">  71</tt>  <tt class="py-line">        <tt class="py-name">h</tt> <tt class="py-op">=</tt> <tt class="py-number">0x345678</tt> </tt>
<a name="L72"></a><tt class="py-lineno">  72</tt>  <tt class="py-line">        <tt class="py-keyword">for</tt> <tt class="py-name">i</tt> <tt class="py-keyword">in</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> </tt>
<a name="L73"></a><tt class="py-lineno">  73</tt>  <tt class="py-line">            <tt class="py-name">h</tt> <tt class="py-op">^=</tt> <tt class="py-name">portable_hash</tt><tt class="py-op">(</tt><tt class="py-name">i</tt><tt class="py-op">)</tt> </tt>
<a name="L74"></a><tt class="py-lineno">  74</tt>  <tt class="py-line">            <tt class="py-name">h</tt> <tt class="py-op">*=</tt> <tt class="py-number">1000003</tt> </tt>
<a name="L75"></a><tt class="py-lineno">  75</tt>  <tt class="py-line">            <tt class="py-name">h</tt> <tt class="py-op">&amp;=</tt> <tt class="py-number">0xffffffff</tt> </tt>
<a name="L76"></a><tt class="py-lineno">  76</tt>  <tt class="py-line">        <tt class="py-name">h</tt> <tt class="py-op">^=</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt> </tt>
<a name="L77"></a><tt class="py-lineno">  77</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">h</tt> <tt class="py-op">==</tt> <tt class="py-op">-</tt><tt class="py-number">1</tt><tt class="py-op">:</tt> </tt>
<a name="L78"></a><tt class="py-lineno">  78</tt>  <tt class="py-line">            <tt class="py-name">h</tt> <tt class="py-op">=</tt> <tt class="py-op">-</tt><tt class="py-number">2</tt> </tt>
<a name="L79"></a><tt class="py-lineno">  79</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">h</tt> </tt>
<a name="L80"></a><tt class="py-lineno">  80</tt>  <tt class="py-line">    <tt class="py-keyword">return</tt> <tt class="py-name">hash</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt> </tt>
</div><a name="L81"></a><tt class="py-lineno">  81</tt>  <tt class="py-line"> </tt>
<a name="_extract_concise_traceback"></a><div id="_extract_concise_traceback-def"><a name="L82"></a><tt class="py-lineno">  82</tt>  <tt class="py-line"> </tt>
<a name="L83"></a><tt class="py-lineno">  83</tt> <a class="py-toggle" href="#" id="_extract_concise_traceback-toggle" onclick="return toggle('_extract_concise_traceback');">-</a><tt class="py-line"><tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd-module.html#_extract_concise_traceback">_extract_concise_traceback</a><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="_extract_concise_traceback-collapsed" style="display:none;" pad="++++" indent="++++"></div><div id="_extract_concise_traceback-expanded"><a name="L84"></a><tt class="py-lineno">  84</tt>  <tt class="py-line">    <tt class="py-docstring">"""</tt> </tt>
<a name="L85"></a><tt class="py-lineno">  85</tt>  <tt class="py-line"><tt class="py-docstring">    This function returns the traceback info for a callsite, returns a dict</tt> </tt>
<a name="L86"></a><tt class="py-lineno">  86</tt>  <tt class="py-line"><tt class="py-docstring">    with function name, file name and line number</tt> </tt>
<a name="L87"></a><tt class="py-lineno">  87</tt>  <tt class="py-line"><tt class="py-docstring">    """</tt> </tt>
<a name="L88"></a><tt class="py-lineno">  88</tt>  <tt class="py-line">    <tt class="py-name">tb</tt> <tt class="py-op">=</tt> <tt class="py-name">traceback</tt><tt class="py-op">.</tt><tt class="py-name">extract_stack</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L89"></a><tt class="py-lineno">  89</tt>  <tt class="py-line">    <tt class="py-name">callsite</tt> <tt class="py-op">=</tt> <tt class="py-name">namedtuple</tt><tt class="py-op">(</tt><tt class="py-string">"Callsite"</tt><tt class="py-op">,</tt> <tt class="py-string">"function file linenum"</tt><tt class="py-op">)</tt> </tt>
<a name="L90"></a><tt class="py-lineno">  90</tt>  <tt class="py-line">    <tt class="py-keyword">if</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">tb</tt><tt class="py-op">)</tt> <tt class="py-op">==</tt> <tt class="py-number">0</tt><tt class="py-op">:</tt> </tt>
<a name="L91"></a><tt class="py-lineno">  91</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">None</tt> </tt>
<a name="L92"></a><tt class="py-lineno">  92</tt>  <tt class="py-line">    <tt class="py-name">file</tt><tt class="py-op">,</tt> <tt class="py-name">line</tt><tt class="py-op">,</tt> <tt class="py-name">module</tt><tt class="py-op">,</tt> <tt class="py-name">what</tt> <tt class="py-op">=</tt> <tt class="py-name">tb</tt><tt class="py-op">[</tt><tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">tb</tt><tt class="py-op">)</tt> <tt class="py-op">-</tt> <tt class="py-number">1</tt><tt class="py-op">]</tt> </tt>
<a name="L93"></a><tt class="py-lineno">  93</tt>  <tt class="py-line">    <tt class="py-name">sparkpath</tt> <tt class="py-op">=</tt> <tt class="py-name">os</tt><tt class="py-op">.</tt><tt class="py-name">path</tt><tt class="py-op">.</tt><tt class="py-name">dirname</tt><tt class="py-op">(</tt><tt class="py-name">file</tt><tt class="py-op">)</tt> </tt>
<a name="L94"></a><tt class="py-lineno">  94</tt>  <tt class="py-line">    <tt class="py-name">first_spark_frame</tt> <tt class="py-op">=</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">tb</tt><tt class="py-op">)</tt> <tt class="py-op">-</tt> <tt class="py-number">1</tt> </tt>
<a name="L95"></a><tt class="py-lineno">  95</tt>  <tt class="py-line">    <tt class="py-keyword">for</tt> <tt class="py-name">i</tt> <tt class="py-keyword">in</tt> <tt class="py-name">range</tt><tt class="py-op">(</tt><tt class="py-number">0</tt><tt class="py-op">,</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">tb</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L96"></a><tt class="py-lineno">  96</tt>  <tt class="py-line">        <tt class="py-name">file</tt><tt class="py-op">,</tt> <tt class="py-name">line</tt><tt class="py-op">,</tt> <tt class="py-name">fun</tt><tt class="py-op">,</tt> <tt class="py-name">what</tt> <tt class="py-op">=</tt> <tt class="py-name">tb</tt><tt class="py-op">[</tt><tt class="py-name">i</tt><tt class="py-op">]</tt> </tt>
<a name="L97"></a><tt class="py-lineno">  97</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">file</tt><tt class="py-op">.</tt><tt class="py-name">startswith</tt><tt class="py-op">(</tt><tt class="py-name">sparkpath</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L98"></a><tt class="py-lineno">  98</tt>  <tt class="py-line">            <tt class="py-name">first_spark_frame</tt> <tt class="py-op">=</tt> <tt class="py-name">i</tt> </tt>
<a name="L99"></a><tt class="py-lineno">  99</tt>  <tt class="py-line">            <tt class="py-keyword">break</tt> </tt>
<a name="L100"></a><tt class="py-lineno"> 100</tt>  <tt class="py-line">    <tt class="py-keyword">if</tt> <tt class="py-name">first_spark_frame</tt> <tt class="py-op">==</tt> <tt class="py-number">0</tt><tt class="py-op">:</tt> </tt>
<a name="L101"></a><tt class="py-lineno"> 101</tt>  <tt class="py-line">        <tt class="py-name">file</tt><tt class="py-op">,</tt> <tt class="py-name">line</tt><tt class="py-op">,</tt> <tt class="py-name">fun</tt><tt class="py-op">,</tt> <tt class="py-name">what</tt> <tt class="py-op">=</tt> <tt class="py-name">tb</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt> </tt>
<a name="L102"></a><tt class="py-lineno"> 102</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">callsite</tt><tt class="py-op">(</tt><tt class="py-name">function</tt><tt class="py-op">=</tt><tt class="py-name">fun</tt><tt class="py-op">,</tt> <tt class="py-name">file</tt><tt class="py-op">=</tt><tt class="py-name">file</tt><tt class="py-op">,</tt> <tt class="py-name">linenum</tt><tt class="py-op">=</tt><tt class="py-name">line</tt><tt class="py-op">)</tt> </tt>
<a name="L103"></a><tt class="py-lineno"> 103</tt>  <tt class="py-line">    <tt class="py-name">sfile</tt><tt class="py-op">,</tt> <tt class="py-name">sline</tt><tt class="py-op">,</tt> <tt class="py-name">sfun</tt><tt class="py-op">,</tt> <tt class="py-name">swhat</tt> <tt class="py-op">=</tt> <tt class="py-name">tb</tt><tt class="py-op">[</tt><tt class="py-name">first_spark_frame</tt><tt class="py-op">]</tt> </tt>
<a name="L104"></a><tt class="py-lineno"> 104</tt>  <tt class="py-line">    <tt class="py-name">ufile</tt><tt class="py-op">,</tt> <tt class="py-name">uline</tt><tt class="py-op">,</tt> <tt class="py-name">ufun</tt><tt class="py-op">,</tt> <tt class="py-name">uwhat</tt> <tt class="py-op">=</tt> <tt class="py-name">tb</tt><tt class="py-op">[</tt><tt class="py-name">first_spark_frame</tt> <tt class="py-op">-</tt> <tt class="py-number">1</tt><tt class="py-op">]</tt> </tt>
<a name="L105"></a><tt class="py-lineno"> 105</tt>  <tt class="py-line">    <tt class="py-keyword">return</tt> <tt class="py-name">callsite</tt><tt class="py-op">(</tt><tt class="py-name">function</tt><tt class="py-op">=</tt><tt class="py-name">sfun</tt><tt class="py-op">,</tt> <tt class="py-name">file</tt><tt class="py-op">=</tt><tt class="py-name">ufile</tt><tt class="py-op">,</tt> <tt class="py-name">linenum</tt><tt class="py-op">=</tt><tt class="py-name">uline</tt><tt class="py-op">)</tt> </tt>
</div><a name="L106"></a><tt class="py-lineno"> 106</tt>  <tt class="py-line"> </tt>
<a name="L107"></a><tt class="py-lineno"> 107</tt>  <tt class="py-line"><tt id="link-19" class="py-name" targets="Variable pyspark.rdd._spark_stack_depth=pyspark.rdd-module.html#_spark_stack_depth"><a title="pyspark.rdd._spark_stack_depth" class="py-name" href="#" onclick="return doclink('link-19', '_spark_stack_depth', 'link-19');">_spark_stack_depth</a></tt> <tt class="py-op">=</tt> <tt class="py-number">0</tt> </tt>
<a name="_JavaStackTrace"></a><div id="_JavaStackTrace-def"><a name="L108"></a><tt class="py-lineno"> 108</tt>  <tt class="py-line"> </tt>
<a name="L109"></a><tt class="py-lineno"> 109</tt>  <tt class="py-line"> </tt>
<a name="L110"></a><tt class="py-lineno"> 110</tt> <a class="py-toggle" href="#" id="_JavaStackTrace-toggle" onclick="return toggle('_JavaStackTrace');">-</a><tt class="py-line"><tt class="py-keyword">class</tt> <a class="py-def-name" href="pyspark.rdd._JavaStackTrace-class.html">_JavaStackTrace</a><tt class="py-op">(</tt><tt class="py-base-class">object</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="_JavaStackTrace-collapsed" style="display:none;" pad="++++" indent="++++"></div><div id="_JavaStackTrace-expanded"><a name="L111"></a><tt class="py-lineno"> 111</tt>  <tt class="py-line"> </tt>
<a name="_JavaStackTrace.__init__"></a><div id="_JavaStackTrace.__init__-def"><a name="L112"></a><tt class="py-lineno"> 112</tt> <a class="py-toggle" href="#" id="_JavaStackTrace.__init__-toggle" onclick="return toggle('_JavaStackTrace.__init__');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd._JavaStackTrace-class.html#__init__">__init__</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">sc</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="_JavaStackTrace.__init__-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="_JavaStackTrace.__init__-expanded"><a name="L113"></a><tt class="py-lineno"> 113</tt>  <tt class="py-line">        <tt class="py-name">tb</tt> <tt class="py-op">=</tt> <tt class="py-name">_extract_concise_traceback</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L114"></a><tt class="py-lineno"> 114</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">tb</tt> <tt class="py-keyword">is</tt> <tt class="py-keyword">not</tt> <tt class="py-name">None</tt><tt class="py-op">:</tt> </tt>
<a name="L115"></a><tt class="py-lineno"> 115</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_traceback</tt> <tt class="py-op">=</tt> <tt class="py-string">"%s at %s:%s"</tt> <tt class="py-op">%</tt> <tt class="py-op">(</tt> </tt>
<a name="L116"></a><tt class="py-lineno"> 116</tt>  <tt class="py-line">                <tt class="py-name">tb</tt><tt class="py-op">.</tt><tt class="py-name">function</tt><tt class="py-op">,</tt> <tt class="py-name">tb</tt><tt class="py-op">.</tt><tt class="py-name">file</tt><tt class="py-op">,</tt> <tt class="py-name">tb</tt><tt class="py-op">.</tt><tt class="py-name">linenum</tt><tt class="py-op">)</tt> </tt>
<a name="L117"></a><tt class="py-lineno"> 117</tt>  <tt class="py-line">        <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L118"></a><tt class="py-lineno"> 118</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_traceback</tt> <tt class="py-op">=</tt> <tt class="py-string">"Error! Could not extract traceback info"</tt> </tt>
<a name="L119"></a><tt class="py-lineno"> 119</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_context</tt> <tt class="py-op">=</tt> <tt class="py-name">sc</tt> </tt>
</div><a name="L120"></a><tt class="py-lineno"> 120</tt>  <tt class="py-line"> </tt>
<a name="_JavaStackTrace.__enter__"></a><div id="_JavaStackTrace.__enter__-def"><a name="L121"></a><tt class="py-lineno"> 121</tt> <a class="py-toggle" href="#" id="_JavaStackTrace.__enter__-toggle" onclick="return toggle('_JavaStackTrace.__enter__');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd._JavaStackTrace-class.html#__enter__">__enter__</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="_JavaStackTrace.__enter__-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="_JavaStackTrace.__enter__-expanded"><a name="L122"></a><tt class="py-lineno"> 122</tt>  <tt class="py-line">        <tt class="py-keyword">global</tt> <tt id="link-20" class="py-name"><a title="pyspark.rdd._spark_stack_depth" class="py-name" href="#" onclick="return doclink('link-20', '_spark_stack_depth', 'link-19');">_spark_stack_depth</a></tt> </tt>
<a name="L123"></a><tt class="py-lineno"> 123</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt id="link-21" class="py-name"><a title="pyspark.rdd._spark_stack_depth" class="py-name" href="#" onclick="return doclink('link-21', '_spark_stack_depth', 'link-19');">_spark_stack_depth</a></tt> <tt class="py-op">==</tt> <tt class="py-number">0</tt><tt class="py-op">:</tt> </tt>
<a name="L124"></a><tt class="py-lineno"> 124</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_context</tt><tt class="py-op">.</tt><tt class="py-name">_jsc</tt><tt class="py-op">.</tt><tt class="py-name">setCallSite</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_traceback</tt><tt class="py-op">)</tt> </tt>
<a name="L125"></a><tt class="py-lineno"> 125</tt>  <tt class="py-line">        <tt id="link-22" class="py-name"><a title="pyspark.rdd._spark_stack_depth" class="py-name" href="#" onclick="return doclink('link-22', '_spark_stack_depth', 'link-19');">_spark_stack_depth</a></tt> <tt class="py-op">+=</tt> <tt class="py-number">1</tt> </tt>
</div><a name="L126"></a><tt class="py-lineno"> 126</tt>  <tt class="py-line"> </tt>
<a name="_JavaStackTrace.__exit__"></a><div id="_JavaStackTrace.__exit__-def"><a name="L127"></a><tt class="py-lineno"> 127</tt> <a class="py-toggle" href="#" id="_JavaStackTrace.__exit__-toggle" onclick="return toggle('_JavaStackTrace.__exit__');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd._JavaStackTrace-class.html#__exit__">__exit__</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">type</tt><tt class="py-op">,</tt> <tt class="py-param">value</tt><tt class="py-op">,</tt> <tt class="py-param">tb</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="_JavaStackTrace.__exit__-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="_JavaStackTrace.__exit__-expanded"><a name="L128"></a><tt class="py-lineno"> 128</tt>  <tt class="py-line">        <tt class="py-keyword">global</tt> <tt id="link-23" class="py-name"><a title="pyspark.rdd._spark_stack_depth" class="py-name" href="#" onclick="return doclink('link-23', '_spark_stack_depth', 'link-19');">_spark_stack_depth</a></tt> </tt>
<a name="L129"></a><tt class="py-lineno"> 129</tt>  <tt class="py-line">        <tt id="link-24" class="py-name"><a title="pyspark.rdd._spark_stack_depth" class="py-name" href="#" onclick="return doclink('link-24', '_spark_stack_depth', 'link-19');">_spark_stack_depth</a></tt> <tt class="py-op">-=</tt> <tt class="py-number">1</tt> </tt>
<a name="L130"></a><tt class="py-lineno"> 130</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt id="link-25" class="py-name"><a title="pyspark.rdd._spark_stack_depth" class="py-name" href="#" onclick="return doclink('link-25', '_spark_stack_depth', 'link-19');">_spark_stack_depth</a></tt> <tt class="py-op">==</tt> <tt class="py-number">0</tt><tt class="py-op">:</tt> </tt>
<a name="L131"></a><tt class="py-lineno"> 131</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_context</tt><tt class="py-op">.</tt><tt class="py-name">_jsc</tt><tt class="py-op">.</tt><tt class="py-name">setCallSite</tt><tt class="py-op">(</tt><tt class="py-name">None</tt><tt class="py-op">)</tt> </tt>
</div></div><a name="L132"></a><tt class="py-lineno"> 132</tt>  <tt class="py-line"> </tt>
<a name="MaxHeapQ"></a><div id="MaxHeapQ-def"><a name="L133"></a><tt class="py-lineno"> 133</tt>  <tt class="py-line"> </tt>
<a name="L134"></a><tt class="py-lineno"> 134</tt> <a class="py-toggle" href="#" id="MaxHeapQ-toggle" onclick="return toggle('MaxHeapQ');">-</a><tt class="py-line"><tt class="py-keyword">class</tt> <a class="py-def-name" href="pyspark.rdd.MaxHeapQ-class.html">MaxHeapQ</a><tt class="py-op">(</tt><tt class="py-base-class">object</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="MaxHeapQ-collapsed" style="display:none;" pad="++++" indent="++++"></div><div id="MaxHeapQ-expanded"><a name="L135"></a><tt class="py-lineno"> 135</tt>  <tt class="py-line"> </tt>
<a name="L136"></a><tt class="py-lineno"> 136</tt>  <tt class="py-line">    <tt class="py-docstring">"""</tt> </tt>
<a name="L137"></a><tt class="py-lineno"> 137</tt>  <tt class="py-line"><tt class="py-docstring">    An implementation of MaxHeap.</tt> </tt>
<a name="L138"></a><tt class="py-lineno"> 138</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L139"></a><tt class="py-lineno"> 139</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; import pyspark.rdd</tt> </tt>
<a name="L140"></a><tt class="py-lineno"> 140</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; heap = pyspark.rdd.MaxHeapQ(5)</tt> </tt>
<a name="L141"></a><tt class="py-lineno"> 141</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; [heap.insert(i) for i in range(10)]</tt> </tt>
<a name="L142"></a><tt class="py-lineno"> 142</tt>  <tt class="py-line"><tt class="py-docstring">    [None, None, None, None, None, None, None, None, None, None]</tt> </tt>
<a name="L143"></a><tt class="py-lineno"> 143</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; sorted(heap.getElements())</tt> </tt>
<a name="L144"></a><tt class="py-lineno"> 144</tt>  <tt class="py-line"><tt class="py-docstring">    [0, 1, 2, 3, 4]</tt> </tt>
<a name="L145"></a><tt class="py-lineno"> 145</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; heap = pyspark.rdd.MaxHeapQ(5)</tt> </tt>
<a name="L146"></a><tt class="py-lineno"> 146</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; [heap.insert(i) for i in range(9, -1, -1)]</tt> </tt>
<a name="L147"></a><tt class="py-lineno"> 147</tt>  <tt class="py-line"><tt class="py-docstring">    [None, None, None, None, None, None, None, None, None, None]</tt> </tt>
<a name="L148"></a><tt class="py-lineno"> 148</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; sorted(heap.getElements())</tt> </tt>
<a name="L149"></a><tt class="py-lineno"> 149</tt>  <tt class="py-line"><tt class="py-docstring">    [0, 1, 2, 3, 4]</tt> </tt>
<a name="L150"></a><tt class="py-lineno"> 150</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; heap = pyspark.rdd.MaxHeapQ(1)</tt> </tt>
<a name="L151"></a><tt class="py-lineno"> 151</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; [heap.insert(i) for i in range(9, -1, -1)]</tt> </tt>
<a name="L152"></a><tt class="py-lineno"> 152</tt>  <tt class="py-line"><tt class="py-docstring">    [None, None, None, None, None, None, None, None, None, None]</tt> </tt>
<a name="L153"></a><tt class="py-lineno"> 153</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; heap.getElements()</tt> </tt>
<a name="L154"></a><tt class="py-lineno"> 154</tt>  <tt class="py-line"><tt class="py-docstring">    [0]</tt> </tt>
<a name="L155"></a><tt class="py-lineno"> 155</tt>  <tt class="py-line"><tt class="py-docstring">    """</tt> </tt>
<a name="L156"></a><tt class="py-lineno"> 156</tt>  <tt class="py-line"> </tt>
<a name="MaxHeapQ.__init__"></a><div id="MaxHeapQ.__init__-def"><a name="L157"></a><tt class="py-lineno"> 157</tt> <a class="py-toggle" href="#" id="MaxHeapQ.__init__-toggle" onclick="return toggle('MaxHeapQ.__init__');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.MaxHeapQ-class.html#__init__">__init__</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">maxsize</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="MaxHeapQ.__init__-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="MaxHeapQ.__init__-expanded"><a name="L158"></a><tt class="py-lineno"> 158</tt>  <tt class="py-line">        <tt class="py-comment"># We start from q[1], so its children are always  2 * k</tt> </tt>
<a name="L159"></a><tt class="py-lineno"> 159</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt> <tt class="py-op">=</tt> <tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt> </tt>
<a name="L160"></a><tt class="py-lineno"> 160</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">maxsize</tt> <tt class="py-op">=</tt> <tt class="py-name">maxsize</tt> </tt>
</div><a name="L161"></a><tt class="py-lineno"> 161</tt>  <tt class="py-line"> </tt>
<a name="MaxHeapQ._swim"></a><div id="MaxHeapQ._swim-def"><a name="L162"></a><tt class="py-lineno"> 162</tt> <a class="py-toggle" href="#" id="MaxHeapQ._swim-toggle" onclick="return toggle('MaxHeapQ._swim');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.MaxHeapQ-class.html#_swim">_swim</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">k</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="MaxHeapQ._swim-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="MaxHeapQ._swim-expanded"><a name="L163"></a><tt class="py-lineno"> 163</tt>  <tt class="py-line">        <tt class="py-keyword">while</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt> <tt class="py-op">&gt;</tt> <tt class="py-number">1</tt><tt class="py-op">)</tt> <tt class="py-keyword">and</tt> <tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-name">k</tt> <tt class="py-op">/</tt> <tt class="py-number">2</tt><tt class="py-op">]</tt> <tt class="py-op">&lt;</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-name">k</tt><tt class="py-op">]</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L164"></a><tt class="py-lineno"> 164</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_swap</tt><tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">k</tt> <tt class="py-op">/</tt> <tt class="py-number">2</tt><tt class="py-op">)</tt> </tt>
<a name="L165"></a><tt class="py-lineno"> 165</tt>  <tt class="py-line">            <tt class="py-name">k</tt> <tt class="py-op">=</tt> <tt class="py-name">k</tt> <tt class="py-op">/</tt> <tt class="py-number">2</tt> </tt>
</div><a name="L166"></a><tt class="py-lineno"> 166</tt>  <tt class="py-line"> </tt>
<a name="MaxHeapQ._swap"></a><div id="MaxHeapQ._swap-def"><a name="L167"></a><tt class="py-lineno"> 167</tt> <a class="py-toggle" href="#" id="MaxHeapQ._swap-toggle" onclick="return toggle('MaxHeapQ._swap');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.MaxHeapQ-class.html#_swap">_swap</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">i</tt><tt class="py-op">,</tt> <tt class="py-param">j</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="MaxHeapQ._swap-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="MaxHeapQ._swap-expanded"><a name="L168"></a><tt class="py-lineno"> 168</tt>  <tt class="py-line">        <tt class="py-name">t</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-name">i</tt><tt class="py-op">]</tt> </tt>
<a name="L169"></a><tt class="py-lineno"> 169</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-name">i</tt><tt class="py-op">]</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-name">j</tt><tt class="py-op">]</tt> </tt>
<a name="L170"></a><tt class="py-lineno"> 170</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-name">j</tt><tt class="py-op">]</tt> <tt class="py-op">=</tt> <tt class="py-name">t</tt> </tt>
</div><a name="L171"></a><tt class="py-lineno"> 171</tt>  <tt class="py-line"> </tt>
<a name="MaxHeapQ._sink"></a><div id="MaxHeapQ._sink-def"><a name="L172"></a><tt class="py-lineno"> 172</tt> <a class="py-toggle" href="#" id="MaxHeapQ._sink-toggle" onclick="return toggle('MaxHeapQ._sink');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.MaxHeapQ-class.html#_sink">_sink</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">k</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="MaxHeapQ._sink-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="MaxHeapQ._sink-expanded"><a name="L173"></a><tt class="py-lineno"> 173</tt>  <tt class="py-line">        <tt class="py-name">N</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">size</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L174"></a><tt class="py-lineno"> 174</tt>  <tt class="py-line">        <tt class="py-keyword">while</tt> <tt class="py-number">2</tt> <tt class="py-op">*</tt> <tt class="py-name">k</tt> <tt class="py-op">&lt;=</tt> <tt class="py-name">N</tt><tt class="py-op">:</tt> </tt>
<a name="L175"></a><tt class="py-lineno"> 175</tt>  <tt class="py-line">            <tt class="py-name">j</tt> <tt class="py-op">=</tt> <tt class="py-number">2</tt> <tt class="py-op">*</tt> <tt class="py-name">k</tt> </tt>
<a name="L176"></a><tt class="py-lineno"> 176</tt>  <tt class="py-line">            <tt class="py-comment"># Here we test if both children are greater than parent</tt> </tt>
<a name="L177"></a><tt class="py-lineno"> 177</tt>  <tt class="py-line">            <tt class="py-comment"># if not swap with larger one.</tt> </tt>
<a name="L178"></a><tt class="py-lineno"> 178</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">j</tt> <tt class="py-op">&lt;</tt> <tt class="py-name">N</tt> <tt class="py-keyword">and</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-name">j</tt><tt class="py-op">]</tt> <tt class="py-op">&lt;</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-name">j</tt> <tt class="py-op">+</tt> <tt class="py-number">1</tt><tt class="py-op">]</tt><tt class="py-op">:</tt> </tt>
<a name="L179"></a><tt class="py-lineno"> 179</tt>  <tt class="py-line">                <tt class="py-name">j</tt> <tt class="py-op">=</tt> <tt class="py-name">j</tt> <tt class="py-op">+</tt> <tt class="py-number">1</tt> </tt>
<a name="L180"></a><tt class="py-lineno"> 180</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-name">k</tt><tt class="py-op">]</tt> <tt class="py-op">&gt;</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-name">j</tt><tt class="py-op">]</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L181"></a><tt class="py-lineno"> 181</tt>  <tt class="py-line">                <tt class="py-keyword">break</tt> </tt>
<a name="L182"></a><tt class="py-lineno"> 182</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_swap</tt><tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">j</tt><tt class="py-op">)</tt> </tt>
<a name="L183"></a><tt class="py-lineno"> 183</tt>  <tt class="py-line">            <tt class="py-name">k</tt> <tt class="py-op">=</tt> <tt class="py-name">j</tt> </tt>
</div><a name="L184"></a><tt class="py-lineno"> 184</tt>  <tt class="py-line"> </tt>
<a name="MaxHeapQ.size"></a><div id="MaxHeapQ.size-def"><a name="L185"></a><tt class="py-lineno"> 185</tt> <a class="py-toggle" href="#" id="MaxHeapQ.size-toggle" onclick="return toggle('MaxHeapQ.size');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.MaxHeapQ-class.html#size">size</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="MaxHeapQ.size-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="MaxHeapQ.size-expanded"><a name="L186"></a><tt class="py-lineno"> 186</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">)</tt> <tt class="py-op">-</tt> <tt class="py-number">1</tt> </tt>
</div><a name="L187"></a><tt class="py-lineno"> 187</tt>  <tt class="py-line"> </tt>
<a name="MaxHeapQ.insert"></a><div id="MaxHeapQ.insert-def"><a name="L188"></a><tt class="py-lineno"> 188</tt> <a class="py-toggle" href="#" id="MaxHeapQ.insert-toggle" onclick="return toggle('MaxHeapQ.insert');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.MaxHeapQ-class.html#insert">insert</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">value</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="MaxHeapQ.insert-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="MaxHeapQ.insert-expanded"><a name="L189"></a><tt class="py-lineno"> 189</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">size</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> <tt class="py-op">&lt;</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">maxsize</tt><tt class="py-op">:</tt> </tt>
<a name="L190"></a><tt class="py-lineno"> 190</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">.</tt><tt class="py-name">append</tt><tt class="py-op">(</tt><tt id="link-26" class="py-name" targets="Method pyspark.accumulators.Accumulator.value()=pyspark.accumulators.Accumulator-class.html#value"><a title="pyspark.accumulators.Accumulator.value" class="py-name" href="#" onclick="return doclink('link-26', 'value', 'link-26');">value</a></tt><tt class="py-op">)</tt> </tt>
<a name="L191"></a><tt class="py-lineno"> 191</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_swim</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">size</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L192"></a><tt class="py-lineno"> 192</tt>  <tt class="py-line">        <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L193"></a><tt class="py-lineno"> 193</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_replaceRoot</tt><tt class="py-op">(</tt><tt id="link-27" class="py-name"><a title="pyspark.accumulators.Accumulator.value" class="py-name" href="#" onclick="return doclink('link-27', 'value', 'link-26');">value</a></tt><tt class="py-op">)</tt> </tt>
</div><a name="L194"></a><tt class="py-lineno"> 194</tt>  <tt class="py-line"> </tt>
<a name="MaxHeapQ.getElements"></a><div id="MaxHeapQ.getElements-def"><a name="L195"></a><tt class="py-lineno"> 195</tt> <a class="py-toggle" href="#" id="MaxHeapQ.getElements-toggle" onclick="return toggle('MaxHeapQ.getElements');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.MaxHeapQ-class.html#getElements">getElements</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="MaxHeapQ.getElements-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="MaxHeapQ.getElements-expanded"><a name="L196"></a><tt class="py-lineno"> 196</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-number">1</tt><tt class="py-op">:</tt><tt class="py-op">]</tt> </tt>
</div><a name="L197"></a><tt class="py-lineno"> 197</tt>  <tt class="py-line"> </tt>
<a name="MaxHeapQ._replaceRoot"></a><div id="MaxHeapQ._replaceRoot-def"><a name="L198"></a><tt class="py-lineno"> 198</tt> <a class="py-toggle" href="#" id="MaxHeapQ._replaceRoot-toggle" onclick="return toggle('MaxHeapQ._replaceRoot');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.MaxHeapQ-class.html#_replaceRoot">_replaceRoot</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">value</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="MaxHeapQ._replaceRoot-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="MaxHeapQ._replaceRoot-expanded"><a name="L199"></a><tt class="py-lineno"> 199</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-number">1</tt><tt class="py-op">]</tt> <tt class="py-op">&gt;</tt> <tt id="link-28" class="py-name"><a title="pyspark.accumulators.Accumulator.value" class="py-name" href="#" onclick="return doclink('link-28', 'value', 'link-26');">value</a></tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L200"></a><tt class="py-lineno"> 200</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">q</tt><tt class="py-op">[</tt><tt class="py-number">1</tt><tt class="py-op">]</tt> <tt class="py-op">=</tt> <tt id="link-29" class="py-name"><a title="pyspark.accumulators.Accumulator.value" class="py-name" href="#" onclick="return doclink('link-29', 'value', 'link-26');">value</a></tt> </tt>
<a name="L201"></a><tt class="py-lineno"> 201</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_sink</tt><tt class="py-op">(</tt><tt class="py-number">1</tt><tt class="py-op">)</tt> </tt>
</div></div><a name="L202"></a><tt class="py-lineno"> 202</tt>  <tt class="py-line"> </tt>
<a name="_parse_memory"></a><div id="_parse_memory-def"><a name="L203"></a><tt class="py-lineno"> 203</tt>  <tt class="py-line"> </tt>
<a name="L204"></a><tt class="py-lineno"> 204</tt> <a class="py-toggle" href="#" id="_parse_memory-toggle" onclick="return toggle('_parse_memory');">-</a><tt class="py-line"><tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd-module.html#_parse_memory">_parse_memory</a><tt class="py-op">(</tt><tt class="py-param">s</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="_parse_memory-collapsed" style="display:none;" pad="++++" indent="++++"></div><div id="_parse_memory-expanded"><a name="L205"></a><tt class="py-lineno"> 205</tt>  <tt class="py-line">    <tt class="py-docstring">"""</tt> </tt>
<a name="L206"></a><tt class="py-lineno"> 206</tt>  <tt class="py-line"><tt class="py-docstring">    Parse a memory string in the format supported by Java (e.g. 1g, 200m) and</tt> </tt>
<a name="L207"></a><tt class="py-lineno"> 207</tt>  <tt class="py-line"><tt class="py-docstring">    return the value in MB</tt> </tt>
<a name="L208"></a><tt class="py-lineno"> 208</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L209"></a><tt class="py-lineno"> 209</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; _parse_memory("256m")</tt> </tt>
<a name="L210"></a><tt class="py-lineno"> 210</tt>  <tt class="py-line"><tt class="py-docstring">    256</tt> </tt>
<a name="L211"></a><tt class="py-lineno"> 211</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; _parse_memory("2g")</tt> </tt>
<a name="L212"></a><tt class="py-lineno"> 212</tt>  <tt class="py-line"><tt class="py-docstring">    2048</tt> </tt>
<a name="L213"></a><tt class="py-lineno"> 213</tt>  <tt class="py-line"><tt class="py-docstring">    """</tt> </tt>
<a name="L214"></a><tt class="py-lineno"> 214</tt>  <tt class="py-line">    <tt class="py-name">units</tt> <tt class="py-op">=</tt> <tt class="py-op">{</tt><tt class="py-string">'g'</tt><tt class="py-op">:</tt> <tt class="py-number">1024</tt><tt class="py-op">,</tt> <tt class="py-string">'m'</tt><tt class="py-op">:</tt> <tt class="py-number">1</tt><tt class="py-op">,</tt> <tt class="py-string">'t'</tt><tt class="py-op">:</tt> <tt class="py-number">1</tt> <tt class="py-op">&lt;&lt;</tt> <tt class="py-number">20</tt><tt class="py-op">,</tt> <tt class="py-string">'k'</tt><tt class="py-op">:</tt> <tt class="py-number">1.0</tt> <tt class="py-op">/</tt> <tt class="py-number">1024</tt><tt class="py-op">}</tt> </tt>
<a name="L215"></a><tt class="py-lineno"> 215</tt>  <tt class="py-line">    <tt class="py-keyword">if</tt> <tt id="link-30" class="py-name" targets="Variable pyspark.s=pyspark-module.html#s"><a title="pyspark.s" class="py-name" href="#" onclick="return doclink('link-30', 's', 'link-30');">s</a></tt><tt class="py-op">[</tt><tt class="py-op">-</tt><tt class="py-number">1</tt><tt class="py-op">]</tt> <tt class="py-keyword">not</tt> <tt class="py-keyword">in</tt> <tt class="py-name">units</tt><tt class="py-op">:</tt> </tt>
<a name="L216"></a><tt class="py-lineno"> 216</tt>  <tt class="py-line">        <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt><tt class="py-string">"invalid format: "</tt> <tt class="py-op">+</tt> <tt id="link-31" class="py-name"><a title="pyspark.s" class="py-name" href="#" onclick="return doclink('link-31', 's', 'link-30');">s</a></tt><tt class="py-op">)</tt> </tt>
<a name="L217"></a><tt class="py-lineno"> 217</tt>  <tt class="py-line">    <tt class="py-keyword">return</tt> <tt class="py-name">int</tt><tt class="py-op">(</tt><tt class="py-name">float</tt><tt class="py-op">(</tt><tt id="link-32" class="py-name"><a title="pyspark.s" class="py-name" href="#" onclick="return doclink('link-32', 's', 'link-30');">s</a></tt><tt class="py-op">[</tt><tt class="py-op">:</tt><tt class="py-op">-</tt><tt class="py-number">1</tt><tt class="py-op">]</tt><tt class="py-op">)</tt> <tt class="py-op">*</tt> <tt class="py-name">units</tt><tt class="py-op">[</tt><tt id="link-33" class="py-name"><a title="pyspark.s" class="py-name" href="#" onclick="return doclink('link-33', 's', 'link-30');">s</a></tt><tt class="py-op">[</tt><tt class="py-op">-</tt><tt class="py-number">1</tt><tt class="py-op">]</tt><tt class="py-op">.</tt><tt class="py-name">lower</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">]</tt><tt class="py-op">)</tt> </tt>
</div><a name="L218"></a><tt class="py-lineno"> 218</tt>  <tt class="py-line"> </tt>
<a name="RDD"></a><div id="RDD-def"><a name="L219"></a><tt class="py-lineno"> 219</tt>  <tt class="py-line"> </tt>
<a name="L220"></a><tt class="py-lineno"> 220</tt> <a class="py-toggle" href="#" id="RDD-toggle" onclick="return toggle('RDD');">-</a><tt class="py-line"><tt class="py-keyword">class</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html">RDD</a><tt class="py-op">(</tt><tt class="py-base-class">object</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD-collapsed" style="display:none;" pad="++++" indent="++++"></div><div id="RDD-expanded"><a name="L221"></a><tt class="py-lineno"> 221</tt>  <tt class="py-line"> </tt>
<a name="L222"></a><tt class="py-lineno"> 222</tt>  <tt class="py-line">    <tt class="py-docstring">"""</tt> </tt>
<a name="L223"></a><tt class="py-lineno"> 223</tt>  <tt class="py-line"><tt class="py-docstring">    A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.</tt> </tt>
<a name="L224"></a><tt class="py-lineno"> 224</tt>  <tt class="py-line"><tt class="py-docstring">    Represents an immutable, partitioned collection of elements that can be</tt> </tt>
<a name="L225"></a><tt class="py-lineno"> 225</tt>  <tt class="py-line"><tt class="py-docstring">    operated on in parallel.</tt> </tt>
<a name="L226"></a><tt class="py-lineno"> 226</tt>  <tt class="py-line"><tt class="py-docstring">    """</tt> </tt>
<a name="L227"></a><tt class="py-lineno"> 227</tt>  <tt class="py-line"> </tt>
<a name="RDD.__init__"></a><div id="RDD.__init__-def"><a name="L228"></a><tt class="py-lineno"> 228</tt> <a class="py-toggle" href="#" id="RDD.__init__-toggle" onclick="return toggle('RDD.__init__');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#__init__">__init__</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">jrdd</tt><tt class="py-op">,</tt> <tt class="py-param">ctx</tt><tt class="py-op">,</tt> <tt class="py-param">jrdd_deserializer</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.__init__-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.__init__-expanded"><a name="L229"></a><tt class="py-lineno"> 229</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt> <tt class="py-op">=</tt> <tt class="py-name">jrdd</tt> </tt>
<a name="L230"></a><tt class="py-lineno"> 230</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">is_cached</tt> <tt class="py-op">=</tt> <tt class="py-name">False</tt> </tt>
<a name="L231"></a><tt class="py-lineno"> 231</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">is_checkpointed</tt> <tt class="py-op">=</tt> <tt class="py-name">False</tt> </tt>
<a name="L232"></a><tt class="py-lineno"> 232</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt> <tt class="py-op">=</tt> <tt class="py-name">ctx</tt> </tt>
<a name="L233"></a><tt class="py-lineno"> 233</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt> <tt class="py-op">=</tt> <tt class="py-name">jrdd_deserializer</tt> </tt>
<a name="L234"></a><tt class="py-lineno"> 234</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_id</tt> <tt class="py-op">=</tt> <tt class="py-name">jrdd</tt><tt class="py-op">.</tt><tt id="link-34" class="py-name" targets="Method pyspark.rdd.RDD.id()=pyspark.rdd.RDD-class.html#id"><a title="pyspark.rdd.RDD.id" class="py-name" href="#" onclick="return doclink('link-34', 'id', 'link-34');">id</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L235"></a><tt class="py-lineno"> 235</tt>  <tt class="py-line"> </tt>
<a name="RDD._toPickleSerialization"></a><div id="RDD._toPickleSerialization-def"><a name="L236"></a><tt class="py-lineno"> 236</tt> <a class="py-toggle" href="#" id="RDD._toPickleSerialization-toggle" onclick="return toggle('RDD._toPickleSerialization');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#_toPickleSerialization">_toPickleSerialization</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD._toPickleSerialization-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD._toPickleSerialization-expanded"><a name="L237"></a><tt class="py-lineno"> 237</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt> <tt class="py-op">==</tt> <tt id="link-35" class="py-name"><a title="pyspark.serializers.PickleSerializer" class="py-name" href="#" onclick="return doclink('link-35', 'PickleSerializer', 'link-5');">PickleSerializer</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> <tt class="py-keyword">or</tt> </tt>
<a name="L238"></a><tt class="py-lineno"> 238</tt>  <tt class="py-line">                <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt> <tt class="py-op">==</tt> <tt class="py-name">BatchedSerializer</tt><tt class="py-op">(</tt><tt id="link-36" class="py-name"><a title="pyspark.serializers.PickleSerializer" class="py-name" href="#" onclick="return doclink('link-36', 'PickleSerializer', 'link-5');">PickleSerializer</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L239"></a><tt class="py-lineno"> 239</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">self</tt> </tt>
<a name="L240"></a><tt class="py-lineno"> 240</tt>  <tt class="py-line">        <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L241"></a><tt class="py-lineno"> 241</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_reserialize</tt><tt class="py-op">(</tt><tt class="py-name">BatchedSerializer</tt><tt class="py-op">(</tt><tt id="link-37" class="py-name"><a title="pyspark.serializers.PickleSerializer" class="py-name" href="#" onclick="return doclink('link-37', 'PickleSerializer', 'link-5');">PickleSerializer</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-number">10</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L242"></a><tt class="py-lineno"> 242</tt>  <tt class="py-line"> </tt>
<a name="RDD.id"></a><div id="RDD.id-def"><a name="L243"></a><tt class="py-lineno"> 243</tt> <a class="py-toggle" href="#" id="RDD.id-toggle" onclick="return toggle('RDD.id');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#id">id</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.id-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.id-expanded"><a name="L244"></a><tt class="py-lineno"> 244</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L245"></a><tt class="py-lineno"> 245</tt>  <tt class="py-line"><tt class="py-docstring">        A unique ID for this RDD (within its SparkContext).</tt> </tt>
<a name="L246"></a><tt class="py-lineno"> 246</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L247"></a><tt class="py-lineno"> 247</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_id</tt> </tt>
</div><a name="L248"></a><tt class="py-lineno"> 248</tt>  <tt class="py-line"> </tt>
<a name="RDD.__repr__"></a><div id="RDD.__repr__-def"><a name="L249"></a><tt class="py-lineno"> 249</tt> <a class="py-toggle" href="#" id="RDD.__repr__-toggle" onclick="return toggle('RDD.__repr__');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#__repr__">__repr__</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.__repr__-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.__repr__-expanded"><a name="L250"></a><tt class="py-lineno"> 250</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt class="py-name">toString</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L251"></a><tt class="py-lineno"> 251</tt>  <tt class="py-line"> </tt>
<a name="L252"></a><tt class="py-lineno"> 252</tt>  <tt class="py-line">    <tt class="py-decorator">@</tt><tt class="py-decorator">property</tt> </tt>
<a name="RDD.context"></a><div id="RDD.context-def"><a name="L253"></a><tt class="py-lineno"> 253</tt> <a class="py-toggle" href="#" id="RDD.context-toggle" onclick="return toggle('RDD.context');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#context">context</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.context-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.context-expanded"><a name="L254"></a><tt class="py-lineno"> 254</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L255"></a><tt class="py-lineno"> 255</tt>  <tt class="py-line"><tt class="py-docstring">        The L{SparkContext} that this RDD was created on.</tt> </tt>
<a name="L256"></a><tt class="py-lineno"> 256</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L257"></a><tt class="py-lineno"> 257</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt> </tt>
</div><a name="L258"></a><tt class="py-lineno"> 258</tt>  <tt class="py-line"> </tt>
<a name="RDD.cache"></a><div id="RDD.cache-def"><a name="L259"></a><tt class="py-lineno"> 259</tt> <a class="py-toggle" href="#" id="RDD.cache-toggle" onclick="return toggle('RDD.cache');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#cache">cache</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.cache-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.cache-expanded"><a name="L260"></a><tt class="py-lineno"> 260</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L261"></a><tt class="py-lineno"> 261</tt>  <tt class="py-line"><tt class="py-docstring">        Persist this RDD with the default storage level (C{MEMORY_ONLY_SER}).</tt> </tt>
<a name="L262"></a><tt class="py-lineno"> 262</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L263"></a><tt class="py-lineno"> 263</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">is_cached</tt> <tt class="py-op">=</tt> <tt class="py-name">True</tt> </tt>
<a name="L264"></a><tt class="py-lineno"> 264</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-38" class="py-name" targets="Method pyspark.rdd.RDD.persist()=pyspark.rdd.RDD-class.html#persist,Method pyspark.sql.SchemaRDD.persist()=pyspark.sql.SchemaRDD-class.html#persist"><a title="pyspark.rdd.RDD.persist
pyspark.sql.SchemaRDD.persist" class="py-name" href="#" onclick="return doclink('link-38', 'persist', 'link-38');">persist</a></tt><tt class="py-op">(</tt><tt id="link-39" class="py-name"><a title="pyspark.storagelevel.StorageLevel" class="py-name" href="#" onclick="return doclink('link-39', 'StorageLevel', 'link-14');">StorageLevel</a></tt><tt class="py-op">.</tt><tt id="link-40" class="py-name" targets="Variable pyspark.storagelevel.StorageLevel.MEMORY_ONLY_SER=pyspark.storagelevel.StorageLevel-class.html#MEMORY_ONLY_SER"><a title="pyspark.storagelevel.StorageLevel.MEMORY_ONLY_SER" class="py-name" href="#" onclick="return doclink('link-40', 'MEMORY_ONLY_SER', 'link-40');">MEMORY_ONLY_SER</a></tt><tt class="py-op">)</tt> </tt>
<a name="L265"></a><tt class="py-lineno"> 265</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt> </tt>
</div><a name="L266"></a><tt class="py-lineno"> 266</tt>  <tt class="py-line"> </tt>
<a name="RDD.persist"></a><div id="RDD.persist-def"><a name="L267"></a><tt class="py-lineno"> 267</tt> <a class="py-toggle" href="#" id="RDD.persist-toggle" onclick="return toggle('RDD.persist');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#persist">persist</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">storageLevel</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.persist-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.persist-expanded"><a name="L268"></a><tt class="py-lineno"> 268</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L269"></a><tt class="py-lineno"> 269</tt>  <tt class="py-line"><tt class="py-docstring">        Set this RDD's storage level to persist its values across operations</tt> </tt>
<a name="L270"></a><tt class="py-lineno"> 270</tt>  <tt class="py-line"><tt class="py-docstring">        after the first time it is computed. This can only be used to assign</tt> </tt>
<a name="L271"></a><tt class="py-lineno"> 271</tt>  <tt class="py-line"><tt class="py-docstring">        a new storage level if the RDD does not have a storage level set yet.</tt> </tt>
<a name="L272"></a><tt class="py-lineno"> 272</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L273"></a><tt class="py-lineno"> 273</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">is_cached</tt> <tt class="py-op">=</tt> <tt class="py-name">True</tt> </tt>
<a name="L274"></a><tt class="py-lineno"> 274</tt>  <tt class="py-line">        <tt class="py-name">javaStorageLevel</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_getJavaStorageLevel</tt><tt class="py-op">(</tt><tt class="py-name">storageLevel</tt><tt class="py-op">)</tt> </tt>
<a name="L275"></a><tt class="py-lineno"> 275</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-41" class="py-name"><a title="pyspark.rdd.RDD.persist
pyspark.sql.SchemaRDD.persist" class="py-name" href="#" onclick="return doclink('link-41', 'persist', 'link-38');">persist</a></tt><tt class="py-op">(</tt><tt class="py-name">javaStorageLevel</tt><tt class="py-op">)</tt> </tt>
<a name="L276"></a><tt class="py-lineno"> 276</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt> </tt>
</div><a name="L277"></a><tt class="py-lineno"> 277</tt>  <tt class="py-line"> </tt>
<a name="RDD.unpersist"></a><div id="RDD.unpersist-def"><a name="L278"></a><tt class="py-lineno"> 278</tt> <a class="py-toggle" href="#" id="RDD.unpersist-toggle" onclick="return toggle('RDD.unpersist');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#unpersist">unpersist</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.unpersist-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.unpersist-expanded"><a name="L279"></a><tt class="py-lineno"> 279</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L280"></a><tt class="py-lineno"> 280</tt>  <tt class="py-line"><tt class="py-docstring">        Mark the RDD as non-persistent, and remove all blocks for it from</tt> </tt>
<a name="L281"></a><tt class="py-lineno"> 281</tt>  <tt class="py-line"><tt class="py-docstring">        memory and disk.</tt> </tt>
<a name="L282"></a><tt class="py-lineno"> 282</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L283"></a><tt class="py-lineno"> 283</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">is_cached</tt> <tt class="py-op">=</tt> <tt class="py-name">False</tt> </tt>
<a name="L284"></a><tt class="py-lineno"> 284</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-42" class="py-name" targets="Method pyspark.broadcast.Broadcast.unpersist()=pyspark.broadcast.Broadcast-class.html#unpersist,Method pyspark.rdd.RDD.unpersist()=pyspark.rdd.RDD-class.html#unpersist,Method pyspark.sql.SchemaRDD.unpersist()=pyspark.sql.SchemaRDD-class.html#unpersist"><a title="pyspark.broadcast.Broadcast.unpersist
pyspark.rdd.RDD.unpersist
pyspark.sql.SchemaRDD.unpersist" class="py-name" href="#" onclick="return doclink('link-42', 'unpersist', 'link-42');">unpersist</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L285"></a><tt class="py-lineno"> 285</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt> </tt>
</div><a name="L286"></a><tt class="py-lineno"> 286</tt>  <tt class="py-line"> </tt>
<a name="RDD.checkpoint"></a><div id="RDD.checkpoint-def"><a name="L287"></a><tt class="py-lineno"> 287</tt> <a class="py-toggle" href="#" id="RDD.checkpoint-toggle" onclick="return toggle('RDD.checkpoint');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#checkpoint">checkpoint</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.checkpoint-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.checkpoint-expanded"><a name="L288"></a><tt class="py-lineno"> 288</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L289"></a><tt class="py-lineno"> 289</tt>  <tt class="py-line"><tt class="py-docstring">        Mark this RDD for checkpointing. It will be saved to a file inside the</tt> </tt>
<a name="L290"></a><tt class="py-lineno"> 290</tt>  <tt class="py-line"><tt class="py-docstring">        checkpoint directory set with L{SparkContext.setCheckpointDir()} and</tt> </tt>
<a name="L291"></a><tt class="py-lineno"> 291</tt>  <tt class="py-line"><tt class="py-docstring">        all references to its parent RDDs will be removed. This function must</tt> </tt>
<a name="L292"></a><tt class="py-lineno"> 292</tt>  <tt class="py-line"><tt class="py-docstring">        be called before any job has been executed on this RDD. It is strongly</tt> </tt>
<a name="L293"></a><tt class="py-lineno"> 293</tt>  <tt class="py-line"><tt class="py-docstring">        recommended that this RDD is persisted in memory, otherwise saving it</tt> </tt>
<a name="L294"></a><tt class="py-lineno"> 294</tt>  <tt class="py-line"><tt class="py-docstring">        on a file will require recomputation.</tt> </tt>
<a name="L295"></a><tt class="py-lineno"> 295</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L296"></a><tt class="py-lineno"> 296</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">is_checkpointed</tt> <tt class="py-op">=</tt> <tt class="py-name">True</tt> </tt>
<a name="L297"></a><tt class="py-lineno"> 297</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-43" class="py-name" targets="Module pyspark.rdd=pyspark.rdd-module.html"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-43', 'rdd', 'link-43');">rdd</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-44" class="py-name" targets="Method pyspark.rdd.RDD.checkpoint()=pyspark.rdd.RDD-class.html#checkpoint,Method pyspark.sql.SchemaRDD.checkpoint()=pyspark.sql.SchemaRDD-class.html#checkpoint"><a title="pyspark.rdd.RDD.checkpoint
pyspark.sql.SchemaRDD.checkpoint" class="py-name" href="#" onclick="return doclink('link-44', 'checkpoint', 'link-44');">checkpoint</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L298"></a><tt class="py-lineno"> 298</tt>  <tt class="py-line"> </tt>
<a name="RDD.isCheckpointed"></a><div id="RDD.isCheckpointed-def"><a name="L299"></a><tt class="py-lineno"> 299</tt> <a class="py-toggle" href="#" id="RDD.isCheckpointed-toggle" onclick="return toggle('RDD.isCheckpointed');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#isCheckpointed">isCheckpointed</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.isCheckpointed-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.isCheckpointed-expanded"><a name="L300"></a><tt class="py-lineno"> 300</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L301"></a><tt class="py-lineno"> 301</tt>  <tt class="py-line"><tt class="py-docstring">        Return whether this RDD has been checkpointed or not</tt> </tt>
<a name="L302"></a><tt class="py-lineno"> 302</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L303"></a><tt class="py-lineno"> 303</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-45" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-45', 'rdd', 'link-43');">rdd</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-46" class="py-name" targets="Method pyspark.rdd.RDD.isCheckpointed()=pyspark.rdd.RDD-class.html#isCheckpointed,Method pyspark.sql.SchemaRDD.isCheckpointed()=pyspark.sql.SchemaRDD-class.html#isCheckpointed"><a title="pyspark.rdd.RDD.isCheckpointed
pyspark.sql.SchemaRDD.isCheckpointed" class="py-name" href="#" onclick="return doclink('link-46', 'isCheckpointed', 'link-46');">isCheckpointed</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L304"></a><tt class="py-lineno"> 304</tt>  <tt class="py-line"> </tt>
<a name="RDD.getCheckpointFile"></a><div id="RDD.getCheckpointFile-def"><a name="L305"></a><tt class="py-lineno"> 305</tt> <a class="py-toggle" href="#" id="RDD.getCheckpointFile-toggle" onclick="return toggle('RDD.getCheckpointFile');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#getCheckpointFile">getCheckpointFile</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.getCheckpointFile-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.getCheckpointFile-expanded"><a name="L306"></a><tt class="py-lineno"> 306</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L307"></a><tt class="py-lineno"> 307</tt>  <tt class="py-line"><tt class="py-docstring">        Gets the name of the file to which this RDD was checkpointed</tt> </tt>
<a name="L308"></a><tt class="py-lineno"> 308</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L309"></a><tt class="py-lineno"> 309</tt>  <tt class="py-line">        <tt class="py-name">checkpointFile</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-47" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-47', 'rdd', 'link-43');">rdd</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-48" class="py-name" targets="Method pyspark.rdd.RDD.getCheckpointFile()=pyspark.rdd.RDD-class.html#getCheckpointFile,Method pyspark.sql.SchemaRDD.getCheckpointFile()=pyspark.sql.SchemaRDD-class.html#getCheckpointFile"><a title="pyspark.rdd.RDD.getCheckpointFile
pyspark.sql.SchemaRDD.getCheckpointFile" class="py-name" href="#" onclick="return doclink('link-48', 'getCheckpointFile', 'link-48');">getCheckpointFile</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L310"></a><tt class="py-lineno"> 310</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">checkpointFile</tt><tt class="py-op">.</tt><tt class="py-name">isDefined</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L311"></a><tt class="py-lineno"> 311</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">checkpointFile</tt><tt class="py-op">.</tt><tt id="link-49" class="py-name" targets="Method pyspark.conf.SparkConf.get()=pyspark.conf.SparkConf-class.html#get,Class Method pyspark.files.SparkFiles.get()=pyspark.files.SparkFiles-class.html#get"><a title="pyspark.conf.SparkConf.get
pyspark.files.SparkFiles.get" class="py-name" href="#" onclick="return doclink('link-49', 'get', 'link-49');">get</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L312"></a><tt class="py-lineno"> 312</tt>  <tt class="py-line">        <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L313"></a><tt class="py-lineno"> 313</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">None</tt> </tt>
</div><a name="L314"></a><tt class="py-lineno"> 314</tt>  <tt class="py-line"> </tt>
<a name="RDD.map"></a><div id="RDD.map-def"><a name="L315"></a><tt class="py-lineno"> 315</tt> <a class="py-toggle" href="#" id="RDD.map-toggle" onclick="return toggle('RDD.map');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#map">map</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">,</tt> <tt class="py-param">preservesPartitioning</tt><tt class="py-op">=</tt><tt class="py-name">False</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.map-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.map-expanded"><a name="L316"></a><tt class="py-lineno"> 316</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L317"></a><tt class="py-lineno"> 317</tt>  <tt class="py-line"><tt class="py-docstring">        Return a new RDD by applying a function to each element of this RDD.</tt> </tt>
<a name="L318"></a><tt class="py-lineno"> 318</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L319"></a><tt class="py-lineno"> 319</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize(["b", "a", "c"])</tt> </tt>
<a name="L320"></a><tt class="py-lineno"> 320</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(rdd.map(lambda x: (x, 1)).collect())</tt> </tt>
<a name="L321"></a><tt class="py-lineno"> 321</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 1), ('b', 1), ('c', 1)]</tt> </tt>
<a name="L322"></a><tt class="py-lineno"> 322</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L323"></a><tt class="py-lineno"> 323</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">_</tt><tt class="py-op">,</tt> <tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L324"></a><tt class="py-lineno"> 324</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">imap</tt><tt class="py-op">(</tt><tt class="py-name">f</tt><tt class="py-op">,</tt> <tt class="py-name">iterator</tt><tt class="py-op">)</tt> </tt>
</div><a name="L325"></a><tt class="py-lineno"> 325</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-50" class="py-name" targets="Method pyspark.rdd.RDD.mapPartitionsWithIndex()=pyspark.rdd.RDD-class.html#mapPartitionsWithIndex,Method pyspark.sql.SchemaRDD.mapPartitionsWithIndex()=pyspark.sql.SchemaRDD-class.html#mapPartitionsWithIndex"><a title="pyspark.rdd.RDD.mapPartitionsWithIndex
pyspark.sql.SchemaRDD.mapPartitionsWithIndex" class="py-name" href="#" onclick="return doclink('link-50', 'mapPartitionsWithIndex', 'link-50');">mapPartitionsWithIndex</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">,</tt> <tt class="py-name">preservesPartitioning</tt><tt class="py-op">)</tt> </tt>
</div><a name="L326"></a><tt class="py-lineno"> 326</tt>  <tt class="py-line"> </tt>
<a name="RDD.flatMap"></a><div id="RDD.flatMap-def"><a name="L327"></a><tt class="py-lineno"> 327</tt> <a class="py-toggle" href="#" id="RDD.flatMap-toggle" onclick="return toggle('RDD.flatMap');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#flatMap">flatMap</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">,</tt> <tt class="py-param">preservesPartitioning</tt><tt class="py-op">=</tt><tt class="py-name">False</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.flatMap-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.flatMap-expanded"><a name="L328"></a><tt class="py-lineno"> 328</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L329"></a><tt class="py-lineno"> 329</tt>  <tt class="py-line"><tt class="py-docstring">        Return a new RDD by first applying a function to all elements of this</tt> </tt>
<a name="L330"></a><tt class="py-lineno"> 330</tt>  <tt class="py-line"><tt class="py-docstring">        RDD, and then flattening the results.</tt> </tt>
<a name="L331"></a><tt class="py-lineno"> 331</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L332"></a><tt class="py-lineno"> 332</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4])</tt> </tt>
<a name="L333"></a><tt class="py-lineno"> 333</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: range(1, x)).collect())</tt> </tt>
<a name="L334"></a><tt class="py-lineno"> 334</tt>  <tt class="py-line"><tt class="py-docstring">        [1, 1, 1, 2, 2, 3]</tt> </tt>
<a name="L335"></a><tt class="py-lineno"> 335</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())</tt> </tt>
<a name="L336"></a><tt class="py-lineno"> 336</tt>  <tt class="py-line"><tt class="py-docstring">        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]</tt> </tt>
<a name="L337"></a><tt class="py-lineno"> 337</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L338"></a><tt class="py-lineno"> 338</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">s</tt><tt class="py-op">,</tt> <tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L339"></a><tt class="py-lineno"> 339</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">chain</tt><tt class="py-op">.</tt><tt class="py-name">from_iterable</tt><tt class="py-op">(</tt><tt class="py-name">imap</tt><tt class="py-op">(</tt><tt class="py-name">f</tt><tt class="py-op">,</tt> <tt class="py-name">iterator</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L340"></a><tt class="py-lineno"> 340</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-51" class="py-name"><a title="pyspark.rdd.RDD.mapPartitionsWithIndex
pyspark.sql.SchemaRDD.mapPartitionsWithIndex" class="py-name" href="#" onclick="return doclink('link-51', 'mapPartitionsWithIndex', 'link-50');">mapPartitionsWithIndex</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">,</tt> <tt class="py-name">preservesPartitioning</tt><tt class="py-op">)</tt> </tt>
</div><a name="L341"></a><tt class="py-lineno"> 341</tt>  <tt class="py-line"> </tt>
<a name="RDD.mapPartitions"></a><div id="RDD.mapPartitions-def"><a name="L342"></a><tt class="py-lineno"> 342</tt> <a class="py-toggle" href="#" id="RDD.mapPartitions-toggle" onclick="return toggle('RDD.mapPartitions');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#mapPartitions">mapPartitions</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">,</tt> <tt class="py-param">preservesPartitioning</tt><tt class="py-op">=</tt><tt class="py-name">False</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.mapPartitions-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.mapPartitions-expanded"><a name="L343"></a><tt class="py-lineno"> 343</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L344"></a><tt class="py-lineno"> 344</tt>  <tt class="py-line"><tt class="py-docstring">        Return a new RDD by applying a function to each partition of this RDD.</tt> </tt>
<a name="L345"></a><tt class="py-lineno"> 345</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L346"></a><tt class="py-lineno"> 346</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2)</tt> </tt>
<a name="L347"></a><tt class="py-lineno"> 347</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; def f(iterator): yield sum(iterator)</tt> </tt>
<a name="L348"></a><tt class="py-lineno"> 348</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.mapPartitions(f).collect()</tt> </tt>
<a name="L349"></a><tt class="py-lineno"> 349</tt>  <tt class="py-line"><tt class="py-docstring">        [3, 7]</tt> </tt>
<a name="L350"></a><tt class="py-lineno"> 350</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L351"></a><tt class="py-lineno"> 351</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">s</tt><tt class="py-op">,</tt> <tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L352"></a><tt class="py-lineno"> 352</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">f</tt><tt class="py-op">(</tt><tt class="py-name">iterator</tt><tt class="py-op">)</tt> </tt>
</div><a name="L353"></a><tt class="py-lineno"> 353</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-52" class="py-name"><a title="pyspark.rdd.RDD.mapPartitionsWithIndex
pyspark.sql.SchemaRDD.mapPartitionsWithIndex" class="py-name" href="#" onclick="return doclink('link-52', 'mapPartitionsWithIndex', 'link-50');">mapPartitionsWithIndex</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">)</tt> </tt>
</div><a name="L354"></a><tt class="py-lineno"> 354</tt>  <tt class="py-line"> </tt>
<a name="RDD.mapPartitionsWithIndex"></a><div id="RDD.mapPartitionsWithIndex-def"><a name="L355"></a><tt class="py-lineno"> 355</tt> <a class="py-toggle" href="#" id="RDD.mapPartitionsWithIndex-toggle" onclick="return toggle('RDD.mapPartitionsWithIndex');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#mapPartitionsWithIndex">mapPartitionsWithIndex</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">,</tt> <tt class="py-param">preservesPartitioning</tt><tt class="py-op">=</tt><tt class="py-name">False</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.mapPartitionsWithIndex-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.mapPartitionsWithIndex-expanded"><a name="L356"></a><tt class="py-lineno"> 356</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L357"></a><tt class="py-lineno"> 357</tt>  <tt class="py-line"><tt class="py-docstring">        Return a new RDD by applying a function to each partition of this RDD,</tt> </tt>
<a name="L358"></a><tt class="py-lineno"> 358</tt>  <tt class="py-line"><tt class="py-docstring">        while tracking the index of the original partition.</tt> </tt>
<a name="L359"></a><tt class="py-lineno"> 359</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L360"></a><tt class="py-lineno"> 360</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 4)</tt> </tt>
<a name="L361"></a><tt class="py-lineno"> 361</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; def f(splitIndex, iterator): yield splitIndex</tt> </tt>
<a name="L362"></a><tt class="py-lineno"> 362</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.mapPartitionsWithIndex(f).sum()</tt> </tt>
<a name="L363"></a><tt class="py-lineno"> 363</tt>  <tt class="py-line"><tt class="py-docstring">        6</tt> </tt>
<a name="L364"></a><tt class="py-lineno"> 364</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L365"></a><tt class="py-lineno"> 365</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">PipelinedRDD</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">,</tt> <tt class="py-name">f</tt><tt class="py-op">,</tt> <tt class="py-name">preservesPartitioning</tt><tt class="py-op">)</tt> </tt>
</div><a name="L366"></a><tt class="py-lineno"> 366</tt>  <tt class="py-line"> </tt>
<a name="RDD.mapPartitionsWithSplit"></a><div id="RDD.mapPartitionsWithSplit-def"><a name="L367"></a><tt class="py-lineno"> 367</tt> <a class="py-toggle" href="#" id="RDD.mapPartitionsWithSplit-toggle" onclick="return toggle('RDD.mapPartitionsWithSplit');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#mapPartitionsWithSplit">mapPartitionsWithSplit</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">,</tt> <tt class="py-param">preservesPartitioning</tt><tt class="py-op">=</tt><tt class="py-name">False</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.mapPartitionsWithSplit-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.mapPartitionsWithSplit-expanded"><a name="L368"></a><tt class="py-lineno"> 368</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L369"></a><tt class="py-lineno"> 369</tt>  <tt class="py-line"><tt class="py-docstring">        Deprecated: use mapPartitionsWithIndex instead.</tt> </tt>
<a name="L370"></a><tt class="py-lineno"> 370</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L371"></a><tt class="py-lineno"> 371</tt>  <tt class="py-line"><tt class="py-docstring">        Return a new RDD by applying a function to each partition of this RDD,</tt> </tt>
<a name="L372"></a><tt class="py-lineno"> 372</tt>  <tt class="py-line"><tt class="py-docstring">        while tracking the index of the original partition.</tt> </tt>
<a name="L373"></a><tt class="py-lineno"> 373</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L374"></a><tt class="py-lineno"> 374</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 4)</tt> </tt>
<a name="L375"></a><tt class="py-lineno"> 375</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; def f(splitIndex, iterator): yield splitIndex</tt> </tt>
<a name="L376"></a><tt class="py-lineno"> 376</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.mapPartitionsWithSplit(f).sum()</tt> </tt>
<a name="L377"></a><tt class="py-lineno"> 377</tt>  <tt class="py-line"><tt class="py-docstring">        6</tt> </tt>
<a name="L378"></a><tt class="py-lineno"> 378</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L379"></a><tt class="py-lineno"> 379</tt>  <tt class="py-line">        <tt class="py-name">warnings</tt><tt class="py-op">.</tt><tt class="py-name">warn</tt><tt class="py-op">(</tt><tt class="py-string">"mapPartitionsWithSplit is deprecated; "</tt> </tt>
<a name="L380"></a><tt class="py-lineno"> 380</tt>  <tt class="py-line">                      <tt class="py-string">"use mapPartitionsWithIndex instead"</tt><tt class="py-op">,</tt> <tt class="py-name">DeprecationWarning</tt><tt class="py-op">,</tt> <tt class="py-name">stacklevel</tt><tt class="py-op">=</tt><tt class="py-number">2</tt><tt class="py-op">)</tt> </tt>
<a name="L381"></a><tt class="py-lineno"> 381</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-53" class="py-name"><a title="pyspark.rdd.RDD.mapPartitionsWithIndex
pyspark.sql.SchemaRDD.mapPartitionsWithIndex" class="py-name" href="#" onclick="return doclink('link-53', 'mapPartitionsWithIndex', 'link-50');">mapPartitionsWithIndex</a></tt><tt class="py-op">(</tt><tt class="py-name">f</tt><tt class="py-op">,</tt> <tt class="py-name">preservesPartitioning</tt><tt class="py-op">)</tt> </tt>
</div><a name="L382"></a><tt class="py-lineno"> 382</tt>  <tt class="py-line"> </tt>
<a name="RDD.getNumPartitions"></a><div id="RDD.getNumPartitions-def"><a name="L383"></a><tt class="py-lineno"> 383</tt> <a class="py-toggle" href="#" id="RDD.getNumPartitions-toggle" onclick="return toggle('RDD.getNumPartitions');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#getNumPartitions">getNumPartitions</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.getNumPartitions-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.getNumPartitions-expanded"><a name="L384"></a><tt class="py-lineno"> 384</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L385"></a><tt class="py-lineno"> 385</tt>  <tt class="py-line"><tt class="py-docstring">        Returns the number of partitions in RDD</tt> </tt>
<a name="L386"></a><tt class="py-lineno"> 386</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L387"></a><tt class="py-lineno"> 387</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2)</tt> </tt>
<a name="L388"></a><tt class="py-lineno"> 388</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.getNumPartitions()</tt> </tt>
<a name="L389"></a><tt class="py-lineno"> 389</tt>  <tt class="py-line"><tt class="py-docstring">        2</tt> </tt>
<a name="L390"></a><tt class="py-lineno"> 390</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L391"></a><tt class="py-lineno"> 391</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt class="py-name">partitions</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">size</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L392"></a><tt class="py-lineno"> 392</tt>  <tt class="py-line"> </tt>
<a name="RDD.filter"></a><div id="RDD.filter-def"><a name="L393"></a><tt class="py-lineno"> 393</tt> <a class="py-toggle" href="#" id="RDD.filter-toggle" onclick="return toggle('RDD.filter');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#filter">filter</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.filter-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.filter-expanded"><a name="L394"></a><tt class="py-lineno"> 394</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L395"></a><tt class="py-lineno"> 395</tt>  <tt class="py-line"><tt class="py-docstring">        Return a new RDD containing only the elements that satisfy a predicate.</tt> </tt>
<a name="L396"></a><tt class="py-lineno"> 396</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L397"></a><tt class="py-lineno"> 397</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4, 5])</tt> </tt>
<a name="L398"></a><tt class="py-lineno"> 398</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.filter(lambda x: x % 2 == 0).collect()</tt> </tt>
<a name="L399"></a><tt class="py-lineno"> 399</tt>  <tt class="py-line"><tt class="py-docstring">        [2, 4]</tt> </tt>
<a name="L400"></a><tt class="py-lineno"> 400</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L401"></a><tt class="py-lineno"> 401</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L402"></a><tt class="py-lineno"> 402</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">ifilter</tt><tt class="py-op">(</tt><tt class="py-name">f</tt><tt class="py-op">,</tt> <tt class="py-name">iterator</tt><tt class="py-op">)</tt> </tt>
</div><a name="L403"></a><tt class="py-lineno"> 403</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-54" class="py-name" targets="Method pyspark.rdd.RDD.mapPartitions()=pyspark.rdd.RDD-class.html#mapPartitions"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-54', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">)</tt> </tt>
</div><a name="L404"></a><tt class="py-lineno"> 404</tt>  <tt class="py-line"> </tt>
<a name="RDD.distinct"></a><div id="RDD.distinct-def"><a name="L405"></a><tt class="py-lineno"> 405</tt> <a class="py-toggle" href="#" id="RDD.distinct-toggle" onclick="return toggle('RDD.distinct');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#distinct">distinct</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.distinct-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.distinct-expanded"><a name="L406"></a><tt class="py-lineno"> 406</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L407"></a><tt class="py-lineno"> 407</tt>  <tt class="py-line"><tt class="py-docstring">        Return a new RDD containing the distinct elements in this RDD.</tt> </tt>
<a name="L408"></a><tt class="py-lineno"> 408</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L409"></a><tt class="py-lineno"> 409</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())</tt> </tt>
<a name="L410"></a><tt class="py-lineno"> 410</tt>  <tt class="py-line"><tt class="py-docstring">        [1, 2, 3]</tt> </tt>
<a name="L411"></a><tt class="py-lineno"> 411</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L412"></a><tt class="py-lineno"> 412</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-55" class="py-name" targets="Method pyspark.rdd.RDD.map()=pyspark.rdd.RDD-class.html#map"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-55', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> \ </tt>
<a name="L413"></a><tt class="py-lineno"> 413</tt>  <tt class="py-line">                   <tt class="py-op">.</tt><tt id="link-56" class="py-name" targets="Method pyspark.rdd.RDD.reduceByKey()=pyspark.rdd.RDD-class.html#reduceByKey"><a title="pyspark.rdd.RDD.reduceByKey" class="py-name" href="#" onclick="return doclink('link-56', 'reduceByKey', 'link-56');">reduceByKey</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">_</tt><tt class="py-op">:</tt> <tt class="py-name">x</tt><tt class="py-op">)</tt> \ </tt>
<a name="L414"></a><tt class="py-lineno"> 414</tt>  <tt class="py-line">                   <tt class="py-op">.</tt><tt id="link-57" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-57', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">_</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> <tt class="py-name">x</tt><tt class="py-op">)</tt> </tt>
</div><a name="L415"></a><tt class="py-lineno"> 415</tt>  <tt class="py-line"> </tt>
<a name="RDD.sample"></a><div id="RDD.sample-def"><a name="L416"></a><tt class="py-lineno"> 416</tt> <a class="py-toggle" href="#" id="RDD.sample-toggle" onclick="return toggle('RDD.sample');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#sample">sample</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">withReplacement</tt><tt class="py-op">,</tt> <tt class="py-param">fraction</tt><tt class="py-op">,</tt> <tt class="py-param">seed</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.sample-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.sample-expanded"><a name="L417"></a><tt class="py-lineno"> 417</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L418"></a><tt class="py-lineno"> 418</tt>  <tt class="py-line"><tt class="py-docstring">        Return a sampled subset of this RDD (relies on numpy and falls back</tt> </tt>
<a name="L419"></a><tt class="py-lineno"> 419</tt>  <tt class="py-line"><tt class="py-docstring">        on default random generator if numpy is unavailable).</tt> </tt>
<a name="L420"></a><tt class="py-lineno"> 420</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L421"></a><tt class="py-lineno"> 421</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(range(0, 100)).sample(False, 0.1, 2).collect() #doctest: +SKIP</tt> </tt>
<a name="L422"></a><tt class="py-lineno"> 422</tt>  <tt class="py-line"><tt class="py-docstring">        [2, 3, 20, 21, 24, 41, 42, 66, 67, 89, 90, 98]</tt> </tt>
<a name="L423"></a><tt class="py-lineno"> 423</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L424"></a><tt class="py-lineno"> 424</tt>  <tt class="py-line">        <tt class="py-keyword">assert</tt> <tt class="py-name">fraction</tt> <tt class="py-op">&gt;=</tt> <tt class="py-number">0.0</tt><tt class="py-op">,</tt> <tt class="py-string">"Negative fraction value: %s"</tt> <tt class="py-op">%</tt> <tt class="py-name">fraction</tt> </tt>
<a name="L425"></a><tt class="py-lineno"> 425</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-58" class="py-name"><a title="pyspark.rdd.RDD.mapPartitionsWithIndex
pyspark.sql.SchemaRDD.mapPartitionsWithIndex" class="py-name" href="#" onclick="return doclink('link-58', 'mapPartitionsWithIndex', 'link-50');">mapPartitionsWithIndex</a></tt><tt class="py-op">(</tt><tt class="py-name">RDDSampler</tt><tt class="py-op">(</tt><tt class="py-name">withReplacement</tt><tt class="py-op">,</tt> <tt class="py-name">fraction</tt><tt class="py-op">,</tt> <tt class="py-name">seed</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">func</tt><tt class="py-op">,</tt> <tt class="py-name">True</tt><tt class="py-op">)</tt> </tt>
</div><a name="L426"></a><tt class="py-lineno"> 426</tt>  <tt class="py-line"> </tt>
<a name="L427"></a><tt class="py-lineno"> 427</tt>  <tt class="py-line">    <tt class="py-comment"># this is ported from scala/spark/RDD.scala</tt> </tt>
<a name="RDD.takeSample"></a><div id="RDD.takeSample-def"><a name="L428"></a><tt class="py-lineno"> 428</tt> <a class="py-toggle" href="#" id="RDD.takeSample-toggle" onclick="return toggle('RDD.takeSample');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#takeSample">takeSample</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">withReplacement</tt><tt class="py-op">,</tt> <tt class="py-param">num</tt><tt class="py-op">,</tt> <tt class="py-param">seed</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.takeSample-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.takeSample-expanded"><a name="L429"></a><tt class="py-lineno"> 429</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L430"></a><tt class="py-lineno"> 430</tt>  <tt class="py-line"><tt class="py-docstring">        Return a fixed-size sampled subset of this RDD (currently requires</tt> </tt>
<a name="L431"></a><tt class="py-lineno"> 431</tt>  <tt class="py-line"><tt class="py-docstring">        numpy).</tt> </tt>
<a name="L432"></a><tt class="py-lineno"> 432</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L433"></a><tt class="py-lineno"> 433</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize(range(0, 10))</tt> </tt>
<a name="L434"></a><tt class="py-lineno"> 434</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; len(rdd.takeSample(True, 20, 1))</tt> </tt>
<a name="L435"></a><tt class="py-lineno"> 435</tt>  <tt class="py-line"><tt class="py-docstring">        20</tt> </tt>
<a name="L436"></a><tt class="py-lineno"> 436</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; len(rdd.takeSample(False, 5, 2))</tt> </tt>
<a name="L437"></a><tt class="py-lineno"> 437</tt>  <tt class="py-line"><tt class="py-docstring">        5</tt> </tt>
<a name="L438"></a><tt class="py-lineno"> 438</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; len(rdd.takeSample(False, 15, 3))</tt> </tt>
<a name="L439"></a><tt class="py-lineno"> 439</tt>  <tt class="py-line"><tt class="py-docstring">        10</tt> </tt>
<a name="L440"></a><tt class="py-lineno"> 440</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L441"></a><tt class="py-lineno"> 441</tt>  <tt class="py-line">        <tt class="py-name">numStDev</tt> <tt class="py-op">=</tt> <tt class="py-number">10.0</tt> </tt>
<a name="L442"></a><tt class="py-lineno"> 442</tt>  <tt class="py-line"> </tt>
<a name="L443"></a><tt class="py-lineno"> 443</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">num</tt> <tt class="py-op">&lt;</tt> <tt class="py-number">0</tt><tt class="py-op">:</tt> </tt>
<a name="L444"></a><tt class="py-lineno"> 444</tt>  <tt class="py-line">            <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt><tt class="py-string">"Sample size cannot be negative."</tt><tt class="py-op">)</tt> </tt>
<a name="L445"></a><tt class="py-lineno"> 445</tt>  <tt class="py-line">        <tt class="py-keyword">elif</tt> <tt class="py-name">num</tt> <tt class="py-op">==</tt> <tt class="py-number">0</tt><tt class="py-op">:</tt> </tt>
<a name="L446"></a><tt class="py-lineno"> 446</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-op">[</tt><tt class="py-op">]</tt> </tt>
<a name="L447"></a><tt class="py-lineno"> 447</tt>  <tt class="py-line"> </tt>
<a name="L448"></a><tt class="py-lineno"> 448</tt>  <tt class="py-line">        <tt class="py-name">initialCount</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-59" class="py-name" targets="Method pyspark.mllib.stat.MultivariateStatisticalSummary.count()=pyspark.mllib.stat.MultivariateStatisticalSummary-class.html#count,Method pyspark.rdd.RDD.count()=pyspark.rdd.RDD-class.html#count,Method pyspark.sql.SchemaRDD.count()=pyspark.sql.SchemaRDD-class.html#count,Method pyspark.statcounter.StatCounter.count()=pyspark.statcounter.StatCounter-class.html#count"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.count
pyspark.rdd.RDD.count
pyspark.sql.SchemaRDD.count
pyspark.statcounter.StatCounter.count" class="py-name" href="#" onclick="return doclink('link-59', 'count', 'link-59');">count</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L449"></a><tt class="py-lineno"> 449</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">initialCount</tt> <tt class="py-op">==</tt> <tt class="py-number">0</tt><tt class="py-op">:</tt> </tt>
<a name="L450"></a><tt class="py-lineno"> 450</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-op">[</tt><tt class="py-op">]</tt> </tt>
<a name="L451"></a><tt class="py-lineno"> 451</tt>  <tt class="py-line"> </tt>
<a name="L452"></a><tt class="py-lineno"> 452</tt>  <tt class="py-line">        <tt class="py-name">rand</tt> <tt class="py-op">=</tt> <tt class="py-name">Random</tt><tt class="py-op">(</tt><tt class="py-name">seed</tt><tt class="py-op">)</tt> </tt>
<a name="L453"></a><tt class="py-lineno"> 453</tt>  <tt class="py-line"> </tt>
<a name="L454"></a><tt class="py-lineno"> 454</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-op">(</tt><tt class="py-keyword">not</tt> <tt class="py-name">withReplacement</tt><tt class="py-op">)</tt> <tt class="py-keyword">and</tt> <tt class="py-name">num</tt> <tt class="py-op">&gt;=</tt> <tt class="py-name">initialCount</tt><tt class="py-op">:</tt> </tt>
<a name="L455"></a><tt class="py-lineno"> 455</tt>  <tt class="py-line">            <tt class="py-comment"># shuffle current RDD and return</tt> </tt>
<a name="L456"></a><tt class="py-lineno"> 456</tt>  <tt class="py-line">            <tt class="py-name">samples</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-60" class="py-name" targets="Method pyspark.rdd.RDD.collect()=pyspark.rdd.RDD-class.html#collect,Method pyspark.sql.SchemaRDD.collect()=pyspark.sql.SchemaRDD-class.html#collect"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-60', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L457"></a><tt class="py-lineno"> 457</tt>  <tt class="py-line">            <tt class="py-name">rand</tt><tt class="py-op">.</tt><tt class="py-name">shuffle</tt><tt class="py-op">(</tt><tt class="py-name">samples</tt><tt class="py-op">)</tt> </tt>
<a name="L458"></a><tt class="py-lineno"> 458</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">samples</tt> </tt>
<a name="L459"></a><tt class="py-lineno"> 459</tt>  <tt class="py-line"> </tt>
<a name="L460"></a><tt class="py-lineno"> 460</tt>  <tt class="py-line">        <tt class="py-name">maxSampleSize</tt> <tt class="py-op">=</tt> <tt class="py-name">sys</tt><tt class="py-op">.</tt><tt class="py-name">maxint</tt> <tt class="py-op">-</tt> <tt class="py-name">int</tt><tt class="py-op">(</tt><tt class="py-name">numStDev</tt> <tt class="py-op">*</tt> <tt id="link-61" class="py-name"><a title="pyspark.statcounter.sqrt" class="py-name" href="#" onclick="return doclink('link-61', 'sqrt', 'link-2');">sqrt</a></tt><tt class="py-op">(</tt><tt class="py-name">sys</tt><tt class="py-op">.</tt><tt class="py-name">maxint</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L461"></a><tt class="py-lineno"> 461</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">num</tt> <tt class="py-op">&gt;</tt> <tt class="py-name">maxSampleSize</tt><tt class="py-op">:</tt> </tt>
<a name="L462"></a><tt class="py-lineno"> 462</tt>  <tt class="py-line">            <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt> </tt>
<a name="L463"></a><tt class="py-lineno"> 463</tt>  <tt class="py-line">                <tt class="py-string">"Sample size cannot be greater than %d."</tt> <tt class="py-op">%</tt> <tt class="py-name">maxSampleSize</tt><tt class="py-op">)</tt> </tt>
<a name="L464"></a><tt class="py-lineno"> 464</tt>  <tt class="py-line"> </tt>
<a name="L465"></a><tt class="py-lineno"> 465</tt>  <tt class="py-line">        <tt class="py-name">fraction</tt> <tt class="py-op">=</tt> <tt id="link-62" class="py-name" targets="Class pyspark.rdd.RDD=pyspark.rdd.RDD-class.html"><a title="pyspark.rdd.RDD" class="py-name" href="#" onclick="return doclink('link-62', 'RDD', 'link-62');">RDD</a></tt><tt class="py-op">.</tt><tt class="py-name">_computeFractionForSampleSize</tt><tt class="py-op">(</tt> </tt>
<a name="L466"></a><tt class="py-lineno"> 466</tt>  <tt class="py-line">            <tt class="py-name">num</tt><tt class="py-op">,</tt> <tt class="py-name">initialCount</tt><tt class="py-op">,</tt> <tt class="py-name">withReplacement</tt><tt class="py-op">)</tt> </tt>
<a name="L467"></a><tt class="py-lineno"> 467</tt>  <tt class="py-line">        <tt class="py-name">samples</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-63" class="py-name" targets="Method pyspark.rdd.RDD.sample()=pyspark.rdd.RDD-class.html#sample"><a title="pyspark.rdd.RDD.sample" class="py-name" href="#" onclick="return doclink('link-63', 'sample', 'link-63');">sample</a></tt><tt class="py-op">(</tt><tt class="py-name">withReplacement</tt><tt class="py-op">,</tt> <tt class="py-name">fraction</tt><tt class="py-op">,</tt> <tt class="py-name">seed</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-64" class="py-name"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-64', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L468"></a><tt class="py-lineno"> 468</tt>  <tt class="py-line"> </tt>
<a name="L469"></a><tt class="py-lineno"> 469</tt>  <tt class="py-line">        <tt class="py-comment"># If the first sample didn't turn out large enough, keep trying to take samples;</tt> </tt>
<a name="L470"></a><tt class="py-lineno"> 470</tt>  <tt class="py-line">        <tt class="py-comment"># this shouldn't happen often because we use a big multiplier for their initial size.</tt> </tt>
<a name="L471"></a><tt class="py-lineno"> 471</tt>  <tt class="py-line">        <tt class="py-comment"># See: scala/spark/RDD.scala</tt> </tt>
<a name="L472"></a><tt class="py-lineno"> 472</tt>  <tt class="py-line">        <tt class="py-keyword">while</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">samples</tt><tt class="py-op">)</tt> <tt class="py-op">&lt;</tt> <tt class="py-name">num</tt><tt class="py-op">:</tt> </tt>
<a name="L473"></a><tt class="py-lineno"> 473</tt>  <tt class="py-line">            <tt class="py-comment"># TODO: add log warning for when more than one iteration was run</tt> </tt>
<a name="L474"></a><tt class="py-lineno"> 474</tt>  <tt class="py-line">            <tt class="py-name">seed</tt> <tt class="py-op">=</tt> <tt class="py-name">rand</tt><tt class="py-op">.</tt><tt class="py-name">randint</tt><tt class="py-op">(</tt><tt class="py-number">0</tt><tt class="py-op">,</tt> <tt class="py-name">sys</tt><tt class="py-op">.</tt><tt class="py-name">maxint</tt><tt class="py-op">)</tt> </tt>
<a name="L475"></a><tt class="py-lineno"> 475</tt>  <tt class="py-line">            <tt class="py-name">samples</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-65" class="py-name"><a title="pyspark.rdd.RDD.sample" class="py-name" href="#" onclick="return doclink('link-65', 'sample', 'link-63');">sample</a></tt><tt class="py-op">(</tt><tt class="py-name">withReplacement</tt><tt class="py-op">,</tt> <tt class="py-name">fraction</tt><tt class="py-op">,</tt> <tt class="py-name">seed</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-66" class="py-name"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-66', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L476"></a><tt class="py-lineno"> 476</tt>  <tt class="py-line"> </tt>
<a name="L477"></a><tt class="py-lineno"> 477</tt>  <tt class="py-line">        <tt class="py-name">rand</tt><tt class="py-op">.</tt><tt class="py-name">shuffle</tt><tt class="py-op">(</tt><tt class="py-name">samples</tt><tt class="py-op">)</tt> </tt>
<a name="L478"></a><tt class="py-lineno"> 478</tt>  <tt class="py-line"> </tt>
<a name="L479"></a><tt class="py-lineno"> 479</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">samples</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">:</tt><tt class="py-name">num</tt><tt class="py-op">]</tt> </tt>
</div><a name="L480"></a><tt class="py-lineno"> 480</tt>  <tt class="py-line"> </tt>
<a name="L481"></a><tt class="py-lineno"> 481</tt>  <tt class="py-line">    <tt class="py-decorator">@</tt><tt class="py-decorator">staticmethod</tt> </tt>
<a name="RDD._computeFractionForSampleSize"></a><div id="RDD._computeFractionForSampleSize-def"><a name="L482"></a><tt class="py-lineno"> 482</tt> <a class="py-toggle" href="#" id="RDD._computeFractionForSampleSize-toggle" onclick="return toggle('RDD._computeFractionForSampleSize');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#_computeFractionForSampleSize">_computeFractionForSampleSize</a><tt class="py-op">(</tt><tt class="py-param">sampleSizeLowerBound</tt><tt class="py-op">,</tt> <tt class="py-param">total</tt><tt class="py-op">,</tt> <tt class="py-param">withReplacement</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD._computeFractionForSampleSize-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD._computeFractionForSampleSize-expanded"><a name="L483"></a><tt class="py-lineno"> 483</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L484"></a><tt class="py-lineno"> 484</tt>  <tt class="py-line"><tt class="py-docstring">        Returns a sampling rate that guarantees a sample of</tt> </tt>
<a name="L485"></a><tt class="py-lineno"> 485</tt>  <tt class="py-line"><tt class="py-docstring">        size &gt;= sampleSizeLowerBound 99.99% of the time.</tt> </tt>
<a name="L486"></a><tt class="py-lineno"> 486</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L487"></a><tt class="py-lineno"> 487</tt>  <tt class="py-line"><tt class="py-docstring">        How the sampling rate is determined:</tt> </tt>
<a name="L488"></a><tt class="py-lineno"> 488</tt>  <tt class="py-line"><tt class="py-docstring">        Let p = num / total, where num is the sample size and total is the</tt> </tt>
<a name="L489"></a><tt class="py-lineno"> 489</tt>  <tt class="py-line"><tt class="py-docstring">        total number of data points in the RDD. We're trying to compute</tt> </tt>
<a name="L490"></a><tt class="py-lineno"> 490</tt>  <tt class="py-line"><tt class="py-docstring">        q &gt; p such that</tt> </tt>
<a name="L491"></a><tt class="py-lineno"> 491</tt>  <tt class="py-line"><tt class="py-docstring">          - when sampling with replacement, we're drawing each data point</tt> </tt>
<a name="L492"></a><tt class="py-lineno"> 492</tt>  <tt class="py-line"><tt class="py-docstring">            with prob_i ~ Pois(q), where we want to guarantee</tt> </tt>
<a name="L493"></a><tt class="py-lineno"> 493</tt>  <tt class="py-line"><tt class="py-docstring">            Pr[s &lt; num] &lt; 0.0001 for s = sum(prob_i for i from 0 to</tt> </tt>
<a name="L494"></a><tt class="py-lineno"> 494</tt>  <tt class="py-line"><tt class="py-docstring">            total), i.e. the failure rate of not having a sufficiently large</tt> </tt>
<a name="L495"></a><tt class="py-lineno"> 495</tt>  <tt class="py-line"><tt class="py-docstring">            sample &lt; 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient</tt> </tt>
<a name="L496"></a><tt class="py-lineno"> 496</tt>  <tt class="py-line"><tt class="py-docstring">            to guarantee 0.9999 success rate for num &gt; 12, but we need a</tt> </tt>
<a name="L497"></a><tt class="py-lineno"> 497</tt>  <tt class="py-line"><tt class="py-docstring">            slightly larger q (9 empirically determined).</tt> </tt>
<a name="L498"></a><tt class="py-lineno"> 498</tt>  <tt class="py-line"><tt class="py-docstring">          - when sampling without replacement, we're drawing each data point</tt> </tt>
<a name="L499"></a><tt class="py-lineno"> 499</tt>  <tt class="py-line"><tt class="py-docstring">            with prob_i ~ Binomial(total, fraction) and our choice of q</tt> </tt>
<a name="L500"></a><tt class="py-lineno"> 500</tt>  <tt class="py-line"><tt class="py-docstring">            guarantees 1-delta, or 0.9999 success rate, where success rate is</tt> </tt>
<a name="L501"></a><tt class="py-lineno"> 501</tt>  <tt class="py-line"><tt class="py-docstring">            defined the same as in sampling with replacement.</tt> </tt>
<a name="L502"></a><tt class="py-lineno"> 502</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L503"></a><tt class="py-lineno"> 503</tt>  <tt class="py-line">        <tt class="py-name">fraction</tt> <tt class="py-op">=</tt> <tt class="py-name">float</tt><tt class="py-op">(</tt><tt class="py-name">sampleSizeLowerBound</tt><tt class="py-op">)</tt> <tt class="py-op">/</tt> <tt class="py-name">total</tt> </tt>
<a name="L504"></a><tt class="py-lineno"> 504</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">withReplacement</tt><tt class="py-op">:</tt> </tt>
<a name="L505"></a><tt class="py-lineno"> 505</tt>  <tt class="py-line">            <tt class="py-name">numStDev</tt> <tt class="py-op">=</tt> <tt class="py-number">5</tt> </tt>
<a name="L506"></a><tt class="py-lineno"> 506</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-op">(</tt><tt class="py-name">sampleSizeLowerBound</tt> <tt class="py-op">&lt;</tt> <tt class="py-number">12</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L507"></a><tt class="py-lineno"> 507</tt>  <tt class="py-line">                <tt class="py-name">numStDev</tt> <tt class="py-op">=</tt> <tt class="py-number">9</tt> </tt>
<a name="L508"></a><tt class="py-lineno"> 508</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">fraction</tt> <tt class="py-op">+</tt> <tt class="py-name">numStDev</tt> <tt class="py-op">*</tt> <tt id="link-67" class="py-name"><a title="pyspark.statcounter.sqrt" class="py-name" href="#" onclick="return doclink('link-67', 'sqrt', 'link-2');">sqrt</a></tt><tt class="py-op">(</tt><tt class="py-name">fraction</tt> <tt class="py-op">/</tt> <tt class="py-name">total</tt><tt class="py-op">)</tt> </tt>
<a name="L509"></a><tt class="py-lineno"> 509</tt>  <tt class="py-line">        <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L510"></a><tt class="py-lineno"> 510</tt>  <tt class="py-line">            <tt class="py-name">delta</tt> <tt class="py-op">=</tt> <tt class="py-number">0.00005</tt> </tt>
<a name="L511"></a><tt class="py-lineno"> 511</tt>  <tt class="py-line">            <tt class="py-name">gamma</tt> <tt class="py-op">=</tt> <tt class="py-op">-</tt> <tt class="py-name">log</tt><tt class="py-op">(</tt><tt class="py-name">delta</tt><tt class="py-op">)</tt> <tt class="py-op">/</tt> <tt class="py-name">total</tt> </tt>
<a name="L512"></a><tt class="py-lineno"> 512</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt id="link-68" class="py-name" targets="Method pyspark.mllib.stat.MultivariateStatisticalSummary.min()=pyspark.mllib.stat.MultivariateStatisticalSummary-class.html#min,Method pyspark.rdd.RDD.min()=pyspark.rdd.RDD-class.html#min,Method pyspark.statcounter.StatCounter.min()=pyspark.statcounter.StatCounter-class.html#min"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.min
pyspark.rdd.RDD.min
pyspark.statcounter.StatCounter.min" class="py-name" href="#" onclick="return doclink('link-68', 'min', 'link-68');">min</a></tt><tt class="py-op">(</tt><tt class="py-number">1</tt><tt class="py-op">,</tt> <tt class="py-name">fraction</tt> <tt class="py-op">+</tt> <tt class="py-name">gamma</tt> <tt class="py-op">+</tt> <tt id="link-69" class="py-name"><a title="pyspark.statcounter.sqrt" class="py-name" href="#" onclick="return doclink('link-69', 'sqrt', 'link-2');">sqrt</a></tt><tt class="py-op">(</tt><tt class="py-name">gamma</tt> <tt class="py-op">*</tt> <tt class="py-name">gamma</tt> <tt class="py-op">+</tt> <tt class="py-number">2</tt> <tt class="py-op">*</tt> <tt class="py-name">gamma</tt> <tt class="py-op">*</tt> <tt class="py-name">fraction</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L513"></a><tt class="py-lineno"> 513</tt>  <tt class="py-line"> </tt>
<a name="RDD.union"></a><div id="RDD.union-def"><a name="L514"></a><tt class="py-lineno"> 514</tt> <a class="py-toggle" href="#" id="RDD.union-toggle" onclick="return toggle('RDD.union');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#union">union</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.union-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.union-expanded"><a name="L515"></a><tt class="py-lineno"> 515</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L516"></a><tt class="py-lineno"> 516</tt>  <tt class="py-line"><tt class="py-docstring">        Return the union of this RDD and another one.</tt> </tt>
<a name="L517"></a><tt class="py-lineno"> 517</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L518"></a><tt class="py-lineno"> 518</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3])</tt> </tt>
<a name="L519"></a><tt class="py-lineno"> 519</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.union(rdd).collect()</tt> </tt>
<a name="L520"></a><tt class="py-lineno"> 520</tt>  <tt class="py-line"><tt class="py-docstring">        [1, 1, 2, 3, 1, 1, 2, 3]</tt> </tt>
<a name="L521"></a><tt class="py-lineno"> 521</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L522"></a><tt class="py-lineno"> 522</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt> <tt class="py-op">==</tt> <tt class="py-name">other</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">:</tt> </tt>
<a name="L523"></a><tt class="py-lineno"> 523</tt>  <tt class="py-line">            <tt id="link-70" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-70', 'rdd', 'link-43');">rdd</a></tt> <tt class="py-op">=</tt> <tt id="link-71" class="py-name"><a title="pyspark.rdd.RDD" class="py-name" href="#" onclick="return doclink('link-71', 'RDD', 'link-62');">RDD</a></tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-72" class="py-name" targets="Method pyspark.context.SparkContext.union()=pyspark.context.SparkContext-class.html#union,Method pyspark.rdd.RDD.union()=pyspark.rdd.RDD-class.html#union"><a title="pyspark.context.SparkContext.union
pyspark.rdd.RDD.union" class="py-name" href="#" onclick="return doclink('link-72', 'union', 'link-72');">union</a></tt><tt class="py-op">(</tt><tt class="py-name">other</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">,</tt> </tt>
<a name="L524"></a><tt class="py-lineno"> 524</tt>  <tt class="py-line">                      <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">)</tt> </tt>
<a name="L525"></a><tt class="py-lineno"> 525</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt id="link-73" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-73', 'rdd', 'link-43');">rdd</a></tt> </tt>
<a name="L526"></a><tt class="py-lineno"> 526</tt>  <tt class="py-line">        <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L527"></a><tt class="py-lineno"> 527</tt>  <tt class="py-line">            <tt class="py-comment"># These RDDs contain data in different serialized formats, so we</tt> </tt>
<a name="L528"></a><tt class="py-lineno"> 528</tt>  <tt class="py-line">            <tt class="py-comment"># must normalize them to the default serializer.</tt> </tt>
<a name="L529"></a><tt class="py-lineno"> 529</tt>  <tt class="py-line">            <tt class="py-name">self_copy</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_reserialize</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L530"></a><tt class="py-lineno"> 530</tt>  <tt class="py-line">            <tt class="py-name">other_copy</tt> <tt class="py-op">=</tt> <tt class="py-name">other</tt><tt class="py-op">.</tt><tt class="py-name">_reserialize</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L531"></a><tt class="py-lineno"> 531</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt id="link-74" class="py-name"><a title="pyspark.rdd.RDD" class="py-name" href="#" onclick="return doclink('link-74', 'RDD', 'link-62');">RDD</a></tt><tt class="py-op">(</tt><tt class="py-name">self_copy</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-75" class="py-name"><a title="pyspark.context.SparkContext.union
pyspark.rdd.RDD.union" class="py-name" href="#" onclick="return doclink('link-75', 'union', 'link-72');">union</a></tt><tt class="py-op">(</tt><tt class="py-name">other_copy</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">,</tt> </tt>
<a name="L532"></a><tt class="py-lineno"> 532</tt>  <tt class="py-line">                       <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">serializer</tt><tt class="py-op">)</tt> </tt>
</div><a name="L533"></a><tt class="py-lineno"> 533</tt>  <tt class="py-line"> </tt>
<a name="RDD.intersection"></a><div id="RDD.intersection-def"><a name="L534"></a><tt class="py-lineno"> 534</tt> <a class="py-toggle" href="#" id="RDD.intersection-toggle" onclick="return toggle('RDD.intersection');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#intersection">intersection</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.intersection-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.intersection-expanded"><a name="L535"></a><tt class="py-lineno"> 535</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L536"></a><tt class="py-lineno"> 536</tt>  <tt class="py-line"><tt class="py-docstring">        Return the intersection of this RDD and another one. The output will</tt> </tt>
<a name="L537"></a><tt class="py-lineno"> 537</tt>  <tt class="py-line"><tt class="py-docstring">        not contain any duplicate elements, even if the input RDDs did.</tt> </tt>
<a name="L538"></a><tt class="py-lineno"> 538</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L539"></a><tt class="py-lineno"> 539</tt>  <tt class="py-line"><tt class="py-docstring">        Note that this method performs a shuffle internally.</tt> </tt>
<a name="L540"></a><tt class="py-lineno"> 540</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L541"></a><tt class="py-lineno"> 541</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])</tt> </tt>
<a name="L542"></a><tt class="py-lineno"> 542</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])</tt> </tt>
<a name="L543"></a><tt class="py-lineno"> 543</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd1.intersection(rdd2).collect()</tt> </tt>
<a name="L544"></a><tt class="py-lineno"> 544</tt>  <tt class="py-line"><tt class="py-docstring">        [1, 2, 3]</tt> </tt>
<a name="L545"></a><tt class="py-lineno"> 545</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L546"></a><tt class="py-lineno"> 546</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-76" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-76', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">v</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-name">v</tt><tt class="py-op">,</tt> <tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> \ </tt>
<a name="L547"></a><tt class="py-lineno"> 547</tt>  <tt class="py-line">            <tt class="py-op">.</tt><tt id="link-77" class="py-name" targets="Method pyspark.rdd.RDD.cogroup()=pyspark.rdd.RDD-class.html#cogroup"><a title="pyspark.rdd.RDD.cogroup" class="py-name" href="#" onclick="return doclink('link-77', 'cogroup', 'link-77');">cogroup</a></tt><tt class="py-op">(</tt><tt class="py-name">other</tt><tt class="py-op">.</tt><tt id="link-78" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-78', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">v</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-name">v</tt><tt class="py-op">,</tt> <tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> \ </tt>
<a name="L548"></a><tt class="py-lineno"> 548</tt>  <tt class="py-line">            <tt class="py-op">.</tt><tt id="link-79" class="py-name" targets="Method pyspark.rdd.RDD.filter()=pyspark.rdd.RDD-class.html#filter"><a title="pyspark.rdd.RDD.filter" class="py-name" href="#" onclick="return doclink('link-79', 'filter', 'link-79');">filter</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">[</tt><tt class="py-number">1</tt><tt class="py-op">]</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt><tt class="py-op">)</tt> <tt class="py-op">!=</tt> <tt class="py-number">0</tt><tt class="py-op">)</tt> <tt class="py-keyword">and</tt> <tt class="py-op">(</tt><tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">[</tt><tt class="py-number">1</tt><tt class="py-op">]</tt><tt class="py-op">[</tt><tt class="py-number">1</tt><tt class="py-op">]</tt><tt class="py-op">)</tt> <tt class="py-op">!=</tt> <tt class="py-number">0</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> \ </tt>
<a name="L549"></a><tt class="py-lineno"> 549</tt>  <tt class="py-line">            <tt class="py-op">.</tt><tt id="link-80" class="py-name" targets="Method pyspark.rdd.RDD.keys()=pyspark.rdd.RDD-class.html#keys"><a title="pyspark.rdd.RDD.keys" class="py-name" href="#" onclick="return doclink('link-80', 'keys', 'link-80');">keys</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L550"></a><tt class="py-lineno"> 550</tt>  <tt class="py-line"> </tt>
<a name="RDD._reserialize"></a><div id="RDD._reserialize-def"><a name="L551"></a><tt class="py-lineno"> 551</tt> <a class="py-toggle" href="#" id="RDD._reserialize-toggle" onclick="return toggle('RDD._reserialize');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#_reserialize">_reserialize</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">serializer</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD._reserialize-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD._reserialize-expanded"><a name="L552"></a><tt class="py-lineno"> 552</tt>  <tt class="py-line">        <tt class="py-name">serializer</tt> <tt class="py-op">=</tt> <tt class="py-name">serializer</tt> <tt class="py-keyword">or</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">serializer</tt> </tt>
<a name="L553"></a><tt class="py-lineno"> 553</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt> <tt class="py-op">==</tt> <tt class="py-name">serializer</tt><tt class="py-op">:</tt> </tt>
<a name="L554"></a><tt class="py-lineno"> 554</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">self</tt> </tt>
<a name="L555"></a><tt class="py-lineno"> 555</tt>  <tt class="py-line">        <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L556"></a><tt class="py-lineno"> 556</tt>  <tt class="py-line">            <tt class="py-name">converted</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-81" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-81', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">preservesPartitioning</tt><tt class="py-op">=</tt><tt class="py-name">True</tt><tt class="py-op">)</tt> </tt>
<a name="L557"></a><tt class="py-lineno"> 557</tt>  <tt class="py-line">            <tt class="py-name">converted</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt> <tt class="py-op">=</tt> <tt class="py-name">serializer</tt> </tt>
<a name="L558"></a><tt class="py-lineno"> 558</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">converted</tt> </tt>
</div><a name="L559"></a><tt class="py-lineno"> 559</tt>  <tt class="py-line"> </tt>
<a name="RDD.__add__"></a><div id="RDD.__add__-def"><a name="L560"></a><tt class="py-lineno"> 560</tt> <a class="py-toggle" href="#" id="RDD.__add__-toggle" onclick="return toggle('RDD.__add__');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#__add__">__add__</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.__add__-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.__add__-expanded"><a name="L561"></a><tt class="py-lineno"> 561</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L562"></a><tt class="py-lineno"> 562</tt>  <tt class="py-line"><tt class="py-docstring">        Return the union of this RDD and another one.</tt> </tt>
<a name="L563"></a><tt class="py-lineno"> 563</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L564"></a><tt class="py-lineno"> 564</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3])</tt> </tt>
<a name="L565"></a><tt class="py-lineno"> 565</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; (rdd + rdd).collect()</tt> </tt>
<a name="L566"></a><tt class="py-lineno"> 566</tt>  <tt class="py-line"><tt class="py-docstring">        [1, 1, 2, 3, 1, 1, 2, 3]</tt> </tt>
<a name="L567"></a><tt class="py-lineno"> 567</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L568"></a><tt class="py-lineno"> 568</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-keyword">not</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">other</tt><tt class="py-op">,</tt> <tt id="link-82" class="py-name"><a title="pyspark.rdd.RDD" class="py-name" href="#" onclick="return doclink('link-82', 'RDD', 'link-62');">RDD</a></tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L569"></a><tt class="py-lineno"> 569</tt>  <tt class="py-line">            <tt class="py-keyword">raise</tt> <tt class="py-name">TypeError</tt> </tt>
<a name="L570"></a><tt class="py-lineno"> 570</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-83" class="py-name"><a title="pyspark.context.SparkContext.union
pyspark.rdd.RDD.union" class="py-name" href="#" onclick="return doclink('link-83', 'union', 'link-72');">union</a></tt><tt class="py-op">(</tt><tt class="py-name">other</tt><tt class="py-op">)</tt> </tt>
</div><a name="L571"></a><tt class="py-lineno"> 571</tt>  <tt class="py-line"> </tt>
<a name="RDD.sortByKey"></a><div id="RDD.sortByKey-def"><a name="L572"></a><tt class="py-lineno"> 572</tt> <a class="py-toggle" href="#" id="RDD.sortByKey-toggle" onclick="return toggle('RDD.sortByKey');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#sortByKey">sortByKey</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">ascending</tt><tt class="py-op">=</tt><tt class="py-name">True</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> <tt class="py-param">keyfunc</tt><tt class="py-op">=</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-name">x</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.sortByKey-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.sortByKey-expanded"><a name="L573"></a><tt class="py-lineno"> 573</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L574"></a><tt class="py-lineno"> 574</tt>  <tt class="py-line"><tt class="py-docstring">        Sorts this RDD, which is assumed to consist of (key, value) pairs.</tt> </tt>
<a name="L575"></a><tt class="py-lineno"> 575</tt>  <tt class="py-line"><tt class="py-docstring">        # noqa</tt> </tt>
<a name="L576"></a><tt class="py-lineno"> 576</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L577"></a><tt class="py-lineno"> 577</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]</tt> </tt>
<a name="L578"></a><tt class="py-lineno"> 578</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(tmp).sortByKey().first()</tt> </tt>
<a name="L579"></a><tt class="py-lineno"> 579</tt>  <tt class="py-line"><tt class="py-docstring">        ('1', 3)</tt> </tt>
<a name="L580"></a><tt class="py-lineno"> 580</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(tmp).sortByKey(True, 1).collect()</tt> </tt>
<a name="L581"></a><tt class="py-lineno"> 581</tt>  <tt class="py-line"><tt class="py-docstring">        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]</tt> </tt>
<a name="L582"></a><tt class="py-lineno"> 582</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(tmp).sortByKey(True, 2).collect()</tt> </tt>
<a name="L583"></a><tt class="py-lineno"> 583</tt>  <tt class="py-line"><tt class="py-docstring">        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]</tt> </tt>
<a name="L584"></a><tt class="py-lineno"> 584</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]</tt> </tt>
<a name="L585"></a><tt class="py-lineno"> 585</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])</tt> </tt>
<a name="L586"></a><tt class="py-lineno"> 586</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()</tt> </tt>
<a name="L587"></a><tt class="py-lineno"> 587</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]</tt> </tt>
<a name="L588"></a><tt class="py-lineno"> 588</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L589"></a><tt class="py-lineno"> 589</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">numPartitions</tt> <tt class="py-keyword">is</tt> <tt class="py-name">None</tt><tt class="py-op">:</tt> </tt>
<a name="L590"></a><tt class="py-lineno"> 590</tt>  <tt class="py-line">            <tt class="py-name">numPartitions</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_defaultReducePartitions</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L591"></a><tt class="py-lineno"> 591</tt>  <tt class="py-line"> </tt>
<a name="L592"></a><tt class="py-lineno"> 592</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">sortPartition</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L593"></a><tt class="py-lineno"> 593</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">iter</tt><tt class="py-op">(</tt><tt class="py-name">sorted</tt><tt class="py-op">(</tt><tt class="py-name">iterator</tt><tt class="py-op">,</tt> <tt class="py-name">key</tt><tt class="py-op">=</tt><tt class="py-keyword">lambda</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> <tt class="py-name">keyfunc</tt><tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">reverse</tt><tt class="py-op">=</tt><tt class="py-keyword">not</tt> <tt class="py-name">ascending</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L594"></a><tt class="py-lineno"> 594</tt>  <tt class="py-line"> </tt>
<a name="L595"></a><tt class="py-lineno"> 595</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">numPartitions</tt> <tt class="py-op">==</tt> <tt class="py-number">1</tt><tt class="py-op">:</tt> </tt>
<a name="L596"></a><tt class="py-lineno"> 596</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-84" class="py-name" targets="Method pyspark.rdd.RDD.getNumPartitions()=pyspark.rdd.RDD-class.html#getNumPartitions"><a title="pyspark.rdd.RDD.getNumPartitions" class="py-name" href="#" onclick="return doclink('link-84', 'getNumPartitions', 'link-84');">getNumPartitions</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> <tt class="py-op">&gt;</tt> <tt class="py-number">1</tt><tt class="py-op">:</tt> </tt>
<a name="L597"></a><tt class="py-lineno"> 597</tt>  <tt class="py-line">                <tt class="py-name">self</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-85" class="py-name" targets="Method pyspark.rdd.RDD.coalesce()=pyspark.rdd.RDD-class.html#coalesce,Method pyspark.sql.SchemaRDD.coalesce()=pyspark.sql.SchemaRDD-class.html#coalesce"><a title="pyspark.rdd.RDD.coalesce
pyspark.sql.SchemaRDD.coalesce" class="py-name" href="#" onclick="return doclink('link-85', 'coalesce', 'link-85');">coalesce</a></tt><tt class="py-op">(</tt><tt class="py-number">1</tt><tt class="py-op">)</tt> </tt>
<a name="L598"></a><tt class="py-lineno"> 598</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-86" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-86', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">sortPartition</tt><tt class="py-op">)</tt> </tt>
<a name="L599"></a><tt class="py-lineno"> 599</tt>  <tt class="py-line"> </tt>
<a name="L600"></a><tt class="py-lineno"> 600</tt>  <tt class="py-line">        <tt class="py-comment"># first compute the boundary of each part via sampling: we want to partition</tt> </tt>
<a name="L601"></a><tt class="py-lineno"> 601</tt>  <tt class="py-line">        <tt class="py-comment"># the key-space into bins such that the bins have roughly the same</tt> </tt>
<a name="L602"></a><tt class="py-lineno"> 602</tt>  <tt class="py-line">        <tt class="py-comment"># number of (key, value) pairs falling into them</tt> </tt>
<a name="L603"></a><tt class="py-lineno"> 603</tt>  <tt class="py-line">        <tt class="py-name">rddSize</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-87" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.count
pyspark.rdd.RDD.count
pyspark.sql.SchemaRDD.count
pyspark.statcounter.StatCounter.count" class="py-name" href="#" onclick="return doclink('link-87', 'count', 'link-59');">count</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L604"></a><tt class="py-lineno"> 604</tt>  <tt class="py-line">        <tt class="py-name">maxSampleSize</tt> <tt class="py-op">=</tt> <tt class="py-name">numPartitions</tt> <tt class="py-op">*</tt> <tt class="py-number">20.0</tt>  <tt class="py-comment"># constant from Spark's RangePartitioner</tt> </tt>
<a name="L605"></a><tt class="py-lineno"> 605</tt>  <tt class="py-line">        <tt class="py-name">fraction</tt> <tt class="py-op">=</tt> <tt id="link-88" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.min
pyspark.rdd.RDD.min
pyspark.statcounter.StatCounter.min" class="py-name" href="#" onclick="return doclink('link-88', 'min', 'link-68');">min</a></tt><tt class="py-op">(</tt><tt class="py-name">maxSampleSize</tt> <tt class="py-op">/</tt> <tt id="link-89" class="py-name" targets="Method pyspark.mllib.stat.MultivariateStatisticalSummary.max()=pyspark.mllib.stat.MultivariateStatisticalSummary-class.html#max,Method pyspark.rdd.RDD.max()=pyspark.rdd.RDD-class.html#max,Method pyspark.statcounter.StatCounter.max()=pyspark.statcounter.StatCounter-class.html#max"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.max
pyspark.rdd.RDD.max
pyspark.statcounter.StatCounter.max" class="py-name" href="#" onclick="return doclink('link-89', 'max', 'link-89');">max</a></tt><tt class="py-op">(</tt><tt class="py-name">rddSize</tt><tt class="py-op">,</tt> <tt class="py-number">1</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-number">1.0</tt><tt class="py-op">)</tt> </tt>
<a name="L606"></a><tt class="py-lineno"> 606</tt>  <tt class="py-line">        <tt class="py-name">samples</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-90" class="py-name"><a title="pyspark.rdd.RDD.sample" class="py-name" href="#" onclick="return doclink('link-90', 'sample', 'link-63');">sample</a></tt><tt class="py-op">(</tt><tt class="py-name">False</tt><tt class="py-op">,</tt> <tt class="py-name">fraction</tt><tt class="py-op">,</tt> <tt class="py-number">1</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-91" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-91', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> <tt class="py-name">k</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-92" class="py-name"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-92', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L607"></a><tt class="py-lineno"> 607</tt>  <tt class="py-line">        <tt class="py-name">samples</tt> <tt class="py-op">=</tt> <tt class="py-name">sorted</tt><tt class="py-op">(</tt><tt class="py-name">samples</tt><tt class="py-op">,</tt> <tt class="py-name">reverse</tt><tt class="py-op">=</tt><tt class="py-op">(</tt><tt class="py-keyword">not</tt> <tt class="py-name">ascending</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">key</tt><tt class="py-op">=</tt><tt class="py-name">keyfunc</tt><tt class="py-op">)</tt> </tt>
<a name="L608"></a><tt class="py-lineno"> 608</tt>  <tt class="py-line"> </tt>
<a name="L609"></a><tt class="py-lineno"> 609</tt>  <tt class="py-line">        <tt class="py-comment"># we have numPartitions many parts but one of the them has</tt> </tt>
<a name="L610"></a><tt class="py-lineno"> 610</tt>  <tt class="py-line">        <tt class="py-comment"># an implicit boundary</tt> </tt>
<a name="L611"></a><tt class="py-lineno"> 611</tt>  <tt class="py-line">        <tt class="py-name">bounds</tt> <tt class="py-op">=</tt> <tt class="py-op">[</tt><tt class="py-name">samples</tt><tt class="py-op">[</tt><tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">samples</tt><tt class="py-op">)</tt> <tt class="py-op">*</tt> <tt class="py-op">(</tt><tt class="py-name">i</tt> <tt class="py-op">+</tt> <tt class="py-number">1</tt><tt class="py-op">)</tt> <tt class="py-op">/</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">]</tt> </tt>
<a name="L612"></a><tt class="py-lineno"> 612</tt>  <tt class="py-line">                  <tt class="py-keyword">for</tt> <tt class="py-name">i</tt> <tt class="py-keyword">in</tt> <tt class="py-name">range</tt><tt class="py-op">(</tt><tt class="py-number">0</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt> <tt class="py-op">-</tt> <tt class="py-number">1</tt><tt class="py-op">)</tt><tt class="py-op">]</tt> </tt>
<a name="L613"></a><tt class="py-lineno"> 613</tt>  <tt class="py-line"> </tt>
<a name="L614"></a><tt class="py-lineno"> 614</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">rangePartitioner</tt><tt class="py-op">(</tt><tt class="py-param">k</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L615"></a><tt class="py-lineno"> 615</tt>  <tt class="py-line">            <tt class="py-name">p</tt> <tt class="py-op">=</tt> <tt class="py-name">bisect</tt><tt class="py-op">.</tt><tt class="py-name">bisect_left</tt><tt class="py-op">(</tt><tt class="py-name">bounds</tt><tt class="py-op">,</tt> <tt class="py-name">keyfunc</tt><tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L616"></a><tt class="py-lineno"> 616</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">ascending</tt><tt class="py-op">:</tt> </tt>
<a name="L617"></a><tt class="py-lineno"> 617</tt>  <tt class="py-line">                <tt class="py-keyword">return</tt> <tt class="py-name">p</tt> </tt>
<a name="L618"></a><tt class="py-lineno"> 618</tt>  <tt class="py-line">            <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L619"></a><tt class="py-lineno"> 619</tt>  <tt class="py-line">                <tt class="py-keyword">return</tt> <tt class="py-name">numPartitions</tt> <tt class="py-op">-</tt> <tt class="py-number">1</tt> <tt class="py-op">-</tt> <tt class="py-name">p</tt> </tt>
</div><a name="L620"></a><tt class="py-lineno"> 620</tt>  <tt class="py-line"> </tt>
<a name="L621"></a><tt class="py-lineno"> 621</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-93" class="py-name" targets="Method pyspark.rdd.RDD.partitionBy()=pyspark.rdd.RDD-class.html#partitionBy"><a title="pyspark.rdd.RDD.partitionBy" class="py-name" href="#" onclick="return doclink('link-93', 'partitionBy', 'link-93');">partitionBy</a></tt><tt class="py-op">(</tt><tt class="py-name">numPartitions</tt><tt class="py-op">,</tt> <tt class="py-name">rangePartitioner</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-94" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-94', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">sortPartition</tt><tt class="py-op">,</tt> <tt class="py-name">True</tt><tt class="py-op">)</tt> </tt>
</div><a name="L622"></a><tt class="py-lineno"> 622</tt>  <tt class="py-line"> </tt>
<a name="RDD.sortBy"></a><div id="RDD.sortBy-def"><a name="L623"></a><tt class="py-lineno"> 623</tt> <a class="py-toggle" href="#" id="RDD.sortBy-toggle" onclick="return toggle('RDD.sortBy');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#sortBy">sortBy</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">keyfunc</tt><tt class="py-op">,</tt> <tt class="py-param">ascending</tt><tt class="py-op">=</tt><tt class="py-name">True</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.sortBy-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.sortBy-expanded"><a name="L624"></a><tt class="py-lineno"> 624</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L625"></a><tt class="py-lineno"> 625</tt>  <tt class="py-line"><tt class="py-docstring">        Sorts this RDD by the given keyfunc</tt> </tt>
<a name="L626"></a><tt class="py-lineno"> 626</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L627"></a><tt class="py-lineno"> 627</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]</tt> </tt>
<a name="L628"></a><tt class="py-lineno"> 628</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()</tt> </tt>
<a name="L629"></a><tt class="py-lineno"> 629</tt>  <tt class="py-line"><tt class="py-docstring">        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]</tt> </tt>
<a name="L630"></a><tt class="py-lineno"> 630</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()</tt> </tt>
<a name="L631"></a><tt class="py-lineno"> 631</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]</tt> </tt>
<a name="L632"></a><tt class="py-lineno"> 632</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L633"></a><tt class="py-lineno"> 633</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-95" class="py-name" targets="Method pyspark.rdd.RDD.keyBy()=pyspark.rdd.RDD-class.html#keyBy"><a title="pyspark.rdd.RDD.keyBy" class="py-name" href="#" onclick="return doclink('link-95', 'keyBy', 'link-95');">keyBy</a></tt><tt class="py-op">(</tt><tt class="py-name">keyfunc</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-96" class="py-name" targets="Method pyspark.rdd.RDD.sortByKey()=pyspark.rdd.RDD-class.html#sortByKey"><a title="pyspark.rdd.RDD.sortByKey" class="py-name" href="#" onclick="return doclink('link-96', 'sortByKey', 'link-96');">sortByKey</a></tt><tt class="py-op">(</tt><tt class="py-name">ascending</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-97" class="py-name" targets="Method pyspark.rdd.RDD.values()=pyspark.rdd.RDD-class.html#values"><a title="pyspark.rdd.RDD.values" class="py-name" href="#" onclick="return doclink('link-97', 'values', 'link-97');">values</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L634"></a><tt class="py-lineno"> 634</tt>  <tt class="py-line"> </tt>
<a name="RDD.glom"></a><div id="RDD.glom-def"><a name="L635"></a><tt class="py-lineno"> 635</tt> <a class="py-toggle" href="#" id="RDD.glom-toggle" onclick="return toggle('RDD.glom');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#glom">glom</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.glom-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.glom-expanded"><a name="L636"></a><tt class="py-lineno"> 636</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L637"></a><tt class="py-lineno"> 637</tt>  <tt class="py-line"><tt class="py-docstring">        Return an RDD created by coalescing all elements within each partition</tt> </tt>
<a name="L638"></a><tt class="py-lineno"> 638</tt>  <tt class="py-line"><tt class="py-docstring">        into a list.</tt> </tt>
<a name="L639"></a><tt class="py-lineno"> 639</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L640"></a><tt class="py-lineno"> 640</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2)</tt> </tt>
<a name="L641"></a><tt class="py-lineno"> 641</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(rdd.glom().collect())</tt> </tt>
<a name="L642"></a><tt class="py-lineno"> 642</tt>  <tt class="py-line"><tt class="py-docstring">        [[1, 2], [3, 4]]</tt> </tt>
<a name="L643"></a><tt class="py-lineno"> 643</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L644"></a><tt class="py-lineno"> 644</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L645"></a><tt class="py-lineno"> 645</tt>  <tt class="py-line">            <tt class="py-keyword">yield</tt> <tt class="py-name">list</tt><tt class="py-op">(</tt><tt class="py-name">iterator</tt><tt class="py-op">)</tt> </tt>
</div><a name="L646"></a><tt class="py-lineno"> 646</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-98" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-98', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">)</tt> </tt>
</div><a name="L647"></a><tt class="py-lineno"> 647</tt>  <tt class="py-line"> </tt>
<a name="RDD.cartesian"></a><div id="RDD.cartesian-def"><a name="L648"></a><tt class="py-lineno"> 648</tt> <a class="py-toggle" href="#" id="RDD.cartesian-toggle" onclick="return toggle('RDD.cartesian');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#cartesian">cartesian</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.cartesian-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.cartesian-expanded"><a name="L649"></a><tt class="py-lineno"> 649</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L650"></a><tt class="py-lineno"> 650</tt>  <tt class="py-line"><tt class="py-docstring">        Return the Cartesian product of this RDD and another one, that is, the</tt> </tt>
<a name="L651"></a><tt class="py-lineno"> 651</tt>  <tt class="py-line"><tt class="py-docstring">        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and</tt> </tt>
<a name="L652"></a><tt class="py-lineno"> 652</tt>  <tt class="py-line"><tt class="py-docstring">        C{b} is in C{other}.</tt> </tt>
<a name="L653"></a><tt class="py-lineno"> 653</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L654"></a><tt class="py-lineno"> 654</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([1, 2])</tt> </tt>
<a name="L655"></a><tt class="py-lineno"> 655</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(rdd.cartesian(rdd).collect())</tt> </tt>
<a name="L656"></a><tt class="py-lineno"> 656</tt>  <tt class="py-line"><tt class="py-docstring">        [(1, 1), (1, 2), (2, 1), (2, 2)]</tt> </tt>
<a name="L657"></a><tt class="py-lineno"> 657</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L658"></a><tt class="py-lineno"> 658</tt>  <tt class="py-line">        <tt class="py-comment"># Due to batching, we can't use the Java cartesian method.</tt> </tt>
<a name="L659"></a><tt class="py-lineno"> 659</tt>  <tt class="py-line">        <tt class="py-name">deserializer</tt> <tt class="py-op">=</tt> <tt class="py-name">CartesianDeserializer</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">,</tt> </tt>
<a name="L660"></a><tt class="py-lineno"> 660</tt>  <tt class="py-line">                                             <tt class="py-name">other</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">)</tt> </tt>
<a name="L661"></a><tt class="py-lineno"> 661</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt id="link-99" class="py-name"><a title="pyspark.rdd.RDD" class="py-name" href="#" onclick="return doclink('link-99', 'RDD', 'link-62');">RDD</a></tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-100" class="py-name" targets="Method pyspark.rdd.RDD.cartesian()=pyspark.rdd.RDD-class.html#cartesian"><a title="pyspark.rdd.RDD.cartesian" class="py-name" href="#" onclick="return doclink('link-100', 'cartesian', 'link-100');">cartesian</a></tt><tt class="py-op">(</tt><tt class="py-name">other</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">,</tt> <tt class="py-name">deserializer</tt><tt class="py-op">)</tt> </tt>
</div><a name="L662"></a><tt class="py-lineno"> 662</tt>  <tt class="py-line"> </tt>
<a name="RDD.groupBy"></a><div id="RDD.groupBy-def"><a name="L663"></a><tt class="py-lineno"> 663</tt> <a class="py-toggle" href="#" id="RDD.groupBy-toggle" onclick="return toggle('RDD.groupBy');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#groupBy">groupBy</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.groupBy-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.groupBy-expanded"><a name="L664"></a><tt class="py-lineno"> 664</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L665"></a><tt class="py-lineno"> 665</tt>  <tt class="py-line"><tt class="py-docstring">        Return an RDD of grouped items.</tt> </tt>
<a name="L666"></a><tt class="py-lineno"> 666</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L667"></a><tt class="py-lineno"> 667</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3, 5, 8])</tt> </tt>
<a name="L668"></a><tt class="py-lineno"> 668</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; result = rdd.groupBy(lambda x: x % 2).collect()</tt> </tt>
<a name="L669"></a><tt class="py-lineno"> 669</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted([(x, sorted(y)) for (x, y) in result])</tt> </tt>
<a name="L670"></a><tt class="py-lineno"> 670</tt>  <tt class="py-line"><tt class="py-docstring">        [(0, [2, 8]), (1, [1, 1, 3, 5])]</tt> </tt>
<a name="L671"></a><tt class="py-lineno"> 671</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L672"></a><tt class="py-lineno"> 672</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-101" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-101', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-name">f</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">x</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-102" class="py-name" targets="Method pyspark.rdd.RDD.groupByKey()=pyspark.rdd.RDD-class.html#groupByKey"><a title="pyspark.rdd.RDD.groupByKey" class="py-name" href="#" onclick="return doclink('link-102', 'groupByKey', 'link-102');">groupByKey</a></tt><tt class="py-op">(</tt><tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
</div><a name="L673"></a><tt class="py-lineno"> 673</tt>  <tt class="py-line"> </tt>
<a name="RDD.pipe"></a><div id="RDD.pipe-def"><a name="L674"></a><tt class="py-lineno"> 674</tt> <a class="py-toggle" href="#" id="RDD.pipe-toggle" onclick="return toggle('RDD.pipe');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#pipe">pipe</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">command</tt><tt class="py-op">,</tt> <tt class="py-param">env</tt><tt class="py-op">=</tt><tt class="py-op">{</tt><tt class="py-op">}</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.pipe-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.pipe-expanded"><a name="L675"></a><tt class="py-lineno"> 675</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L676"></a><tt class="py-lineno"> 676</tt>  <tt class="py-line"><tt class="py-docstring">        Return an RDD created by piping elements to a forked external process.</tt> </tt>
<a name="L677"></a><tt class="py-lineno"> 677</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L678"></a><tt class="py-lineno"> 678</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()</tt> </tt>
<a name="L679"></a><tt class="py-lineno"> 679</tt>  <tt class="py-line"><tt class="py-docstring">        ['1', '2', '', '3']</tt> </tt>
<a name="L680"></a><tt class="py-lineno"> 680</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L681"></a><tt class="py-lineno"> 681</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L682"></a><tt class="py-lineno"> 682</tt>  <tt class="py-line">            <tt id="link-103" class="py-name" targets="Method pyspark.rdd.RDD.pipe()=pyspark.rdd.RDD-class.html#pipe"><a title="pyspark.rdd.RDD.pipe" class="py-name" href="#" onclick="return doclink('link-103', 'pipe', 'link-103');">pipe</a></tt> <tt class="py-op">=</tt> <tt class="py-name">Popen</tt><tt class="py-op">(</tt> </tt>
<a name="L683"></a><tt class="py-lineno"> 683</tt>  <tt class="py-line">                <tt class="py-name">shlex</tt><tt class="py-op">.</tt><tt class="py-name">split</tt><tt class="py-op">(</tt><tt class="py-name">command</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">env</tt><tt class="py-op">=</tt><tt class="py-name">env</tt><tt class="py-op">,</tt> <tt class="py-name">stdin</tt><tt class="py-op">=</tt><tt class="py-name">PIPE</tt><tt class="py-op">,</tt> <tt class="py-name">stdout</tt><tt class="py-op">=</tt><tt class="py-name">PIPE</tt><tt class="py-op">)</tt> </tt>
<a name="L684"></a><tt class="py-lineno"> 684</tt>  <tt class="py-line"> </tt>
<a name="L685"></a><tt class="py-lineno"> 685</tt>  <tt class="py-line">            <tt class="py-keyword">def</tt> <tt class="py-def-name">pipe_objs</tt><tt class="py-op">(</tt><tt class="py-param">out</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L686"></a><tt class="py-lineno"> 686</tt>  <tt class="py-line">                <tt class="py-keyword">for</tt> <tt class="py-name">obj</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L687"></a><tt class="py-lineno"> 687</tt>  <tt class="py-line">                    <tt class="py-name">out</tt><tt class="py-op">.</tt><tt class="py-name">write</tt><tt class="py-op">(</tt><tt class="py-name">str</tt><tt class="py-op">(</tt><tt class="py-name">obj</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">rstrip</tt><tt class="py-op">(</tt><tt class="py-string">'\n'</tt><tt class="py-op">)</tt> <tt class="py-op">+</tt> <tt class="py-string">'\n'</tt><tt class="py-op">)</tt> </tt>
<a name="L688"></a><tt class="py-lineno"> 688</tt>  <tt class="py-line">                <tt class="py-name">out</tt><tt class="py-op">.</tt><tt class="py-name">close</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L689"></a><tt class="py-lineno"> 689</tt>  <tt class="py-line">            <tt class="py-name">Thread</tt><tt class="py-op">(</tt><tt class="py-name">target</tt><tt class="py-op">=</tt><tt class="py-name">pipe_objs</tt><tt class="py-op">,</tt> <tt class="py-name">args</tt><tt class="py-op">=</tt><tt class="py-op">[</tt><tt id="link-104" class="py-name"><a title="pyspark.rdd.RDD.pipe" class="py-name" href="#" onclick="return doclink('link-104', 'pipe', 'link-103');">pipe</a></tt><tt class="py-op">.</tt><tt class="py-name">stdin</tt><tt class="py-op">]</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">start</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L690"></a><tt class="py-lineno"> 690</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">.</tt><tt class="py-name">rstrip</tt><tt class="py-op">(</tt><tt class="py-string">'\n'</tt><tt class="py-op">)</tt> <tt class="py-keyword">for</tt> <tt class="py-name">x</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iter</tt><tt class="py-op">(</tt><tt id="link-105" class="py-name"><a title="pyspark.rdd.RDD.pipe" class="py-name" href="#" onclick="return doclink('link-105', 'pipe', 'link-103');">pipe</a></tt><tt class="py-op">.</tt><tt class="py-name">stdout</tt><tt class="py-op">.</tt><tt class="py-name">readline</tt><tt class="py-op">,</tt> <tt class="py-string">''</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L691"></a><tt class="py-lineno"> 691</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-106" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-106', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">)</tt> </tt>
</div><a name="L692"></a><tt class="py-lineno"> 692</tt>  <tt class="py-line"> </tt>
<a name="RDD.foreach"></a><div id="RDD.foreach-def"><a name="L693"></a><tt class="py-lineno"> 693</tt> <a class="py-toggle" href="#" id="RDD.foreach-toggle" onclick="return toggle('RDD.foreach');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#foreach">foreach</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.foreach-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.foreach-expanded"><a name="L694"></a><tt class="py-lineno"> 694</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L695"></a><tt class="py-lineno"> 695</tt>  <tt class="py-line"><tt class="py-docstring">        Applies a function to all elements of this RDD.</tt> </tt>
<a name="L696"></a><tt class="py-lineno"> 696</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L697"></a><tt class="py-lineno"> 697</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; def f(x): print x</tt> </tt>
<a name="L698"></a><tt class="py-lineno"> 698</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).foreach(f)</tt> </tt>
<a name="L699"></a><tt class="py-lineno"> 699</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L700"></a><tt class="py-lineno"> 700</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">processPartition</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L701"></a><tt class="py-lineno"> 701</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">x</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L702"></a><tt class="py-lineno"> 702</tt>  <tt class="py-line">                <tt class="py-name">f</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt> </tt>
<a name="L703"></a><tt class="py-lineno"> 703</tt>  <tt class="py-line">            <tt class="py-keyword">yield</tt> <tt class="py-name">None</tt> </tt>
</div><a name="L704"></a><tt class="py-lineno"> 704</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-107" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-107', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">processPartition</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-108" class="py-name"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-108', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt>  <tt class="py-comment"># Force evaluation</tt> </tt>
</div><a name="L705"></a><tt class="py-lineno"> 705</tt>  <tt class="py-line"> </tt>
<a name="RDD.foreachPartition"></a><div id="RDD.foreachPartition-def"><a name="L706"></a><tt class="py-lineno"> 706</tt> <a class="py-toggle" href="#" id="RDD.foreachPartition-toggle" onclick="return toggle('RDD.foreachPartition');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#foreachPartition">foreachPartition</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.foreachPartition-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.foreachPartition-expanded"><a name="L707"></a><tt class="py-lineno"> 707</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L708"></a><tt class="py-lineno"> 708</tt>  <tt class="py-line"><tt class="py-docstring">        Applies a function to each partition of this RDD.</tt> </tt>
<a name="L709"></a><tt class="py-lineno"> 709</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L710"></a><tt class="py-lineno"> 710</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; def f(iterator):</tt> </tt>
<a name="L711"></a><tt class="py-lineno"> 711</tt>  <tt class="py-line"><tt class="py-docstring">        ...      for x in iterator:</tt> </tt>
<a name="L712"></a><tt class="py-lineno"> 712</tt>  <tt class="py-line"><tt class="py-docstring">        ...           print x</tt> </tt>
<a name="L713"></a><tt class="py-lineno"> 713</tt>  <tt class="py-line"><tt class="py-docstring">        ...      yield None</tt> </tt>
<a name="L714"></a><tt class="py-lineno"> 714</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)</tt> </tt>
<a name="L715"></a><tt class="py-lineno"> 715</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L716"></a><tt class="py-lineno"> 716</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-109" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-109', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">f</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-110" class="py-name"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-110', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt>  <tt class="py-comment"># Force evaluation</tt> </tt>
</div><a name="L717"></a><tt class="py-lineno"> 717</tt>  <tt class="py-line"> </tt>
<a name="RDD.collect"></a><div id="RDD.collect-def"><a name="L718"></a><tt class="py-lineno"> 718</tt> <a class="py-toggle" href="#" id="RDD.collect-toggle" onclick="return toggle('RDD.collect');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#collect">collect</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.collect-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.collect-expanded"><a name="L719"></a><tt class="py-lineno"> 719</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L720"></a><tt class="py-lineno"> 720</tt>  <tt class="py-line"><tt class="py-docstring">        Return a list that contains all of the elements in this RDD.</tt> </tt>
<a name="L721"></a><tt class="py-lineno"> 721</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L722"></a><tt class="py-lineno"> 722</tt>  <tt class="py-line">        <tt class="py-keyword">with</tt> <tt class="py-name">_JavaStackTrace</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-111" class="py-name" targets="Module pyspark.context=pyspark.context-module.html,Method pyspark.rdd.RDD.context()=pyspark.rdd.RDD-class.html#context"><a title="pyspark.context
pyspark.rdd.RDD.context" class="py-name" href="#" onclick="return doclink('link-111', 'context', 'link-111');">context</a></tt><tt class="py-op">)</tt> <tt class="py-keyword">as</tt> <tt class="py-name">st</tt><tt class="py-op">:</tt> </tt>
<a name="L723"></a><tt class="py-lineno"> 723</tt>  <tt class="py-line">            <tt class="py-name">bytesInJava</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-112" class="py-name"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-112', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">iterator</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L724"></a><tt class="py-lineno"> 724</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">list</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_collect_iterator_through_file</tt><tt class="py-op">(</tt><tt class="py-name">bytesInJava</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L725"></a><tt class="py-lineno"> 725</tt>  <tt class="py-line"> </tt>
<a name="RDD._collect_iterator_through_file"></a><div id="RDD._collect_iterator_through_file-def"><a name="L726"></a><tt class="py-lineno"> 726</tt> <a class="py-toggle" href="#" id="RDD._collect_iterator_through_file-toggle" onclick="return toggle('RDD._collect_iterator_through_file');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#_collect_iterator_through_file">_collect_iterator_through_file</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD._collect_iterator_through_file-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD._collect_iterator_through_file-expanded"><a name="L727"></a><tt class="py-lineno"> 727</tt>  <tt class="py-line">        <tt class="py-comment"># Transferring lots of data through Py4J can be slow because</tt> </tt>
<a name="L728"></a><tt class="py-lineno"> 728</tt>  <tt class="py-line">        <tt class="py-comment"># socket.readline() is inefficient.  Instead, we'll dump the data to a</tt> </tt>
<a name="L729"></a><tt class="py-lineno"> 729</tt>  <tt class="py-line">        <tt class="py-comment"># file and read it back.</tt> </tt>
<a name="L730"></a><tt class="py-lineno"> 730</tt>  <tt class="py-line">        <tt class="py-name">tempFile</tt> <tt class="py-op">=</tt> <tt class="py-name">NamedTemporaryFile</tt><tt class="py-op">(</tt><tt class="py-name">delete</tt><tt class="py-op">=</tt><tt class="py-name">False</tt><tt class="py-op">,</tt> <tt class="py-name">dir</tt><tt class="py-op">=</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_temp_dir</tt><tt class="py-op">)</tt> </tt>
<a name="L731"></a><tt class="py-lineno"> 731</tt>  <tt class="py-line">        <tt class="py-name">tempFile</tt><tt class="py-op">.</tt><tt class="py-name">close</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L732"></a><tt class="py-lineno"> 732</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-113" class="py-name" targets="Variable pyspark.context.SparkContext._writeToFile=pyspark.context.SparkContext-class.html#_writeToFile"><a title="pyspark.context.SparkContext._writeToFile" class="py-name" href="#" onclick="return doclink('link-113', '_writeToFile', 'link-113');">_writeToFile</a></tt><tt class="py-op">(</tt><tt class="py-name">iterator</tt><tt class="py-op">,</tt> <tt class="py-name">tempFile</tt><tt class="py-op">.</tt><tt id="link-114" class="py-name" targets="Method pyspark.rdd.RDD.name()=pyspark.rdd.RDD-class.html#name"><a title="pyspark.rdd.RDD.name" class="py-name" href="#" onclick="return doclink('link-114', 'name', 'link-114');">name</a></tt><tt class="py-op">)</tt> </tt>
<a name="L733"></a><tt class="py-lineno"> 733</tt>  <tt class="py-line">        <tt class="py-comment"># Read the data into Python and deserialize it:</tt> </tt>
<a name="L734"></a><tt class="py-lineno"> 734</tt>  <tt class="py-line">        <tt class="py-keyword">with</tt> <tt class="py-name">open</tt><tt class="py-op">(</tt><tt class="py-name">tempFile</tt><tt class="py-op">.</tt><tt id="link-115" class="py-name"><a title="pyspark.rdd.RDD.name" class="py-name" href="#" onclick="return doclink('link-115', 'name', 'link-114');">name</a></tt><tt class="py-op">,</tt> <tt class="py-string">'rb'</tt><tt class="py-op">)</tt> <tt class="py-keyword">as</tt> <tt class="py-name">tempFile</tt><tt class="py-op">:</tt> </tt>
<a name="L735"></a><tt class="py-lineno"> 735</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">item</tt> <tt class="py-keyword">in</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">.</tt><tt class="py-name">load_stream</tt><tt class="py-op">(</tt><tt class="py-name">tempFile</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L736"></a><tt class="py-lineno"> 736</tt>  <tt class="py-line">                <tt class="py-keyword">yield</tt> <tt class="py-name">item</tt> </tt>
<a name="L737"></a><tt class="py-lineno"> 737</tt>  <tt class="py-line">        <tt class="py-name">os</tt><tt class="py-op">.</tt><tt class="py-name">unlink</tt><tt class="py-op">(</tt><tt class="py-name">tempFile</tt><tt class="py-op">.</tt><tt id="link-116" class="py-name"><a title="pyspark.rdd.RDD.name" class="py-name" href="#" onclick="return doclink('link-116', 'name', 'link-114');">name</a></tt><tt class="py-op">)</tt> </tt>
</div><a name="L738"></a><tt class="py-lineno"> 738</tt>  <tt class="py-line"> </tt>
<a name="RDD.reduce"></a><div id="RDD.reduce-def"><a name="L739"></a><tt class="py-lineno"> 739</tt> <a class="py-toggle" href="#" id="RDD.reduce-toggle" onclick="return toggle('RDD.reduce');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#reduce">reduce</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.reduce-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.reduce-expanded"><a name="L740"></a><tt class="py-lineno"> 740</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L741"></a><tt class="py-lineno"> 741</tt>  <tt class="py-line"><tt class="py-docstring">        Reduces the elements of this RDD using the specified commutative and</tt> </tt>
<a name="L742"></a><tt class="py-lineno"> 742</tt>  <tt class="py-line"><tt class="py-docstring">        associative binary operator. Currently reduces partitions locally.</tt> </tt>
<a name="L743"></a><tt class="py-lineno"> 743</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L744"></a><tt class="py-lineno"> 744</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; from operator import add</tt> </tt>
<a name="L745"></a><tt class="py-lineno"> 745</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).reduce(add)</tt> </tt>
<a name="L746"></a><tt class="py-lineno"> 746</tt>  <tt class="py-line"><tt class="py-docstring">        15</tt> </tt>
<a name="L747"></a><tt class="py-lineno"> 747</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)</tt> </tt>
<a name="L748"></a><tt class="py-lineno"> 748</tt>  <tt class="py-line"><tt class="py-docstring">        10</tt> </tt>
<a name="L749"></a><tt class="py-lineno"> 749</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L750"></a><tt class="py-lineno"> 750</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L751"></a><tt class="py-lineno"> 751</tt>  <tt class="py-line">            <tt class="py-name">acc</tt> <tt class="py-op">=</tt> <tt class="py-name">None</tt> </tt>
<a name="L752"></a><tt class="py-lineno"> 752</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">obj</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L753"></a><tt class="py-lineno"> 753</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-name">acc</tt> <tt class="py-keyword">is</tt> <tt class="py-name">None</tt><tt class="py-op">:</tt> </tt>
<a name="L754"></a><tt class="py-lineno"> 754</tt>  <tt class="py-line">                    <tt class="py-name">acc</tt> <tt class="py-op">=</tt> <tt class="py-name">obj</tt> </tt>
<a name="L755"></a><tt class="py-lineno"> 755</tt>  <tt class="py-line">                <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L756"></a><tt class="py-lineno"> 756</tt>  <tt class="py-line">                    <tt class="py-name">acc</tt> <tt class="py-op">=</tt> <tt class="py-name">f</tt><tt class="py-op">(</tt><tt class="py-name">obj</tt><tt class="py-op">,</tt> <tt class="py-name">acc</tt><tt class="py-op">)</tt> </tt>
<a name="L757"></a><tt class="py-lineno"> 757</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">acc</tt> <tt class="py-keyword">is</tt> <tt class="py-keyword">not</tt> <tt class="py-name">None</tt><tt class="py-op">:</tt> </tt>
<a name="L758"></a><tt class="py-lineno"> 758</tt>  <tt class="py-line">                <tt class="py-keyword">yield</tt> <tt class="py-name">acc</tt> </tt>
</div><a name="L759"></a><tt class="py-lineno"> 759</tt>  <tt class="py-line">        <tt class="py-name">vals</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-117" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-117', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-118" class="py-name"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-118', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L760"></a><tt class="py-lineno"> 760</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt id="link-119" class="py-name" targets="Method pyspark.rdd.RDD.reduce()=pyspark.rdd.RDD-class.html#reduce"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-119', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt class="py-name">f</tt><tt class="py-op">,</tt> <tt class="py-name">vals</tt><tt class="py-op">)</tt> </tt>
</div><a name="L761"></a><tt class="py-lineno"> 761</tt>  <tt class="py-line"> </tt>
<a name="RDD.fold"></a><div id="RDD.fold-def"><a name="L762"></a><tt class="py-lineno"> 762</tt> <a class="py-toggle" href="#" id="RDD.fold-toggle" onclick="return toggle('RDD.fold');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#fold">fold</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">zeroValue</tt><tt class="py-op">,</tt> <tt class="py-param">op</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.fold-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.fold-expanded"><a name="L763"></a><tt class="py-lineno"> 763</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L764"></a><tt class="py-lineno"> 764</tt>  <tt class="py-line"><tt class="py-docstring">        Aggregate the elements of each partition, and then the results for all</tt> </tt>
<a name="L765"></a><tt class="py-lineno"> 765</tt>  <tt class="py-line"><tt class="py-docstring">        the partitions, using a given associative function and a neutral "zero</tt> </tt>
<a name="L766"></a><tt class="py-lineno"> 766</tt>  <tt class="py-line"><tt class="py-docstring">        value."</tt> </tt>
<a name="L767"></a><tt class="py-lineno"> 767</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L768"></a><tt class="py-lineno"> 768</tt>  <tt class="py-line"><tt class="py-docstring">        The function C{op(t1, t2)} is allowed to modify C{t1} and return it</tt> </tt>
<a name="L769"></a><tt class="py-lineno"> 769</tt>  <tt class="py-line"><tt class="py-docstring">        as its result value to avoid object allocation; however, it should not</tt> </tt>
<a name="L770"></a><tt class="py-lineno"> 770</tt>  <tt class="py-line"><tt class="py-docstring">        modify C{t2}.</tt> </tt>
<a name="L771"></a><tt class="py-lineno"> 771</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L772"></a><tt class="py-lineno"> 772</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; from operator import add</tt> </tt>
<a name="L773"></a><tt class="py-lineno"> 773</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)</tt> </tt>
<a name="L774"></a><tt class="py-lineno"> 774</tt>  <tt class="py-line"><tt class="py-docstring">        15</tt> </tt>
<a name="L775"></a><tt class="py-lineno"> 775</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L776"></a><tt class="py-lineno"> 776</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L777"></a><tt class="py-lineno"> 777</tt>  <tt class="py-line">            <tt class="py-name">acc</tt> <tt class="py-op">=</tt> <tt class="py-name">zeroValue</tt> </tt>
<a name="L778"></a><tt class="py-lineno"> 778</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">obj</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L779"></a><tt class="py-lineno"> 779</tt>  <tt class="py-line">                <tt class="py-name">acc</tt> <tt class="py-op">=</tt> <tt class="py-name">op</tt><tt class="py-op">(</tt><tt class="py-name">obj</tt><tt class="py-op">,</tt> <tt class="py-name">acc</tt><tt class="py-op">)</tt> </tt>
<a name="L780"></a><tt class="py-lineno"> 780</tt>  <tt class="py-line">            <tt class="py-keyword">yield</tt> <tt class="py-name">acc</tt> </tt>
</div><a name="L781"></a><tt class="py-lineno"> 781</tt>  <tt class="py-line">        <tt class="py-name">vals</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-120" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-120', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-121" class="py-name"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-121', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L782"></a><tt class="py-lineno"> 782</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt id="link-122" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-122', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt class="py-name">op</tt><tt class="py-op">,</tt> <tt class="py-name">vals</tt><tt class="py-op">,</tt> <tt class="py-name">zeroValue</tt><tt class="py-op">)</tt> </tt>
</div><a name="L783"></a><tt class="py-lineno"> 783</tt>  <tt class="py-line"> </tt>
<a name="RDD.aggregate"></a><div id="RDD.aggregate-def"><a name="L784"></a><tt class="py-lineno"> 784</tt> <a class="py-toggle" href="#" id="RDD.aggregate-toggle" onclick="return toggle('RDD.aggregate');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#aggregate">aggregate</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">zeroValue</tt><tt class="py-op">,</tt> <tt class="py-param">seqOp</tt><tt class="py-op">,</tt> <tt class="py-param">combOp</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.aggregate-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.aggregate-expanded"><a name="L785"></a><tt class="py-lineno"> 785</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L786"></a><tt class="py-lineno"> 786</tt>  <tt class="py-line"><tt class="py-docstring">        Aggregate the elements of each partition, and then the results for all</tt> </tt>
<a name="L787"></a><tt class="py-lineno"> 787</tt>  <tt class="py-line"><tt class="py-docstring">        the partitions, using a given combine functions and a neutral "zero</tt> </tt>
<a name="L788"></a><tt class="py-lineno"> 788</tt>  <tt class="py-line"><tt class="py-docstring">        value."</tt> </tt>
<a name="L789"></a><tt class="py-lineno"> 789</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L790"></a><tt class="py-lineno"> 790</tt>  <tt class="py-line"><tt class="py-docstring">        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it</tt> </tt>
<a name="L791"></a><tt class="py-lineno"> 791</tt>  <tt class="py-line"><tt class="py-docstring">        as its result value to avoid object allocation; however, it should not</tt> </tt>
<a name="L792"></a><tt class="py-lineno"> 792</tt>  <tt class="py-line"><tt class="py-docstring">        modify C{t2}.</tt> </tt>
<a name="L793"></a><tt class="py-lineno"> 793</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L794"></a><tt class="py-lineno"> 794</tt>  <tt class="py-line"><tt class="py-docstring">        The first function (seqOp) can return a different result type, U, than</tt> </tt>
<a name="L795"></a><tt class="py-lineno"> 795</tt>  <tt class="py-line"><tt class="py-docstring">        the type of this RDD. Thus, we need one operation for merging a T into</tt> </tt>
<a name="L796"></a><tt class="py-lineno"> 796</tt>  <tt class="py-line"><tt class="py-docstring">        an U and one operation for merging two U</tt> </tt>
<a name="L797"></a><tt class="py-lineno"> 797</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L798"></a><tt class="py-lineno"> 798</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))</tt> </tt>
<a name="L799"></a><tt class="py-lineno"> 799</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))</tt> </tt>
<a name="L800"></a><tt class="py-lineno"> 800</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)</tt> </tt>
<a name="L801"></a><tt class="py-lineno"> 801</tt>  <tt class="py-line"><tt class="py-docstring">        (10, 4)</tt> </tt>
<a name="L802"></a><tt class="py-lineno"> 802</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([]).aggregate((0, 0), seqOp, combOp)</tt> </tt>
<a name="L803"></a><tt class="py-lineno"> 803</tt>  <tt class="py-line"><tt class="py-docstring">        (0, 0)</tt> </tt>
<a name="L804"></a><tt class="py-lineno"> 804</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L805"></a><tt class="py-lineno"> 805</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L806"></a><tt class="py-lineno"> 806</tt>  <tt class="py-line">            <tt class="py-name">acc</tt> <tt class="py-op">=</tt> <tt class="py-name">zeroValue</tt> </tt>
<a name="L807"></a><tt class="py-lineno"> 807</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">obj</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L808"></a><tt class="py-lineno"> 808</tt>  <tt class="py-line">                <tt class="py-name">acc</tt> <tt class="py-op">=</tt> <tt class="py-name">seqOp</tt><tt class="py-op">(</tt><tt class="py-name">acc</tt><tt class="py-op">,</tt> <tt class="py-name">obj</tt><tt class="py-op">)</tt> </tt>
<a name="L809"></a><tt class="py-lineno"> 809</tt>  <tt class="py-line">            <tt class="py-keyword">yield</tt> <tt class="py-name">acc</tt> </tt>
</div><a name="L810"></a><tt class="py-lineno"> 810</tt>  <tt class="py-line"> </tt>
<a name="L811"></a><tt class="py-lineno"> 811</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-123" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-123', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-124" class="py-name" targets="Method pyspark.rdd.RDD.fold()=pyspark.rdd.RDD-class.html#fold"><a title="pyspark.rdd.RDD.fold" class="py-name" href="#" onclick="return doclink('link-124', 'fold', 'link-124');">fold</a></tt><tt class="py-op">(</tt><tt class="py-name">zeroValue</tt><tt class="py-op">,</tt> <tt class="py-name">combOp</tt><tt class="py-op">)</tt> </tt>
</div><a name="L812"></a><tt class="py-lineno"> 812</tt>  <tt class="py-line"> </tt>
<a name="RDD.max"></a><div id="RDD.max-def"><a name="L813"></a><tt class="py-lineno"> 813</tt> <a class="py-toggle" href="#" id="RDD.max-toggle" onclick="return toggle('RDD.max');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#max">max</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.max-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.max-expanded"><a name="L814"></a><tt class="py-lineno"> 814</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L815"></a><tt class="py-lineno"> 815</tt>  <tt class="py-line"><tt class="py-docstring">        Find the maximum item in this RDD.</tt> </tt>
<a name="L816"></a><tt class="py-lineno"> 816</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L817"></a><tt class="py-lineno"> 817</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1.0, 5.0, 43.0, 10.0]).max()</tt> </tt>
<a name="L818"></a><tt class="py-lineno"> 818</tt>  <tt class="py-line"><tt class="py-docstring">        43.0</tt> </tt>
<a name="L819"></a><tt class="py-lineno"> 819</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L820"></a><tt class="py-lineno"> 820</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-125" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-125', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt id="link-126" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.max
pyspark.rdd.RDD.max
pyspark.statcounter.StatCounter.max" class="py-name" href="#" onclick="return doclink('link-126', 'max', 'link-89');">max</a></tt><tt class="py-op">)</tt> </tt>
</div><a name="L821"></a><tt class="py-lineno"> 821</tt>  <tt class="py-line"> </tt>
<a name="RDD.min"></a><div id="RDD.min-def"><a name="L822"></a><tt class="py-lineno"> 822</tt> <a class="py-toggle" href="#" id="RDD.min-toggle" onclick="return toggle('RDD.min');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#min">min</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.min-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.min-expanded"><a name="L823"></a><tt class="py-lineno"> 823</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L824"></a><tt class="py-lineno"> 824</tt>  <tt class="py-line"><tt class="py-docstring">        Find the minimum item in this RDD.</tt> </tt>
<a name="L825"></a><tt class="py-lineno"> 825</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L826"></a><tt class="py-lineno"> 826</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1.0, 5.0, 43.0, 10.0]).min()</tt> </tt>
<a name="L827"></a><tt class="py-lineno"> 827</tt>  <tt class="py-line"><tt class="py-docstring">        1.0</tt> </tt>
<a name="L828"></a><tt class="py-lineno"> 828</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L829"></a><tt class="py-lineno"> 829</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-127" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-127', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt id="link-128" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.min
pyspark.rdd.RDD.min
pyspark.statcounter.StatCounter.min" class="py-name" href="#" onclick="return doclink('link-128', 'min', 'link-68');">min</a></tt><tt class="py-op">)</tt> </tt>
</div><a name="L830"></a><tt class="py-lineno"> 830</tt>  <tt class="py-line"> </tt>
<a name="RDD.sum"></a><div id="RDD.sum-def"><a name="L831"></a><tt class="py-lineno"> 831</tt> <a class="py-toggle" href="#" id="RDD.sum-toggle" onclick="return toggle('RDD.sum');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#sum">sum</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.sum-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.sum-expanded"><a name="L832"></a><tt class="py-lineno"> 832</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L833"></a><tt class="py-lineno"> 833</tt>  <tt class="py-line"><tt class="py-docstring">        Add up the elements in this RDD.</tt> </tt>
<a name="L834"></a><tt class="py-lineno"> 834</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L835"></a><tt class="py-lineno"> 835</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1.0, 2.0, 3.0]).sum()</tt> </tt>
<a name="L836"></a><tt class="py-lineno"> 836</tt>  <tt class="py-line"><tt class="py-docstring">        6.0</tt> </tt>
<a name="L837"></a><tt class="py-lineno"> 837</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L838"></a><tt class="py-lineno"> 838</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-129" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-129', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-op">[</tt><tt id="link-130" class="py-name" targets="Method pyspark.rdd.RDD.sum()=pyspark.rdd.RDD-class.html#sum,Method pyspark.statcounter.StatCounter.sum()=pyspark.statcounter.StatCounter-class.html#sum"><a title="pyspark.rdd.RDD.sum
pyspark.statcounter.StatCounter.sum" class="py-name" href="#" onclick="return doclink('link-130', 'sum', 'link-130');">sum</a></tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt><tt class="py-op">]</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-131" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-131', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt class="py-name">operator</tt><tt class="py-op">.</tt><tt id="link-132" class="py-name" targets="Method pyspark.accumulators.Accumulator.add()=pyspark.accumulators.Accumulator-class.html#add"><a title="pyspark.accumulators.Accumulator.add" class="py-name" href="#" onclick="return doclink('link-132', 'add', 'link-132');">add</a></tt><tt class="py-op">)</tt> </tt>
</div><a name="L839"></a><tt class="py-lineno"> 839</tt>  <tt class="py-line"> </tt>
<a name="RDD.count"></a><div id="RDD.count-def"><a name="L840"></a><tt class="py-lineno"> 840</tt> <a class="py-toggle" href="#" id="RDD.count-toggle" onclick="return toggle('RDD.count');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#count">count</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.count-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.count-expanded"><a name="L841"></a><tt class="py-lineno"> 841</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L842"></a><tt class="py-lineno"> 842</tt>  <tt class="py-line"><tt class="py-docstring">        Return the number of elements in this RDD.</tt> </tt>
<a name="L843"></a><tt class="py-lineno"> 843</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L844"></a><tt class="py-lineno"> 844</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([2, 3, 4]).count()</tt> </tt>
<a name="L845"></a><tt class="py-lineno"> 845</tt>  <tt class="py-line"><tt class="py-docstring">        3</tt> </tt>
<a name="L846"></a><tt class="py-lineno"> 846</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L847"></a><tt class="py-lineno"> 847</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-133" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-133', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">i</tt><tt class="py-op">:</tt> <tt class="py-op">[</tt><tt id="link-134" class="py-name"><a title="pyspark.rdd.RDD.sum
pyspark.statcounter.StatCounter.sum" class="py-name" href="#" onclick="return doclink('link-134', 'sum', 'link-130');">sum</a></tt><tt class="py-op">(</tt><tt class="py-number">1</tt> <tt class="py-keyword">for</tt> <tt class="py-name">_</tt> <tt class="py-keyword">in</tt> <tt class="py-name">i</tt><tt class="py-op">)</tt><tt class="py-op">]</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-135" class="py-name"><a title="pyspark.rdd.RDD.sum
pyspark.statcounter.StatCounter.sum" class="py-name" href="#" onclick="return doclink('link-135', 'sum', 'link-130');">sum</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L848"></a><tt class="py-lineno"> 848</tt>  <tt class="py-line"> </tt>
<a name="RDD.stats"></a><div id="RDD.stats-def"><a name="L849"></a><tt class="py-lineno"> 849</tt> <a class="py-toggle" href="#" id="RDD.stats-toggle" onclick="return toggle('RDD.stats');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#stats">stats</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.stats-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.stats-expanded"><a name="L850"></a><tt class="py-lineno"> 850</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L851"></a><tt class="py-lineno"> 851</tt>  <tt class="py-line"><tt class="py-docstring">        Return a L{StatCounter} object that captures the mean, variance</tt> </tt>
<a name="L852"></a><tt class="py-lineno"> 852</tt>  <tt class="py-line"><tt class="py-docstring">        and count of the RDD's elements in one operation.</tt> </tt>
<a name="L853"></a><tt class="py-lineno"> 853</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L854"></a><tt class="py-lineno"> 854</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">redFunc</tt><tt class="py-op">(</tt><tt class="py-param">left_counter</tt><tt class="py-op">,</tt> <tt class="py-param">right_counter</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L855"></a><tt class="py-lineno"> 855</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">left_counter</tt><tt class="py-op">.</tt><tt id="link-136" class="py-name" targets="Method pyspark.statcounter.StatCounter.mergeStats()=pyspark.statcounter.StatCounter-class.html#mergeStats"><a title="pyspark.statcounter.StatCounter.mergeStats" class="py-name" href="#" onclick="return doclink('link-136', 'mergeStats', 'link-136');">mergeStats</a></tt><tt class="py-op">(</tt><tt class="py-name">right_counter</tt><tt class="py-op">)</tt> </tt>
</div><a name="L856"></a><tt class="py-lineno"> 856</tt>  <tt class="py-line"> </tt>
<a name="L857"></a><tt class="py-lineno"> 857</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-137" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-137', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">i</tt><tt class="py-op">:</tt> <tt class="py-op">[</tt><tt id="link-138" class="py-name"><a title="pyspark.statcounter.StatCounter" class="py-name" href="#" onclick="return doclink('link-138', 'StatCounter', 'link-10');">StatCounter</a></tt><tt class="py-op">(</tt><tt class="py-name">i</tt><tt class="py-op">)</tt><tt class="py-op">]</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-139" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-139', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt class="py-name">redFunc</tt><tt class="py-op">)</tt> </tt>
</div><a name="L858"></a><tt class="py-lineno"> 858</tt>  <tt class="py-line"> </tt>
<a name="RDD.histogram"></a><div id="RDD.histogram-def"><a name="L859"></a><tt class="py-lineno"> 859</tt> <a class="py-toggle" href="#" id="RDD.histogram-toggle" onclick="return toggle('RDD.histogram');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#histogram">histogram</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">buckets</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.histogram-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.histogram-expanded"><a name="L860"></a><tt class="py-lineno"> 860</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L861"></a><tt class="py-lineno"> 861</tt>  <tt class="py-line"><tt class="py-docstring">        Compute a histogram using the provided buckets. The buckets</tt> </tt>
<a name="L862"></a><tt class="py-lineno"> 862</tt>  <tt class="py-line"><tt class="py-docstring">        are all open to the right except for the last which is closed.</tt> </tt>
<a name="L863"></a><tt class="py-lineno"> 863</tt>  <tt class="py-line"><tt class="py-docstring">        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],</tt> </tt>
<a name="L864"></a><tt class="py-lineno"> 864</tt>  <tt class="py-line"><tt class="py-docstring">        which means 1&lt;=x&lt;10, 10&lt;=x&lt;20, 20&lt;=x&lt;=50. And on the input of 1</tt> </tt>
<a name="L865"></a><tt class="py-lineno"> 865</tt>  <tt class="py-line"><tt class="py-docstring">        and 50 we would have a histogram of 1,0,1.</tt> </tt>
<a name="L866"></a><tt class="py-lineno"> 866</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L867"></a><tt class="py-lineno"> 867</tt>  <tt class="py-line"><tt class="py-docstring">        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),</tt> </tt>
<a name="L868"></a><tt class="py-lineno"> 868</tt>  <tt class="py-line"><tt class="py-docstring">        this can be switched from an O(log n) inseration to O(1) per</tt> </tt>
<a name="L869"></a><tt class="py-lineno"> 869</tt>  <tt class="py-line"><tt class="py-docstring">        element(where n = # buckets).</tt> </tt>
<a name="L870"></a><tt class="py-lineno"> 870</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L871"></a><tt class="py-lineno"> 871</tt>  <tt class="py-line"><tt class="py-docstring">        Buckets must be sorted and not contain any duplicates, must be</tt> </tt>
<a name="L872"></a><tt class="py-lineno"> 872</tt>  <tt class="py-line"><tt class="py-docstring">        at least two elements.</tt> </tt>
<a name="L873"></a><tt class="py-lineno"> 873</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L874"></a><tt class="py-lineno"> 874</tt>  <tt class="py-line"><tt class="py-docstring">        If `buckets` is a number, it will generates buckets which are</tt> </tt>
<a name="L875"></a><tt class="py-lineno"> 875</tt>  <tt class="py-line"><tt class="py-docstring">        evenly spaced between the minimum and maximum of the RDD. For</tt> </tt>
<a name="L876"></a><tt class="py-lineno"> 876</tt>  <tt class="py-line"><tt class="py-docstring">        example, if the min value is 0 and the max is 100, given buckets</tt> </tt>
<a name="L877"></a><tt class="py-lineno"> 877</tt>  <tt class="py-line"><tt class="py-docstring">        as 2, the resulting buckets will be [0,50) [50,100]. buckets must</tt> </tt>
<a name="L878"></a><tt class="py-lineno"> 878</tt>  <tt class="py-line"><tt class="py-docstring">        be at least 1 If the RDD contains infinity, NaN throws an exception</tt> </tt>
<a name="L879"></a><tt class="py-lineno"> 879</tt>  <tt class="py-line"><tt class="py-docstring">        If the elements in RDD do not vary (max == min) always returns</tt> </tt>
<a name="L880"></a><tt class="py-lineno"> 880</tt>  <tt class="py-line"><tt class="py-docstring">        a single bucket.</tt> </tt>
<a name="L881"></a><tt class="py-lineno"> 881</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L882"></a><tt class="py-lineno"> 882</tt>  <tt class="py-line"><tt class="py-docstring">        It will return an tuple of buckets and histogram.</tt> </tt>
<a name="L883"></a><tt class="py-lineno"> 883</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L884"></a><tt class="py-lineno"> 884</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize(range(51))</tt> </tt>
<a name="L885"></a><tt class="py-lineno"> 885</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.histogram(2)</tt> </tt>
<a name="L886"></a><tt class="py-lineno"> 886</tt>  <tt class="py-line"><tt class="py-docstring">        ([0, 25, 50], [25, 26])</tt> </tt>
<a name="L887"></a><tt class="py-lineno"> 887</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.histogram([0, 5, 25, 50])</tt> </tt>
<a name="L888"></a><tt class="py-lineno"> 888</tt>  <tt class="py-line"><tt class="py-docstring">        ([0, 5, 25, 50], [5, 20, 26])</tt> </tt>
<a name="L889"></a><tt class="py-lineno"> 889</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets</tt> </tt>
<a name="L890"></a><tt class="py-lineno"> 890</tt>  <tt class="py-line"><tt class="py-docstring">        ([0, 15, 30, 45, 60], [15, 15, 15, 6])</tt> </tt>
<a name="L891"></a><tt class="py-lineno"> 891</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize(["ab", "ac", "b", "bd", "ef"])</tt> </tt>
<a name="L892"></a><tt class="py-lineno"> 892</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.histogram(("a", "b", "c"))</tt> </tt>
<a name="L893"></a><tt class="py-lineno"> 893</tt>  <tt class="py-line"><tt class="py-docstring">        (('a', 'b', 'c'), [2, 2])</tt> </tt>
<a name="L894"></a><tt class="py-lineno"> 894</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L895"></a><tt class="py-lineno"> 895</tt>  <tt class="py-line"> </tt>
<a name="L896"></a><tt class="py-lineno"> 896</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">,</tt> <tt class="py-op">(</tt><tt class="py-name">int</tt><tt class="py-op">,</tt> <tt class="py-name">long</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L897"></a><tt class="py-lineno"> 897</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">buckets</tt> <tt class="py-op">&lt;</tt> <tt class="py-number">1</tt><tt class="py-op">:</tt> </tt>
<a name="L898"></a><tt class="py-lineno"> 898</tt>  <tt class="py-line">                <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt><tt class="py-string">"number of buckets must be &gt;= 1"</tt><tt class="py-op">)</tt> </tt>
<a name="L899"></a><tt class="py-lineno"> 899</tt>  <tt class="py-line"> </tt>
<a name="L900"></a><tt class="py-lineno"> 900</tt>  <tt class="py-line">            <tt class="py-comment"># filter out non-comparable elements</tt> </tt>
<a name="L901"></a><tt class="py-lineno"> 901</tt>  <tt class="py-line">            <tt class="py-keyword">def</tt> <tt class="py-def-name">comparable</tt><tt class="py-op">(</tt><tt class="py-param">x</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L902"></a><tt class="py-lineno"> 902</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-name">x</tt> <tt class="py-keyword">is</tt> <tt class="py-name">None</tt><tt class="py-op">:</tt> </tt>
<a name="L903"></a><tt class="py-lineno"> 903</tt>  <tt class="py-line">                    <tt class="py-keyword">return</tt> <tt class="py-name">False</tt> </tt>
<a name="L904"></a><tt class="py-lineno"> 904</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-name">type</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt> <tt class="py-keyword">is</tt> <tt class="py-name">float</tt> <tt class="py-keyword">and</tt> <tt class="py-name">isnan</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L905"></a><tt class="py-lineno"> 905</tt>  <tt class="py-line">                    <tt class="py-keyword">return</tt> <tt class="py-name">False</tt> </tt>
<a name="L906"></a><tt class="py-lineno"> 906</tt>  <tt class="py-line">                <tt class="py-keyword">return</tt> <tt class="py-name">True</tt> </tt>
</div><a name="L907"></a><tt class="py-lineno"> 907</tt>  <tt class="py-line"> </tt>
<a name="L908"></a><tt class="py-lineno"> 908</tt>  <tt class="py-line">            <tt class="py-name">filtered</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-140" class="py-name"><a title="pyspark.rdd.RDD.filter" class="py-name" href="#" onclick="return doclink('link-140', 'filter', 'link-79');">filter</a></tt><tt class="py-op">(</tt><tt class="py-name">comparable</tt><tt class="py-op">)</tt> </tt>
<a name="L909"></a><tt class="py-lineno"> 909</tt>  <tt class="py-line"> </tt>
<a name="L910"></a><tt class="py-lineno"> 910</tt>  <tt class="py-line">            <tt class="py-comment"># faster than stats()</tt> </tt>
<a name="L911"></a><tt class="py-lineno"> 911</tt>  <tt class="py-line">            <tt class="py-keyword">def</tt> <tt class="py-def-name">minmax</tt><tt class="py-op">(</tt><tt class="py-param">a</tt><tt class="py-op">,</tt> <tt class="py-param">b</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L912"></a><tt class="py-lineno"> 912</tt>  <tt class="py-line">                <tt class="py-keyword">return</tt> <tt id="link-141" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.min
pyspark.rdd.RDD.min
pyspark.statcounter.StatCounter.min" class="py-name" href="#" onclick="return doclink('link-141', 'min', 'link-68');">min</a></tt><tt class="py-op">(</tt><tt class="py-name">a</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt><tt class="py-op">,</tt> <tt class="py-name">b</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt id="link-142" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.max
pyspark.rdd.RDD.max
pyspark.statcounter.StatCounter.max" class="py-name" href="#" onclick="return doclink('link-142', 'max', 'link-89');">max</a></tt><tt class="py-op">(</tt><tt class="py-name">a</tt><tt class="py-op">[</tt><tt class="py-number">1</tt><tt class="py-op">]</tt><tt class="py-op">,</tt> <tt class="py-name">b</tt><tt class="py-op">[</tt><tt class="py-number">1</tt><tt class="py-op">]</tt><tt class="py-op">)</tt> </tt>
</div><a name="L913"></a><tt class="py-lineno"> 913</tt>  <tt class="py-line">            <tt class="py-keyword">try</tt><tt class="py-op">:</tt> </tt>
<a name="L914"></a><tt class="py-lineno"> 914</tt>  <tt class="py-line">                <tt class="py-name">minv</tt><tt class="py-op">,</tt> <tt class="py-name">maxv</tt> <tt class="py-op">=</tt> <tt class="py-name">filtered</tt><tt class="py-op">.</tt><tt id="link-143" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-143', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">x</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-144" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-144', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt class="py-name">minmax</tt><tt class="py-op">)</tt> </tt>
<a name="L915"></a><tt class="py-lineno"> 915</tt>  <tt class="py-line">            <tt class="py-keyword">except</tt> <tt class="py-name">TypeError</tt> <tt class="py-keyword">as</tt> <tt class="py-name">e</tt><tt class="py-op">:</tt> </tt>
<a name="L916"></a><tt class="py-lineno"> 916</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-string">" empty "</tt> <tt class="py-keyword">in</tt> <tt class="py-name">str</tt><tt class="py-op">(</tt><tt class="py-name">e</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L917"></a><tt class="py-lineno"> 917</tt>  <tt class="py-line">                    <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt><tt class="py-string">"can not generate buckets from empty RDD"</tt><tt class="py-op">)</tt> </tt>
<a name="L918"></a><tt class="py-lineno"> 918</tt>  <tt class="py-line">                <tt class="py-keyword">raise</tt> </tt>
<a name="L919"></a><tt class="py-lineno"> 919</tt>  <tt class="py-line"> </tt>
<a name="L920"></a><tt class="py-lineno"> 920</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">minv</tt> <tt class="py-op">==</tt> <tt class="py-name">maxv</tt> <tt class="py-keyword">or</tt> <tt class="py-name">buckets</tt> <tt class="py-op">==</tt> <tt class="py-number">1</tt><tt class="py-op">:</tt> </tt>
<a name="L921"></a><tt class="py-lineno"> 921</tt>  <tt class="py-line">                <tt class="py-keyword">return</tt> <tt class="py-op">[</tt><tt class="py-name">minv</tt><tt class="py-op">,</tt> <tt class="py-name">maxv</tt><tt class="py-op">]</tt><tt class="py-op">,</tt> <tt class="py-op">[</tt><tt class="py-name">filtered</tt><tt class="py-op">.</tt><tt id="link-145" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.count
pyspark.rdd.RDD.count
pyspark.sql.SchemaRDD.count
pyspark.statcounter.StatCounter.count" class="py-name" href="#" onclick="return doclink('link-145', 'count', 'link-59');">count</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">]</tt> </tt>
<a name="L922"></a><tt class="py-lineno"> 922</tt>  <tt class="py-line"> </tt>
<a name="L923"></a><tt class="py-lineno"> 923</tt>  <tt class="py-line">            <tt class="py-keyword">try</tt><tt class="py-op">:</tt> </tt>
<a name="L924"></a><tt class="py-lineno"> 924</tt>  <tt class="py-line">                <tt class="py-name">inc</tt> <tt class="py-op">=</tt> <tt class="py-op">(</tt><tt class="py-name">maxv</tt> <tt class="py-op">-</tt> <tt class="py-name">minv</tt><tt class="py-op">)</tt> <tt class="py-op">/</tt> <tt class="py-name">buckets</tt> </tt>
<a name="L925"></a><tt class="py-lineno"> 925</tt>  <tt class="py-line">            <tt class="py-keyword">except</tt> <tt class="py-name">TypeError</tt><tt class="py-op">:</tt> </tt>
<a name="L926"></a><tt class="py-lineno"> 926</tt>  <tt class="py-line">                <tt class="py-keyword">raise</tt> <tt class="py-name">TypeError</tt><tt class="py-op">(</tt><tt class="py-string">"Can not generate buckets with non-number in RDD"</tt><tt class="py-op">)</tt> </tt>
<a name="L927"></a><tt class="py-lineno"> 927</tt>  <tt class="py-line"> </tt>
<a name="L928"></a><tt class="py-lineno"> 928</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">isinf</tt><tt class="py-op">(</tt><tt class="py-name">inc</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L929"></a><tt class="py-lineno"> 929</tt>  <tt class="py-line">                <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt><tt class="py-string">"Can not generate buckets with infinite value"</tt><tt class="py-op">)</tt> </tt>
<a name="L930"></a><tt class="py-lineno"> 930</tt>  <tt class="py-line"> </tt>
<a name="L931"></a><tt class="py-lineno"> 931</tt>  <tt class="py-line">            <tt class="py-comment"># keep them as integer if possible</tt> </tt>
<a name="L932"></a><tt class="py-lineno"> 932</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">inc</tt> <tt class="py-op">*</tt> <tt class="py-name">buckets</tt> <tt class="py-op">!=</tt> <tt class="py-name">maxv</tt> <tt class="py-op">-</tt> <tt class="py-name">minv</tt><tt class="py-op">:</tt> </tt>
<a name="L933"></a><tt class="py-lineno"> 933</tt>  <tt class="py-line">                <tt class="py-name">inc</tt> <tt class="py-op">=</tt> <tt class="py-op">(</tt><tt class="py-name">maxv</tt> <tt class="py-op">-</tt> <tt class="py-name">minv</tt><tt class="py-op">)</tt> <tt class="py-op">*</tt> <tt class="py-number">1.0</tt> <tt class="py-op">/</tt> <tt class="py-name">buckets</tt> </tt>
<a name="L934"></a><tt class="py-lineno"> 934</tt>  <tt class="py-line"> </tt>
<a name="L935"></a><tt class="py-lineno"> 935</tt>  <tt class="py-line">            <tt class="py-name">buckets</tt> <tt class="py-op">=</tt> <tt class="py-op">[</tt><tt class="py-name">i</tt> <tt class="py-op">*</tt> <tt class="py-name">inc</tt> <tt class="py-op">+</tt> <tt class="py-name">minv</tt> <tt class="py-keyword">for</tt> <tt class="py-name">i</tt> <tt class="py-keyword">in</tt> <tt class="py-name">range</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">)</tt><tt class="py-op">]</tt> </tt>
<a name="L936"></a><tt class="py-lineno"> 936</tt>  <tt class="py-line">            <tt class="py-name">buckets</tt><tt class="py-op">.</tt><tt class="py-name">append</tt><tt class="py-op">(</tt><tt class="py-name">maxv</tt><tt class="py-op">)</tt>  <tt class="py-comment"># fix accumulated error</tt> </tt>
<a name="L937"></a><tt class="py-lineno"> 937</tt>  <tt class="py-line">            <tt class="py-name">even</tt> <tt class="py-op">=</tt> <tt class="py-name">True</tt> </tt>
<a name="L938"></a><tt class="py-lineno"> 938</tt>  <tt class="py-line"> </tt>
<a name="L939"></a><tt class="py-lineno"> 939</tt>  <tt class="py-line">        <tt class="py-keyword">elif</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">,</tt> <tt class="py-op">(</tt><tt class="py-name">list</tt><tt class="py-op">,</tt> <tt class="py-name">tuple</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L940"></a><tt class="py-lineno"> 940</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">)</tt> <tt class="py-op">&lt;</tt> <tt class="py-number">2</tt><tt class="py-op">:</tt> </tt>
<a name="L941"></a><tt class="py-lineno"> 941</tt>  <tt class="py-line">                <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt><tt class="py-string">"buckets should have more than one value"</tt><tt class="py-op">)</tt> </tt>
<a name="L942"></a><tt class="py-lineno"> 942</tt>  <tt class="py-line"> </tt>
<a name="L943"></a><tt class="py-lineno"> 943</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">any</tt><tt class="py-op">(</tt><tt class="py-name">i</tt> <tt class="py-keyword">is</tt> <tt class="py-name">None</tt> <tt class="py-keyword">or</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">i</tt><tt class="py-op">,</tt> <tt class="py-name">float</tt><tt class="py-op">)</tt> <tt class="py-keyword">and</tt> <tt class="py-name">isnan</tt><tt class="py-op">(</tt><tt class="py-name">i</tt><tt class="py-op">)</tt> <tt class="py-keyword">for</tt> <tt class="py-name">i</tt> <tt class="py-keyword">in</tt> <tt class="py-name">buckets</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L944"></a><tt class="py-lineno"> 944</tt>  <tt class="py-line">                <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt><tt class="py-string">"can not have None or NaN in buckets"</tt><tt class="py-op">)</tt> </tt>
<a name="L945"></a><tt class="py-lineno"> 945</tt>  <tt class="py-line"> </tt>
<a name="L946"></a><tt class="py-lineno"> 946</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">sorted</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">)</tt> <tt class="py-op">!=</tt> <tt class="py-name">list</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L947"></a><tt class="py-lineno"> 947</tt>  <tt class="py-line">                <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt><tt class="py-string">"buckets should be sorted"</tt><tt class="py-op">)</tt> </tt>
<a name="L948"></a><tt class="py-lineno"> 948</tt>  <tt class="py-line"> </tt>
<a name="L949"></a><tt class="py-lineno"> 949</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt id="link-146" class="py-name" targets="Method pyspark.conf.SparkConf.set()=pyspark.conf.SparkConf-class.html#set"><a title="pyspark.conf.SparkConf.set" class="py-name" href="#" onclick="return doclink('link-146', 'set', 'link-146');">set</a></tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> <tt class="py-op">!=</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L950"></a><tt class="py-lineno"> 950</tt>  <tt class="py-line">                <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt><tt class="py-string">"buckets should not contain duplicated values"</tt><tt class="py-op">)</tt> </tt>
<a name="L951"></a><tt class="py-lineno"> 951</tt>  <tt class="py-line"> </tt>
<a name="L952"></a><tt class="py-lineno"> 952</tt>  <tt class="py-line">            <tt class="py-name">minv</tt> <tt class="py-op">=</tt> <tt class="py-name">buckets</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt> </tt>
<a name="L953"></a><tt class="py-lineno"> 953</tt>  <tt class="py-line">            <tt class="py-name">maxv</tt> <tt class="py-op">=</tt> <tt class="py-name">buckets</tt><tt class="py-op">[</tt><tt class="py-op">-</tt><tt class="py-number">1</tt><tt class="py-op">]</tt> </tt>
<a name="L954"></a><tt class="py-lineno"> 954</tt>  <tt class="py-line">            <tt class="py-name">even</tt> <tt class="py-op">=</tt> <tt class="py-name">False</tt> </tt>
<a name="L955"></a><tt class="py-lineno"> 955</tt>  <tt class="py-line">            <tt class="py-name">inc</tt> <tt class="py-op">=</tt> <tt class="py-name">None</tt> </tt>
<a name="L956"></a><tt class="py-lineno"> 956</tt>  <tt class="py-line">            <tt class="py-keyword">try</tt><tt class="py-op">:</tt> </tt>
<a name="L957"></a><tt class="py-lineno"> 957</tt>  <tt class="py-line">                <tt class="py-name">steps</tt> <tt class="py-op">=</tt> <tt class="py-op">[</tt><tt class="py-name">buckets</tt><tt class="py-op">[</tt><tt class="py-name">i</tt> <tt class="py-op">+</tt> <tt class="py-number">1</tt><tt class="py-op">]</tt> <tt class="py-op">-</tt> <tt class="py-name">buckets</tt><tt class="py-op">[</tt><tt class="py-name">i</tt><tt class="py-op">]</tt> <tt class="py-keyword">for</tt> <tt class="py-name">i</tt> <tt class="py-keyword">in</tt> <tt class="py-name">range</tt><tt class="py-op">(</tt><tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">)</tt> <tt class="py-op">-</tt> <tt class="py-number">1</tt><tt class="py-op">)</tt><tt class="py-op">]</tt> </tt>
<a name="L958"></a><tt class="py-lineno"> 958</tt>  <tt class="py-line">            <tt class="py-keyword">except</tt> <tt class="py-name">TypeError</tt><tt class="py-op">:</tt> </tt>
<a name="L959"></a><tt class="py-lineno"> 959</tt>  <tt class="py-line">                <tt class="py-keyword">pass</tt>  <tt class="py-comment"># objects in buckets do not support '-'</tt> </tt>
<a name="L960"></a><tt class="py-lineno"> 960</tt>  <tt class="py-line">            <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L961"></a><tt class="py-lineno"> 961</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt id="link-147" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.max
pyspark.rdd.RDD.max
pyspark.statcounter.StatCounter.max" class="py-name" href="#" onclick="return doclink('link-147', 'max', 'link-89');">max</a></tt><tt class="py-op">(</tt><tt class="py-name">steps</tt><tt class="py-op">)</tt> <tt class="py-op">-</tt> <tt id="link-148" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.min
pyspark.rdd.RDD.min
pyspark.statcounter.StatCounter.min" class="py-name" href="#" onclick="return doclink('link-148', 'min', 'link-68');">min</a></tt><tt class="py-op">(</tt><tt class="py-name">steps</tt><tt class="py-op">)</tt> <tt class="py-op">&lt;</tt> <tt class="py-number">1e-10</tt><tt class="py-op">:</tt>  <tt class="py-comment"># handle precision errors</tt> </tt>
<a name="L962"></a><tt class="py-lineno"> 962</tt>  <tt class="py-line">                    <tt class="py-name">even</tt> <tt class="py-op">=</tt> <tt class="py-name">True</tt> </tt>
<a name="L963"></a><tt class="py-lineno"> 963</tt>  <tt class="py-line">                    <tt class="py-name">inc</tt> <tt class="py-op">=</tt> <tt class="py-op">(</tt><tt class="py-name">maxv</tt> <tt class="py-op">-</tt> <tt class="py-name">minv</tt><tt class="py-op">)</tt> <tt class="py-op">/</tt> <tt class="py-op">(</tt><tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">)</tt> <tt class="py-op">-</tt> <tt class="py-number">1</tt><tt class="py-op">)</tt> </tt>
<a name="L964"></a><tt class="py-lineno"> 964</tt>  <tt class="py-line"> </tt>
<a name="L965"></a><tt class="py-lineno"> 965</tt>  <tt class="py-line">        <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L966"></a><tt class="py-lineno"> 966</tt>  <tt class="py-line">            <tt class="py-keyword">raise</tt> <tt class="py-name">TypeError</tt><tt class="py-op">(</tt><tt class="py-string">"buckets should be a list or tuple or number(int or long)"</tt><tt class="py-op">)</tt> </tt>
<a name="L967"></a><tt class="py-lineno"> 967</tt>  <tt class="py-line"> </tt>
<a name="L968"></a><tt class="py-lineno"> 968</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">histogram</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L969"></a><tt class="py-lineno"> 969</tt>  <tt class="py-line">            <tt class="py-name">counters</tt> <tt class="py-op">=</tt> <tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt> <tt class="py-op">*</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">)</tt> </tt>
<a name="L970"></a><tt class="py-lineno"> 970</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">i</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L971"></a><tt class="py-lineno"> 971</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-name">i</tt> <tt class="py-keyword">is</tt> <tt class="py-name">None</tt> <tt class="py-keyword">or</tt> <tt class="py-op">(</tt><tt class="py-name">type</tt><tt class="py-op">(</tt><tt class="py-name">i</tt><tt class="py-op">)</tt> <tt class="py-keyword">is</tt> <tt class="py-name">float</tt> <tt class="py-keyword">and</tt> <tt class="py-name">isnan</tt><tt class="py-op">(</tt><tt class="py-name">i</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> <tt class="py-keyword">or</tt> <tt class="py-name">i</tt> <tt class="py-op">&gt;</tt> <tt class="py-name">maxv</tt> <tt class="py-keyword">or</tt> <tt class="py-name">i</tt> <tt class="py-op">&lt;</tt> <tt class="py-name">minv</tt><tt class="py-op">:</tt> </tt>
<a name="L972"></a><tt class="py-lineno"> 972</tt>  <tt class="py-line">                    <tt class="py-keyword">continue</tt> </tt>
<a name="L973"></a><tt class="py-lineno"> 973</tt>  <tt class="py-line">                <tt class="py-name">t</tt> <tt class="py-op">=</tt> <tt class="py-op">(</tt><tt class="py-name">int</tt><tt class="py-op">(</tt><tt class="py-op">(</tt><tt class="py-name">i</tt> <tt class="py-op">-</tt> <tt class="py-name">minv</tt><tt class="py-op">)</tt> <tt class="py-op">/</tt> <tt class="py-name">inc</tt><tt class="py-op">)</tt> <tt class="py-keyword">if</tt> <tt class="py-name">even</tt> </tt>
<a name="L974"></a><tt class="py-lineno"> 974</tt>  <tt class="py-line">                     <tt class="py-keyword">else</tt> <tt class="py-name">bisect</tt><tt class="py-op">.</tt><tt class="py-name">bisect_right</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">,</tt> <tt class="py-name">i</tt><tt class="py-op">)</tt> <tt class="py-op">-</tt> <tt class="py-number">1</tt><tt class="py-op">)</tt> </tt>
<a name="L975"></a><tt class="py-lineno"> 975</tt>  <tt class="py-line">                <tt class="py-name">counters</tt><tt class="py-op">[</tt><tt class="py-name">t</tt><tt class="py-op">]</tt> <tt class="py-op">+=</tt> <tt class="py-number">1</tt> </tt>
<a name="L976"></a><tt class="py-lineno"> 976</tt>  <tt class="py-line">            <tt class="py-comment"># add last two together</tt> </tt>
<a name="L977"></a><tt class="py-lineno"> 977</tt>  <tt class="py-line">            <tt class="py-name">last</tt> <tt class="py-op">=</tt> <tt class="py-name">counters</tt><tt class="py-op">.</tt><tt class="py-name">pop</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L978"></a><tt class="py-lineno"> 978</tt>  <tt class="py-line">            <tt class="py-name">counters</tt><tt class="py-op">[</tt><tt class="py-op">-</tt><tt class="py-number">1</tt><tt class="py-op">]</tt> <tt class="py-op">+=</tt> <tt class="py-name">last</tt> </tt>
<a name="L979"></a><tt class="py-lineno"> 979</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-op">[</tt><tt class="py-name">counters</tt><tt class="py-op">]</tt> </tt>
</div><a name="L980"></a><tt class="py-lineno"> 980</tt>  <tt class="py-line"> </tt>
<a name="L981"></a><tt class="py-lineno"> 981</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">mergeCounters</tt><tt class="py-op">(</tt><tt class="py-param">a</tt><tt class="py-op">,</tt> <tt class="py-param">b</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L982"></a><tt class="py-lineno"> 982</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-op">[</tt><tt class="py-name">i</tt> <tt class="py-op">+</tt> <tt class="py-name">j</tt> <tt class="py-keyword">for</tt> <tt class="py-name">i</tt><tt class="py-op">,</tt> <tt class="py-name">j</tt> <tt class="py-keyword">in</tt> <tt id="link-149" class="py-name" targets="Method pyspark.rdd.RDD.zip()=pyspark.rdd.RDD-class.html#zip"><a title="pyspark.rdd.RDD.zip" class="py-name" href="#" onclick="return doclink('link-149', 'zip', 'link-149');">zip</a></tt><tt class="py-op">(</tt><tt class="py-name">a</tt><tt class="py-op">,</tt> <tt class="py-name">b</tt><tt class="py-op">)</tt><tt class="py-op">]</tt> </tt>
</div><a name="L983"></a><tt class="py-lineno"> 983</tt>  <tt class="py-line"> </tt>
<a name="L984"></a><tt class="py-lineno"> 984</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">buckets</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-150" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-150', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt id="link-151" class="py-name" targets="Method pyspark.rdd.RDD.histogram()=pyspark.rdd.RDD-class.html#histogram"><a title="pyspark.rdd.RDD.histogram" class="py-name" href="#" onclick="return doclink('link-151', 'histogram', 'link-151');">histogram</a></tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-152" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-152', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt class="py-name">mergeCounters</tt><tt class="py-op">)</tt> </tt>
</div><a name="L985"></a><tt class="py-lineno"> 985</tt>  <tt class="py-line"> </tt>
<a name="RDD.mean"></a><div id="RDD.mean-def"><a name="L986"></a><tt class="py-lineno"> 986</tt> <a class="py-toggle" href="#" id="RDD.mean-toggle" onclick="return toggle('RDD.mean');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#mean">mean</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.mean-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.mean-expanded"><a name="L987"></a><tt class="py-lineno"> 987</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L988"></a><tt class="py-lineno"> 988</tt>  <tt class="py-line"><tt class="py-docstring">        Compute the mean of this RDD's elements.</tt> </tt>
<a name="L989"></a><tt class="py-lineno"> 989</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L990"></a><tt class="py-lineno"> 990</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3]).mean()</tt> </tt>
<a name="L991"></a><tt class="py-lineno"> 991</tt>  <tt class="py-line"><tt class="py-docstring">        2.0</tt> </tt>
<a name="L992"></a><tt class="py-lineno"> 992</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L993"></a><tt class="py-lineno"> 993</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-153" class="py-name" targets="Method pyspark.rdd.RDD.stats()=pyspark.rdd.RDD-class.html#stats"><a title="pyspark.rdd.RDD.stats" class="py-name" href="#" onclick="return doclink('link-153', 'stats', 'link-153');">stats</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-154" class="py-name" targets="Method pyspark.mllib.stat.MultivariateStatisticalSummary.mean()=pyspark.mllib.stat.MultivariateStatisticalSummary-class.html#mean,Method pyspark.rdd.RDD.mean()=pyspark.rdd.RDD-class.html#mean,Method pyspark.statcounter.StatCounter.mean()=pyspark.statcounter.StatCounter-class.html#mean"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.mean
pyspark.rdd.RDD.mean
pyspark.statcounter.StatCounter.mean" class="py-name" href="#" onclick="return doclink('link-154', 'mean', 'link-154');">mean</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L994"></a><tt class="py-lineno"> 994</tt>  <tt class="py-line"> </tt>
<a name="RDD.variance"></a><div id="RDD.variance-def"><a name="L995"></a><tt class="py-lineno"> 995</tt> <a class="py-toggle" href="#" id="RDD.variance-toggle" onclick="return toggle('RDD.variance');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#variance">variance</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.variance-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.variance-expanded"><a name="L996"></a><tt class="py-lineno"> 996</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L997"></a><tt class="py-lineno"> 997</tt>  <tt class="py-line"><tt class="py-docstring">        Compute the variance of this RDD's elements.</tt> </tt>
<a name="L998"></a><tt class="py-lineno"> 998</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L999"></a><tt class="py-lineno"> 999</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3]).variance()</tt> </tt>
<a name="L1000"></a><tt class="py-lineno">1000</tt>  <tt class="py-line"><tt class="py-docstring">        0.666...</tt> </tt>
<a name="L1001"></a><tt class="py-lineno">1001</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1002"></a><tt class="py-lineno">1002</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-155" class="py-name"><a title="pyspark.rdd.RDD.stats" class="py-name" href="#" onclick="return doclink('link-155', 'stats', 'link-153');">stats</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-156" class="py-name" targets="Method pyspark.mllib.stat.MultivariateStatisticalSummary.variance()=pyspark.mllib.stat.MultivariateStatisticalSummary-class.html#variance,Method pyspark.rdd.RDD.variance()=pyspark.rdd.RDD-class.html#variance,Method pyspark.statcounter.StatCounter.variance()=pyspark.statcounter.StatCounter-class.html#variance"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.variance
pyspark.rdd.RDD.variance
pyspark.statcounter.StatCounter.variance" class="py-name" href="#" onclick="return doclink('link-156', 'variance', 'link-156');">variance</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1003"></a><tt class="py-lineno">1003</tt>  <tt class="py-line"> </tt>
<a name="RDD.stdev"></a><div id="RDD.stdev-def"><a name="L1004"></a><tt class="py-lineno">1004</tt> <a class="py-toggle" href="#" id="RDD.stdev-toggle" onclick="return toggle('RDD.stdev');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#stdev">stdev</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.stdev-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.stdev-expanded"><a name="L1005"></a><tt class="py-lineno">1005</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1006"></a><tt class="py-lineno">1006</tt>  <tt class="py-line"><tt class="py-docstring">        Compute the standard deviation of this RDD's elements.</tt> </tt>
<a name="L1007"></a><tt class="py-lineno">1007</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1008"></a><tt class="py-lineno">1008</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3]).stdev()</tt> </tt>
<a name="L1009"></a><tt class="py-lineno">1009</tt>  <tt class="py-line"><tt class="py-docstring">        0.816...</tt> </tt>
<a name="L1010"></a><tt class="py-lineno">1010</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1011"></a><tt class="py-lineno">1011</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-157" class="py-name"><a title="pyspark.rdd.RDD.stats" class="py-name" href="#" onclick="return doclink('link-157', 'stats', 'link-153');">stats</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-158" class="py-name" targets="Method pyspark.rdd.RDD.stdev()=pyspark.rdd.RDD-class.html#stdev,Method pyspark.statcounter.StatCounter.stdev()=pyspark.statcounter.StatCounter-class.html#stdev"><a title="pyspark.rdd.RDD.stdev
pyspark.statcounter.StatCounter.stdev" class="py-name" href="#" onclick="return doclink('link-158', 'stdev', 'link-158');">stdev</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1012"></a><tt class="py-lineno">1012</tt>  <tt class="py-line"> </tt>
<a name="RDD.sampleStdev"></a><div id="RDD.sampleStdev-def"><a name="L1013"></a><tt class="py-lineno">1013</tt> <a class="py-toggle" href="#" id="RDD.sampleStdev-toggle" onclick="return toggle('RDD.sampleStdev');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#sampleStdev">sampleStdev</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.sampleStdev-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.sampleStdev-expanded"><a name="L1014"></a><tt class="py-lineno">1014</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1015"></a><tt class="py-lineno">1015</tt>  <tt class="py-line"><tt class="py-docstring">        Compute the sample standard deviation of this RDD's elements (which</tt> </tt>
<a name="L1016"></a><tt class="py-lineno">1016</tt>  <tt class="py-line"><tt class="py-docstring">        corrects for bias in estimating the standard deviation by dividing by</tt> </tt>
<a name="L1017"></a><tt class="py-lineno">1017</tt>  <tt class="py-line"><tt class="py-docstring">        N-1 instead of N).</tt> </tt>
<a name="L1018"></a><tt class="py-lineno">1018</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1019"></a><tt class="py-lineno">1019</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3]).sampleStdev()</tt> </tt>
<a name="L1020"></a><tt class="py-lineno">1020</tt>  <tt class="py-line"><tt class="py-docstring">        1.0</tt> </tt>
<a name="L1021"></a><tt class="py-lineno">1021</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1022"></a><tt class="py-lineno">1022</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-159" class="py-name"><a title="pyspark.rdd.RDD.stats" class="py-name" href="#" onclick="return doclink('link-159', 'stats', 'link-153');">stats</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-160" class="py-name" targets="Method pyspark.rdd.RDD.sampleStdev()=pyspark.rdd.RDD-class.html#sampleStdev,Method pyspark.statcounter.StatCounter.sampleStdev()=pyspark.statcounter.StatCounter-class.html#sampleStdev"><a title="pyspark.rdd.RDD.sampleStdev
pyspark.statcounter.StatCounter.sampleStdev" class="py-name" href="#" onclick="return doclink('link-160', 'sampleStdev', 'link-160');">sampleStdev</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1023"></a><tt class="py-lineno">1023</tt>  <tt class="py-line"> </tt>
<a name="RDD.sampleVariance"></a><div id="RDD.sampleVariance-def"><a name="L1024"></a><tt class="py-lineno">1024</tt> <a class="py-toggle" href="#" id="RDD.sampleVariance-toggle" onclick="return toggle('RDD.sampleVariance');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#sampleVariance">sampleVariance</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.sampleVariance-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.sampleVariance-expanded"><a name="L1025"></a><tt class="py-lineno">1025</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1026"></a><tt class="py-lineno">1026</tt>  <tt class="py-line"><tt class="py-docstring">        Compute the sample variance of this RDD's elements (which corrects</tt> </tt>
<a name="L1027"></a><tt class="py-lineno">1027</tt>  <tt class="py-line"><tt class="py-docstring">        for bias in estimating the variance by dividing by N-1 instead of N).</tt> </tt>
<a name="L1028"></a><tt class="py-lineno">1028</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1029"></a><tt class="py-lineno">1029</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3]).sampleVariance()</tt> </tt>
<a name="L1030"></a><tt class="py-lineno">1030</tt>  <tt class="py-line"><tt class="py-docstring">        1.0</tt> </tt>
<a name="L1031"></a><tt class="py-lineno">1031</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1032"></a><tt class="py-lineno">1032</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-161" class="py-name"><a title="pyspark.rdd.RDD.stats" class="py-name" href="#" onclick="return doclink('link-161', 'stats', 'link-153');">stats</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-162" class="py-name" targets="Method pyspark.rdd.RDD.sampleVariance()=pyspark.rdd.RDD-class.html#sampleVariance,Method pyspark.statcounter.StatCounter.sampleVariance()=pyspark.statcounter.StatCounter-class.html#sampleVariance"><a title="pyspark.rdd.RDD.sampleVariance
pyspark.statcounter.StatCounter.sampleVariance" class="py-name" href="#" onclick="return doclink('link-162', 'sampleVariance', 'link-162');">sampleVariance</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1033"></a><tt class="py-lineno">1033</tt>  <tt class="py-line"> </tt>
<a name="RDD.countByValue"></a><div id="RDD.countByValue-def"><a name="L1034"></a><tt class="py-lineno">1034</tt> <a class="py-toggle" href="#" id="RDD.countByValue-toggle" onclick="return toggle('RDD.countByValue');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#countByValue">countByValue</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.countByValue-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.countByValue-expanded"><a name="L1035"></a><tt class="py-lineno">1035</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1036"></a><tt class="py-lineno">1036</tt>  <tt class="py-line"><tt class="py-docstring">        Return the count of each unique value in this RDD as a dictionary of</tt> </tt>
<a name="L1037"></a><tt class="py-lineno">1037</tt>  <tt class="py-line"><tt class="py-docstring">        (value, count) pairs.</tt> </tt>
<a name="L1038"></a><tt class="py-lineno">1038</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1039"></a><tt class="py-lineno">1039</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())</tt> </tt>
<a name="L1040"></a><tt class="py-lineno">1040</tt>  <tt class="py-line"><tt class="py-docstring">        [(1, 2), (2, 3)]</tt> </tt>
<a name="L1041"></a><tt class="py-lineno">1041</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1042"></a><tt class="py-lineno">1042</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">countPartition</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1043"></a><tt class="py-lineno">1043</tt>  <tt class="py-line">            <tt class="py-name">counts</tt> <tt class="py-op">=</tt> <tt class="py-name">defaultdict</tt><tt class="py-op">(</tt><tt class="py-name">int</tt><tt class="py-op">)</tt> </tt>
<a name="L1044"></a><tt class="py-lineno">1044</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">obj</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L1045"></a><tt class="py-lineno">1045</tt>  <tt class="py-line">                <tt class="py-name">counts</tt><tt class="py-op">[</tt><tt class="py-name">obj</tt><tt class="py-op">]</tt> <tt class="py-op">+=</tt> <tt class="py-number">1</tt> </tt>
<a name="L1046"></a><tt class="py-lineno">1046</tt>  <tt class="py-line">            <tt class="py-keyword">yield</tt> <tt class="py-name">counts</tt> </tt>
</div><a name="L1047"></a><tt class="py-lineno">1047</tt>  <tt class="py-line"> </tt>
<a name="L1048"></a><tt class="py-lineno">1048</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">mergeMaps</tt><tt class="py-op">(</tt><tt class="py-param">m1</tt><tt class="py-op">,</tt> <tt class="py-param">m2</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1049"></a><tt class="py-lineno">1049</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt> <tt class="py-keyword">in</tt> <tt class="py-name">m2</tt><tt class="py-op">.</tt><tt class="py-name">iteritems</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1050"></a><tt class="py-lineno">1050</tt>  <tt class="py-line">                <tt class="py-name">m1</tt><tt class="py-op">[</tt><tt class="py-name">k</tt><tt class="py-op">]</tt> <tt class="py-op">+=</tt> <tt class="py-name">v</tt> </tt>
<a name="L1051"></a><tt class="py-lineno">1051</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">m1</tt> </tt>
</div><a name="L1052"></a><tt class="py-lineno">1052</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-163" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-163', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">countPartition</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-164" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-164', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt class="py-name">mergeMaps</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1053"></a><tt class="py-lineno">1053</tt>  <tt class="py-line"> </tt>
<a name="RDD.top"></a><div id="RDD.top-def"><a name="L1054"></a><tt class="py-lineno">1054</tt> <a class="py-toggle" href="#" id="RDD.top-toggle" onclick="return toggle('RDD.top');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#top">top</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">num</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.top-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.top-expanded"><a name="L1055"></a><tt class="py-lineno">1055</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1056"></a><tt class="py-lineno">1056</tt>  <tt class="py-line"><tt class="py-docstring">        Get the top N elements from a RDD.</tt> </tt>
<a name="L1057"></a><tt class="py-lineno">1057</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1058"></a><tt class="py-lineno">1058</tt>  <tt class="py-line"><tt class="py-docstring">        Note: It returns the list sorted in descending order.</tt> </tt>
<a name="L1059"></a><tt class="py-lineno">1059</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([10, 4, 2, 12, 3]).top(1)</tt> </tt>
<a name="L1060"></a><tt class="py-lineno">1060</tt>  <tt class="py-line"><tt class="py-docstring">        [12]</tt> </tt>
<a name="L1061"></a><tt class="py-lineno">1061</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6], 2).top(2)</tt> </tt>
<a name="L1062"></a><tt class="py-lineno">1062</tt>  <tt class="py-line"><tt class="py-docstring">        [6, 5]</tt> </tt>
<a name="L1063"></a><tt class="py-lineno">1063</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1064"></a><tt class="py-lineno">1064</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">topIterator</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1065"></a><tt class="py-lineno">1065</tt>  <tt class="py-line">            <tt class="py-name">q</tt> <tt class="py-op">=</tt> <tt class="py-op">[</tt><tt class="py-op">]</tt> </tt>
<a name="L1066"></a><tt class="py-lineno">1066</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">k</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L1067"></a><tt class="py-lineno">1067</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">q</tt><tt class="py-op">)</tt> <tt class="py-op">&lt;</tt> <tt class="py-name">num</tt><tt class="py-op">:</tt> </tt>
<a name="L1068"></a><tt class="py-lineno">1068</tt>  <tt class="py-line">                    <tt class="py-name">heapq</tt><tt class="py-op">.</tt><tt class="py-name">heappush</tt><tt class="py-op">(</tt><tt class="py-name">q</tt><tt class="py-op">,</tt> <tt class="py-name">k</tt><tt class="py-op">)</tt> </tt>
<a name="L1069"></a><tt class="py-lineno">1069</tt>  <tt class="py-line">                <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L1070"></a><tt class="py-lineno">1070</tt>  <tt class="py-line">                    <tt class="py-name">heapq</tt><tt class="py-op">.</tt><tt class="py-name">heappushpop</tt><tt class="py-op">(</tt><tt class="py-name">q</tt><tt class="py-op">,</tt> <tt class="py-name">k</tt><tt class="py-op">)</tt> </tt>
<a name="L1071"></a><tt class="py-lineno">1071</tt>  <tt class="py-line">            <tt class="py-keyword">yield</tt> <tt class="py-name">q</tt> </tt>
</div><a name="L1072"></a><tt class="py-lineno">1072</tt>  <tt class="py-line"> </tt>
<a name="L1073"></a><tt class="py-lineno">1073</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">merge</tt><tt class="py-op">(</tt><tt class="py-param">a</tt><tt class="py-op">,</tt> <tt class="py-param">b</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1074"></a><tt class="py-lineno">1074</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">next</tt><tt class="py-op">(</tt><tt class="py-name">topIterator</tt><tt class="py-op">(</tt><tt class="py-name">a</tt> <tt class="py-op">+</tt> <tt class="py-name">b</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1075"></a><tt class="py-lineno">1075</tt>  <tt class="py-line"> </tt>
<a name="L1076"></a><tt class="py-lineno">1076</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">sorted</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-165" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-165', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">topIterator</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-166" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-166', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt id="link-167" class="py-name" targets="Method pyspark.statcounter.StatCounter.merge()=pyspark.statcounter.StatCounter-class.html#merge"><a title="pyspark.statcounter.StatCounter.merge" class="py-name" href="#" onclick="return doclink('link-167', 'merge', 'link-167');">merge</a></tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">reverse</tt><tt class="py-op">=</tt><tt class="py-name">True</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1077"></a><tt class="py-lineno">1077</tt>  <tt class="py-line"> </tt>
<a name="RDD.takeOrdered"></a><div id="RDD.takeOrdered-def"><a name="L1078"></a><tt class="py-lineno">1078</tt> <a class="py-toggle" href="#" id="RDD.takeOrdered-toggle" onclick="return toggle('RDD.takeOrdered');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#takeOrdered">takeOrdered</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">num</tt><tt class="py-op">,</tt> <tt class="py-param">key</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.takeOrdered-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.takeOrdered-expanded"><a name="L1079"></a><tt class="py-lineno">1079</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1080"></a><tt class="py-lineno">1080</tt>  <tt class="py-line"><tt class="py-docstring">        Get the N elements from a RDD ordered in ascending order or as</tt> </tt>
<a name="L1081"></a><tt class="py-lineno">1081</tt>  <tt class="py-line"><tt class="py-docstring">        specified by the optional key function.</tt> </tt>
<a name="L1082"></a><tt class="py-lineno">1082</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1083"></a><tt class="py-lineno">1083</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)</tt> </tt>
<a name="L1084"></a><tt class="py-lineno">1084</tt>  <tt class="py-line"><tt class="py-docstring">        [1, 2, 3, 4, 5, 6]</tt> </tt>
<a name="L1085"></a><tt class="py-lineno">1085</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)</tt> </tt>
<a name="L1086"></a><tt class="py-lineno">1086</tt>  <tt class="py-line"><tt class="py-docstring">        [10, 9, 7, 6, 5, 4]</tt> </tt>
<a name="L1087"></a><tt class="py-lineno">1087</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1088"></a><tt class="py-lineno">1088</tt>  <tt class="py-line"> </tt>
<a name="L1089"></a><tt class="py-lineno">1089</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">topNKeyedElems</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">,</tt> <tt class="py-param">key_</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1090"></a><tt class="py-lineno">1090</tt>  <tt class="py-line">            <tt class="py-name">q</tt> <tt class="py-op">=</tt> <tt class="py-name">MaxHeapQ</tt><tt class="py-op">(</tt><tt class="py-name">num</tt><tt class="py-op">)</tt> </tt>
<a name="L1091"></a><tt class="py-lineno">1091</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">k</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L1092"></a><tt class="py-lineno">1092</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-name">key_</tt> <tt class="py-keyword">is</tt> <tt class="py-keyword">not</tt> <tt class="py-name">None</tt><tt class="py-op">:</tt> </tt>
<a name="L1093"></a><tt class="py-lineno">1093</tt>  <tt class="py-line">                    <tt class="py-name">k</tt> <tt class="py-op">=</tt> <tt class="py-op">(</tt><tt class="py-name">key_</tt><tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">k</tt><tt class="py-op">)</tt> </tt>
<a name="L1094"></a><tt class="py-lineno">1094</tt>  <tt class="py-line">                <tt class="py-name">q</tt><tt class="py-op">.</tt><tt class="py-name">insert</tt><tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">)</tt> </tt>
<a name="L1095"></a><tt class="py-lineno">1095</tt>  <tt class="py-line">            <tt class="py-keyword">yield</tt> <tt class="py-name">q</tt><tt class="py-op">.</tt><tt class="py-name">getElements</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1096"></a><tt class="py-lineno">1096</tt>  <tt class="py-line"> </tt>
<a name="L1097"></a><tt class="py-lineno">1097</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">unKey</tt><tt class="py-op">(</tt><tt class="py-param">x</tt><tt class="py-op">,</tt> <tt class="py-param">key_</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1098"></a><tt class="py-lineno">1098</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">key_</tt> <tt class="py-keyword">is</tt> <tt class="py-keyword">not</tt> <tt class="py-name">None</tt><tt class="py-op">:</tt> </tt>
<a name="L1099"></a><tt class="py-lineno">1099</tt>  <tt class="py-line">                <tt class="py-name">x</tt> <tt class="py-op">=</tt> <tt class="py-op">[</tt><tt class="py-name">i</tt><tt class="py-op">[</tt><tt class="py-number">1</tt><tt class="py-op">]</tt> <tt class="py-keyword">for</tt> <tt class="py-name">i</tt> <tt class="py-keyword">in</tt> <tt class="py-name">x</tt><tt class="py-op">]</tt> </tt>
<a name="L1100"></a><tt class="py-lineno">1100</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">x</tt> </tt>
</div><a name="L1101"></a><tt class="py-lineno">1101</tt>  <tt class="py-line"> </tt>
<a name="L1102"></a><tt class="py-lineno">1102</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">merge</tt><tt class="py-op">(</tt><tt class="py-param">a</tt><tt class="py-op">,</tt> <tt class="py-param">b</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1103"></a><tt class="py-lineno">1103</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">next</tt><tt class="py-op">(</tt><tt class="py-name">topNKeyedElems</tt><tt class="py-op">(</tt><tt class="py-name">a</tt> <tt class="py-op">+</tt> <tt class="py-name">b</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1104"></a><tt class="py-lineno">1104</tt>  <tt class="py-line">        <tt class="py-name">result</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-168" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-168', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt> </tt>
<a name="L1105"></a><tt class="py-lineno">1105</tt>  <tt class="py-line">            <tt class="py-keyword">lambda</tt> <tt class="py-name">i</tt><tt class="py-op">:</tt> <tt class="py-name">topNKeyedElems</tt><tt class="py-op">(</tt><tt class="py-name">i</tt><tt class="py-op">,</tt> <tt class="py-name">key</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-169" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-169', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt id="link-170" class="py-name"><a title="pyspark.statcounter.StatCounter.merge" class="py-name" href="#" onclick="return doclink('link-170', 'merge', 'link-167');">merge</a></tt><tt class="py-op">)</tt> </tt>
<a name="L1106"></a><tt class="py-lineno">1106</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">sorted</tt><tt class="py-op">(</tt><tt class="py-name">unKey</tt><tt class="py-op">(</tt><tt class="py-name">result</tt><tt class="py-op">,</tt> <tt class="py-name">key</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">key</tt><tt class="py-op">=</tt><tt class="py-name">key</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1107"></a><tt class="py-lineno">1107</tt>  <tt class="py-line"> </tt>
<a name="RDD.take"></a><div id="RDD.take-def"><a name="L1108"></a><tt class="py-lineno">1108</tt> <a class="py-toggle" href="#" id="RDD.take-toggle" onclick="return toggle('RDD.take');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#take">take</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">num</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.take-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.take-expanded"><a name="L1109"></a><tt class="py-lineno">1109</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1110"></a><tt class="py-lineno">1110</tt>  <tt class="py-line"><tt class="py-docstring">        Take the first num elements of the RDD.</tt> </tt>
<a name="L1111"></a><tt class="py-lineno">1111</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1112"></a><tt class="py-lineno">1112</tt>  <tt class="py-line"><tt class="py-docstring">        It works by first scanning one partition, and use the results from</tt> </tt>
<a name="L1113"></a><tt class="py-lineno">1113</tt>  <tt class="py-line"><tt class="py-docstring">        that partition to estimate the number of additional partitions needed</tt> </tt>
<a name="L1114"></a><tt class="py-lineno">1114</tt>  <tt class="py-line"><tt class="py-docstring">        to satisfy the limit.</tt> </tt>
<a name="L1115"></a><tt class="py-lineno">1115</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1116"></a><tt class="py-lineno">1116</tt>  <tt class="py-line"><tt class="py-docstring">        Translated from the Scala implementation in RDD#take().</tt> </tt>
<a name="L1117"></a><tt class="py-lineno">1117</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1118"></a><tt class="py-lineno">1118</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)</tt> </tt>
<a name="L1119"></a><tt class="py-lineno">1119</tt>  <tt class="py-line"><tt class="py-docstring">        [2, 3]</tt> </tt>
<a name="L1120"></a><tt class="py-lineno">1120</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6]).take(10)</tt> </tt>
<a name="L1121"></a><tt class="py-lineno">1121</tt>  <tt class="py-line"><tt class="py-docstring">        [2, 3, 4, 5, 6]</tt> </tt>
<a name="L1122"></a><tt class="py-lineno">1122</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(range(100), 100).filter(lambda x: x &gt; 90).take(3)</tt> </tt>
<a name="L1123"></a><tt class="py-lineno">1123</tt>  <tt class="py-line"><tt class="py-docstring">        [91, 92, 93]</tt> </tt>
<a name="L1124"></a><tt class="py-lineno">1124</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1125"></a><tt class="py-lineno">1125</tt>  <tt class="py-line">        <tt class="py-name">items</tt> <tt class="py-op">=</tt> <tt class="py-op">[</tt><tt class="py-op">]</tt> </tt>
<a name="L1126"></a><tt class="py-lineno">1126</tt>  <tt class="py-line">        <tt class="py-name">totalParts</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt class="py-name">partitions</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">size</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1127"></a><tt class="py-lineno">1127</tt>  <tt class="py-line">        <tt class="py-name">partsScanned</tt> <tt class="py-op">=</tt> <tt class="py-number">0</tt> </tt>
<a name="L1128"></a><tt class="py-lineno">1128</tt>  <tt class="py-line"> </tt>
<a name="L1129"></a><tt class="py-lineno">1129</tt>  <tt class="py-line">        <tt class="py-keyword">while</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">items</tt><tt class="py-op">)</tt> <tt class="py-op">&lt;</tt> <tt class="py-name">num</tt> <tt class="py-keyword">and</tt> <tt class="py-name">partsScanned</tt> <tt class="py-op">&lt;</tt> <tt class="py-name">totalParts</tt><tt class="py-op">:</tt> </tt>
<a name="L1130"></a><tt class="py-lineno">1130</tt>  <tt class="py-line">            <tt class="py-comment"># The number of partitions to try in this iteration.</tt> </tt>
<a name="L1131"></a><tt class="py-lineno">1131</tt>  <tt class="py-line">            <tt class="py-comment"># It is ok for this number to be greater than totalParts because</tt> </tt>
<a name="L1132"></a><tt class="py-lineno">1132</tt>  <tt class="py-line">            <tt class="py-comment"># we actually cap it at totalParts in runJob.</tt> </tt>
<a name="L1133"></a><tt class="py-lineno">1133</tt>  <tt class="py-line">            <tt class="py-name">numPartsToTry</tt> <tt class="py-op">=</tt> <tt class="py-number">1</tt> </tt>
<a name="L1134"></a><tt class="py-lineno">1134</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">partsScanned</tt> <tt class="py-op">&gt;</tt> <tt class="py-number">0</tt><tt class="py-op">:</tt> </tt>
<a name="L1135"></a><tt class="py-lineno">1135</tt>  <tt class="py-line">                <tt class="py-comment"># If we didn't find any rows after the previous iteration,</tt> </tt>
<a name="L1136"></a><tt class="py-lineno">1136</tt>  <tt class="py-line">                <tt class="py-comment"># quadruple and retry.  Otherwise, interpolate the number of</tt> </tt>
<a name="L1137"></a><tt class="py-lineno">1137</tt>  <tt class="py-line">                <tt class="py-comment"># partitions we need to try, but overestimate it by 50%.</tt> </tt>
<a name="L1138"></a><tt class="py-lineno">1138</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">items</tt><tt class="py-op">)</tt> <tt class="py-op">==</tt> <tt class="py-number">0</tt><tt class="py-op">:</tt> </tt>
<a name="L1139"></a><tt class="py-lineno">1139</tt>  <tt class="py-line">                    <tt class="py-name">numPartsToTry</tt> <tt class="py-op">=</tt> <tt class="py-name">partsScanned</tt> <tt class="py-op">*</tt> <tt class="py-number">4</tt> </tt>
<a name="L1140"></a><tt class="py-lineno">1140</tt>  <tt class="py-line">                <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L1141"></a><tt class="py-lineno">1141</tt>  <tt class="py-line">                    <tt class="py-name">numPartsToTry</tt> <tt class="py-op">=</tt> <tt class="py-name">int</tt><tt class="py-op">(</tt><tt class="py-number">1.5</tt> <tt class="py-op">*</tt> <tt class="py-name">num</tt> <tt class="py-op">*</tt> <tt class="py-name">partsScanned</tt> <tt class="py-op">/</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">items</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L1142"></a><tt class="py-lineno">1142</tt>  <tt class="py-line"> </tt>
<a name="L1143"></a><tt class="py-lineno">1143</tt>  <tt class="py-line">            <tt class="py-name">left</tt> <tt class="py-op">=</tt> <tt class="py-name">num</tt> <tt class="py-op">-</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">items</tt><tt class="py-op">)</tt> </tt>
<a name="L1144"></a><tt class="py-lineno">1144</tt>  <tt class="py-line"> </tt>
<a name="L1145"></a><tt class="py-lineno">1145</tt>  <tt class="py-line">            <tt class="py-keyword">def</tt> <tt class="py-def-name">takeUpToNumLeft</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1146"></a><tt class="py-lineno">1146</tt>  <tt class="py-line">                <tt class="py-name">taken</tt> <tt class="py-op">=</tt> <tt class="py-number">0</tt> </tt>
<a name="L1147"></a><tt class="py-lineno">1147</tt>  <tt class="py-line">                <tt class="py-keyword">while</tt> <tt class="py-name">taken</tt> <tt class="py-op">&lt;</tt> <tt class="py-name">left</tt><tt class="py-op">:</tt> </tt>
<a name="L1148"></a><tt class="py-lineno">1148</tt>  <tt class="py-line">                    <tt class="py-keyword">yield</tt> <tt class="py-name">next</tt><tt class="py-op">(</tt><tt class="py-name">iterator</tt><tt class="py-op">)</tt> </tt>
<a name="L1149"></a><tt class="py-lineno">1149</tt>  <tt class="py-line">                    <tt class="py-name">taken</tt> <tt class="py-op">+=</tt> <tt class="py-number">1</tt> </tt>
</div><a name="L1150"></a><tt class="py-lineno">1150</tt>  <tt class="py-line"> </tt>
<a name="L1151"></a><tt class="py-lineno">1151</tt>  <tt class="py-line">            <tt class="py-name">p</tt> <tt class="py-op">=</tt> <tt class="py-name">range</tt><tt class="py-op">(</tt> </tt>
<a name="L1152"></a><tt class="py-lineno">1152</tt>  <tt class="py-line">                <tt class="py-name">partsScanned</tt><tt class="py-op">,</tt> <tt id="link-171" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.min
pyspark.rdd.RDD.min
pyspark.statcounter.StatCounter.min" class="py-name" href="#" onclick="return doclink('link-171', 'min', 'link-68');">min</a></tt><tt class="py-op">(</tt><tt class="py-name">partsScanned</tt> <tt class="py-op">+</tt> <tt class="py-name">numPartsToTry</tt><tt class="py-op">,</tt> <tt class="py-name">totalParts</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L1153"></a><tt class="py-lineno">1153</tt>  <tt class="py-line">            <tt class="py-name">res</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-172" class="py-name"><a title="pyspark.context
pyspark.rdd.RDD.context" class="py-name" href="#" onclick="return doclink('link-172', 'context', 'link-111');">context</a></tt><tt class="py-op">.</tt><tt id="link-173" class="py-name" targets="Method pyspark.context.SparkContext.runJob()=pyspark.context.SparkContext-class.html#runJob"><a title="pyspark.context.SparkContext.runJob" class="py-name" href="#" onclick="return doclink('link-173', 'runJob', 'link-173');">runJob</a></tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">,</tt> <tt class="py-name">takeUpToNumLeft</tt><tt class="py-op">,</tt> <tt class="py-name">p</tt><tt class="py-op">,</tt> <tt class="py-name">True</tt><tt class="py-op">)</tt> </tt>
<a name="L1154"></a><tt class="py-lineno">1154</tt>  <tt class="py-line"> </tt>
<a name="L1155"></a><tt class="py-lineno">1155</tt>  <tt class="py-line">            <tt class="py-name">items</tt> <tt class="py-op">+=</tt> <tt class="py-name">res</tt> </tt>
<a name="L1156"></a><tt class="py-lineno">1156</tt>  <tt class="py-line">            <tt class="py-name">partsScanned</tt> <tt class="py-op">+=</tt> <tt class="py-name">numPartsToTry</tt> </tt>
<a name="L1157"></a><tt class="py-lineno">1157</tt>  <tt class="py-line"> </tt>
<a name="L1158"></a><tt class="py-lineno">1158</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">items</tt><tt class="py-op">[</tt><tt class="py-op">:</tt><tt class="py-name">num</tt><tt class="py-op">]</tt> </tt>
</div><a name="L1159"></a><tt class="py-lineno">1159</tt>  <tt class="py-line"> </tt>
<a name="RDD.first"></a><div id="RDD.first-def"><a name="L1160"></a><tt class="py-lineno">1160</tt> <a class="py-toggle" href="#" id="RDD.first-toggle" onclick="return toggle('RDD.first');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#first">first</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.first-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.first-expanded"><a name="L1161"></a><tt class="py-lineno">1161</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1162"></a><tt class="py-lineno">1162</tt>  <tt class="py-line"><tt class="py-docstring">        Return the first element in this RDD.</tt> </tt>
<a name="L1163"></a><tt class="py-lineno">1163</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1164"></a><tt class="py-lineno">1164</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([2, 3, 4]).first()</tt> </tt>
<a name="L1165"></a><tt class="py-lineno">1165</tt>  <tt class="py-line"><tt class="py-docstring">        2</tt> </tt>
<a name="L1166"></a><tt class="py-lineno">1166</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1167"></a><tt class="py-lineno">1167</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-174" class="py-name" targets="Method pyspark.rdd.RDD.take()=pyspark.rdd.RDD-class.html#take"><a title="pyspark.rdd.RDD.take" class="py-name" href="#" onclick="return doclink('link-174', 'take', 'link-174');">take</a></tt><tt class="py-op">(</tt><tt class="py-number">1</tt><tt class="py-op">)</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt> </tt>
</div><a name="L1168"></a><tt class="py-lineno">1168</tt>  <tt class="py-line"> </tt>
<a name="RDD.saveAsNewAPIHadoopDataset"></a><div id="RDD.saveAsNewAPIHadoopDataset-def"><a name="L1169"></a><tt class="py-lineno">1169</tt> <a class="py-toggle" href="#" id="RDD.saveAsNewAPIHadoopDataset-toggle" onclick="return toggle('RDD.saveAsNewAPIHadoopDataset');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#saveAsNewAPIHadoopDataset">saveAsNewAPIHadoopDataset</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">conf</tt><tt class="py-op">,</tt> <tt class="py-param">keyConverter</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> <tt class="py-param">valueConverter</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.saveAsNewAPIHadoopDataset-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.saveAsNewAPIHadoopDataset-expanded"><a name="L1170"></a><tt class="py-lineno">1170</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1171"></a><tt class="py-lineno">1171</tt>  <tt class="py-line"><tt class="py-docstring">        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file</tt> </tt>
<a name="L1172"></a><tt class="py-lineno">1172</tt>  <tt class="py-line"><tt class="py-docstring">        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are</tt> </tt>
<a name="L1173"></a><tt class="py-lineno">1173</tt>  <tt class="py-line"><tt class="py-docstring">        converted for output using either user specified converters or, by default,</tt> </tt>
<a name="L1174"></a><tt class="py-lineno">1174</tt>  <tt class="py-line"><tt class="py-docstring">        L{org.apache.spark.api.python.JavaToWritableConverter}.</tt> </tt>
<a name="L1175"></a><tt class="py-lineno">1175</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1176"></a><tt class="py-lineno">1176</tt>  <tt class="py-line"><tt class="py-docstring">        @param conf: Hadoop job configuration, passed in as a dict</tt> </tt>
<a name="L1177"></a><tt class="py-lineno">1177</tt>  <tt class="py-line"><tt class="py-docstring">        @param keyConverter: (None by default)</tt> </tt>
<a name="L1178"></a><tt class="py-lineno">1178</tt>  <tt class="py-line"><tt class="py-docstring">        @param valueConverter: (None by default)</tt> </tt>
<a name="L1179"></a><tt class="py-lineno">1179</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1180"></a><tt class="py-lineno">1180</tt>  <tt class="py-line">        <tt class="py-name">jconf</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_dictToJavaMap</tt><tt class="py-op">(</tt><tt id="link-175" class="py-name" targets="Module pyspark.conf=pyspark.conf-module.html"><a title="pyspark.conf" class="py-name" href="#" onclick="return doclink('link-175', 'conf', 'link-175');">conf</a></tt><tt class="py-op">)</tt> </tt>
<a name="L1181"></a><tt class="py-lineno">1181</tt>  <tt class="py-line">        <tt class="py-name">pickledRDD</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_toPickleSerialization</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1182"></a><tt class="py-lineno">1182</tt>  <tt class="py-line">        <tt class="py-name">batched</tt> <tt class="py-op">=</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">pickledRDD</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">,</tt> <tt class="py-name">BatchedSerializer</tt><tt class="py-op">)</tt> </tt>
<a name="L1183"></a><tt class="py-lineno">1183</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-176" class="py-name" targets="Variable pyspark.context.SparkContext._jvm=pyspark.context.SparkContext-class.html#_jvm"><a title="pyspark.context.SparkContext._jvm" class="py-name" href="#" onclick="return doclink('link-176', '_jvm', 'link-176');">_jvm</a></tt><tt class="py-op">.</tt><tt class="py-name">PythonRDD</tt><tt class="py-op">.</tt><tt id="link-177" class="py-name" targets="Method pyspark.rdd.RDD.saveAsHadoopDataset()=pyspark.rdd.RDD-class.html#saveAsHadoopDataset"><a title="pyspark.rdd.RDD.saveAsHadoopDataset" class="py-name" href="#" onclick="return doclink('link-177', 'saveAsHadoopDataset', 'link-177');">saveAsHadoopDataset</a></tt><tt class="py-op">(</tt><tt class="py-name">pickledRDD</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">,</tt> <tt class="py-name">batched</tt><tt class="py-op">,</tt> <tt class="py-name">jconf</tt><tt class="py-op">,</tt> </tt>
<a name="L1184"></a><tt class="py-lineno">1184</tt>  <tt class="py-line">                                                    <tt class="py-name">keyConverter</tt><tt class="py-op">,</tt> <tt class="py-name">valueConverter</tt><tt class="py-op">,</tt> <tt class="py-name">True</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1185"></a><tt class="py-lineno">1185</tt>  <tt class="py-line"> </tt>
<a name="RDD.saveAsNewAPIHadoopFile"></a><div id="RDD.saveAsNewAPIHadoopFile-def"><a name="L1186"></a><tt class="py-lineno">1186</tt> <a class="py-toggle" href="#" id="RDD.saveAsNewAPIHadoopFile-toggle" onclick="return toggle('RDD.saveAsNewAPIHadoopFile');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#saveAsNewAPIHadoopFile">saveAsNewAPIHadoopFile</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">path</tt><tt class="py-op">,</tt> <tt class="py-param">outputFormatClass</tt><tt class="py-op">,</tt> <tt class="py-param">keyClass</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> <tt class="py-param">valueClass</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> </tt>
<a name="L1187"></a><tt class="py-lineno">1187</tt>  <tt class="py-line">                               <tt class="py-param">keyConverter</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> <tt class="py-param">valueConverter</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> <tt class="py-param">conf</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.saveAsNewAPIHadoopFile-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.saveAsNewAPIHadoopFile-expanded"><a name="L1188"></a><tt class="py-lineno">1188</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1189"></a><tt class="py-lineno">1189</tt>  <tt class="py-line"><tt class="py-docstring">        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file</tt> </tt>
<a name="L1190"></a><tt class="py-lineno">1190</tt>  <tt class="py-line"><tt class="py-docstring">        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types</tt> </tt>
<a name="L1191"></a><tt class="py-lineno">1191</tt>  <tt class="py-line"><tt class="py-docstring">        will be inferred if not specified. Keys and values are converted for output using either</tt> </tt>
<a name="L1192"></a><tt class="py-lineno">1192</tt>  <tt class="py-line"><tt class="py-docstring">        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The</tt> </tt>
<a name="L1193"></a><tt class="py-lineno">1193</tt>  <tt class="py-line"><tt class="py-docstring">        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext</tt> </tt>
<a name="L1194"></a><tt class="py-lineno">1194</tt>  <tt class="py-line"><tt class="py-docstring">        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.</tt> </tt>
<a name="L1195"></a><tt class="py-lineno">1195</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1196"></a><tt class="py-lineno">1196</tt>  <tt class="py-line"><tt class="py-docstring">        @param path: path to Hadoop file</tt> </tt>
<a name="L1197"></a><tt class="py-lineno">1197</tt>  <tt class="py-line"><tt class="py-docstring">        @param outputFormatClass: fully qualified classname of Hadoop OutputFormat</tt> </tt>
<a name="L1198"></a><tt class="py-lineno">1198</tt>  <tt class="py-line"><tt class="py-docstring">               (e.g. "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat")</tt> </tt>
<a name="L1199"></a><tt class="py-lineno">1199</tt>  <tt class="py-line"><tt class="py-docstring">        @param keyClass: fully qualified classname of key Writable class</tt> </tt>
<a name="L1200"></a><tt class="py-lineno">1200</tt>  <tt class="py-line"><tt class="py-docstring">               (e.g. "org.apache.hadoop.io.IntWritable", None by default)</tt> </tt>
<a name="L1201"></a><tt class="py-lineno">1201</tt>  <tt class="py-line"><tt class="py-docstring">        @param valueClass: fully qualified classname of value Writable class</tt> </tt>
<a name="L1202"></a><tt class="py-lineno">1202</tt>  <tt class="py-line"><tt class="py-docstring">               (e.g. "org.apache.hadoop.io.Text", None by default)</tt> </tt>
<a name="L1203"></a><tt class="py-lineno">1203</tt>  <tt class="py-line"><tt class="py-docstring">        @param keyConverter: (None by default)</tt> </tt>
<a name="L1204"></a><tt class="py-lineno">1204</tt>  <tt class="py-line"><tt class="py-docstring">        @param valueConverter: (None by default)</tt> </tt>
<a name="L1205"></a><tt class="py-lineno">1205</tt>  <tt class="py-line"><tt class="py-docstring">        @param conf: Hadoop job configuration, passed in as a dict (None by default)</tt> </tt>
<a name="L1206"></a><tt class="py-lineno">1206</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1207"></a><tt class="py-lineno">1207</tt>  <tt class="py-line">        <tt class="py-name">jconf</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_dictToJavaMap</tt><tt class="py-op">(</tt><tt id="link-178" class="py-name"><a title="pyspark.conf" class="py-name" href="#" onclick="return doclink('link-178', 'conf', 'link-175');">conf</a></tt><tt class="py-op">)</tt> </tt>
<a name="L1208"></a><tt class="py-lineno">1208</tt>  <tt class="py-line">        <tt class="py-name">pickledRDD</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_toPickleSerialization</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1209"></a><tt class="py-lineno">1209</tt>  <tt class="py-line">        <tt class="py-name">batched</tt> <tt class="py-op">=</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">pickledRDD</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">,</tt> <tt class="py-name">BatchedSerializer</tt><tt class="py-op">)</tt> </tt>
<a name="L1210"></a><tt class="py-lineno">1210</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-179" class="py-name"><a title="pyspark.context.SparkContext._jvm" class="py-name" href="#" onclick="return doclink('link-179', '_jvm', 'link-176');">_jvm</a></tt><tt class="py-op">.</tt><tt class="py-name">PythonRDD</tt><tt class="py-op">.</tt><tt id="link-180" class="py-name" targets="Method pyspark.rdd.RDD.saveAsNewAPIHadoopFile()=pyspark.rdd.RDD-class.html#saveAsNewAPIHadoopFile"><a title="pyspark.rdd.RDD.saveAsNewAPIHadoopFile" class="py-name" href="#" onclick="return doclink('link-180', 'saveAsNewAPIHadoopFile', 'link-180');">saveAsNewAPIHadoopFile</a></tt><tt class="py-op">(</tt><tt class="py-name">pickledRDD</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">,</tt> <tt class="py-name">batched</tt><tt class="py-op">,</tt> <tt class="py-name">path</tt><tt class="py-op">,</tt> </tt>
<a name="L1211"></a><tt class="py-lineno">1211</tt>  <tt class="py-line">                                                       <tt class="py-name">outputFormatClass</tt><tt class="py-op">,</tt> </tt>
<a name="L1212"></a><tt class="py-lineno">1212</tt>  <tt class="py-line">                                                       <tt class="py-name">keyClass</tt><tt class="py-op">,</tt> <tt class="py-name">valueClass</tt><tt class="py-op">,</tt> </tt>
<a name="L1213"></a><tt class="py-lineno">1213</tt>  <tt class="py-line">                                                       <tt class="py-name">keyConverter</tt><tt class="py-op">,</tt> <tt class="py-name">valueConverter</tt><tt class="py-op">,</tt> <tt class="py-name">jconf</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1214"></a><tt class="py-lineno">1214</tt>  <tt class="py-line"> </tt>
<a name="RDD.saveAsHadoopDataset"></a><div id="RDD.saveAsHadoopDataset-def"><a name="L1215"></a><tt class="py-lineno">1215</tt> <a class="py-toggle" href="#" id="RDD.saveAsHadoopDataset-toggle" onclick="return toggle('RDD.saveAsHadoopDataset');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#saveAsHadoopDataset">saveAsHadoopDataset</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">conf</tt><tt class="py-op">,</tt> <tt class="py-param">keyConverter</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> <tt class="py-param">valueConverter</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.saveAsHadoopDataset-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.saveAsHadoopDataset-expanded"><a name="L1216"></a><tt class="py-lineno">1216</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1217"></a><tt class="py-lineno">1217</tt>  <tt class="py-line"><tt class="py-docstring">        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file</tt> </tt>
<a name="L1218"></a><tt class="py-lineno">1218</tt>  <tt class="py-line"><tt class="py-docstring">        system, using the old Hadoop OutputFormat API (mapred package). Keys/values are</tt> </tt>
<a name="L1219"></a><tt class="py-lineno">1219</tt>  <tt class="py-line"><tt class="py-docstring">        converted for output using either user specified converters or, by default,</tt> </tt>
<a name="L1220"></a><tt class="py-lineno">1220</tt>  <tt class="py-line"><tt class="py-docstring">        L{org.apache.spark.api.python.JavaToWritableConverter}.</tt> </tt>
<a name="L1221"></a><tt class="py-lineno">1221</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1222"></a><tt class="py-lineno">1222</tt>  <tt class="py-line"><tt class="py-docstring">        @param conf: Hadoop job configuration, passed in as a dict</tt> </tt>
<a name="L1223"></a><tt class="py-lineno">1223</tt>  <tt class="py-line"><tt class="py-docstring">        @param keyConverter: (None by default)</tt> </tt>
<a name="L1224"></a><tt class="py-lineno">1224</tt>  <tt class="py-line"><tt class="py-docstring">        @param valueConverter: (None by default)</tt> </tt>
<a name="L1225"></a><tt class="py-lineno">1225</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1226"></a><tt class="py-lineno">1226</tt>  <tt class="py-line">        <tt class="py-name">jconf</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_dictToJavaMap</tt><tt class="py-op">(</tt><tt id="link-181" class="py-name"><a title="pyspark.conf" class="py-name" href="#" onclick="return doclink('link-181', 'conf', 'link-175');">conf</a></tt><tt class="py-op">)</tt> </tt>
<a name="L1227"></a><tt class="py-lineno">1227</tt>  <tt class="py-line">        <tt class="py-name">pickledRDD</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_toPickleSerialization</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1228"></a><tt class="py-lineno">1228</tt>  <tt class="py-line">        <tt class="py-name">batched</tt> <tt class="py-op">=</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">pickledRDD</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">,</tt> <tt class="py-name">BatchedSerializer</tt><tt class="py-op">)</tt> </tt>
<a name="L1229"></a><tt class="py-lineno">1229</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-182" class="py-name"><a title="pyspark.context.SparkContext._jvm" class="py-name" href="#" onclick="return doclink('link-182', '_jvm', 'link-176');">_jvm</a></tt><tt class="py-op">.</tt><tt class="py-name">PythonRDD</tt><tt class="py-op">.</tt><tt id="link-183" class="py-name"><a title="pyspark.rdd.RDD.saveAsHadoopDataset" class="py-name" href="#" onclick="return doclink('link-183', 'saveAsHadoopDataset', 'link-177');">saveAsHadoopDataset</a></tt><tt class="py-op">(</tt><tt class="py-name">pickledRDD</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">,</tt> <tt class="py-name">batched</tt><tt class="py-op">,</tt> <tt class="py-name">jconf</tt><tt class="py-op">,</tt> </tt>
<a name="L1230"></a><tt class="py-lineno">1230</tt>  <tt class="py-line">                                                    <tt class="py-name">keyConverter</tt><tt class="py-op">,</tt> <tt class="py-name">valueConverter</tt><tt class="py-op">,</tt> <tt class="py-name">False</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1231"></a><tt class="py-lineno">1231</tt>  <tt class="py-line"> </tt>
<a name="RDD.saveAsHadoopFile"></a><div id="RDD.saveAsHadoopFile-def"><a name="L1232"></a><tt class="py-lineno">1232</tt> <a class="py-toggle" href="#" id="RDD.saveAsHadoopFile-toggle" onclick="return toggle('RDD.saveAsHadoopFile');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#saveAsHadoopFile">saveAsHadoopFile</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">path</tt><tt class="py-op">,</tt> <tt class="py-param">outputFormatClass</tt><tt class="py-op">,</tt> <tt class="py-param">keyClass</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> <tt class="py-param">valueClass</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> </tt>
<a name="L1233"></a><tt class="py-lineno">1233</tt>  <tt class="py-line">                         <tt class="py-param">keyConverter</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> <tt class="py-param">valueConverter</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> <tt class="py-param">conf</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">,</tt> </tt>
<a name="L1234"></a><tt class="py-lineno">1234</tt>  <tt class="py-line">                         <tt class="py-param">compressionCodecClass</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.saveAsHadoopFile-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.saveAsHadoopFile-expanded"><a name="L1235"></a><tt class="py-lineno">1235</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1236"></a><tt class="py-lineno">1236</tt>  <tt class="py-line"><tt class="py-docstring">        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file</tt> </tt>
<a name="L1237"></a><tt class="py-lineno">1237</tt>  <tt class="py-line"><tt class="py-docstring">        system, using the old Hadoop OutputFormat API (mapred package). Key and value types</tt> </tt>
<a name="L1238"></a><tt class="py-lineno">1238</tt>  <tt class="py-line"><tt class="py-docstring">        will be inferred if not specified. Keys and values are converted for output using either</tt> </tt>
<a name="L1239"></a><tt class="py-lineno">1239</tt>  <tt class="py-line"><tt class="py-docstring">        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The</tt> </tt>
<a name="L1240"></a><tt class="py-lineno">1240</tt>  <tt class="py-line"><tt class="py-docstring">        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext</tt> </tt>
<a name="L1241"></a><tt class="py-lineno">1241</tt>  <tt class="py-line"><tt class="py-docstring">        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.</tt> </tt>
<a name="L1242"></a><tt class="py-lineno">1242</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1243"></a><tt class="py-lineno">1243</tt>  <tt class="py-line"><tt class="py-docstring">        @param path: path to Hadoop file</tt> </tt>
<a name="L1244"></a><tt class="py-lineno">1244</tt>  <tt class="py-line"><tt class="py-docstring">        @param outputFormatClass: fully qualified classname of Hadoop OutputFormat</tt> </tt>
<a name="L1245"></a><tt class="py-lineno">1245</tt>  <tt class="py-line"><tt class="py-docstring">               (e.g. "org.apache.hadoop.mapred.SequenceFileOutputFormat")</tt> </tt>
<a name="L1246"></a><tt class="py-lineno">1246</tt>  <tt class="py-line"><tt class="py-docstring">        @param keyClass: fully qualified classname of key Writable class</tt> </tt>
<a name="L1247"></a><tt class="py-lineno">1247</tt>  <tt class="py-line"><tt class="py-docstring">               (e.g. "org.apache.hadoop.io.IntWritable", None by default)</tt> </tt>
<a name="L1248"></a><tt class="py-lineno">1248</tt>  <tt class="py-line"><tt class="py-docstring">        @param valueClass: fully qualified classname of value Writable class</tt> </tt>
<a name="L1249"></a><tt class="py-lineno">1249</tt>  <tt class="py-line"><tt class="py-docstring">               (e.g. "org.apache.hadoop.io.Text", None by default)</tt> </tt>
<a name="L1250"></a><tt class="py-lineno">1250</tt>  <tt class="py-line"><tt class="py-docstring">        @param keyConverter: (None by default)</tt> </tt>
<a name="L1251"></a><tt class="py-lineno">1251</tt>  <tt class="py-line"><tt class="py-docstring">        @param valueConverter: (None by default)</tt> </tt>
<a name="L1252"></a><tt class="py-lineno">1252</tt>  <tt class="py-line"><tt class="py-docstring">        @param conf: (None by default)</tt> </tt>
<a name="L1253"></a><tt class="py-lineno">1253</tt>  <tt class="py-line"><tt class="py-docstring">        @param compressionCodecClass: (None by default)</tt> </tt>
<a name="L1254"></a><tt class="py-lineno">1254</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1255"></a><tt class="py-lineno">1255</tt>  <tt class="py-line">        <tt class="py-name">jconf</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_dictToJavaMap</tt><tt class="py-op">(</tt><tt id="link-184" class="py-name"><a title="pyspark.conf" class="py-name" href="#" onclick="return doclink('link-184', 'conf', 'link-175');">conf</a></tt><tt class="py-op">)</tt> </tt>
<a name="L1256"></a><tt class="py-lineno">1256</tt>  <tt class="py-line">        <tt class="py-name">pickledRDD</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_toPickleSerialization</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1257"></a><tt class="py-lineno">1257</tt>  <tt class="py-line">        <tt class="py-name">batched</tt> <tt class="py-op">=</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">pickledRDD</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">,</tt> <tt class="py-name">BatchedSerializer</tt><tt class="py-op">)</tt> </tt>
<a name="L1258"></a><tt class="py-lineno">1258</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-185" class="py-name"><a title="pyspark.context.SparkContext._jvm" class="py-name" href="#" onclick="return doclink('link-185', '_jvm', 'link-176');">_jvm</a></tt><tt class="py-op">.</tt><tt class="py-name">PythonRDD</tt><tt class="py-op">.</tt><tt id="link-186" class="py-name" targets="Method pyspark.rdd.RDD.saveAsHadoopFile()=pyspark.rdd.RDD-class.html#saveAsHadoopFile"><a title="pyspark.rdd.RDD.saveAsHadoopFile" class="py-name" href="#" onclick="return doclink('link-186', 'saveAsHadoopFile', 'link-186');">saveAsHadoopFile</a></tt><tt class="py-op">(</tt><tt class="py-name">pickledRDD</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">,</tt> <tt class="py-name">batched</tt><tt class="py-op">,</tt> <tt class="py-name">path</tt><tt class="py-op">,</tt> </tt>
<a name="L1259"></a><tt class="py-lineno">1259</tt>  <tt class="py-line">                                                 <tt class="py-name">outputFormatClass</tt><tt class="py-op">,</tt> </tt>
<a name="L1260"></a><tt class="py-lineno">1260</tt>  <tt class="py-line">                                                 <tt class="py-name">keyClass</tt><tt class="py-op">,</tt> <tt class="py-name">valueClass</tt><tt class="py-op">,</tt> </tt>
<a name="L1261"></a><tt class="py-lineno">1261</tt>  <tt class="py-line">                                                 <tt class="py-name">keyConverter</tt><tt class="py-op">,</tt> <tt class="py-name">valueConverter</tt><tt class="py-op">,</tt> </tt>
<a name="L1262"></a><tt class="py-lineno">1262</tt>  <tt class="py-line">                                                 <tt class="py-name">jconf</tt><tt class="py-op">,</tt> <tt class="py-name">compressionCodecClass</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1263"></a><tt class="py-lineno">1263</tt>  <tt class="py-line"> </tt>
<a name="RDD.saveAsSequenceFile"></a><div id="RDD.saveAsSequenceFile-def"><a name="L1264"></a><tt class="py-lineno">1264</tt> <a class="py-toggle" href="#" id="RDD.saveAsSequenceFile-toggle" onclick="return toggle('RDD.saveAsSequenceFile');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#saveAsSequenceFile">saveAsSequenceFile</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">path</tt><tt class="py-op">,</tt> <tt class="py-param">compressionCodecClass</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.saveAsSequenceFile-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.saveAsSequenceFile-expanded"><a name="L1265"></a><tt class="py-lineno">1265</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1266"></a><tt class="py-lineno">1266</tt>  <tt class="py-line"><tt class="py-docstring">        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file</tt> </tt>
<a name="L1267"></a><tt class="py-lineno">1267</tt>  <tt class="py-line"><tt class="py-docstring">        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the</tt> </tt>
<a name="L1268"></a><tt class="py-lineno">1268</tt>  <tt class="py-line"><tt class="py-docstring">        RDD's key and value types. The mechanism is as follows:</tt> </tt>
<a name="L1269"></a><tt class="py-lineno">1269</tt>  <tt class="py-line"><tt class="py-docstring">            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.</tt> </tt>
<a name="L1270"></a><tt class="py-lineno">1270</tt>  <tt class="py-line"><tt class="py-docstring">            2. Keys and values of this Java RDD are converted to Writables and written out.</tt> </tt>
<a name="L1271"></a><tt class="py-lineno">1271</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1272"></a><tt class="py-lineno">1272</tt>  <tt class="py-line"><tt class="py-docstring">        @param path: path to sequence file</tt> </tt>
<a name="L1273"></a><tt class="py-lineno">1273</tt>  <tt class="py-line"><tt class="py-docstring">        @param compressionCodecClass: (None by default)</tt> </tt>
<a name="L1274"></a><tt class="py-lineno">1274</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1275"></a><tt class="py-lineno">1275</tt>  <tt class="py-line">        <tt class="py-name">pickledRDD</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_toPickleSerialization</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1276"></a><tt class="py-lineno">1276</tt>  <tt class="py-line">        <tt class="py-name">batched</tt> <tt class="py-op">=</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">pickledRDD</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">,</tt> <tt class="py-name">BatchedSerializer</tt><tt class="py-op">)</tt> </tt>
<a name="L1277"></a><tt class="py-lineno">1277</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-187" class="py-name"><a title="pyspark.context.SparkContext._jvm" class="py-name" href="#" onclick="return doclink('link-187', '_jvm', 'link-176');">_jvm</a></tt><tt class="py-op">.</tt><tt class="py-name">PythonRDD</tt><tt class="py-op">.</tt><tt id="link-188" class="py-name" targets="Method pyspark.rdd.RDD.saveAsSequenceFile()=pyspark.rdd.RDD-class.html#saveAsSequenceFile"><a title="pyspark.rdd.RDD.saveAsSequenceFile" class="py-name" href="#" onclick="return doclink('link-188', 'saveAsSequenceFile', 'link-188');">saveAsSequenceFile</a></tt><tt class="py-op">(</tt><tt class="py-name">pickledRDD</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">,</tt> <tt class="py-name">batched</tt><tt class="py-op">,</tt> </tt>
<a name="L1278"></a><tt class="py-lineno">1278</tt>  <tt class="py-line">                                                   <tt class="py-name">path</tt><tt class="py-op">,</tt> <tt class="py-name">compressionCodecClass</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1279"></a><tt class="py-lineno">1279</tt>  <tt class="py-line"> </tt>
<a name="RDD.saveAsPickleFile"></a><div id="RDD.saveAsPickleFile-def"><a name="L1280"></a><tt class="py-lineno">1280</tt> <a class="py-toggle" href="#" id="RDD.saveAsPickleFile-toggle" onclick="return toggle('RDD.saveAsPickleFile');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#saveAsPickleFile">saveAsPickleFile</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">path</tt><tt class="py-op">,</tt> <tt class="py-param">batchSize</tt><tt class="py-op">=</tt><tt class="py-number">10</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.saveAsPickleFile-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.saveAsPickleFile-expanded"><a name="L1281"></a><tt class="py-lineno">1281</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1282"></a><tt class="py-lineno">1282</tt>  <tt class="py-line"><tt class="py-docstring">        Save this RDD as a SequenceFile of serialized objects. The serializer</tt> </tt>
<a name="L1283"></a><tt class="py-lineno">1283</tt>  <tt class="py-line"><tt class="py-docstring">        used is L{pyspark.serializers.PickleSerializer}, default batch size</tt> </tt>
<a name="L1284"></a><tt class="py-lineno">1284</tt>  <tt class="py-line"><tt class="py-docstring">        is 10.</tt> </tt>
<a name="L1285"></a><tt class="py-lineno">1285</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1286"></a><tt class="py-lineno">1286</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; tmpFile = NamedTemporaryFile(delete=True)</tt> </tt>
<a name="L1287"></a><tt class="py-lineno">1287</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; tmpFile.close()</tt> </tt>
<a name="L1288"></a><tt class="py-lineno">1288</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)</tt> </tt>
<a name="L1289"></a><tt class="py-lineno">1289</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(sc.pickleFile(tmpFile.name, 5).collect())</tt> </tt>
<a name="L1290"></a><tt class="py-lineno">1290</tt>  <tt class="py-line"><tt class="py-docstring">        [1, 2, 'rdd', 'spark']</tt> </tt>
<a name="L1291"></a><tt class="py-lineno">1291</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1292"></a><tt class="py-lineno">1292</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_reserialize</tt><tt class="py-op">(</tt><tt class="py-name">BatchedSerializer</tt><tt class="py-op">(</tt><tt id="link-189" class="py-name"><a title="pyspark.serializers.PickleSerializer" class="py-name" href="#" onclick="return doclink('link-189', 'PickleSerializer', 'link-5');">PickleSerializer</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> </tt>
<a name="L1293"></a><tt class="py-lineno">1293</tt>  <tt class="py-line">                                            <tt class="py-name">batchSize</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt class="py-name">saveAsObjectFile</tt><tt class="py-op">(</tt><tt class="py-name">path</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1294"></a><tt class="py-lineno">1294</tt>  <tt class="py-line"> </tt>
<a name="RDD.saveAsTextFile"></a><div id="RDD.saveAsTextFile-def"><a name="L1295"></a><tt class="py-lineno">1295</tt> <a class="py-toggle" href="#" id="RDD.saveAsTextFile-toggle" onclick="return toggle('RDD.saveAsTextFile');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#saveAsTextFile">saveAsTextFile</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">path</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.saveAsTextFile-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.saveAsTextFile-expanded"><a name="L1296"></a><tt class="py-lineno">1296</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1297"></a><tt class="py-lineno">1297</tt>  <tt class="py-line"><tt class="py-docstring">        Save this RDD as a text file, using string representations of elements.</tt> </tt>
<a name="L1298"></a><tt class="py-lineno">1298</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1299"></a><tt class="py-lineno">1299</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; tempFile = NamedTemporaryFile(delete=True)</tt> </tt>
<a name="L1300"></a><tt class="py-lineno">1300</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; tempFile.close()</tt> </tt>
<a name="L1301"></a><tt class="py-lineno">1301</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(range(10)).saveAsTextFile(tempFile.name)</tt> </tt>
<a name="L1302"></a><tt class="py-lineno">1302</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; from fileinput import input</tt> </tt>
<a name="L1303"></a><tt class="py-lineno">1303</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; from glob import glob</tt> </tt>
<a name="L1304"></a><tt class="py-lineno">1304</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; ''.join(sorted(input(glob(tempFile.name + "/part-0000*"))))</tt> </tt>
<a name="L1305"></a><tt class="py-lineno">1305</tt>  <tt class="py-line"><tt class="py-docstring">        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'</tt> </tt>
<a name="L1306"></a><tt class="py-lineno">1306</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1307"></a><tt class="py-lineno">1307</tt>  <tt class="py-line"><tt class="py-docstring">        Empty lines are tolerated when saving to text files.</tt> </tt>
<a name="L1308"></a><tt class="py-lineno">1308</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1309"></a><tt class="py-lineno">1309</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; tempFile2 = NamedTemporaryFile(delete=True)</tt> </tt>
<a name="L1310"></a><tt class="py-lineno">1310</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; tempFile2.close()</tt> </tt>
<a name="L1311"></a><tt class="py-lineno">1311</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)</tt> </tt>
<a name="L1312"></a><tt class="py-lineno">1312</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; ''.join(sorted(input(glob(tempFile2.name + "/part-0000*"))))</tt> </tt>
<a name="L1313"></a><tt class="py-lineno">1313</tt>  <tt class="py-line"><tt class="py-docstring">        '\\n\\n\\nbar\\nfoo\\n'</tt> </tt>
<a name="L1314"></a><tt class="py-lineno">1314</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1315"></a><tt class="py-lineno">1315</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">split</tt><tt class="py-op">,</tt> <tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1316"></a><tt class="py-lineno">1316</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">x</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L1317"></a><tt class="py-lineno">1317</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-keyword">not</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">basestring</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1318"></a><tt class="py-lineno">1318</tt>  <tt class="py-line">                    <tt class="py-name">x</tt> <tt class="py-op">=</tt> <tt class="py-name">unicode</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt> </tt>
<a name="L1319"></a><tt class="py-lineno">1319</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">unicode</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1320"></a><tt class="py-lineno">1320</tt>  <tt class="py-line">                    <tt class="py-name">x</tt> <tt class="py-op">=</tt> <tt class="py-name">x</tt><tt class="py-op">.</tt><tt class="py-name">encode</tt><tt class="py-op">(</tt><tt class="py-string">"utf-8"</tt><tt class="py-op">)</tt> </tt>
<a name="L1321"></a><tt class="py-lineno">1321</tt>  <tt class="py-line">                <tt class="py-keyword">yield</tt> <tt class="py-name">x</tt> </tt>
</div><a name="L1322"></a><tt class="py-lineno">1322</tt>  <tt class="py-line">        <tt class="py-name">keyed</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-190" class="py-name"><a title="pyspark.rdd.RDD.mapPartitionsWithIndex
pyspark.sql.SchemaRDD.mapPartitionsWithIndex" class="py-name" href="#" onclick="return doclink('link-190', 'mapPartitionsWithIndex', 'link-50');">mapPartitionsWithIndex</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">)</tt> </tt>
<a name="L1323"></a><tt class="py-lineno">1323</tt>  <tt class="py-line">        <tt class="py-name">keyed</tt><tt class="py-op">.</tt><tt class="py-name">_bypass_serializer</tt> <tt class="py-op">=</tt> <tt class="py-name">True</tt> </tt>
<a name="L1324"></a><tt class="py-lineno">1324</tt>  <tt class="py-line">        <tt class="py-name">keyed</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-191" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-191', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-192" class="py-name"><a title="pyspark.context.SparkContext._jvm" class="py-name" href="#" onclick="return doclink('link-192', '_jvm', 'link-176');">_jvm</a></tt><tt class="py-op">.</tt><tt class="py-name">BytesToString</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-193" class="py-name" targets="Method pyspark.rdd.RDD.saveAsTextFile()=pyspark.rdd.RDD-class.html#saveAsTextFile"><a title="pyspark.rdd.RDD.saveAsTextFile" class="py-name" href="#" onclick="return doclink('link-193', 'saveAsTextFile', 'link-193');">saveAsTextFile</a></tt><tt class="py-op">(</tt><tt class="py-name">path</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1325"></a><tt class="py-lineno">1325</tt>  <tt class="py-line"> </tt>
<a name="L1326"></a><tt class="py-lineno">1326</tt>  <tt class="py-line">    <tt class="py-comment"># Pair functions</tt> </tt>
<a name="L1327"></a><tt class="py-lineno">1327</tt>  <tt class="py-line"> </tt>
<a name="RDD.collectAsMap"></a><div id="RDD.collectAsMap-def"><a name="L1328"></a><tt class="py-lineno">1328</tt> <a class="py-toggle" href="#" id="RDD.collectAsMap-toggle" onclick="return toggle('RDD.collectAsMap');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#collectAsMap">collectAsMap</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.collectAsMap-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.collectAsMap-expanded"><a name="L1329"></a><tt class="py-lineno">1329</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1330"></a><tt class="py-lineno">1330</tt>  <tt class="py-line"><tt class="py-docstring">        Return the key-value pairs in this RDD to the master as a dictionary.</tt> </tt>
<a name="L1331"></a><tt class="py-lineno">1331</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1332"></a><tt class="py-lineno">1332</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()</tt> </tt>
<a name="L1333"></a><tt class="py-lineno">1333</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; m[1]</tt> </tt>
<a name="L1334"></a><tt class="py-lineno">1334</tt>  <tt class="py-line"><tt class="py-docstring">        2</tt> </tt>
<a name="L1335"></a><tt class="py-lineno">1335</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; m[3]</tt> </tt>
<a name="L1336"></a><tt class="py-lineno">1336</tt>  <tt class="py-line"><tt class="py-docstring">        4</tt> </tt>
<a name="L1337"></a><tt class="py-lineno">1337</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1338"></a><tt class="py-lineno">1338</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">dict</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-194" class="py-name"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-194', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1339"></a><tt class="py-lineno">1339</tt>  <tt class="py-line"> </tt>
<a name="RDD.keys"></a><div id="RDD.keys-def"><a name="L1340"></a><tt class="py-lineno">1340</tt> <a class="py-toggle" href="#" id="RDD.keys-toggle" onclick="return toggle('RDD.keys');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#keys">keys</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.keys-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.keys-expanded"><a name="L1341"></a><tt class="py-lineno">1341</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1342"></a><tt class="py-lineno">1342</tt>  <tt class="py-line"><tt class="py-docstring">        Return an RDD with the keys of each tuple.</tt> </tt>
<a name="L1343"></a><tt class="py-lineno">1343</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1344"></a><tt class="py-lineno">1344</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).keys()</tt> </tt>
<a name="L1345"></a><tt class="py-lineno">1345</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; m.collect()</tt> </tt>
<a name="L1346"></a><tt class="py-lineno">1346</tt>  <tt class="py-line"><tt class="py-docstring">        [1, 3]</tt> </tt>
<a name="L1347"></a><tt class="py-lineno">1347</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1348"></a><tt class="py-lineno">1348</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-195" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-195', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> <tt class="py-name">k</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1349"></a><tt class="py-lineno">1349</tt>  <tt class="py-line"> </tt>
<a name="RDD.values"></a><div id="RDD.values-def"><a name="L1350"></a><tt class="py-lineno">1350</tt> <a class="py-toggle" href="#" id="RDD.values-toggle" onclick="return toggle('RDD.values');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#values">values</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.values-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.values-expanded"><a name="L1351"></a><tt class="py-lineno">1351</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1352"></a><tt class="py-lineno">1352</tt>  <tt class="py-line"><tt class="py-docstring">        Return an RDD with the values of each tuple.</tt> </tt>
<a name="L1353"></a><tt class="py-lineno">1353</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1354"></a><tt class="py-lineno">1354</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).values()</tt> </tt>
<a name="L1355"></a><tt class="py-lineno">1355</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; m.collect()</tt> </tt>
<a name="L1356"></a><tt class="py-lineno">1356</tt>  <tt class="py-line"><tt class="py-docstring">        [2, 4]</tt> </tt>
<a name="L1357"></a><tt class="py-lineno">1357</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1358"></a><tt class="py-lineno">1358</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-196" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-196', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1359"></a><tt class="py-lineno">1359</tt>  <tt class="py-line"> </tt>
<a name="RDD.reduceByKey"></a><div id="RDD.reduceByKey-def"><a name="L1360"></a><tt class="py-lineno">1360</tt> <a class="py-toggle" href="#" id="RDD.reduceByKey-toggle" onclick="return toggle('RDD.reduceByKey');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#reduceByKey">reduceByKey</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">func</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.reduceByKey-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.reduceByKey-expanded"><a name="L1361"></a><tt class="py-lineno">1361</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1362"></a><tt class="py-lineno">1362</tt>  <tt class="py-line"><tt class="py-docstring">        Merge the values for each key using an associative reduce function.</tt> </tt>
<a name="L1363"></a><tt class="py-lineno">1363</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1364"></a><tt class="py-lineno">1364</tt>  <tt class="py-line"><tt class="py-docstring">        This will also perform the merging locally on each mapper before</tt> </tt>
<a name="L1365"></a><tt class="py-lineno">1365</tt>  <tt class="py-line"><tt class="py-docstring">        sending results to a reducer, similarly to a "combiner" in MapReduce.</tt> </tt>
<a name="L1366"></a><tt class="py-lineno">1366</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1367"></a><tt class="py-lineno">1367</tt>  <tt class="py-line"><tt class="py-docstring">        Output will be hash-partitioned with C{numPartitions} partitions, or</tt> </tt>
<a name="L1368"></a><tt class="py-lineno">1368</tt>  <tt class="py-line"><tt class="py-docstring">        the default parallelism level if C{numPartitions} is not specified.</tt> </tt>
<a name="L1369"></a><tt class="py-lineno">1369</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1370"></a><tt class="py-lineno">1370</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; from operator import add</tt> </tt>
<a name="L1371"></a><tt class="py-lineno">1371</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])</tt> </tt>
<a name="L1372"></a><tt class="py-lineno">1372</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(rdd.reduceByKey(add).collect())</tt> </tt>
<a name="L1373"></a><tt class="py-lineno">1373</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 2), ('b', 1)]</tt> </tt>
<a name="L1374"></a><tt class="py-lineno">1374</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1375"></a><tt class="py-lineno">1375</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-197" class="py-name" targets="Method pyspark.rdd.RDD.combineByKey()=pyspark.rdd.RDD-class.html#combineByKey"><a title="pyspark.rdd.RDD.combineByKey" class="py-name" href="#" onclick="return doclink('link-197', 'combineByKey', 'link-197');">combineByKey</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">func</tt><tt class="py-op">,</tt> <tt class="py-name">func</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1376"></a><tt class="py-lineno">1376</tt>  <tt class="py-line"> </tt>
<a name="RDD.reduceByKeyLocally"></a><div id="RDD.reduceByKeyLocally-def"><a name="L1377"></a><tt class="py-lineno">1377</tt> <a class="py-toggle" href="#" id="RDD.reduceByKeyLocally-toggle" onclick="return toggle('RDD.reduceByKeyLocally');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#reduceByKeyLocally">reduceByKeyLocally</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">func</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.reduceByKeyLocally-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.reduceByKeyLocally-expanded"><a name="L1378"></a><tt class="py-lineno">1378</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1379"></a><tt class="py-lineno">1379</tt>  <tt class="py-line"><tt class="py-docstring">        Merge the values for each key using an associative reduce function, but</tt> </tt>
<a name="L1380"></a><tt class="py-lineno">1380</tt>  <tt class="py-line"><tt class="py-docstring">        return the results immediately to the master as a dictionary.</tt> </tt>
<a name="L1381"></a><tt class="py-lineno">1381</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1382"></a><tt class="py-lineno">1382</tt>  <tt class="py-line"><tt class="py-docstring">        This will also perform the merging locally on each mapper before</tt> </tt>
<a name="L1383"></a><tt class="py-lineno">1383</tt>  <tt class="py-line"><tt class="py-docstring">        sending results to a reducer, similarly to a "combiner" in MapReduce.</tt> </tt>
<a name="L1384"></a><tt class="py-lineno">1384</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1385"></a><tt class="py-lineno">1385</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; from operator import add</tt> </tt>
<a name="L1386"></a><tt class="py-lineno">1386</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])</tt> </tt>
<a name="L1387"></a><tt class="py-lineno">1387</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(rdd.reduceByKeyLocally(add).items())</tt> </tt>
<a name="L1388"></a><tt class="py-lineno">1388</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 2), ('b', 1)]</tt> </tt>
<a name="L1389"></a><tt class="py-lineno">1389</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1390"></a><tt class="py-lineno">1390</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">reducePartition</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1391"></a><tt class="py-lineno">1391</tt>  <tt class="py-line">            <tt class="py-name">m</tt> <tt class="py-op">=</tt> <tt class="py-op">{</tt><tt class="py-op">}</tt> </tt>
<a name="L1392"></a><tt class="py-lineno">1392</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L1393"></a><tt class="py-lineno">1393</tt>  <tt class="py-line">                <tt class="py-name">m</tt><tt class="py-op">[</tt><tt class="py-name">k</tt><tt class="py-op">]</tt> <tt class="py-op">=</tt> <tt class="py-name">v</tt> <tt class="py-keyword">if</tt> <tt class="py-name">k</tt> <tt class="py-keyword">not</tt> <tt class="py-keyword">in</tt> <tt class="py-name">m</tt> <tt class="py-keyword">else</tt> <tt class="py-name">func</tt><tt class="py-op">(</tt><tt class="py-name">m</tt><tt class="py-op">[</tt><tt class="py-name">k</tt><tt class="py-op">]</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt> </tt>
<a name="L1394"></a><tt class="py-lineno">1394</tt>  <tt class="py-line">            <tt class="py-keyword">yield</tt> <tt class="py-name">m</tt> </tt>
</div><a name="L1395"></a><tt class="py-lineno">1395</tt>  <tt class="py-line"> </tt>
<a name="L1396"></a><tt class="py-lineno">1396</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">mergeMaps</tt><tt class="py-op">(</tt><tt class="py-param">m1</tt><tt class="py-op">,</tt> <tt class="py-param">m2</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1397"></a><tt class="py-lineno">1397</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt> <tt class="py-keyword">in</tt> <tt class="py-name">m2</tt><tt class="py-op">.</tt><tt class="py-name">iteritems</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1398"></a><tt class="py-lineno">1398</tt>  <tt class="py-line">                <tt class="py-name">m1</tt><tt class="py-op">[</tt><tt class="py-name">k</tt><tt class="py-op">]</tt> <tt class="py-op">=</tt> <tt class="py-name">v</tt> <tt class="py-keyword">if</tt> <tt class="py-name">k</tt> <tt class="py-keyword">not</tt> <tt class="py-keyword">in</tt> <tt class="py-name">m1</tt> <tt class="py-keyword">else</tt> <tt class="py-name">func</tt><tt class="py-op">(</tt><tt class="py-name">m1</tt><tt class="py-op">[</tt><tt class="py-name">k</tt><tt class="py-op">]</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt> </tt>
<a name="L1399"></a><tt class="py-lineno">1399</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">m1</tt> </tt>
</div><a name="L1400"></a><tt class="py-lineno">1400</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-198" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-198', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">reducePartition</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-199" class="py-name"><a title="pyspark.rdd.RDD.reduce" class="py-name" href="#" onclick="return doclink('link-199', 'reduce', 'link-119');">reduce</a></tt><tt class="py-op">(</tt><tt class="py-name">mergeMaps</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1401"></a><tt class="py-lineno">1401</tt>  <tt class="py-line"> </tt>
<a name="RDD.countByKey"></a><div id="RDD.countByKey-def"><a name="L1402"></a><tt class="py-lineno">1402</tt> <a class="py-toggle" href="#" id="RDD.countByKey-toggle" onclick="return toggle('RDD.countByKey');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#countByKey">countByKey</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.countByKey-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.countByKey-expanded"><a name="L1403"></a><tt class="py-lineno">1403</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1404"></a><tt class="py-lineno">1404</tt>  <tt class="py-line"><tt class="py-docstring">        Count the number of elements for each key, and return the result to the</tt> </tt>
<a name="L1405"></a><tt class="py-lineno">1405</tt>  <tt class="py-line"><tt class="py-docstring">        master as a dictionary.</tt> </tt>
<a name="L1406"></a><tt class="py-lineno">1406</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1407"></a><tt class="py-lineno">1407</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])</tt> </tt>
<a name="L1408"></a><tt class="py-lineno">1408</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(rdd.countByKey().items())</tt> </tt>
<a name="L1409"></a><tt class="py-lineno">1409</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 2), ('b', 1)]</tt> </tt>
<a name="L1410"></a><tt class="py-lineno">1410</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1411"></a><tt class="py-lineno">1411</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-200" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-200', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-name">x</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-201" class="py-name" targets="Method pyspark.rdd.RDD.countByValue()=pyspark.rdd.RDD-class.html#countByValue"><a title="pyspark.rdd.RDD.countByValue" class="py-name" href="#" onclick="return doclink('link-201', 'countByValue', 'link-201');">countByValue</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1412"></a><tt class="py-lineno">1412</tt>  <tt class="py-line"> </tt>
<a name="RDD.join"></a><div id="RDD.join-def"><a name="L1413"></a><tt class="py-lineno">1413</tt> <a class="py-toggle" href="#" id="RDD.join-toggle" onclick="return toggle('RDD.join');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#join">join</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.join-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.join-expanded"><a name="L1414"></a><tt class="py-lineno">1414</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1415"></a><tt class="py-lineno">1415</tt>  <tt class="py-line"><tt class="py-docstring">        Return an RDD containing all pairs of elements with matching keys in</tt> </tt>
<a name="L1416"></a><tt class="py-lineno">1416</tt>  <tt class="py-line"><tt class="py-docstring">        C{self} and C{other}.</tt> </tt>
<a name="L1417"></a><tt class="py-lineno">1417</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1418"></a><tt class="py-lineno">1418</tt>  <tt class="py-line"><tt class="py-docstring">        Each pair of elements will be returned as a (k, (v1, v2)) tuple, where</tt> </tt>
<a name="L1419"></a><tt class="py-lineno">1419</tt>  <tt class="py-line"><tt class="py-docstring">        (k, v1) is in C{self} and (k, v2) is in C{other}.</tt> </tt>
<a name="L1420"></a><tt class="py-lineno">1420</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1421"></a><tt class="py-lineno">1421</tt>  <tt class="py-line"><tt class="py-docstring">        Performs a hash join across the cluster.</tt> </tt>
<a name="L1422"></a><tt class="py-lineno">1422</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1423"></a><tt class="py-lineno">1423</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", 1), ("b", 4)])</tt> </tt>
<a name="L1424"></a><tt class="py-lineno">1424</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; y = sc.parallelize([("a", 2), ("a", 3)])</tt> </tt>
<a name="L1425"></a><tt class="py-lineno">1425</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(x.join(y).collect())</tt> </tt>
<a name="L1426"></a><tt class="py-lineno">1426</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', (1, 2)), ('a', (1, 3))]</tt> </tt>
<a name="L1427"></a><tt class="py-lineno">1427</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1428"></a><tt class="py-lineno">1428</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">python_join</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">,</tt> <tt class="py-name">other</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1429"></a><tt class="py-lineno">1429</tt>  <tt class="py-line"> </tt>
<a name="RDD.leftOuterJoin"></a><div id="RDD.leftOuterJoin-def"><a name="L1430"></a><tt class="py-lineno">1430</tt> <a class="py-toggle" href="#" id="RDD.leftOuterJoin-toggle" onclick="return toggle('RDD.leftOuterJoin');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#leftOuterJoin">leftOuterJoin</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.leftOuterJoin-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.leftOuterJoin-expanded"><a name="L1431"></a><tt class="py-lineno">1431</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1432"></a><tt class="py-lineno">1432</tt>  <tt class="py-line"><tt class="py-docstring">        Perform a left outer join of C{self} and C{other}.</tt> </tt>
<a name="L1433"></a><tt class="py-lineno">1433</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1434"></a><tt class="py-lineno">1434</tt>  <tt class="py-line"><tt class="py-docstring">        For each element (k, v) in C{self}, the resulting RDD will either</tt> </tt>
<a name="L1435"></a><tt class="py-lineno">1435</tt>  <tt class="py-line"><tt class="py-docstring">        contain all pairs (k, (v, w)) for w in C{other}, or the pair</tt> </tt>
<a name="L1436"></a><tt class="py-lineno">1436</tt>  <tt class="py-line"><tt class="py-docstring">        (k, (v, None)) if no elements in other have key k.</tt> </tt>
<a name="L1437"></a><tt class="py-lineno">1437</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1438"></a><tt class="py-lineno">1438</tt>  <tt class="py-line"><tt class="py-docstring">        Hash-partitions the resulting RDD into the given number of partitions.</tt> </tt>
<a name="L1439"></a><tt class="py-lineno">1439</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1440"></a><tt class="py-lineno">1440</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", 1), ("b", 4)])</tt> </tt>
<a name="L1441"></a><tt class="py-lineno">1441</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; y = sc.parallelize([("a", 2)])</tt> </tt>
<a name="L1442"></a><tt class="py-lineno">1442</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(x.leftOuterJoin(y).collect())</tt> </tt>
<a name="L1443"></a><tt class="py-lineno">1443</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', (1, 2)), ('b', (4, None))]</tt> </tt>
<a name="L1444"></a><tt class="py-lineno">1444</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1445"></a><tt class="py-lineno">1445</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">python_left_outer_join</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">,</tt> <tt class="py-name">other</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1446"></a><tt class="py-lineno">1446</tt>  <tt class="py-line"> </tt>
<a name="RDD.rightOuterJoin"></a><div id="RDD.rightOuterJoin-def"><a name="L1447"></a><tt class="py-lineno">1447</tt> <a class="py-toggle" href="#" id="RDD.rightOuterJoin-toggle" onclick="return toggle('RDD.rightOuterJoin');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#rightOuterJoin">rightOuterJoin</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.rightOuterJoin-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.rightOuterJoin-expanded"><a name="L1448"></a><tt class="py-lineno">1448</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1449"></a><tt class="py-lineno">1449</tt>  <tt class="py-line"><tt class="py-docstring">        Perform a right outer join of C{self} and C{other}.</tt> </tt>
<a name="L1450"></a><tt class="py-lineno">1450</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1451"></a><tt class="py-lineno">1451</tt>  <tt class="py-line"><tt class="py-docstring">        For each element (k, w) in C{other}, the resulting RDD will either</tt> </tt>
<a name="L1452"></a><tt class="py-lineno">1452</tt>  <tt class="py-line"><tt class="py-docstring">        contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))</tt> </tt>
<a name="L1453"></a><tt class="py-lineno">1453</tt>  <tt class="py-line"><tt class="py-docstring">        if no elements in C{self} have key k.</tt> </tt>
<a name="L1454"></a><tt class="py-lineno">1454</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1455"></a><tt class="py-lineno">1455</tt>  <tt class="py-line"><tt class="py-docstring">        Hash-partitions the resulting RDD into the given number of partitions.</tt> </tt>
<a name="L1456"></a><tt class="py-lineno">1456</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1457"></a><tt class="py-lineno">1457</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", 1), ("b", 4)])</tt> </tt>
<a name="L1458"></a><tt class="py-lineno">1458</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; y = sc.parallelize([("a", 2)])</tt> </tt>
<a name="L1459"></a><tt class="py-lineno">1459</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(y.rightOuterJoin(x).collect())</tt> </tt>
<a name="L1460"></a><tt class="py-lineno">1460</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', (2, 1)), ('b', (None, 4))]</tt> </tt>
<a name="L1461"></a><tt class="py-lineno">1461</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1462"></a><tt class="py-lineno">1462</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">python_right_outer_join</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">,</tt> <tt class="py-name">other</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1463"></a><tt class="py-lineno">1463</tt>  <tt class="py-line"> </tt>
<a name="L1464"></a><tt class="py-lineno">1464</tt>  <tt class="py-line">    <tt class="py-comment"># TODO: add option to control map-side combining</tt> </tt>
<a name="L1465"></a><tt class="py-lineno">1465</tt>  <tt class="py-line">    <tt class="py-comment"># portable_hash is used as default, because builtin hash of None is different</tt> </tt>
<a name="L1466"></a><tt class="py-lineno">1466</tt>  <tt class="py-line">    <tt class="py-comment"># cross machines.</tt> </tt>
<a name="RDD.partitionBy"></a><div id="RDD.partitionBy-def"><a name="L1467"></a><tt class="py-lineno">1467</tt> <a class="py-toggle" href="#" id="RDD.partitionBy-toggle" onclick="return toggle('RDD.partitionBy');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#partitionBy">partitionBy</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">,</tt> <tt class="py-param">partitionFunc</tt><tt class="py-op">=</tt><tt class="py-name">portable_hash</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.partitionBy-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.partitionBy-expanded"><a name="L1468"></a><tt class="py-lineno">1468</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1469"></a><tt class="py-lineno">1469</tt>  <tt class="py-line"><tt class="py-docstring">        Return a copy of the RDD partitioned using the specified partitioner.</tt> </tt>
<a name="L1470"></a><tt class="py-lineno">1470</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1471"></a><tt class="py-lineno">1471</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))</tt> </tt>
<a name="L1472"></a><tt class="py-lineno">1472</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sets = pairs.partitionBy(2).glom().collect()</tt> </tt>
<a name="L1473"></a><tt class="py-lineno">1473</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; set(sets[0]).intersection(set(sets[1]))</tt> </tt>
<a name="L1474"></a><tt class="py-lineno">1474</tt>  <tt class="py-line"><tt class="py-docstring">        set([])</tt> </tt>
<a name="L1475"></a><tt class="py-lineno">1475</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1476"></a><tt class="py-lineno">1476</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">numPartitions</tt> <tt class="py-keyword">is</tt> <tt class="py-name">None</tt><tt class="py-op">:</tt> </tt>
<a name="L1477"></a><tt class="py-lineno">1477</tt>  <tt class="py-line">            <tt class="py-name">numPartitions</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_defaultReducePartitions</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1478"></a><tt class="py-lineno">1478</tt>  <tt class="py-line"> </tt>
<a name="L1479"></a><tt class="py-lineno">1479</tt>  <tt class="py-line">        <tt class="py-comment"># Transferring O(n) objects to Java is too expensive.</tt> </tt>
<a name="L1480"></a><tt class="py-lineno">1480</tt>  <tt class="py-line">        <tt class="py-comment"># Instead, we'll form the hash buckets in Python,</tt> </tt>
<a name="L1481"></a><tt class="py-lineno">1481</tt>  <tt class="py-line">        <tt class="py-comment"># transferring O(numPartitions) objects to Java.</tt> </tt>
<a name="L1482"></a><tt class="py-lineno">1482</tt>  <tt class="py-line">        <tt class="py-comment"># Each object is a (splitNumber, [objects]) pair.</tt> </tt>
<a name="L1483"></a><tt class="py-lineno">1483</tt>  <tt class="py-line">        <tt class="py-comment"># In order to avoid too huge objects, the objects are</tt> </tt>
<a name="L1484"></a><tt class="py-lineno">1484</tt>  <tt class="py-line">        <tt class="py-comment"># grouped into chunks.</tt> </tt>
<a name="L1485"></a><tt class="py-lineno">1485</tt>  <tt class="py-line">        <tt class="py-name">outputSerializer</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_unbatched_serializer</tt> </tt>
<a name="L1486"></a><tt class="py-lineno">1486</tt>  <tt class="py-line"> </tt>
<a name="L1487"></a><tt class="py-lineno">1487</tt>  <tt class="py-line">        <tt class="py-name">limit</tt> <tt class="py-op">=</tt> <tt class="py-op">(</tt><tt class="py-name">_parse_memory</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_conf</tt><tt class="py-op">.</tt><tt id="link-202" class="py-name"><a title="pyspark.conf.SparkConf.get
pyspark.files.SparkFiles.get" class="py-name" href="#" onclick="return doclink('link-202', 'get', 'link-49');">get</a></tt><tt class="py-op">(</tt> </tt>
<a name="L1488"></a><tt class="py-lineno">1488</tt>  <tt class="py-line">            <tt class="py-string">"spark.python.worker.memory"</tt><tt class="py-op">,</tt> <tt class="py-string">"512m"</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> <tt class="py-op">/</tt> <tt class="py-number">2</tt><tt class="py-op">)</tt> </tt>
<a name="L1489"></a><tt class="py-lineno">1489</tt>  <tt class="py-line"> </tt>
<a name="L1490"></a><tt class="py-lineno">1490</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">add_shuffle_key</tt><tt class="py-op">(</tt><tt class="py-param">split</tt><tt class="py-op">,</tt> <tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1491"></a><tt class="py-lineno">1491</tt>  <tt class="py-line"> </tt>
<a name="L1492"></a><tt class="py-lineno">1492</tt>  <tt class="py-line">            <tt class="py-name">buckets</tt> <tt class="py-op">=</tt> <tt class="py-name">defaultdict</tt><tt class="py-op">(</tt><tt class="py-name">list</tt><tt class="py-op">)</tt> </tt>
<a name="L1493"></a><tt class="py-lineno">1493</tt>  <tt class="py-line">            <tt class="py-name">c</tt><tt class="py-op">,</tt> <tt class="py-name">batch</tt> <tt class="py-op">=</tt> <tt class="py-number">0</tt><tt class="py-op">,</tt> <tt id="link-203" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.min
pyspark.rdd.RDD.min
pyspark.statcounter.StatCounter.min" class="py-name" href="#" onclick="return doclink('link-203', 'min', 'link-68');">min</a></tt><tt class="py-op">(</tt><tt class="py-number">10</tt> <tt class="py-op">*</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">,</tt> <tt class="py-number">1000</tt><tt class="py-op">)</tt> </tt>
<a name="L1494"></a><tt class="py-lineno">1494</tt>  <tt class="py-line"> </tt>
<a name="L1495"></a><tt class="py-lineno">1495</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt> <tt class="py-keyword">in</tt> <tt class="py-name">iterator</tt><tt class="py-op">:</tt> </tt>
<a name="L1496"></a><tt class="py-lineno">1496</tt>  <tt class="py-line">                <tt class="py-name">buckets</tt><tt class="py-op">[</tt><tt class="py-name">partitionFunc</tt><tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">)</tt> <tt class="py-op">%</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">]</tt><tt class="py-op">.</tt><tt class="py-name">append</tt><tt class="py-op">(</tt><tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L1497"></a><tt class="py-lineno">1497</tt>  <tt class="py-line">                <tt class="py-name">c</tt> <tt class="py-op">+=</tt> <tt class="py-number">1</tt> </tt>
<a name="L1498"></a><tt class="py-lineno">1498</tt>  <tt class="py-line"> </tt>
<a name="L1499"></a><tt class="py-lineno">1499</tt>  <tt class="py-line">                <tt class="py-comment"># check used memory and avg size of chunk of objects</tt> </tt>
<a name="L1500"></a><tt class="py-lineno">1500</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-op">(</tt><tt class="py-name">c</tt> <tt class="py-op">%</tt> <tt class="py-number">1000</tt> <tt class="py-op">==</tt> <tt class="py-number">0</tt> <tt class="py-keyword">and</tt> <tt class="py-name">get_used_memory</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> <tt class="py-op">&gt;</tt> <tt class="py-name">limit</tt> </tt>
<a name="L1501"></a><tt class="py-lineno">1501</tt>  <tt class="py-line">                        <tt class="py-keyword">or</tt> <tt class="py-name">c</tt> <tt class="py-op">&gt;</tt> <tt class="py-name">batch</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1502"></a><tt class="py-lineno">1502</tt>  <tt class="py-line">                    <tt class="py-name">n</tt><tt class="py-op">,</tt> <tt class="py-name">size</tt> <tt class="py-op">=</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-number">0</tt> </tt>
<a name="L1503"></a><tt class="py-lineno">1503</tt>  <tt class="py-line">                    <tt class="py-keyword">for</tt> <tt class="py-name">split</tt> <tt class="py-keyword">in</tt> <tt class="py-name">buckets</tt><tt class="py-op">.</tt><tt id="link-204" class="py-name"><a title="pyspark.rdd.RDD.keys" class="py-name" href="#" onclick="return doclink('link-204', 'keys', 'link-80');">keys</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1504"></a><tt class="py-lineno">1504</tt>  <tt class="py-line">                        <tt class="py-keyword">yield</tt> <tt class="py-name">pack_long</tt><tt class="py-op">(</tt><tt class="py-name">split</tt><tt class="py-op">)</tt> </tt>
<a name="L1505"></a><tt class="py-lineno">1505</tt>  <tt class="py-line">                        <tt class="py-name">d</tt> <tt class="py-op">=</tt> <tt class="py-name">outputSerializer</tt><tt class="py-op">.</tt><tt id="link-205" class="py-name" targets="Variable pyspark.serializers.MarshalSerializer.dumps=pyspark.serializers.MarshalSerializer-class.html#dumps,Method pyspark.serializers.PickleSerializer.dumps()=pyspark.serializers.PickleSerializer-class.html#dumps"><a title="pyspark.serializers.MarshalSerializer.dumps
pyspark.serializers.PickleSerializer.dumps" class="py-name" href="#" onclick="return doclink('link-205', 'dumps', 'link-205');">dumps</a></tt><tt class="py-op">(</tt><tt class="py-name">buckets</tt><tt class="py-op">[</tt><tt class="py-name">split</tt><tt class="py-op">]</tt><tt class="py-op">)</tt> </tt>
<a name="L1506"></a><tt class="py-lineno">1506</tt>  <tt class="py-line">                        <tt class="py-keyword">del</tt> <tt class="py-name">buckets</tt><tt class="py-op">[</tt><tt class="py-name">split</tt><tt class="py-op">]</tt> </tt>
<a name="L1507"></a><tt class="py-lineno">1507</tt>  <tt class="py-line">                        <tt class="py-keyword">yield</tt> <tt class="py-name">d</tt> </tt>
<a name="L1508"></a><tt class="py-lineno">1508</tt>  <tt class="py-line">                        <tt class="py-name">size</tt> <tt class="py-op">+=</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">d</tt><tt class="py-op">)</tt> </tt>
<a name="L1509"></a><tt class="py-lineno">1509</tt>  <tt class="py-line"> </tt>
<a name="L1510"></a><tt class="py-lineno">1510</tt>  <tt class="py-line">                    <tt class="py-name">avg</tt> <tt class="py-op">=</tt> <tt class="py-op">(</tt><tt class="py-name">size</tt> <tt class="py-op">/</tt> <tt class="py-name">n</tt><tt class="py-op">)</tt> <tt class="py-op">&gt;&gt;</tt> <tt class="py-number">20</tt> </tt>
<a name="L1511"></a><tt class="py-lineno">1511</tt>  <tt class="py-line">                    <tt class="py-comment"># let 1M &lt; avg &lt; 10M</tt> </tt>
<a name="L1512"></a><tt class="py-lineno">1512</tt>  <tt class="py-line">                    <tt class="py-keyword">if</tt> <tt class="py-name">avg</tt> <tt class="py-op">&lt;</tt> <tt class="py-number">1</tt><tt class="py-op">:</tt> </tt>
<a name="L1513"></a><tt class="py-lineno">1513</tt>  <tt class="py-line">                        <tt class="py-name">batch</tt> <tt class="py-op">*=</tt> <tt class="py-number">1.5</tt> </tt>
<a name="L1514"></a><tt class="py-lineno">1514</tt>  <tt class="py-line">                    <tt class="py-keyword">elif</tt> <tt class="py-name">avg</tt> <tt class="py-op">&gt;</tt> <tt class="py-number">10</tt><tt class="py-op">:</tt> </tt>
<a name="L1515"></a><tt class="py-lineno">1515</tt>  <tt class="py-line">                        <tt class="py-name">batch</tt> <tt class="py-op">=</tt> <tt id="link-206" class="py-name"><a title="pyspark.mllib.stat.MultivariateStatisticalSummary.max
pyspark.rdd.RDD.max
pyspark.statcounter.StatCounter.max" class="py-name" href="#" onclick="return doclink('link-206', 'max', 'link-89');">max</a></tt><tt class="py-op">(</tt><tt class="py-name">batch</tt> <tt class="py-op">/</tt> <tt class="py-number">1.5</tt><tt class="py-op">,</tt> <tt class="py-number">1</tt><tt class="py-op">)</tt> </tt>
<a name="L1516"></a><tt class="py-lineno">1516</tt>  <tt class="py-line">                    <tt class="py-name">c</tt> <tt class="py-op">=</tt> <tt class="py-number">0</tt> </tt>
<a name="L1517"></a><tt class="py-lineno">1517</tt>  <tt class="py-line"> </tt>
<a name="L1518"></a><tt class="py-lineno">1518</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-op">(</tt><tt class="py-name">split</tt><tt class="py-op">,</tt> <tt class="py-name">items</tt><tt class="py-op">)</tt> <tt class="py-keyword">in</tt> <tt class="py-name">buckets</tt><tt class="py-op">.</tt><tt class="py-name">iteritems</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1519"></a><tt class="py-lineno">1519</tt>  <tt class="py-line">                <tt class="py-keyword">yield</tt> <tt class="py-name">pack_long</tt><tt class="py-op">(</tt><tt class="py-name">split</tt><tt class="py-op">)</tt> </tt>
<a name="L1520"></a><tt class="py-lineno">1520</tt>  <tt class="py-line">                <tt class="py-keyword">yield</tt> <tt class="py-name">outputSerializer</tt><tt class="py-op">.</tt><tt id="link-207" class="py-name"><a title="pyspark.serializers.MarshalSerializer.dumps
pyspark.serializers.PickleSerializer.dumps" class="py-name" href="#" onclick="return doclink('link-207', 'dumps', 'link-205');">dumps</a></tt><tt class="py-op">(</tt><tt class="py-name">items</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1521"></a><tt class="py-lineno">1521</tt>  <tt class="py-line"> </tt>
<a name="L1522"></a><tt class="py-lineno">1522</tt>  <tt class="py-line">        <tt class="py-name">keyed</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-208" class="py-name"><a title="pyspark.rdd.RDD.mapPartitionsWithIndex
pyspark.sql.SchemaRDD.mapPartitionsWithIndex" class="py-name" href="#" onclick="return doclink('link-208', 'mapPartitionsWithIndex', 'link-50');">mapPartitionsWithIndex</a></tt><tt class="py-op">(</tt><tt class="py-name">add_shuffle_key</tt><tt class="py-op">)</tt> </tt>
<a name="L1523"></a><tt class="py-lineno">1523</tt>  <tt class="py-line">        <tt class="py-name">keyed</tt><tt class="py-op">.</tt><tt class="py-name">_bypass_serializer</tt> <tt class="py-op">=</tt> <tt class="py-name">True</tt> </tt>
<a name="L1524"></a><tt class="py-lineno">1524</tt>  <tt class="py-line">        <tt class="py-keyword">with</tt> <tt class="py-name">_JavaStackTrace</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-209" class="py-name"><a title="pyspark.context
pyspark.rdd.RDD.context" class="py-name" href="#" onclick="return doclink('link-209', 'context', 'link-111');">context</a></tt><tt class="py-op">)</tt> <tt class="py-keyword">as</tt> <tt class="py-name">st</tt><tt class="py-op">:</tt> </tt>
<a name="L1525"></a><tt class="py-lineno">1525</tt>  <tt class="py-line">            <tt class="py-name">pairRDD</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-210" class="py-name"><a title="pyspark.context.SparkContext._jvm" class="py-name" href="#" onclick="return doclink('link-210', '_jvm', 'link-176');">_jvm</a></tt><tt class="py-op">.</tt><tt class="py-name">PairwiseRDD</tt><tt class="py-op">(</tt> </tt>
<a name="L1526"></a><tt class="py-lineno">1526</tt>  <tt class="py-line">                <tt class="py-name">keyed</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-211" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-211', 'rdd', 'link-43');">rdd</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">asJavaPairRDD</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1527"></a><tt class="py-lineno">1527</tt>  <tt class="py-line">            <tt class="py-name">partitioner</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-212" class="py-name"><a title="pyspark.context.SparkContext._jvm" class="py-name" href="#" onclick="return doclink('link-212', '_jvm', 'link-176');">_jvm</a></tt><tt class="py-op">.</tt><tt class="py-name">PythonPartitioner</tt><tt class="py-op">(</tt><tt class="py-name">numPartitions</tt><tt class="py-op">,</tt> </tt>
<a name="L1528"></a><tt class="py-lineno">1528</tt>  <tt class="py-line">                                                          <tt id="link-213" class="py-name"><a title="pyspark.rdd.RDD.id" class="py-name" href="#" onclick="return doclink('link-213', 'id', 'link-34');">id</a></tt><tt class="py-op">(</tt><tt class="py-name">partitionFunc</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L1529"></a><tt class="py-lineno">1529</tt>  <tt class="py-line">        <tt class="py-name">jrdd</tt> <tt class="py-op">=</tt> <tt class="py-name">pairRDD</tt><tt class="py-op">.</tt><tt id="link-214" class="py-name"><a title="pyspark.rdd.RDD.partitionBy" class="py-name" href="#" onclick="return doclink('link-214', 'partitionBy', 'link-93');">partitionBy</a></tt><tt class="py-op">(</tt><tt class="py-name">partitioner</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-215" class="py-name"><a title="pyspark.rdd.RDD.values" class="py-name" href="#" onclick="return doclink('link-215', 'values', 'link-97');">values</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1530"></a><tt class="py-lineno">1530</tt>  <tt class="py-line">        <tt id="link-216" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-216', 'rdd', 'link-43');">rdd</a></tt> <tt class="py-op">=</tt> <tt id="link-217" class="py-name"><a title="pyspark.rdd.RDD" class="py-name" href="#" onclick="return doclink('link-217', 'RDD', 'link-62');">RDD</a></tt><tt class="py-op">(</tt><tt class="py-name">jrdd</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">,</tt> <tt class="py-name">BatchedSerializer</tt><tt class="py-op">(</tt><tt class="py-name">outputSerializer</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L1531"></a><tt class="py-lineno">1531</tt>  <tt class="py-line">        <tt class="py-comment"># This is required so that id(partitionFunc) remains unique,</tt> </tt>
<a name="L1532"></a><tt class="py-lineno">1532</tt>  <tt class="py-line">        <tt class="py-comment"># even if partitionFunc is a lambda:</tt> </tt>
<a name="L1533"></a><tt class="py-lineno">1533</tt>  <tt class="py-line">        <tt id="link-218" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-218', 'rdd', 'link-43');">rdd</a></tt><tt class="py-op">.</tt><tt class="py-name">_partitionFunc</tt> <tt class="py-op">=</tt> <tt class="py-name">partitionFunc</tt> </tt>
<a name="L1534"></a><tt class="py-lineno">1534</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt id="link-219" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-219', 'rdd', 'link-43');">rdd</a></tt> </tt>
</div><a name="L1535"></a><tt class="py-lineno">1535</tt>  <tt class="py-line"> </tt>
<a name="L1536"></a><tt class="py-lineno">1536</tt>  <tt class="py-line">    <tt class="py-comment"># TODO: add control over map-side aggregation</tt> </tt>
<a name="RDD.combineByKey"></a><div id="RDD.combineByKey-def"><a name="L1537"></a><tt class="py-lineno">1537</tt> <a class="py-toggle" href="#" id="RDD.combineByKey-toggle" onclick="return toggle('RDD.combineByKey');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#combineByKey">combineByKey</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">createCombiner</tt><tt class="py-op">,</tt> <tt class="py-param">mergeValue</tt><tt class="py-op">,</tt> <tt class="py-param">mergeCombiners</tt><tt class="py-op">,</tt> </tt>
<a name="L1538"></a><tt class="py-lineno">1538</tt>  <tt class="py-line">                     <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.combineByKey-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.combineByKey-expanded"><a name="L1539"></a><tt class="py-lineno">1539</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1540"></a><tt class="py-lineno">1540</tt>  <tt class="py-line"><tt class="py-docstring">        Generic function to combine the elements for each key using a custom</tt> </tt>
<a name="L1541"></a><tt class="py-lineno">1541</tt>  <tt class="py-line"><tt class="py-docstring">        set of aggregation functions.</tt> </tt>
<a name="L1542"></a><tt class="py-lineno">1542</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1543"></a><tt class="py-lineno">1543</tt>  <tt class="py-line"><tt class="py-docstring">        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined</tt> </tt>
<a name="L1544"></a><tt class="py-lineno">1544</tt>  <tt class="py-line"><tt class="py-docstring">        type" C.  Note that V and C can be different -- for example, one might</tt> </tt>
<a name="L1545"></a><tt class="py-lineno">1545</tt>  <tt class="py-line"><tt class="py-docstring">        group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).</tt> </tt>
<a name="L1546"></a><tt class="py-lineno">1546</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1547"></a><tt class="py-lineno">1547</tt>  <tt class="py-line"><tt class="py-docstring">        Users provide three functions:</tt> </tt>
<a name="L1548"></a><tt class="py-lineno">1548</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1549"></a><tt class="py-lineno">1549</tt>  <tt class="py-line"><tt class="py-docstring">            - C{createCombiner}, which turns a V into a C (e.g., creates</tt> </tt>
<a name="L1550"></a><tt class="py-lineno">1550</tt>  <tt class="py-line"><tt class="py-docstring">              a one-element list)</tt> </tt>
<a name="L1551"></a><tt class="py-lineno">1551</tt>  <tt class="py-line"><tt class="py-docstring">            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of</tt> </tt>
<a name="L1552"></a><tt class="py-lineno">1552</tt>  <tt class="py-line"><tt class="py-docstring">              a list)</tt> </tt>
<a name="L1553"></a><tt class="py-lineno">1553</tt>  <tt class="py-line"><tt class="py-docstring">            - C{mergeCombiners}, to combine two C's into a single one.</tt> </tt>
<a name="L1554"></a><tt class="py-lineno">1554</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1555"></a><tt class="py-lineno">1555</tt>  <tt class="py-line"><tt class="py-docstring">        In addition, users can control the partitioning of the output RDD.</tt> </tt>
<a name="L1556"></a><tt class="py-lineno">1556</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1557"></a><tt class="py-lineno">1557</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])</tt> </tt>
<a name="L1558"></a><tt class="py-lineno">1558</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; def f(x): return x</tt> </tt>
<a name="L1559"></a><tt class="py-lineno">1559</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; def add(a, b): return a + str(b)</tt> </tt>
<a name="L1560"></a><tt class="py-lineno">1560</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(x.combineByKey(str, add, add).collect())</tt> </tt>
<a name="L1561"></a><tt class="py-lineno">1561</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', '11'), ('b', '1')]</tt> </tt>
<a name="L1562"></a><tt class="py-lineno">1562</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1563"></a><tt class="py-lineno">1563</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">numPartitions</tt> <tt class="py-keyword">is</tt> <tt class="py-name">None</tt><tt class="py-op">:</tt> </tt>
<a name="L1564"></a><tt class="py-lineno">1564</tt>  <tt class="py-line">            <tt class="py-name">numPartitions</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_defaultReducePartitions</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1565"></a><tt class="py-lineno">1565</tt>  <tt class="py-line"> </tt>
<a name="L1566"></a><tt class="py-lineno">1566</tt>  <tt class="py-line">        <tt class="py-name">serializer</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">serializer</tt> </tt>
<a name="L1567"></a><tt class="py-lineno">1567</tt>  <tt class="py-line">        <tt class="py-name">spill</tt> <tt class="py-op">=</tt> <tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_conf</tt><tt class="py-op">.</tt><tt id="link-220" class="py-name"><a title="pyspark.conf.SparkConf.get
pyspark.files.SparkFiles.get" class="py-name" href="#" onclick="return doclink('link-220', 'get', 'link-49');">get</a></tt><tt class="py-op">(</tt><tt class="py-string">"spark.shuffle.spill"</tt><tt class="py-op">,</tt> <tt class="py-string">'True'</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">lower</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1568"></a><tt class="py-lineno">1568</tt>  <tt class="py-line">                 <tt class="py-op">==</tt> <tt class="py-string">'true'</tt><tt class="py-op">)</tt> </tt>
<a name="L1569"></a><tt class="py-lineno">1569</tt>  <tt class="py-line">        <tt class="py-name">memory</tt> <tt class="py-op">=</tt> <tt class="py-name">_parse_memory</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_conf</tt><tt class="py-op">.</tt><tt id="link-221" class="py-name"><a title="pyspark.conf.SparkConf.get
pyspark.files.SparkFiles.get" class="py-name" href="#" onclick="return doclink('link-221', 'get', 'link-49');">get</a></tt><tt class="py-op">(</tt> </tt>
<a name="L1570"></a><tt class="py-lineno">1570</tt>  <tt class="py-line">            <tt class="py-string">"spark.python.worker.memory"</tt><tt class="py-op">,</tt> <tt class="py-string">"512m"</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L1571"></a><tt class="py-lineno">1571</tt>  <tt class="py-line">        <tt class="py-name">agg</tt> <tt class="py-op">=</tt> <tt class="py-name">Aggregator</tt><tt class="py-op">(</tt><tt class="py-name">createCombiner</tt><tt class="py-op">,</tt> <tt class="py-name">mergeValue</tt><tt class="py-op">,</tt> <tt class="py-name">mergeCombiners</tt><tt class="py-op">)</tt> </tt>
<a name="L1572"></a><tt class="py-lineno">1572</tt>  <tt class="py-line"> </tt>
<a name="L1573"></a><tt class="py-lineno">1573</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">combineLocally</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1574"></a><tt class="py-lineno">1574</tt>  <tt class="py-line">            <tt class="py-name">merger</tt> <tt class="py-op">=</tt> <tt class="py-name">ExternalMerger</tt><tt class="py-op">(</tt><tt class="py-name">agg</tt><tt class="py-op">,</tt> <tt class="py-name">memory</tt> <tt class="py-op">*</tt> <tt class="py-number">0.9</tt><tt class="py-op">,</tt> <tt class="py-name">serializer</tt><tt class="py-op">)</tt> \ </tt>
<a name="L1575"></a><tt class="py-lineno">1575</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-name">spill</tt> <tt class="py-keyword">else</tt> <tt class="py-name">InMemoryMerger</tt><tt class="py-op">(</tt><tt class="py-name">agg</tt><tt class="py-op">)</tt> </tt>
<a name="L1576"></a><tt class="py-lineno">1576</tt>  <tt class="py-line">            <tt class="py-name">merger</tt><tt class="py-op">.</tt><tt class="py-name">mergeValues</tt><tt class="py-op">(</tt><tt class="py-name">iterator</tt><tt class="py-op">)</tt> </tt>
<a name="L1577"></a><tt class="py-lineno">1577</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">merger</tt><tt class="py-op">.</tt><tt class="py-name">iteritems</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1578"></a><tt class="py-lineno">1578</tt>  <tt class="py-line"> </tt>
<a name="L1579"></a><tt class="py-lineno">1579</tt>  <tt class="py-line">        <tt class="py-name">locally_combined</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-222" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-222', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">combineLocally</tt><tt class="py-op">)</tt> </tt>
<a name="L1580"></a><tt class="py-lineno">1580</tt>  <tt class="py-line">        <tt class="py-name">shuffled</tt> <tt class="py-op">=</tt> <tt class="py-name">locally_combined</tt><tt class="py-op">.</tt><tt id="link-223" class="py-name"><a title="pyspark.rdd.RDD.partitionBy" class="py-name" href="#" onclick="return doclink('link-223', 'partitionBy', 'link-93');">partitionBy</a></tt><tt class="py-op">(</tt><tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
<a name="L1581"></a><tt class="py-lineno">1581</tt>  <tt class="py-line"> </tt>
<a name="L1582"></a><tt class="py-lineno">1582</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">_mergeCombiners</tt><tt class="py-op">(</tt><tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1583"></a><tt class="py-lineno">1583</tt>  <tt class="py-line">            <tt class="py-name">merger</tt> <tt class="py-op">=</tt> <tt class="py-name">ExternalMerger</tt><tt class="py-op">(</tt><tt class="py-name">agg</tt><tt class="py-op">,</tt> <tt class="py-name">memory</tt><tt class="py-op">,</tt> <tt class="py-name">serializer</tt><tt class="py-op">)</tt> \ </tt>
<a name="L1584"></a><tt class="py-lineno">1584</tt>  <tt class="py-line">                <tt class="py-keyword">if</tt> <tt class="py-name">spill</tt> <tt class="py-keyword">else</tt> <tt class="py-name">InMemoryMerger</tt><tt class="py-op">(</tt><tt class="py-name">agg</tt><tt class="py-op">)</tt> </tt>
<a name="L1585"></a><tt class="py-lineno">1585</tt>  <tt class="py-line">            <tt class="py-name">merger</tt><tt class="py-op">.</tt><tt class="py-name">mergeCombiners</tt><tt class="py-op">(</tt><tt class="py-name">iterator</tt><tt class="py-op">)</tt> </tt>
<a name="L1586"></a><tt class="py-lineno">1586</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">merger</tt><tt class="py-op">.</tt><tt class="py-name">iteritems</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1587"></a><tt class="py-lineno">1587</tt>  <tt class="py-line"> </tt>
<a name="L1588"></a><tt class="py-lineno">1588</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">shuffled</tt><tt class="py-op">.</tt><tt id="link-224" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-224', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-name">_mergeCombiners</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1589"></a><tt class="py-lineno">1589</tt>  <tt class="py-line"> </tt>
<a name="RDD.aggregateByKey"></a><div id="RDD.aggregateByKey-def"><a name="L1590"></a><tt class="py-lineno">1590</tt> <a class="py-toggle" href="#" id="RDD.aggregateByKey-toggle" onclick="return toggle('RDD.aggregateByKey');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#aggregateByKey">aggregateByKey</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">zeroValue</tt><tt class="py-op">,</tt> <tt class="py-param">seqFunc</tt><tt class="py-op">,</tt> <tt class="py-param">combFunc</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.aggregateByKey-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.aggregateByKey-expanded"><a name="L1591"></a><tt class="py-lineno">1591</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1592"></a><tt class="py-lineno">1592</tt>  <tt class="py-line"><tt class="py-docstring">        Aggregate the values of each key, using given combine functions and a neutral</tt> </tt>
<a name="L1593"></a><tt class="py-lineno">1593</tt>  <tt class="py-line"><tt class="py-docstring">        "zero value". This function can return a different result type, U, than the type</tt> </tt>
<a name="L1594"></a><tt class="py-lineno">1594</tt>  <tt class="py-line"><tt class="py-docstring">        of the values in this RDD, V. Thus, we need one operation for merging a V into</tt> </tt>
<a name="L1595"></a><tt class="py-lineno">1595</tt>  <tt class="py-line"><tt class="py-docstring">        a U and one operation for merging two U's, The former operation is used for merging</tt> </tt>
<a name="L1596"></a><tt class="py-lineno">1596</tt>  <tt class="py-line"><tt class="py-docstring">        values within a partition, and the latter is used for merging values between</tt> </tt>
<a name="L1597"></a><tt class="py-lineno">1597</tt>  <tt class="py-line"><tt class="py-docstring">        partitions. To avoid memory allocation, both of these functions are</tt> </tt>
<a name="L1598"></a><tt class="py-lineno">1598</tt>  <tt class="py-line"><tt class="py-docstring">        allowed to modify and return their first argument instead of creating a new U.</tt> </tt>
<a name="L1599"></a><tt class="py-lineno">1599</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1600"></a><tt class="py-lineno">1600</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">createZero</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1601"></a><tt class="py-lineno">1601</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt id="link-225" class="py-name"><a title="pyspark.statcounter.StatCounter.copy" class="py-name" href="#" onclick="return doclink('link-225', 'copy', 'link-0');">copy</a></tt><tt class="py-op">.</tt><tt class="py-name">deepcopy</tt><tt class="py-op">(</tt><tt class="py-name">zeroValue</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1602"></a><tt class="py-lineno">1602</tt>  <tt class="py-line"> </tt>
<a name="L1603"></a><tt class="py-lineno">1603</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-226" class="py-name"><a title="pyspark.rdd.RDD.combineByKey" class="py-name" href="#" onclick="return doclink('link-226', 'combineByKey', 'link-197');">combineByKey</a></tt><tt class="py-op">(</tt> </tt>
<a name="L1604"></a><tt class="py-lineno">1604</tt>  <tt class="py-line">            <tt class="py-keyword">lambda</tt> <tt class="py-name">v</tt><tt class="py-op">:</tt> <tt class="py-name">seqFunc</tt><tt class="py-op">(</tt><tt class="py-name">createZero</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">seqFunc</tt><tt class="py-op">,</tt> <tt class="py-name">combFunc</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1605"></a><tt class="py-lineno">1605</tt>  <tt class="py-line"> </tt>
<a name="RDD.foldByKey"></a><div id="RDD.foldByKey-def"><a name="L1606"></a><tt class="py-lineno">1606</tt> <a class="py-toggle" href="#" id="RDD.foldByKey-toggle" onclick="return toggle('RDD.foldByKey');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#foldByKey">foldByKey</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">zeroValue</tt><tt class="py-op">,</tt> <tt class="py-param">func</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.foldByKey-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.foldByKey-expanded"><a name="L1607"></a><tt class="py-lineno">1607</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1608"></a><tt class="py-lineno">1608</tt>  <tt class="py-line"><tt class="py-docstring">        Merge the values for each key using an associative function "func"</tt> </tt>
<a name="L1609"></a><tt class="py-lineno">1609</tt>  <tt class="py-line"><tt class="py-docstring">        and a neutral "zeroValue" which may be added to the result an</tt> </tt>
<a name="L1610"></a><tt class="py-lineno">1610</tt>  <tt class="py-line"><tt class="py-docstring">        arbitrary number of times, and must not change the result</tt> </tt>
<a name="L1611"></a><tt class="py-lineno">1611</tt>  <tt class="py-line"><tt class="py-docstring">        (e.g., 0 for addition, or 1 for multiplication.).</tt> </tt>
<a name="L1612"></a><tt class="py-lineno">1612</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1613"></a><tt class="py-lineno">1613</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])</tt> </tt>
<a name="L1614"></a><tt class="py-lineno">1614</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; from operator import add</tt> </tt>
<a name="L1615"></a><tt class="py-lineno">1615</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd.foldByKey(0, add).collect()</tt> </tt>
<a name="L1616"></a><tt class="py-lineno">1616</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 2), ('b', 1)]</tt> </tt>
<a name="L1617"></a><tt class="py-lineno">1617</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1618"></a><tt class="py-lineno">1618</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">createZero</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1619"></a><tt class="py-lineno">1619</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt id="link-227" class="py-name"><a title="pyspark.statcounter.StatCounter.copy" class="py-name" href="#" onclick="return doclink('link-227', 'copy', 'link-0');">copy</a></tt><tt class="py-op">.</tt><tt class="py-name">deepcopy</tt><tt class="py-op">(</tt><tt class="py-name">zeroValue</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1620"></a><tt class="py-lineno">1620</tt>  <tt class="py-line"> </tt>
<a name="L1621"></a><tt class="py-lineno">1621</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-228" class="py-name"><a title="pyspark.rdd.RDD.combineByKey" class="py-name" href="#" onclick="return doclink('link-228', 'combineByKey', 'link-197');">combineByKey</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">v</tt><tt class="py-op">:</tt> <tt class="py-name">func</tt><tt class="py-op">(</tt><tt class="py-name">createZero</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">func</tt><tt class="py-op">,</tt> <tt class="py-name">func</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1622"></a><tt class="py-lineno">1622</tt>  <tt class="py-line"> </tt>
<a name="L1623"></a><tt class="py-lineno">1623</tt>  <tt class="py-line">    <tt class="py-comment"># TODO: support variant with custom partitioner</tt> </tt>
<a name="RDD.groupByKey"></a><div id="RDD.groupByKey-def"><a name="L1624"></a><tt class="py-lineno">1624</tt> <a class="py-toggle" href="#" id="RDD.groupByKey-toggle" onclick="return toggle('RDD.groupByKey');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#groupByKey">groupByKey</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.groupByKey-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.groupByKey-expanded"><a name="L1625"></a><tt class="py-lineno">1625</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1626"></a><tt class="py-lineno">1626</tt>  <tt class="py-line"><tt class="py-docstring">        Group the values for each key in the RDD into a single sequence.</tt> </tt>
<a name="L1627"></a><tt class="py-lineno">1627</tt>  <tt class="py-line"><tt class="py-docstring">        Hash-partitions the resulting RDD with into numPartitions partitions.</tt> </tt>
<a name="L1628"></a><tt class="py-lineno">1628</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1629"></a><tt class="py-lineno">1629</tt>  <tt class="py-line"><tt class="py-docstring">        Note: If you are grouping in order to perform an aggregation (such as a</tt> </tt>
<a name="L1630"></a><tt class="py-lineno">1630</tt>  <tt class="py-line"><tt class="py-docstring">        sum or average) over each key, using reduceByKey will provide much</tt> </tt>
<a name="L1631"></a><tt class="py-lineno">1631</tt>  <tt class="py-line"><tt class="py-docstring">        better performance.</tt> </tt>
<a name="L1632"></a><tt class="py-lineno">1632</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1633"></a><tt class="py-lineno">1633</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])</tt> </tt>
<a name="L1634"></a><tt class="py-lineno">1634</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; map((lambda (x,y): (x, list(y))), sorted(x.groupByKey().collect()))</tt> </tt>
<a name="L1635"></a><tt class="py-lineno">1635</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', [1, 1]), ('b', [1])]</tt> </tt>
<a name="L1636"></a><tt class="py-lineno">1636</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1637"></a><tt class="py-lineno">1637</tt>  <tt class="py-line"> </tt>
<a name="L1638"></a><tt class="py-lineno">1638</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">createCombiner</tt><tt class="py-op">(</tt><tt class="py-param">x</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1639"></a><tt class="py-lineno">1639</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-op">[</tt><tt class="py-name">x</tt><tt class="py-op">]</tt> </tt>
</div><a name="L1640"></a><tt class="py-lineno">1640</tt>  <tt class="py-line"> </tt>
<a name="L1641"></a><tt class="py-lineno">1641</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">mergeValue</tt><tt class="py-op">(</tt><tt class="py-param">xs</tt><tt class="py-op">,</tt> <tt class="py-param">x</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1642"></a><tt class="py-lineno">1642</tt>  <tt class="py-line">            <tt class="py-name">xs</tt><tt class="py-op">.</tt><tt class="py-name">append</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt> </tt>
<a name="L1643"></a><tt class="py-lineno">1643</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">xs</tt> </tt>
</div><a name="L1644"></a><tt class="py-lineno">1644</tt>  <tt class="py-line"> </tt>
<a name="L1645"></a><tt class="py-lineno">1645</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">mergeCombiners</tt><tt class="py-op">(</tt><tt class="py-param">a</tt><tt class="py-op">,</tt> <tt class="py-param">b</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1646"></a><tt class="py-lineno">1646</tt>  <tt class="py-line">            <tt class="py-name">a</tt><tt class="py-op">.</tt><tt class="py-name">extend</tt><tt class="py-op">(</tt><tt class="py-name">b</tt><tt class="py-op">)</tt> </tt>
<a name="L1647"></a><tt class="py-lineno">1647</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">a</tt> </tt>
</div><a name="L1648"></a><tt class="py-lineno">1648</tt>  <tt class="py-line"> </tt>
<a name="L1649"></a><tt class="py-lineno">1649</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-229" class="py-name"><a title="pyspark.rdd.RDD.combineByKey" class="py-name" href="#" onclick="return doclink('link-229', 'combineByKey', 'link-197');">combineByKey</a></tt><tt class="py-op">(</tt><tt class="py-name">createCombiner</tt><tt class="py-op">,</tt> <tt class="py-name">mergeValue</tt><tt class="py-op">,</tt> <tt class="py-name">mergeCombiners</tt><tt class="py-op">,</tt> </tt>
<a name="L1650"></a><tt class="py-lineno">1650</tt>  <tt class="py-line">                                 <tt class="py-name">numPartitions</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-230" class="py-name" targets="Method pyspark.rdd.RDD.mapValues()=pyspark.rdd.RDD-class.html#mapValues"><a title="pyspark.rdd.RDD.mapValues" class="py-name" href="#" onclick="return doclink('link-230', 'mapValues', 'link-230');">mapValues</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt id="link-231" class="py-name"><a title="pyspark.resultiterable.ResultIterable" class="py-name" href="#" onclick="return doclink('link-231', 'ResultIterable', 'link-17');">ResultIterable</a></tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1651"></a><tt class="py-lineno">1651</tt>  <tt class="py-line"> </tt>
<a name="L1652"></a><tt class="py-lineno">1652</tt>  <tt class="py-line">    <tt class="py-comment"># TODO: add tests</tt> </tt>
<a name="RDD.flatMapValues"></a><div id="RDD.flatMapValues-def"><a name="L1653"></a><tt class="py-lineno">1653</tt> <a class="py-toggle" href="#" id="RDD.flatMapValues-toggle" onclick="return toggle('RDD.flatMapValues');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#flatMapValues">flatMapValues</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.flatMapValues-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.flatMapValues-expanded"><a name="L1654"></a><tt class="py-lineno">1654</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1655"></a><tt class="py-lineno">1655</tt>  <tt class="py-line"><tt class="py-docstring">        Pass each value in the key-value pair RDD through a flatMap function</tt> </tt>
<a name="L1656"></a><tt class="py-lineno">1656</tt>  <tt class="py-line"><tt class="py-docstring">        without changing the keys; this also retains the original RDD's</tt> </tt>
<a name="L1657"></a><tt class="py-lineno">1657</tt>  <tt class="py-line"><tt class="py-docstring">        partitioning.</tt> </tt>
<a name="L1658"></a><tt class="py-lineno">1658</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1659"></a><tt class="py-lineno">1659</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", ["x", "y", "z"]), ("b", ["p", "r"])])</tt> </tt>
<a name="L1660"></a><tt class="py-lineno">1660</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; def f(x): return x</tt> </tt>
<a name="L1661"></a><tt class="py-lineno">1661</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x.flatMapValues(f).collect()</tt> </tt>
<a name="L1662"></a><tt class="py-lineno">1662</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]</tt> </tt>
<a name="L1663"></a><tt class="py-lineno">1663</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1664"></a><tt class="py-lineno">1664</tt>  <tt class="py-line">        <tt class="py-name">flat_map_fn</tt> <tt class="py-op">=</tt> <tt class="py-keyword">lambda</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">x</tt><tt class="py-op">)</tt> <tt class="py-keyword">for</tt> <tt class="py-name">x</tt> <tt class="py-keyword">in</tt> <tt class="py-name">f</tt><tt class="py-op">(</tt><tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L1665"></a><tt class="py-lineno">1665</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-232" class="py-name" targets="Method pyspark.rdd.RDD.flatMap()=pyspark.rdd.RDD-class.html#flatMap"><a title="pyspark.rdd.RDD.flatMap" class="py-name" href="#" onclick="return doclink('link-232', 'flatMap', 'link-232');">flatMap</a></tt><tt class="py-op">(</tt><tt class="py-name">flat_map_fn</tt><tt class="py-op">,</tt> <tt class="py-name">preservesPartitioning</tt><tt class="py-op">=</tt><tt class="py-name">True</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1666"></a><tt class="py-lineno">1666</tt>  <tt class="py-line"> </tt>
<a name="RDD.mapValues"></a><div id="RDD.mapValues-def"><a name="L1667"></a><tt class="py-lineno">1667</tt> <a class="py-toggle" href="#" id="RDD.mapValues-toggle" onclick="return toggle('RDD.mapValues');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#mapValues">mapValues</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.mapValues-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.mapValues-expanded"><a name="L1668"></a><tt class="py-lineno">1668</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1669"></a><tt class="py-lineno">1669</tt>  <tt class="py-line"><tt class="py-docstring">        Pass each value in the key-value pair RDD through a map function</tt> </tt>
<a name="L1670"></a><tt class="py-lineno">1670</tt>  <tt class="py-line"><tt class="py-docstring">        without changing the keys; this also retains the original RDD's</tt> </tt>
<a name="L1671"></a><tt class="py-lineno">1671</tt>  <tt class="py-line"><tt class="py-docstring">        partitioning.</tt> </tt>
<a name="L1672"></a><tt class="py-lineno">1672</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1673"></a><tt class="py-lineno">1673</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", ["apple", "banana", "lemon"]), ("b", ["grapes"])])</tt> </tt>
<a name="L1674"></a><tt class="py-lineno">1674</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; def f(x): return len(x)</tt> </tt>
<a name="L1675"></a><tt class="py-lineno">1675</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x.mapValues(f).collect()</tt> </tt>
<a name="L1676"></a><tt class="py-lineno">1676</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 3), ('b', 1)]</tt> </tt>
<a name="L1677"></a><tt class="py-lineno">1677</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1678"></a><tt class="py-lineno">1678</tt>  <tt class="py-line">        <tt class="py-name">map_values_fn</tt> <tt class="py-op">=</tt> <tt class="py-keyword">lambda</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-name">k</tt><tt class="py-op">,</tt> <tt class="py-name">f</tt><tt class="py-op">(</tt><tt class="py-name">v</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L1679"></a><tt class="py-lineno">1679</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-233" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-233', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-name">map_values_fn</tt><tt class="py-op">,</tt> <tt class="py-name">preservesPartitioning</tt><tt class="py-op">=</tt><tt class="py-name">True</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1680"></a><tt class="py-lineno">1680</tt>  <tt class="py-line"> </tt>
<a name="RDD.groupWith"></a><div id="RDD.groupWith-def"><a name="L1681"></a><tt class="py-lineno">1681</tt> <a class="py-toggle" href="#" id="RDD.groupWith-toggle" onclick="return toggle('RDD.groupWith');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#groupWith">groupWith</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">,</tt> <tt class="py-op">*</tt><tt class="py-param">others</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.groupWith-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.groupWith-expanded"><a name="L1682"></a><tt class="py-lineno">1682</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1683"></a><tt class="py-lineno">1683</tt>  <tt class="py-line"><tt class="py-docstring">        Alias for cogroup but with support for multiple RDDs.</tt> </tt>
<a name="L1684"></a><tt class="py-lineno">1684</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1685"></a><tt class="py-lineno">1685</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; w = sc.parallelize([("a", 5), ("b", 6)])</tt> </tt>
<a name="L1686"></a><tt class="py-lineno">1686</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", 1), ("b", 4)])</tt> </tt>
<a name="L1687"></a><tt class="py-lineno">1687</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; y = sc.parallelize([("a", 2)])</tt> </tt>
<a name="L1688"></a><tt class="py-lineno">1688</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; z = sc.parallelize([("b", 42)])</tt> </tt>
<a name="L1689"></a><tt class="py-lineno">1689</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; map((lambda (x,y): (x, (list(y[0]), list(y[1]), list(y[2]), list(y[3])))), \</tt> </tt>
<a name="L1690"></a><tt class="py-lineno">1690</tt>  <tt class="py-line"><tt class="py-docstring">                sorted(list(w.groupWith(x, y, z).collect())))</tt> </tt>
<a name="L1691"></a><tt class="py-lineno">1691</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]</tt> </tt>
<a name="L1692"></a><tt class="py-lineno">1692</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1693"></a><tt class="py-lineno">1693</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1694"></a><tt class="py-lineno">1694</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">python_cogroup</tt><tt class="py-op">(</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">,</tt> <tt class="py-name">other</tt><tt class="py-op">)</tt> <tt class="py-op">+</tt> <tt class="py-name">others</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1695"></a><tt class="py-lineno">1695</tt>  <tt class="py-line"> </tt>
<a name="L1696"></a><tt class="py-lineno">1696</tt>  <tt class="py-line">    <tt class="py-comment"># TODO: add variant with custom parittioner</tt> </tt>
<a name="RDD.cogroup"></a><div id="RDD.cogroup-def"><a name="L1697"></a><tt class="py-lineno">1697</tt> <a class="py-toggle" href="#" id="RDD.cogroup-toggle" onclick="return toggle('RDD.cogroup');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#cogroup">cogroup</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.cogroup-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.cogroup-expanded"><a name="L1698"></a><tt class="py-lineno">1698</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1699"></a><tt class="py-lineno">1699</tt>  <tt class="py-line"><tt class="py-docstring">        For each key k in C{self} or C{other}, return a resulting RDD that</tt> </tt>
<a name="L1700"></a><tt class="py-lineno">1700</tt>  <tt class="py-line"><tt class="py-docstring">        contains a tuple with the list of values for that key in C{self} as</tt> </tt>
<a name="L1701"></a><tt class="py-lineno">1701</tt>  <tt class="py-line"><tt class="py-docstring">        well as C{other}.</tt> </tt>
<a name="L1702"></a><tt class="py-lineno">1702</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1703"></a><tt class="py-lineno">1703</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", 1), ("b", 4)])</tt> </tt>
<a name="L1704"></a><tt class="py-lineno">1704</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; y = sc.parallelize([("a", 2)])</tt> </tt>
<a name="L1705"></a><tt class="py-lineno">1705</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; map((lambda (x,y): (x, (list(y[0]), list(y[1])))), sorted(list(x.cogroup(y).collect())))</tt> </tt>
<a name="L1706"></a><tt class="py-lineno">1706</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', ([1], [2])), ('b', ([4], []))]</tt> </tt>
<a name="L1707"></a><tt class="py-lineno">1707</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1708"></a><tt class="py-lineno">1708</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">python_cogroup</tt><tt class="py-op">(</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">,</tt> <tt class="py-name">other</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1709"></a><tt class="py-lineno">1709</tt>  <tt class="py-line"> </tt>
<a name="RDD.sampleByKey"></a><div id="RDD.sampleByKey-def"><a name="L1710"></a><tt class="py-lineno">1710</tt> <a class="py-toggle" href="#" id="RDD.sampleByKey-toggle" onclick="return toggle('RDD.sampleByKey');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#sampleByKey">sampleByKey</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">withReplacement</tt><tt class="py-op">,</tt> <tt class="py-param">fractions</tt><tt class="py-op">,</tt> <tt class="py-param">seed</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.sampleByKey-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.sampleByKey-expanded"><a name="L1711"></a><tt class="py-lineno">1711</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1712"></a><tt class="py-lineno">1712</tt>  <tt class="py-line"><tt class="py-docstring">        Return a subset of this RDD sampled by key (via stratified sampling).</tt> </tt>
<a name="L1713"></a><tt class="py-lineno">1713</tt>  <tt class="py-line"><tt class="py-docstring">        Create a sample of this RDD using variable sampling rates for</tt> </tt>
<a name="L1714"></a><tt class="py-lineno">1714</tt>  <tt class="py-line"><tt class="py-docstring">        different keys as specified by fractions, a key to sampling rate map.</tt> </tt>
<a name="L1715"></a><tt class="py-lineno">1715</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1716"></a><tt class="py-lineno">1716</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; fractions = {"a": 0.2, "b": 0.1}</tt> </tt>
<a name="L1717"></a><tt class="py-lineno">1717</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))</tt> </tt>
<a name="L1718"></a><tt class="py-lineno">1718</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())</tt> </tt>
<a name="L1719"></a><tt class="py-lineno">1719</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; 100 &lt; len(sample["a"]) &lt; 300 and 50 &lt; len(sample["b"]) &lt; 150</tt> </tt>
<a name="L1720"></a><tt class="py-lineno">1720</tt>  <tt class="py-line"><tt class="py-docstring">        True</tt> </tt>
<a name="L1721"></a><tt class="py-lineno">1721</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; max(sample["a"]) &lt;= 999 and min(sample["a"]) &gt;= 0</tt> </tt>
<a name="L1722"></a><tt class="py-lineno">1722</tt>  <tt class="py-line"><tt class="py-docstring">        True</tt> </tt>
<a name="L1723"></a><tt class="py-lineno">1723</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; max(sample["b"]) &lt;= 999 and min(sample["b"]) &gt;= 0</tt> </tt>
<a name="L1724"></a><tt class="py-lineno">1724</tt>  <tt class="py-line"><tt class="py-docstring">        True</tt> </tt>
<a name="L1725"></a><tt class="py-lineno">1725</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1726"></a><tt class="py-lineno">1726</tt>  <tt class="py-line">        <tt class="py-keyword">for</tt> <tt class="py-name">fraction</tt> <tt class="py-keyword">in</tt> <tt class="py-name">fractions</tt><tt class="py-op">.</tt><tt id="link-234" class="py-name"><a title="pyspark.rdd.RDD.values" class="py-name" href="#" onclick="return doclink('link-234', 'values', 'link-97');">values</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1727"></a><tt class="py-lineno">1727</tt>  <tt class="py-line">            <tt class="py-keyword">assert</tt> <tt class="py-name">fraction</tt> <tt class="py-op">&gt;=</tt> <tt class="py-number">0.0</tt><tt class="py-op">,</tt> <tt class="py-string">"Negative fraction value: %s"</tt> <tt class="py-op">%</tt> <tt class="py-name">fraction</tt> </tt>
<a name="L1728"></a><tt class="py-lineno">1728</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-235" class="py-name"><a title="pyspark.rdd.RDD.mapPartitionsWithIndex
pyspark.sql.SchemaRDD.mapPartitionsWithIndex" class="py-name" href="#" onclick="return doclink('link-235', 'mapPartitionsWithIndex', 'link-50');">mapPartitionsWithIndex</a></tt><tt class="py-op">(</tt> </tt>
<a name="L1729"></a><tt class="py-lineno">1729</tt>  <tt class="py-line">            <tt class="py-name">RDDStratifiedSampler</tt><tt class="py-op">(</tt><tt class="py-name">withReplacement</tt><tt class="py-op">,</tt> <tt class="py-name">fractions</tt><tt class="py-op">,</tt> <tt class="py-name">seed</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">func</tt><tt class="py-op">,</tt> <tt class="py-name">True</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1730"></a><tt class="py-lineno">1730</tt>  <tt class="py-line"> </tt>
<a name="RDD.subtractByKey"></a><div id="RDD.subtractByKey-def"><a name="L1731"></a><tt class="py-lineno">1731</tt> <a class="py-toggle" href="#" id="RDD.subtractByKey-toggle" onclick="return toggle('RDD.subtractByKey');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#subtractByKey">subtractByKey</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.subtractByKey-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.subtractByKey-expanded"><a name="L1732"></a><tt class="py-lineno">1732</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1733"></a><tt class="py-lineno">1733</tt>  <tt class="py-line"><tt class="py-docstring">        Return each (key, value) pair in C{self} that has no pair with matching</tt> </tt>
<a name="L1734"></a><tt class="py-lineno">1734</tt>  <tt class="py-line"><tt class="py-docstring">        key in C{other}.</tt> </tt>
<a name="L1735"></a><tt class="py-lineno">1735</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1736"></a><tt class="py-lineno">1736</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 2)])</tt> </tt>
<a name="L1737"></a><tt class="py-lineno">1737</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; y = sc.parallelize([("a", 3), ("c", None)])</tt> </tt>
<a name="L1738"></a><tt class="py-lineno">1738</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(x.subtractByKey(y).collect())</tt> </tt>
<a name="L1739"></a><tt class="py-lineno">1739</tt>  <tt class="py-line"><tt class="py-docstring">        [('b', 4), ('b', 5)]</tt> </tt>
<a name="L1740"></a><tt class="py-lineno">1740</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1741"></a><tt class="py-lineno">1741</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">filter_func</tt><tt class="py-op">(</tt><tt class="py-op">(</tt><tt class="py-param">key</tt><tt class="py-op">,</tt> <tt class="py-param">vals</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1742"></a><tt class="py-lineno">1742</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">vals</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt><tt class="py-op">)</tt> <tt class="py-op">&gt;</tt> <tt class="py-number">0</tt> <tt class="py-keyword">and</tt> <tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">vals</tt><tt class="py-op">[</tt><tt class="py-number">1</tt><tt class="py-op">]</tt><tt class="py-op">)</tt> <tt class="py-op">==</tt> <tt class="py-number">0</tt> </tt>
</div><a name="L1743"></a><tt class="py-lineno">1743</tt>  <tt class="py-line">        <tt class="py-name">map_func</tt> <tt class="py-op">=</tt> <tt class="py-keyword">lambda</tt> <tt class="py-op">(</tt><tt class="py-name">key</tt><tt class="py-op">,</tt> <tt class="py-name">vals</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> <tt class="py-op">[</tt><tt class="py-op">(</tt><tt class="py-name">key</tt><tt class="py-op">,</tt> <tt class="py-name">val</tt><tt class="py-op">)</tt> <tt class="py-keyword">for</tt> <tt class="py-name">val</tt> <tt class="py-keyword">in</tt> <tt class="py-name">vals</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt><tt class="py-op">]</tt> </tt>
<a name="L1744"></a><tt class="py-lineno">1744</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-236" class="py-name"><a title="pyspark.rdd.RDD.cogroup" class="py-name" href="#" onclick="return doclink('link-236', 'cogroup', 'link-77');">cogroup</a></tt><tt class="py-op">(</tt><tt class="py-name">other</tt><tt class="py-op">,</tt> <tt class="py-name">numPartitions</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-237" class="py-name"><a title="pyspark.rdd.RDD.filter" class="py-name" href="#" onclick="return doclink('link-237', 'filter', 'link-79');">filter</a></tt><tt class="py-op">(</tt><tt class="py-name">filter_func</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-238" class="py-name"><a title="pyspark.rdd.RDD.flatMap" class="py-name" href="#" onclick="return doclink('link-238', 'flatMap', 'link-232');">flatMap</a></tt><tt class="py-op">(</tt><tt class="py-name">map_func</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1745"></a><tt class="py-lineno">1745</tt>  <tt class="py-line"> </tt>
<a name="RDD.subtract"></a><div id="RDD.subtract-def"><a name="L1746"></a><tt class="py-lineno">1746</tt> <a class="py-toggle" href="#" id="RDD.subtract-toggle" onclick="return toggle('RDD.subtract');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#subtract">subtract</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">=</tt><tt class="py-name">None</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.subtract-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.subtract-expanded"><a name="L1747"></a><tt class="py-lineno">1747</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1748"></a><tt class="py-lineno">1748</tt>  <tt class="py-line"><tt class="py-docstring">        Return each value in C{self} that is not contained in C{other}.</tt> </tt>
<a name="L1749"></a><tt class="py-lineno">1749</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1750"></a><tt class="py-lineno">1750</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 3)])</tt> </tt>
<a name="L1751"></a><tt class="py-lineno">1751</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; y = sc.parallelize([("a", 3), ("c", None)])</tt> </tt>
<a name="L1752"></a><tt class="py-lineno">1752</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sorted(x.subtract(y).collect())</tt> </tt>
<a name="L1753"></a><tt class="py-lineno">1753</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 1), ('b', 4), ('b', 5)]</tt> </tt>
<a name="L1754"></a><tt class="py-lineno">1754</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1755"></a><tt class="py-lineno">1755</tt>  <tt class="py-line">        <tt class="py-comment"># note: here 'True' is just a placeholder</tt> </tt>
<a name="L1756"></a><tt class="py-lineno">1756</tt>  <tt class="py-line">        <tt id="link-239" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-239', 'rdd', 'link-43');">rdd</a></tt> <tt class="py-op">=</tt> <tt class="py-name">other</tt><tt class="py-op">.</tt><tt id="link-240" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-240', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">True</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L1757"></a><tt class="py-lineno">1757</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-241" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-241', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">,</tt> <tt class="py-name">True</tt><tt class="py-op">)</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-242" class="py-name" targets="Method pyspark.rdd.RDD.subtractByKey()=pyspark.rdd.RDD-class.html#subtractByKey"><a title="pyspark.rdd.RDD.subtractByKey" class="py-name" href="#" onclick="return doclink('link-242', 'subtractByKey', 'link-242');">subtractByKey</a></tt><tt class="py-op">(</tt><tt id="link-243" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-243', 'rdd', 'link-43');">rdd</a></tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-244" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-244', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">tpl</tt><tt class="py-op">:</tt> <tt class="py-name">tpl</tt><tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1758"></a><tt class="py-lineno">1758</tt>  <tt class="py-line"> </tt>
<a name="RDD.keyBy"></a><div id="RDD.keyBy-def"><a name="L1759"></a><tt class="py-lineno">1759</tt> <a class="py-toggle" href="#" id="RDD.keyBy-toggle" onclick="return toggle('RDD.keyBy');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#keyBy">keyBy</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">f</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.keyBy-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.keyBy-expanded"><a name="L1760"></a><tt class="py-lineno">1760</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1761"></a><tt class="py-lineno">1761</tt>  <tt class="py-line"><tt class="py-docstring">        Creates tuples of the elements in this RDD by applying C{f}.</tt> </tt>
<a name="L1762"></a><tt class="py-lineno">1762</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1763"></a><tt class="py-lineno">1763</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)</tt> </tt>
<a name="L1764"></a><tt class="py-lineno">1764</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; y = sc.parallelize(zip(range(0,5), range(0,5)))</tt> </tt>
<a name="L1765"></a><tt class="py-lineno">1765</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; map((lambda (x,y): (x, (list(y[0]), (list(y[1]))))), sorted(x.cogroup(y).collect()))</tt> </tt>
<a name="L1766"></a><tt class="py-lineno">1766</tt>  <tt class="py-line"><tt class="py-docstring">        [(0, ([0], [0])), (1, ([1], [1])), (2, ([], [2])), (3, ([], [3])), (4, ([2], [4]))]</tt> </tt>
<a name="L1767"></a><tt class="py-lineno">1767</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1768"></a><tt class="py-lineno">1768</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-245" class="py-name"><a title="pyspark.rdd.RDD.map" class="py-name" href="#" onclick="return doclink('link-245', 'map', 'link-55');">map</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">x</tt><tt class="py-op">:</tt> <tt class="py-op">(</tt><tt class="py-name">f</tt><tt class="py-op">(</tt><tt class="py-name">x</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> <tt class="py-name">x</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1769"></a><tt class="py-lineno">1769</tt>  <tt class="py-line"> </tt>
<a name="RDD.repartition"></a><div id="RDD.repartition-def"><a name="L1770"></a><tt class="py-lineno">1770</tt> <a class="py-toggle" href="#" id="RDD.repartition-toggle" onclick="return toggle('RDD.repartition');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#repartition">repartition</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.repartition-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.repartition-expanded"><a name="L1771"></a><tt class="py-lineno">1771</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1772"></a><tt class="py-lineno">1772</tt>  <tt class="py-line"><tt class="py-docstring">         Return a new RDD that has exactly numPartitions partitions.</tt> </tt>
<a name="L1773"></a><tt class="py-lineno">1773</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1774"></a><tt class="py-lineno">1774</tt>  <tt class="py-line"><tt class="py-docstring">         Can increase or decrease the level of parallelism in this RDD.</tt> </tt>
<a name="L1775"></a><tt class="py-lineno">1775</tt>  <tt class="py-line"><tt class="py-docstring">         Internally, this uses a shuffle to redistribute data.</tt> </tt>
<a name="L1776"></a><tt class="py-lineno">1776</tt>  <tt class="py-line"><tt class="py-docstring">         If you are decreasing the number of partitions in this RDD, consider</tt> </tt>
<a name="L1777"></a><tt class="py-lineno">1777</tt>  <tt class="py-line"><tt class="py-docstring">         using `coalesce`, which can avoid performing a shuffle.</tt> </tt>
<a name="L1778"></a><tt class="py-lineno">1778</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1779"></a><tt class="py-lineno">1779</tt>  <tt class="py-line"><tt class="py-docstring">         &gt;&gt;&gt; rdd = sc.parallelize([1,2,3,4,5,6,7], 4)</tt> </tt>
<a name="L1780"></a><tt class="py-lineno">1780</tt>  <tt class="py-line"><tt class="py-docstring">         &gt;&gt;&gt; sorted(rdd.glom().collect())</tt> </tt>
<a name="L1781"></a><tt class="py-lineno">1781</tt>  <tt class="py-line"><tt class="py-docstring">         [[1], [2, 3], [4, 5], [6, 7]]</tt> </tt>
<a name="L1782"></a><tt class="py-lineno">1782</tt>  <tt class="py-line"><tt class="py-docstring">         &gt;&gt;&gt; len(rdd.repartition(2).glom().collect())</tt> </tt>
<a name="L1783"></a><tt class="py-lineno">1783</tt>  <tt class="py-line"><tt class="py-docstring">         2</tt> </tt>
<a name="L1784"></a><tt class="py-lineno">1784</tt>  <tt class="py-line"><tt class="py-docstring">         &gt;&gt;&gt; len(rdd.repartition(10).glom().collect())</tt> </tt>
<a name="L1785"></a><tt class="py-lineno">1785</tt>  <tt class="py-line"><tt class="py-docstring">         10</tt> </tt>
<a name="L1786"></a><tt class="py-lineno">1786</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1787"></a><tt class="py-lineno">1787</tt>  <tt class="py-line">        <tt class="py-name">jrdd</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-246" class="py-name" targets="Method pyspark.rdd.RDD.repartition()=pyspark.rdd.RDD-class.html#repartition,Method pyspark.sql.SchemaRDD.repartition()=pyspark.sql.SchemaRDD-class.html#repartition"><a title="pyspark.rdd.RDD.repartition
pyspark.sql.SchemaRDD.repartition" class="py-name" href="#" onclick="return doclink('link-246', 'repartition', 'link-246');">repartition</a></tt><tt class="py-op">(</tt><tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
<a name="L1788"></a><tt class="py-lineno">1788</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt id="link-247" class="py-name"><a title="pyspark.rdd.RDD" class="py-name" href="#" onclick="return doclink('link-247', 'RDD', 'link-62');">RDD</a></tt><tt class="py-op">(</tt><tt class="py-name">jrdd</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1789"></a><tt class="py-lineno">1789</tt>  <tt class="py-line"> </tt>
<a name="RDD.coalesce"></a><div id="RDD.coalesce-def"><a name="L1790"></a><tt class="py-lineno">1790</tt> <a class="py-toggle" href="#" id="RDD.coalesce-toggle" onclick="return toggle('RDD.coalesce');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#coalesce">coalesce</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">numPartitions</tt><tt class="py-op">,</tt> <tt class="py-param">shuffle</tt><tt class="py-op">=</tt><tt class="py-name">False</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.coalesce-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.coalesce-expanded"><a name="L1791"></a><tt class="py-lineno">1791</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1792"></a><tt class="py-lineno">1792</tt>  <tt class="py-line"><tt class="py-docstring">        Return a new RDD that is reduced into `numPartitions` partitions.</tt> </tt>
<a name="L1793"></a><tt class="py-lineno">1793</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1794"></a><tt class="py-lineno">1794</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()</tt> </tt>
<a name="L1795"></a><tt class="py-lineno">1795</tt>  <tt class="py-line"><tt class="py-docstring">        [[1], [2, 3], [4, 5]]</tt> </tt>
<a name="L1796"></a><tt class="py-lineno">1796</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()</tt> </tt>
<a name="L1797"></a><tt class="py-lineno">1797</tt>  <tt class="py-line"><tt class="py-docstring">        [[1, 2, 3, 4, 5]]</tt> </tt>
<a name="L1798"></a><tt class="py-lineno">1798</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1799"></a><tt class="py-lineno">1799</tt>  <tt class="py-line">        <tt class="py-name">jrdd</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-248" class="py-name"><a title="pyspark.rdd.RDD.coalesce
pyspark.sql.SchemaRDD.coalesce" class="py-name" href="#" onclick="return doclink('link-248', 'coalesce', 'link-85');">coalesce</a></tt><tt class="py-op">(</tt><tt class="py-name">numPartitions</tt><tt class="py-op">)</tt> </tt>
<a name="L1800"></a><tt class="py-lineno">1800</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt id="link-249" class="py-name"><a title="pyspark.rdd.RDD" class="py-name" href="#" onclick="return doclink('link-249', 'RDD', 'link-62');">RDD</a></tt><tt class="py-op">(</tt><tt class="py-name">jrdd</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1801"></a><tt class="py-lineno">1801</tt>  <tt class="py-line"> </tt>
<a name="RDD.zip"></a><div id="RDD.zip-def"><a name="L1802"></a><tt class="py-lineno">1802</tt> <a class="py-toggle" href="#" id="RDD.zip-toggle" onclick="return toggle('RDD.zip');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#zip">zip</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">other</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.zip-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.zip-expanded"><a name="L1803"></a><tt class="py-lineno">1803</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1804"></a><tt class="py-lineno">1804</tt>  <tt class="py-line"><tt class="py-docstring">        Zips this RDD with another one, returning key-value pairs with the</tt> </tt>
<a name="L1805"></a><tt class="py-lineno">1805</tt>  <tt class="py-line"><tt class="py-docstring">        first element in each RDD second element in each RDD, etc. Assumes</tt> </tt>
<a name="L1806"></a><tt class="py-lineno">1806</tt>  <tt class="py-line"><tt class="py-docstring">        that the two RDDs have the same number of partitions and the same</tt> </tt>
<a name="L1807"></a><tt class="py-lineno">1807</tt>  <tt class="py-line"><tt class="py-docstring">        number of elements in each partition (e.g. one was made through</tt> </tt>
<a name="L1808"></a><tt class="py-lineno">1808</tt>  <tt class="py-line"><tt class="py-docstring">        a map on the other).</tt> </tt>
<a name="L1809"></a><tt class="py-lineno">1809</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1810"></a><tt class="py-lineno">1810</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x = sc.parallelize(range(0,5))</tt> </tt>
<a name="L1811"></a><tt class="py-lineno">1811</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; y = sc.parallelize(range(1000, 1005))</tt> </tt>
<a name="L1812"></a><tt class="py-lineno">1812</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; x.zip(y).collect()</tt> </tt>
<a name="L1813"></a><tt class="py-lineno">1813</tt>  <tt class="py-line"><tt class="py-docstring">        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]</tt> </tt>
<a name="L1814"></a><tt class="py-lineno">1814</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1815"></a><tt class="py-lineno">1815</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-250" class="py-name"><a title="pyspark.rdd.RDD.getNumPartitions" class="py-name" href="#" onclick="return doclink('link-250', 'getNumPartitions', 'link-84');">getNumPartitions</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> <tt class="py-op">!=</tt> <tt class="py-name">other</tt><tt class="py-op">.</tt><tt id="link-251" class="py-name"><a title="pyspark.rdd.RDD.getNumPartitions" class="py-name" href="#" onclick="return doclink('link-251', 'getNumPartitions', 'link-84');">getNumPartitions</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1816"></a><tt class="py-lineno">1816</tt>  <tt class="py-line">            <tt class="py-keyword">raise</tt> <tt class="py-name">ValueError</tt><tt class="py-op">(</tt><tt class="py-string">"Can only zip with RDD which has the same number of partitions"</tt><tt class="py-op">)</tt> </tt>
<a name="L1817"></a><tt class="py-lineno">1817</tt>  <tt class="py-line"> </tt>
<a name="L1818"></a><tt class="py-lineno">1818</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">get_batch_size</tt><tt class="py-op">(</tt><tt class="py-param">ser</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1819"></a><tt class="py-lineno">1819</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">ser</tt><tt class="py-op">,</tt> <tt class="py-name">BatchedSerializer</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1820"></a><tt class="py-lineno">1820</tt>  <tt class="py-line">                <tt class="py-keyword">return</tt> <tt class="py-name">ser</tt><tt class="py-op">.</tt><tt class="py-name">batchSize</tt> </tt>
<a name="L1821"></a><tt class="py-lineno">1821</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-number">0</tt> </tt>
</div><a name="L1822"></a><tt class="py-lineno">1822</tt>  <tt class="py-line"> </tt>
<a name="L1823"></a><tt class="py-lineno">1823</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">batch_as</tt><tt class="py-op">(</tt><tt class="py-param">rdd</tt><tt class="py-op">,</tt> <tt class="py-param">batchSize</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1824"></a><tt class="py-lineno">1824</tt>  <tt class="py-line">            <tt class="py-name">ser</tt> <tt class="py-op">=</tt> <tt id="link-252" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-252', 'rdd', 'link-43');">rdd</a></tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt> </tt>
<a name="L1825"></a><tt class="py-lineno">1825</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">ser</tt><tt class="py-op">,</tt> <tt class="py-name">BatchedSerializer</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1826"></a><tt class="py-lineno">1826</tt>  <tt class="py-line">                <tt class="py-name">ser</tt> <tt class="py-op">=</tt> <tt class="py-name">ser</tt><tt class="py-op">.</tt><tt class="py-name">serializer</tt> </tt>
<a name="L1827"></a><tt class="py-lineno">1827</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt id="link-253" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-253', 'rdd', 'link-43');">rdd</a></tt><tt class="py-op">.</tt><tt class="py-name">_reserialize</tt><tt class="py-op">(</tt><tt class="py-name">BatchedSerializer</tt><tt class="py-op">(</tt><tt class="py-name">ser</tt><tt class="py-op">,</tt> <tt class="py-name">batchSize</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1828"></a><tt class="py-lineno">1828</tt>  <tt class="py-line"> </tt>
<a name="L1829"></a><tt class="py-lineno">1829</tt>  <tt class="py-line">        <tt class="py-name">my_batch</tt> <tt class="py-op">=</tt> <tt class="py-name">get_batch_size</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">)</tt> </tt>
<a name="L1830"></a><tt class="py-lineno">1830</tt>  <tt class="py-line">        <tt class="py-name">other_batch</tt> <tt class="py-op">=</tt> <tt class="py-name">get_batch_size</tt><tt class="py-op">(</tt><tt class="py-name">other</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">)</tt> </tt>
<a name="L1831"></a><tt class="py-lineno">1831</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">my_batch</tt> <tt class="py-op">!=</tt> <tt class="py-name">other_batch</tt><tt class="py-op">:</tt> </tt>
<a name="L1832"></a><tt class="py-lineno">1832</tt>  <tt class="py-line">            <tt class="py-comment"># use the greatest batchSize to batch the other one.</tt> </tt>
<a name="L1833"></a><tt class="py-lineno">1833</tt>  <tt class="py-line">            <tt class="py-keyword">if</tt> <tt class="py-name">my_batch</tt> <tt class="py-op">&gt;</tt> <tt class="py-name">other_batch</tt><tt class="py-op">:</tt> </tt>
<a name="L1834"></a><tt class="py-lineno">1834</tt>  <tt class="py-line">                <tt class="py-name">other</tt> <tt class="py-op">=</tt> <tt class="py-name">batch_as</tt><tt class="py-op">(</tt><tt class="py-name">other</tt><tt class="py-op">,</tt> <tt class="py-name">my_batch</tt><tt class="py-op">)</tt> </tt>
<a name="L1835"></a><tt class="py-lineno">1835</tt>  <tt class="py-line">            <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L1836"></a><tt class="py-lineno">1836</tt>  <tt class="py-line">                <tt class="py-name">self</tt> <tt class="py-op">=</tt> <tt class="py-name">batch_as</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">,</tt> <tt class="py-name">other_batch</tt><tt class="py-op">)</tt> </tt>
<a name="L1837"></a><tt class="py-lineno">1837</tt>  <tt class="py-line"> </tt>
<a name="L1838"></a><tt class="py-lineno">1838</tt>  <tt class="py-line">        <tt class="py-comment"># There will be an Exception in JVM if there are different number</tt> </tt>
<a name="L1839"></a><tt class="py-lineno">1839</tt>  <tt class="py-line">        <tt class="py-comment"># of items in each partitions.</tt> </tt>
<a name="L1840"></a><tt class="py-lineno">1840</tt>  <tt class="py-line">        <tt class="py-name">pairRDD</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-254" class="py-name"><a title="pyspark.rdd.RDD.zip" class="py-name" href="#" onclick="return doclink('link-254', 'zip', 'link-149');">zip</a></tt><tt class="py-op">(</tt><tt class="py-name">other</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">)</tt> </tt>
<a name="L1841"></a><tt class="py-lineno">1841</tt>  <tt class="py-line">        <tt class="py-name">deserializer</tt> <tt class="py-op">=</tt> <tt class="py-name">PairDeserializer</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">,</tt> </tt>
<a name="L1842"></a><tt class="py-lineno">1842</tt>  <tt class="py-line">                                        <tt class="py-name">other</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">)</tt> </tt>
<a name="L1843"></a><tt class="py-lineno">1843</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt id="link-255" class="py-name"><a title="pyspark.rdd.RDD" class="py-name" href="#" onclick="return doclink('link-255', 'RDD', 'link-62');">RDD</a></tt><tt class="py-op">(</tt><tt class="py-name">pairRDD</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">,</tt> <tt class="py-name">deserializer</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1844"></a><tt class="py-lineno">1844</tt>  <tt class="py-line"> </tt>
<a name="RDD.zipWithIndex"></a><div id="RDD.zipWithIndex-def"><a name="L1845"></a><tt class="py-lineno">1845</tt> <a class="py-toggle" href="#" id="RDD.zipWithIndex-toggle" onclick="return toggle('RDD.zipWithIndex');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#zipWithIndex">zipWithIndex</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.zipWithIndex-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.zipWithIndex-expanded"><a name="L1846"></a><tt class="py-lineno">1846</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1847"></a><tt class="py-lineno">1847</tt>  <tt class="py-line"><tt class="py-docstring">        Zips this RDD with its element indices.</tt> </tt>
<a name="L1848"></a><tt class="py-lineno">1848</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1849"></a><tt class="py-lineno">1849</tt>  <tt class="py-line"><tt class="py-docstring">        The ordering is first based on the partition index and then the</tt> </tt>
<a name="L1850"></a><tt class="py-lineno">1850</tt>  <tt class="py-line"><tt class="py-docstring">        ordering of items within each partition. So the first item in</tt> </tt>
<a name="L1851"></a><tt class="py-lineno">1851</tt>  <tt class="py-line"><tt class="py-docstring">        the first partition gets index 0, and the last item in the last</tt> </tt>
<a name="L1852"></a><tt class="py-lineno">1852</tt>  <tt class="py-line"><tt class="py-docstring">        partition receives the largest index.</tt> </tt>
<a name="L1853"></a><tt class="py-lineno">1853</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1854"></a><tt class="py-lineno">1854</tt>  <tt class="py-line"><tt class="py-docstring">        This method needs to trigger a spark job when this RDD contains</tt> </tt>
<a name="L1855"></a><tt class="py-lineno">1855</tt>  <tt class="py-line"><tt class="py-docstring">        more than one partitions.</tt> </tt>
<a name="L1856"></a><tt class="py-lineno">1856</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1857"></a><tt class="py-lineno">1857</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(["a", "b", "c", "d"], 3).zipWithIndex().collect()</tt> </tt>
<a name="L1858"></a><tt class="py-lineno">1858</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 0), ('b', 1), ('c', 2), ('d', 3)]</tt> </tt>
<a name="L1859"></a><tt class="py-lineno">1859</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1860"></a><tt class="py-lineno">1860</tt>  <tt class="py-line">        <tt class="py-name">starts</tt> <tt class="py-op">=</tt> <tt class="py-op">[</tt><tt class="py-number">0</tt><tt class="py-op">]</tt> </tt>
<a name="L1861"></a><tt class="py-lineno">1861</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-256" class="py-name"><a title="pyspark.rdd.RDD.getNumPartitions" class="py-name" href="#" onclick="return doclink('link-256', 'getNumPartitions', 'link-84');">getNumPartitions</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> <tt class="py-op">&gt;</tt> <tt class="py-number">1</tt><tt class="py-op">:</tt> </tt>
<a name="L1862"></a><tt class="py-lineno">1862</tt>  <tt class="py-line">            <tt class="py-name">nums</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-257" class="py-name"><a title="pyspark.rdd.RDD.mapPartitions" class="py-name" href="#" onclick="return doclink('link-257', 'mapPartitions', 'link-54');">mapPartitions</a></tt><tt class="py-op">(</tt><tt class="py-keyword">lambda</tt> <tt class="py-name">it</tt><tt class="py-op">:</tt> <tt class="py-op">[</tt><tt id="link-258" class="py-name"><a title="pyspark.rdd.RDD.sum
pyspark.statcounter.StatCounter.sum" class="py-name" href="#" onclick="return doclink('link-258', 'sum', 'link-130');">sum</a></tt><tt class="py-op">(</tt><tt class="py-number">1</tt> <tt class="py-keyword">for</tt> <tt class="py-name">i</tt> <tt class="py-keyword">in</tt> <tt class="py-name">it</tt><tt class="py-op">)</tt><tt class="py-op">]</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-259" class="py-name"><a title="pyspark.rdd.RDD.collect
pyspark.sql.SchemaRDD.collect" class="py-name" href="#" onclick="return doclink('link-259', 'collect', 'link-60');">collect</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1863"></a><tt class="py-lineno">1863</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">i</tt> <tt class="py-keyword">in</tt> <tt class="py-name">range</tt><tt class="py-op">(</tt><tt class="py-name">len</tt><tt class="py-op">(</tt><tt class="py-name">nums</tt><tt class="py-op">)</tt> <tt class="py-op">-</tt> <tt class="py-number">1</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1864"></a><tt class="py-lineno">1864</tt>  <tt class="py-line">                <tt class="py-name">starts</tt><tt class="py-op">.</tt><tt class="py-name">append</tt><tt class="py-op">(</tt><tt class="py-name">starts</tt><tt class="py-op">[</tt><tt class="py-op">-</tt><tt class="py-number">1</tt><tt class="py-op">]</tt> <tt class="py-op">+</tt> <tt class="py-name">nums</tt><tt class="py-op">[</tt><tt class="py-name">i</tt><tt class="py-op">]</tt><tt class="py-op">)</tt> </tt>
<a name="L1865"></a><tt class="py-lineno">1865</tt>  <tt class="py-line"> </tt>
<a name="L1866"></a><tt class="py-lineno">1866</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">k</tt><tt class="py-op">,</tt> <tt class="py-param">it</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1867"></a><tt class="py-lineno">1867</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">i</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt> <tt class="py-keyword">in</tt> <tt class="py-name">enumerate</tt><tt class="py-op">(</tt><tt class="py-name">it</tt><tt class="py-op">,</tt> <tt class="py-name">starts</tt><tt class="py-op">[</tt><tt class="py-name">k</tt><tt class="py-op">]</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1868"></a><tt class="py-lineno">1868</tt>  <tt class="py-line">                <tt class="py-keyword">yield</tt> <tt class="py-name">v</tt><tt class="py-op">,</tt> <tt class="py-name">i</tt> </tt>
</div><a name="L1869"></a><tt class="py-lineno">1869</tt>  <tt class="py-line"> </tt>
<a name="L1870"></a><tt class="py-lineno">1870</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-260" class="py-name"><a title="pyspark.rdd.RDD.mapPartitionsWithIndex
pyspark.sql.SchemaRDD.mapPartitionsWithIndex" class="py-name" href="#" onclick="return doclink('link-260', 'mapPartitionsWithIndex', 'link-50');">mapPartitionsWithIndex</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1871"></a><tt class="py-lineno">1871</tt>  <tt class="py-line"> </tt>
<a name="RDD.zipWithUniqueId"></a><div id="RDD.zipWithUniqueId-def"><a name="L1872"></a><tt class="py-lineno">1872</tt> <a class="py-toggle" href="#" id="RDD.zipWithUniqueId-toggle" onclick="return toggle('RDD.zipWithUniqueId');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#zipWithUniqueId">zipWithUniqueId</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.zipWithUniqueId-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.zipWithUniqueId-expanded"><a name="L1873"></a><tt class="py-lineno">1873</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1874"></a><tt class="py-lineno">1874</tt>  <tt class="py-line"><tt class="py-docstring">        Zips this RDD with generated unique Long ids.</tt> </tt>
<a name="L1875"></a><tt class="py-lineno">1875</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1876"></a><tt class="py-lineno">1876</tt>  <tt class="py-line"><tt class="py-docstring">        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where</tt> </tt>
<a name="L1877"></a><tt class="py-lineno">1877</tt>  <tt class="py-line"><tt class="py-docstring">        n is the number of partitions. So there may exist gaps, but this</tt> </tt>
<a name="L1878"></a><tt class="py-lineno">1878</tt>  <tt class="py-line"><tt class="py-docstring">        method won't trigger a spark job, which is different from</tt> </tt>
<a name="L1879"></a><tt class="py-lineno">1879</tt>  <tt class="py-line"><tt class="py-docstring">        L{zipWithIndex}</tt> </tt>
<a name="L1880"></a><tt class="py-lineno">1880</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1881"></a><tt class="py-lineno">1881</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; sc.parallelize(["a", "b", "c", "d", "e"], 3).zipWithUniqueId().collect()</tt> </tt>
<a name="L1882"></a><tt class="py-lineno">1882</tt>  <tt class="py-line"><tt class="py-docstring">        [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]</tt> </tt>
<a name="L1883"></a><tt class="py-lineno">1883</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1884"></a><tt class="py-lineno">1884</tt>  <tt class="py-line">        <tt class="py-name">n</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-261" class="py-name"><a title="pyspark.rdd.RDD.getNumPartitions" class="py-name" href="#" onclick="return doclink('link-261', 'getNumPartitions', 'link-84');">getNumPartitions</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1885"></a><tt class="py-lineno">1885</tt>  <tt class="py-line"> </tt>
<a name="L1886"></a><tt class="py-lineno">1886</tt>  <tt class="py-line">        <tt class="py-keyword">def</tt> <tt class="py-def-name">func</tt><tt class="py-op">(</tt><tt class="py-param">k</tt><tt class="py-op">,</tt> <tt class="py-param">it</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1887"></a><tt class="py-lineno">1887</tt>  <tt class="py-line">            <tt class="py-keyword">for</tt> <tt class="py-name">i</tt><tt class="py-op">,</tt> <tt class="py-name">v</tt> <tt class="py-keyword">in</tt> <tt class="py-name">enumerate</tt><tt class="py-op">(</tt><tt class="py-name">it</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1888"></a><tt class="py-lineno">1888</tt>  <tt class="py-line">                <tt class="py-keyword">yield</tt> <tt class="py-name">v</tt><tt class="py-op">,</tt> <tt class="py-name">i</tt> <tt class="py-op">*</tt> <tt class="py-name">n</tt> <tt class="py-op">+</tt> <tt class="py-name">k</tt> </tt>
</div><a name="L1889"></a><tt class="py-lineno">1889</tt>  <tt class="py-line"> </tt>
<a name="L1890"></a><tt class="py-lineno">1890</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-262" class="py-name"><a title="pyspark.rdd.RDD.mapPartitionsWithIndex
pyspark.sql.SchemaRDD.mapPartitionsWithIndex" class="py-name" href="#" onclick="return doclink('link-262', 'mapPartitionsWithIndex', 'link-50');">mapPartitionsWithIndex</a></tt><tt class="py-op">(</tt><tt class="py-name">func</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1891"></a><tt class="py-lineno">1891</tt>  <tt class="py-line"> </tt>
<a name="RDD.name"></a><div id="RDD.name-def"><a name="L1892"></a><tt class="py-lineno">1892</tt> <a class="py-toggle" href="#" id="RDD.name-toggle" onclick="return toggle('RDD.name');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#name">name</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.name-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.name-expanded"><a name="L1893"></a><tt class="py-lineno">1893</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1894"></a><tt class="py-lineno">1894</tt>  <tt class="py-line"><tt class="py-docstring">        Return the name of this RDD.</tt> </tt>
<a name="L1895"></a><tt class="py-lineno">1895</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1896"></a><tt class="py-lineno">1896</tt>  <tt class="py-line">        <tt class="py-name">name_</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-263" class="py-name"><a title="pyspark.rdd.RDD.name" class="py-name" href="#" onclick="return doclink('link-263', 'name', 'link-114');">name</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1897"></a><tt class="py-lineno">1897</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-keyword">not</tt> <tt class="py-name">name_</tt><tt class="py-op">:</tt> </tt>
<a name="L1898"></a><tt class="py-lineno">1898</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">None</tt> </tt>
<a name="L1899"></a><tt class="py-lineno">1899</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">name_</tt><tt class="py-op">.</tt><tt class="py-name">encode</tt><tt class="py-op">(</tt><tt class="py-string">'utf-8'</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1900"></a><tt class="py-lineno">1900</tt>  <tt class="py-line"> </tt>
<a name="RDD.setName"></a><div id="RDD.setName-def"><a name="L1901"></a><tt class="py-lineno">1901</tt> <a class="py-toggle" href="#" id="RDD.setName-toggle" onclick="return toggle('RDD.setName');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#setName">setName</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">name</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.setName-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.setName-expanded"><a name="L1902"></a><tt class="py-lineno">1902</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1903"></a><tt class="py-lineno">1903</tt>  <tt class="py-line"><tt class="py-docstring">        Assign a name to this RDD.</tt> </tt>
<a name="L1904"></a><tt class="py-lineno">1904</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1905"></a><tt class="py-lineno">1905</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd1 = sc.parallelize([1,2])</tt> </tt>
<a name="L1906"></a><tt class="py-lineno">1906</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd1.setName('RDD1')</tt> </tt>
<a name="L1907"></a><tt class="py-lineno">1907</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd1.name()</tt> </tt>
<a name="L1908"></a><tt class="py-lineno">1908</tt>  <tt class="py-line"><tt class="py-docstring">        'RDD1'</tt> </tt>
<a name="L1909"></a><tt class="py-lineno">1909</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1910"></a><tt class="py-lineno">1910</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-264" class="py-name" targets="Method pyspark.rdd.RDD.setName()=pyspark.rdd.RDD-class.html#setName"><a title="pyspark.rdd.RDD.setName" class="py-name" href="#" onclick="return doclink('link-264', 'setName', 'link-264');">setName</a></tt><tt class="py-op">(</tt><tt id="link-265" class="py-name"><a title="pyspark.rdd.RDD.name" class="py-name" href="#" onclick="return doclink('link-265', 'name', 'link-114');">name</a></tt><tt class="py-op">)</tt> </tt>
</div><a name="L1911"></a><tt class="py-lineno">1911</tt>  <tt class="py-line"> </tt>
<a name="RDD.toDebugString"></a><div id="RDD.toDebugString-def"><a name="L1912"></a><tt class="py-lineno">1912</tt> <a class="py-toggle" href="#" id="RDD.toDebugString-toggle" onclick="return toggle('RDD.toDebugString');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#toDebugString">toDebugString</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.toDebugString-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.toDebugString-expanded"><a name="L1913"></a><tt class="py-lineno">1913</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1914"></a><tt class="py-lineno">1914</tt>  <tt class="py-line"><tt class="py-docstring">        A description of this RDD and its recursive dependencies for debugging.</tt> </tt>
<a name="L1915"></a><tt class="py-lineno">1915</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1916"></a><tt class="py-lineno">1916</tt>  <tt class="py-line">        <tt class="py-name">debug_string</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-266" class="py-name" targets="Method pyspark.conf.SparkConf.toDebugString()=pyspark.conf.SparkConf-class.html#toDebugString,Method pyspark.rdd.RDD.toDebugString()=pyspark.rdd.RDD-class.html#toDebugString"><a title="pyspark.conf.SparkConf.toDebugString
pyspark.rdd.RDD.toDebugString" class="py-name" href="#" onclick="return doclink('link-266', 'toDebugString', 'link-266');">toDebugString</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1917"></a><tt class="py-lineno">1917</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-keyword">not</tt> <tt class="py-name">debug_string</tt><tt class="py-op">:</tt> </tt>
<a name="L1918"></a><tt class="py-lineno">1918</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">None</tt> </tt>
<a name="L1919"></a><tt class="py-lineno">1919</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">debug_string</tt><tt class="py-op">.</tt><tt class="py-name">encode</tt><tt class="py-op">(</tt><tt class="py-string">'utf-8'</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1920"></a><tt class="py-lineno">1920</tt>  <tt class="py-line"> </tt>
<a name="RDD.getStorageLevel"></a><div id="RDD.getStorageLevel-def"><a name="L1921"></a><tt class="py-lineno">1921</tt> <a class="py-toggle" href="#" id="RDD.getStorageLevel-toggle" onclick="return toggle('RDD.getStorageLevel');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#getStorageLevel">getStorageLevel</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD.getStorageLevel-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD.getStorageLevel-expanded"><a name="L1922"></a><tt class="py-lineno">1922</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1923"></a><tt class="py-lineno">1923</tt>  <tt class="py-line"><tt class="py-docstring">        Get the RDD's current storage level.</tt> </tt>
<a name="L1924"></a><tt class="py-lineno">1924</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1925"></a><tt class="py-lineno">1925</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd1 = sc.parallelize([1,2])</tt> </tt>
<a name="L1926"></a><tt class="py-lineno">1926</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; rdd1.getStorageLevel()</tt> </tt>
<a name="L1927"></a><tt class="py-lineno">1927</tt>  <tt class="py-line"><tt class="py-docstring">        StorageLevel(False, False, False, False, 1)</tt> </tt>
<a name="L1928"></a><tt class="py-lineno">1928</tt>  <tt class="py-line"><tt class="py-docstring">        &gt;&gt;&gt; print(rdd1.getStorageLevel())</tt> </tt>
<a name="L1929"></a><tt class="py-lineno">1929</tt>  <tt class="py-line"><tt class="py-docstring">        Serialized 1x Replicated</tt> </tt>
<a name="L1930"></a><tt class="py-lineno">1930</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1931"></a><tt class="py-lineno">1931</tt>  <tt class="py-line">        <tt class="py-name">java_storage_level</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt><tt class="py-op">.</tt><tt id="link-267" class="py-name" targets="Method pyspark.rdd.RDD.getStorageLevel()=pyspark.rdd.RDD-class.html#getStorageLevel"><a title="pyspark.rdd.RDD.getStorageLevel" class="py-name" href="#" onclick="return doclink('link-267', 'getStorageLevel', 'link-267');">getStorageLevel</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L1932"></a><tt class="py-lineno">1932</tt>  <tt class="py-line">        <tt class="py-name">storage_level</tt> <tt class="py-op">=</tt> <tt id="link-268" class="py-name"><a title="pyspark.storagelevel.StorageLevel" class="py-name" href="#" onclick="return doclink('link-268', 'StorageLevel', 'link-14');">StorageLevel</a></tt><tt class="py-op">(</tt><tt class="py-name">java_storage_level</tt><tt class="py-op">.</tt><tt class="py-name">useDisk</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> </tt>
<a name="L1933"></a><tt class="py-lineno">1933</tt>  <tt class="py-line">                                     <tt class="py-name">java_storage_level</tt><tt class="py-op">.</tt><tt class="py-name">useMemory</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> </tt>
<a name="L1934"></a><tt class="py-lineno">1934</tt>  <tt class="py-line">                                     <tt class="py-name">java_storage_level</tt><tt class="py-op">.</tt><tt class="py-name">useOffHeap</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> </tt>
<a name="L1935"></a><tt class="py-lineno">1935</tt>  <tt class="py-line">                                     <tt class="py-name">java_storage_level</tt><tt class="py-op">.</tt><tt class="py-name">deserialized</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> </tt>
<a name="L1936"></a><tt class="py-lineno">1936</tt>  <tt class="py-line">                                     <tt class="py-name">java_storage_level</tt><tt class="py-op">.</tt><tt class="py-name">replication</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
<a name="L1937"></a><tt class="py-lineno">1937</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">storage_level</tt> </tt>
</div><a name="L1938"></a><tt class="py-lineno">1938</tt>  <tt class="py-line"> </tt>
<a name="RDD._defaultReducePartitions"></a><div id="RDD._defaultReducePartitions-def"><a name="L1939"></a><tt class="py-lineno">1939</tt> <a class="py-toggle" href="#" id="RDD._defaultReducePartitions-toggle" onclick="return toggle('RDD._defaultReducePartitions');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.RDD-class.html#_defaultReducePartitions">_defaultReducePartitions</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="RDD._defaultReducePartitions-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="RDD._defaultReducePartitions-expanded"><a name="L1940"></a><tt class="py-lineno">1940</tt>  <tt class="py-line">        <tt class="py-docstring">"""</tt> </tt>
<a name="L1941"></a><tt class="py-lineno">1941</tt>  <tt class="py-line"><tt class="py-docstring">        Returns the default number of partitions to use during reduce tasks (e.g., groupBy).</tt> </tt>
<a name="L1942"></a><tt class="py-lineno">1942</tt>  <tt class="py-line"><tt class="py-docstring">        If spark.default.parallelism is set, then we'll use the value from SparkContext</tt> </tt>
<a name="L1943"></a><tt class="py-lineno">1943</tt>  <tt class="py-line"><tt class="py-docstring">        defaultParallelism, otherwise we'll use the number of partitions in this RDD.</tt> </tt>
<a name="L1944"></a><tt class="py-lineno">1944</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1945"></a><tt class="py-lineno">1945</tt>  <tt class="py-line"><tt class="py-docstring">        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce</tt> </tt>
<a name="L1946"></a><tt class="py-lineno">1946</tt>  <tt class="py-line"><tt class="py-docstring">        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will</tt> </tt>
<a name="L1947"></a><tt class="py-lineno">1947</tt>  <tt class="py-line"><tt class="py-docstring">        be inherent.</tt> </tt>
<a name="L1948"></a><tt class="py-lineno">1948</tt>  <tt class="py-line"><tt class="py-docstring">        """</tt> </tt>
<a name="L1949"></a><tt class="py-lineno">1949</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_conf</tt><tt class="py-op">.</tt><tt id="link-269" class="py-name" targets="Method pyspark.conf.SparkConf.contains()=pyspark.conf.SparkConf-class.html#contains"><a title="pyspark.conf.SparkConf.contains" class="py-name" href="#" onclick="return doclink('link-269', 'contains', 'link-269');">contains</a></tt><tt class="py-op">(</tt><tt class="py-string">"spark.default.parallelism"</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1950"></a><tt class="py-lineno">1950</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-270" class="py-name" targets="Method pyspark.context.SparkContext.defaultParallelism()=pyspark.context.SparkContext-class.html#defaultParallelism"><a title="pyspark.context.SparkContext.defaultParallelism" class="py-name" href="#" onclick="return doclink('link-270', 'defaultParallelism', 'link-270');">defaultParallelism</a></tt> </tt>
<a name="L1951"></a><tt class="py-lineno">1951</tt>  <tt class="py-line">        <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L1952"></a><tt class="py-lineno">1952</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt id="link-271" class="py-name"><a title="pyspark.rdd.RDD.getNumPartitions" class="py-name" href="#" onclick="return doclink('link-271', 'getNumPartitions', 'link-84');">getNumPartitions</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
</div></div><a name="L1953"></a><tt class="py-lineno">1953</tt>  <tt class="py-line"> </tt>
<a name="PipelinedRDD"></a><div id="PipelinedRDD-def"><a name="L1954"></a><tt class="py-lineno">1954</tt>  <tt class="py-line">    <tt class="py-comment"># TODO: `lookup` is disabled because we can't make direct comparisons based</tt> </tt>
<a name="L1955"></a><tt class="py-lineno">1955</tt>  <tt class="py-line">    <tt class="py-comment"># on the key; we need to compare the hash of the key to the hash of the</tt> </tt>
<a name="L1956"></a><tt class="py-lineno">1956</tt>  <tt class="py-line">    <tt class="py-comment"># keys in the pairs.  This could be an expensive operation, since those</tt> </tt>
<a name="L1957"></a><tt class="py-lineno">1957</tt>  <tt class="py-line">    <tt class="py-comment"># hashes aren't retained.</tt> </tt>
<a name="L1958"></a><tt class="py-lineno">1958</tt>  <tt class="py-line"> </tt>
<a name="L1959"></a><tt class="py-lineno">1959</tt>  <tt class="py-line"> </tt>
<a name="L1960"></a><tt class="py-lineno">1960</tt> <a class="py-toggle" href="#" id="PipelinedRDD-toggle" onclick="return toggle('PipelinedRDD');">-</a><tt class="py-line"><tt class="py-keyword">class</tt> <a class="py-def-name" href="pyspark.rdd.PipelinedRDD-class.html">PipelinedRDD</a><tt class="py-op">(</tt><tt class="py-base-class">RDD</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="PipelinedRDD-collapsed" style="display:none;" pad="++++" indent="++++"></div><div id="PipelinedRDD-expanded"><a name="L1961"></a><tt class="py-lineno">1961</tt>  <tt class="py-line"> </tt>
<a name="L1962"></a><tt class="py-lineno">1962</tt>  <tt class="py-line">    <tt class="py-docstring">"""</tt> </tt>
<a name="L1963"></a><tt class="py-lineno">1963</tt>  <tt class="py-line"><tt class="py-docstring">    Pipelined maps:</tt> </tt>
<a name="L1964"></a><tt class="py-lineno">1964</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1965"></a><tt class="py-lineno">1965</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4])</tt> </tt>
<a name="L1966"></a><tt class="py-lineno">1966</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()</tt> </tt>
<a name="L1967"></a><tt class="py-lineno">1967</tt>  <tt class="py-line"><tt class="py-docstring">    [4, 8, 12, 16]</tt> </tt>
<a name="L1968"></a><tt class="py-lineno">1968</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).map(lambda x: 2 * x).collect()</tt> </tt>
<a name="L1969"></a><tt class="py-lineno">1969</tt>  <tt class="py-line"><tt class="py-docstring">    [4, 8, 12, 16]</tt> </tt>
<a name="L1970"></a><tt class="py-lineno">1970</tt>  <tt class="py-line"><tt class="py-docstring"></tt> </tt>
<a name="L1971"></a><tt class="py-lineno">1971</tt>  <tt class="py-line"><tt class="py-docstring">    Pipelined reduces:</tt> </tt>
<a name="L1972"></a><tt class="py-lineno">1972</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; from operator import add</tt> </tt>
<a name="L1973"></a><tt class="py-lineno">1973</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).reduce(add)</tt> </tt>
<a name="L1974"></a><tt class="py-lineno">1974</tt>  <tt class="py-line"><tt class="py-docstring">    20</tt> </tt>
<a name="L1975"></a><tt class="py-lineno">1975</tt>  <tt class="py-line"><tt class="py-docstring">    &gt;&gt;&gt; rdd.flatMap(lambda x: [x, x]).reduce(add)</tt> </tt>
<a name="L1976"></a><tt class="py-lineno">1976</tt>  <tt class="py-line"><tt class="py-docstring">    20</tt> </tt>
<a name="L1977"></a><tt class="py-lineno">1977</tt>  <tt class="py-line"><tt class="py-docstring">    """</tt> </tt>
<a name="L1978"></a><tt class="py-lineno">1978</tt>  <tt class="py-line"> </tt>
<a name="PipelinedRDD.__init__"></a><div id="PipelinedRDD.__init__-def"><a name="L1979"></a><tt class="py-lineno">1979</tt> <a class="py-toggle" href="#" id="PipelinedRDD.__init__-toggle" onclick="return toggle('PipelinedRDD.__init__');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.PipelinedRDD-class.html#__init__">__init__</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">,</tt> <tt class="py-param">prev</tt><tt class="py-op">,</tt> <tt class="py-param">func</tt><tt class="py-op">,</tt> <tt class="py-param">preservesPartitioning</tt><tt class="py-op">=</tt><tt class="py-name">False</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="PipelinedRDD.__init__-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="PipelinedRDD.__init__-expanded"><a name="L1980"></a><tt class="py-lineno">1980</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-keyword">not</tt> <tt class="py-name">isinstance</tt><tt class="py-op">(</tt><tt class="py-name">prev</tt><tt class="py-op">,</tt> <tt class="py-name">PipelinedRDD</tt><tt class="py-op">)</tt> <tt class="py-keyword">or</tt> <tt class="py-keyword">not</tt> <tt class="py-name">prev</tt><tt class="py-op">.</tt><tt class="py-name">_is_pipelinable</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1981"></a><tt class="py-lineno">1981</tt>  <tt class="py-line">            <tt class="py-comment"># This transformation is the first in its stage:</tt> </tt>
<a name="L1982"></a><tt class="py-lineno">1982</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">func</tt> <tt class="py-op">=</tt> <tt class="py-name">func</tt> </tt>
<a name="L1983"></a><tt class="py-lineno">1983</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">preservesPartitioning</tt> <tt class="py-op">=</tt> <tt class="py-name">preservesPartitioning</tt> </tt>
<a name="L1984"></a><tt class="py-lineno">1984</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_prev_jrdd</tt> <tt class="py-op">=</tt> <tt class="py-name">prev</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd</tt> </tt>
<a name="L1985"></a><tt class="py-lineno">1985</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_prev_jrdd_deserializer</tt> <tt class="py-op">=</tt> <tt class="py-name">prev</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt> </tt>
<a name="L1986"></a><tt class="py-lineno">1986</tt>  <tt class="py-line">        <tt class="py-keyword">else</tt><tt class="py-op">:</tt> </tt>
<a name="L1987"></a><tt class="py-lineno">1987</tt>  <tt class="py-line">            <tt class="py-name">prev_func</tt> <tt class="py-op">=</tt> <tt class="py-name">prev</tt><tt class="py-op">.</tt><tt class="py-name">func</tt> </tt>
<a name="L1988"></a><tt class="py-lineno">1988</tt>  <tt class="py-line"> </tt>
<a name="L1989"></a><tt class="py-lineno">1989</tt>  <tt class="py-line">            <tt class="py-keyword">def</tt> <tt class="py-def-name">pipeline_func</tt><tt class="py-op">(</tt><tt class="py-param">split</tt><tt class="py-op">,</tt> <tt class="py-param">iterator</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
<a name="L1990"></a><tt class="py-lineno">1990</tt>  <tt class="py-line">                <tt class="py-keyword">return</tt> <tt class="py-name">func</tt><tt class="py-op">(</tt><tt class="py-name">split</tt><tt class="py-op">,</tt> <tt class="py-name">prev_func</tt><tt class="py-op">(</tt><tt class="py-name">split</tt><tt class="py-op">,</tt> <tt class="py-name">iterator</tt><tt class="py-op">)</tt><tt class="py-op">)</tt> </tt>
</div><a name="L1991"></a><tt class="py-lineno">1991</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">func</tt> <tt class="py-op">=</tt> <tt class="py-name">pipeline_func</tt> </tt>
<a name="L1992"></a><tt class="py-lineno">1992</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">preservesPartitioning</tt> <tt class="py-op">=</tt> \ </tt>
<a name="L1993"></a><tt class="py-lineno">1993</tt>  <tt class="py-line">                <tt class="py-name">prev</tt><tt class="py-op">.</tt><tt class="py-name">preservesPartitioning</tt> <tt class="py-keyword">and</tt> <tt class="py-name">preservesPartitioning</tt> </tt>
<a name="L1994"></a><tt class="py-lineno">1994</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_prev_jrdd</tt> <tt class="py-op">=</tt> <tt class="py-name">prev</tt><tt class="py-op">.</tt><tt class="py-name">_prev_jrdd</tt>  <tt class="py-comment"># maintain the pipeline</tt> </tt>
<a name="L1995"></a><tt class="py-lineno">1995</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_prev_jrdd_deserializer</tt> <tt class="py-op">=</tt> <tt class="py-name">prev</tt><tt class="py-op">.</tt><tt class="py-name">_prev_jrdd_deserializer</tt> </tt>
<a name="L1996"></a><tt class="py-lineno">1996</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">is_cached</tt> <tt class="py-op">=</tt> <tt class="py-name">False</tt> </tt>
<a name="L1997"></a><tt class="py-lineno">1997</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">is_checkpointed</tt> <tt class="py-op">=</tt> <tt class="py-name">False</tt> </tt>
<a name="L1998"></a><tt class="py-lineno">1998</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt> <tt class="py-op">=</tt> <tt class="py-name">prev</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt> </tt>
<a name="L1999"></a><tt class="py-lineno">1999</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">prev</tt> <tt class="py-op">=</tt> <tt class="py-name">prev</tt> </tt>
<a name="L2000"></a><tt class="py-lineno">2000</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_val</tt> <tt class="py-op">=</tt> <tt class="py-name">None</tt> </tt>
<a name="L2001"></a><tt class="py-lineno">2001</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">serializer</tt> </tt>
<a name="L2002"></a><tt class="py-lineno">2002</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_bypass_serializer</tt> <tt class="py-op">=</tt> <tt class="py-name">False</tt> </tt>
</div><a name="L2003"></a><tt class="py-lineno">2003</tt>  <tt class="py-line"> </tt>
<a name="L2004"></a><tt class="py-lineno">2004</tt>  <tt class="py-line">    <tt class="py-decorator">@</tt><tt class="py-decorator">property</tt> </tt>
<a name="PipelinedRDD._jrdd"></a><div id="PipelinedRDD._jrdd-def"><a name="L2005"></a><tt class="py-lineno">2005</tt> <a class="py-toggle" href="#" id="PipelinedRDD._jrdd-toggle" onclick="return toggle('PipelinedRDD._jrdd');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.PipelinedRDD-class.html#_jrdd">_jrdd</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="PipelinedRDD._jrdd-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="PipelinedRDD._jrdd-expanded"><a name="L2006"></a><tt class="py-lineno">2006</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_val</tt><tt class="py-op">:</tt> </tt>
<a name="L2007"></a><tt class="py-lineno">2007</tt>  <tt class="py-line">            <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_val</tt> </tt>
<a name="L2008"></a><tt class="py-lineno">2008</tt>  <tt class="py-line">        <tt class="py-keyword">if</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_bypass_serializer</tt><tt class="py-op">:</tt> </tt>
<a name="L2009"></a><tt class="py-lineno">2009</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt> <tt class="py-op">=</tt> <tt class="py-name">NoOpSerializer</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L2010"></a><tt class="py-lineno">2010</tt>  <tt class="py-line">        <tt class="py-name">command</tt> <tt class="py-op">=</tt> <tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">func</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_prev_jrdd_deserializer</tt><tt class="py-op">,</tt> </tt>
<a name="L2011"></a><tt class="py-lineno">2011</tt>  <tt class="py-line">                   <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_deserializer</tt><tt class="py-op">)</tt> </tt>
<a name="L2012"></a><tt class="py-lineno">2012</tt>  <tt class="py-line">        <tt class="py-name">ser</tt> <tt class="py-op">=</tt> <tt class="py-name">CloudPickleSerializer</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L2013"></a><tt class="py-lineno">2013</tt>  <tt class="py-line">        <tt class="py-name">pickled_command</tt> <tt class="py-op">=</tt> <tt class="py-name">ser</tt><tt class="py-op">.</tt><tt id="link-272" class="py-name"><a title="pyspark.serializers.MarshalSerializer.dumps
pyspark.serializers.PickleSerializer.dumps" class="py-name" href="#" onclick="return doclink('link-272', 'dumps', 'link-205');">dumps</a></tt><tt class="py-op">(</tt><tt class="py-name">command</tt><tt class="py-op">)</tt> </tt>
<a name="L2014"></a><tt class="py-lineno">2014</tt>  <tt class="py-line">        <tt class="py-name">broadcast_vars</tt> <tt class="py-op">=</tt> <tt class="py-name">ListConverter</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">convert</tt><tt class="py-op">(</tt> </tt>
<a name="L2015"></a><tt class="py-lineno">2015</tt>  <tt class="py-line">            <tt class="py-op">[</tt><tt class="py-name">x</tt><tt class="py-op">.</tt><tt class="py-name">_jbroadcast</tt> <tt class="py-keyword">for</tt> <tt class="py-name">x</tt> <tt class="py-keyword">in</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_pickled_broadcast_vars</tt><tt class="py-op">]</tt><tt class="py-op">,</tt> </tt>
<a name="L2016"></a><tt class="py-lineno">2016</tt>  <tt class="py-line">            <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-273" class="py-name" targets="Variable pyspark.context.SparkContext._gateway=pyspark.context.SparkContext-class.html#_gateway"><a title="pyspark.context.SparkContext._gateway" class="py-name" href="#" onclick="return doclink('link-273', '_gateway', 'link-273');">_gateway</a></tt><tt class="py-op">.</tt><tt class="py-name">_gateway_client</tt><tt class="py-op">)</tt> </tt>
<a name="L2017"></a><tt class="py-lineno">2017</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_pickled_broadcast_vars</tt><tt class="py-op">.</tt><tt class="py-name">clear</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L2018"></a><tt class="py-lineno">2018</tt>  <tt class="py-line">        <tt class="py-name">env</tt> <tt class="py-op">=</tt> <tt class="py-name">MapConverter</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">convert</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">environment</tt><tt class="py-op">,</tt> </tt>
<a name="L2019"></a><tt class="py-lineno">2019</tt>  <tt class="py-line">                                     <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-274" class="py-name"><a title="pyspark.context.SparkContext._gateway" class="py-name" href="#" onclick="return doclink('link-274', '_gateway', 'link-273');">_gateway</a></tt><tt class="py-op">.</tt><tt class="py-name">_gateway_client</tt><tt class="py-op">)</tt> </tt>
<a name="L2020"></a><tt class="py-lineno">2020</tt>  <tt class="py-line">        <tt class="py-name">includes</tt> <tt class="py-op">=</tt> <tt class="py-name">ListConverter</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt class="py-name">convert</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-275" class="py-name" targets="Variable pyspark.context.SparkContext._python_includes=pyspark.context.SparkContext-class.html#_python_includes"><a title="pyspark.context.SparkContext._python_includes" class="py-name" href="#" onclick="return doclink('link-275', '_python_includes', 'link-275');">_python_includes</a></tt><tt class="py-op">,</tt> </tt>
<a name="L2021"></a><tt class="py-lineno">2021</tt>  <tt class="py-line">                                           <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-276" class="py-name"><a title="pyspark.context.SparkContext._gateway" class="py-name" href="#" onclick="return doclink('link-276', '_gateway', 'link-273');">_gateway</a></tt><tt class="py-op">.</tt><tt class="py-name">_gateway_client</tt><tt class="py-op">)</tt> </tt>
<a name="L2022"></a><tt class="py-lineno">2022</tt>  <tt class="py-line">        <tt class="py-name">python_rdd</tt> <tt class="py-op">=</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt id="link-277" class="py-name"><a title="pyspark.context.SparkContext._jvm" class="py-name" href="#" onclick="return doclink('link-277', '_jvm', 'link-176');">_jvm</a></tt><tt class="py-op">.</tt><tt class="py-name">PythonRDD</tt><tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_prev_jrdd</tt><tt class="py-op">.</tt><tt id="link-278" class="py-name"><a title="pyspark.rdd" class="py-name" href="#" onclick="return doclink('link-278', 'rdd', 'link-43');">rdd</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> </tt>
<a name="L2023"></a><tt class="py-lineno">2023</tt>  <tt class="py-line">                                             <tt class="py-name">bytearray</tt><tt class="py-op">(</tt><tt class="py-name">pickled_command</tt><tt class="py-op">)</tt><tt class="py-op">,</tt> </tt>
<a name="L2024"></a><tt class="py-lineno">2024</tt>  <tt class="py-line">                                             <tt class="py-name">env</tt><tt class="py-op">,</tt> <tt class="py-name">includes</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">preservesPartitioning</tt><tt class="py-op">,</tt> </tt>
<a name="L2025"></a><tt class="py-lineno">2025</tt>  <tt class="py-line">                                             <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">pythonExec</tt><tt class="py-op">,</tt> </tt>
<a name="L2026"></a><tt class="py-lineno">2026</tt>  <tt class="py-line">                                             <tt class="py-name">broadcast_vars</tt><tt class="py-op">,</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">ctx</tt><tt class="py-op">.</tt><tt class="py-name">_javaAccumulator</tt><tt class="py-op">)</tt> </tt>
<a name="L2027"></a><tt class="py-lineno">2027</tt>  <tt class="py-line">        <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_val</tt> <tt class="py-op">=</tt> <tt class="py-name">python_rdd</tt><tt class="py-op">.</tt><tt class="py-name">asJavaRDD</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L2028"></a><tt class="py-lineno">2028</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">_jrdd_val</tt> </tt>
</div><a name="L2029"></a><tt class="py-lineno">2029</tt>  <tt class="py-line"> </tt>
<a name="PipelinedRDD._is_pipelinable"></a><div id="PipelinedRDD._is_pipelinable-def"><a name="L2030"></a><tt class="py-lineno">2030</tt> <a class="py-toggle" href="#" id="PipelinedRDD._is_pipelinable-toggle" onclick="return toggle('PipelinedRDD._is_pipelinable');">-</a><tt class="py-line">    <tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd.PipelinedRDD-class.html#_is_pipelinable">_is_pipelinable</a><tt class="py-op">(</tt><tt class="py-param">self</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="PipelinedRDD._is_pipelinable-collapsed" style="display:none;" pad="++++" indent="++++++++"></div><div id="PipelinedRDD._is_pipelinable-expanded"><a name="L2031"></a><tt class="py-lineno">2031</tt>  <tt class="py-line">        <tt class="py-keyword">return</tt> <tt class="py-keyword">not</tt> <tt class="py-op">(</tt><tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">is_cached</tt> <tt class="py-keyword">or</tt> <tt class="py-name">self</tt><tt class="py-op">.</tt><tt class="py-name">is_checkpointed</tt><tt class="py-op">)</tt> </tt>
</div></div><a name="L2032"></a><tt class="py-lineno">2032</tt>  <tt class="py-line"> </tt>
<a name="_test"></a><div id="_test-def"><a name="L2033"></a><tt class="py-lineno">2033</tt>  <tt class="py-line"> </tt>
<a name="L2034"></a><tt class="py-lineno">2034</tt> <a class="py-toggle" href="#" id="_test-toggle" onclick="return toggle('_test');">-</a><tt class="py-line"><tt class="py-keyword">def</tt> <a class="py-def-name" href="pyspark.rdd-module.html#_test">_test</a><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">:</tt> </tt>
</div><div id="_test-collapsed" style="display:none;" pad="++++" indent="++++"></div><div id="_test-expanded"><a name="L2035"></a><tt class="py-lineno">2035</tt>  <tt class="py-line">    <tt class="py-keyword">import</tt> <tt class="py-name">doctest</tt> </tt>
<a name="L2036"></a><tt class="py-lineno">2036</tt>  <tt class="py-line">    <tt class="py-keyword">from</tt> <tt id="link-279" class="py-name"><a title="pyspark" class="py-name" href="#" onclick="return doclink('link-279', 'pyspark', 'link-3');">pyspark</a></tt><tt class="py-op">.</tt><tt id="link-280" class="py-name"><a title="pyspark.context
pyspark.rdd.RDD.context" class="py-name" href="#" onclick="return doclink('link-280', 'context', 'link-111');">context</a></tt> <tt class="py-keyword">import</tt> <tt id="link-281" class="py-name" targets="Class pyspark.context.SparkContext=pyspark.context.SparkContext-class.html"><a title="pyspark.context.SparkContext" class="py-name" href="#" onclick="return doclink('link-281', 'SparkContext', 'link-281');">SparkContext</a></tt> </tt>
<a name="L2037"></a><tt class="py-lineno">2037</tt>  <tt class="py-line">    <tt class="py-name">globs</tt> <tt class="py-op">=</tt> <tt class="py-name">globals</tt><tt class="py-op">(</tt><tt class="py-op">)</tt><tt class="py-op">.</tt><tt id="link-282" class="py-name"><a title="pyspark.statcounter.StatCounter.copy" class="py-name" href="#" onclick="return doclink('link-282', 'copy', 'link-0');">copy</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L2038"></a><tt class="py-lineno">2038</tt>  <tt class="py-line">    <tt class="py-comment"># The small batch size here ensures that we see multiple batches,</tt> </tt>
<a name="L2039"></a><tt class="py-lineno">2039</tt>  <tt class="py-line">    <tt class="py-comment"># even in these small test examples:</tt> </tt>
<a name="L2040"></a><tt class="py-lineno">2040</tt>  <tt class="py-line">    <tt class="py-name">globs</tt><tt class="py-op">[</tt><tt class="py-string">'sc'</tt><tt class="py-op">]</tt> <tt class="py-op">=</tt> <tt id="link-283" class="py-name"><a title="pyspark.context.SparkContext" class="py-name" href="#" onclick="return doclink('link-283', 'SparkContext', 'link-281');">SparkContext</a></tt><tt class="py-op">(</tt><tt class="py-string">'local[4]'</tt><tt class="py-op">,</tt> <tt class="py-string">'PythonTest'</tt><tt class="py-op">,</tt> <tt class="py-name">batchSize</tt><tt class="py-op">=</tt><tt class="py-number">2</tt><tt class="py-op">)</tt> </tt>
<a name="L2041"></a><tt class="py-lineno">2041</tt>  <tt class="py-line">    <tt class="py-op">(</tt><tt class="py-name">failure_count</tt><tt class="py-op">,</tt> <tt class="py-name">test_count</tt><tt class="py-op">)</tt> <tt class="py-op">=</tt> <tt class="py-name">doctest</tt><tt class="py-op">.</tt><tt class="py-name">testmod</tt><tt class="py-op">(</tt> </tt>
<a name="L2042"></a><tt class="py-lineno">2042</tt>  <tt class="py-line">        <tt class="py-name">globs</tt><tt class="py-op">=</tt><tt class="py-name">globs</tt><tt class="py-op">,</tt> <tt class="py-name">optionflags</tt><tt class="py-op">=</tt><tt class="py-name">doctest</tt><tt class="py-op">.</tt><tt class="py-name">ELLIPSIS</tt><tt class="py-op">)</tt> </tt>
<a name="L2043"></a><tt class="py-lineno">2043</tt>  <tt class="py-line">    <tt class="py-name">globs</tt><tt class="py-op">[</tt><tt class="py-string">'sc'</tt><tt class="py-op">]</tt><tt class="py-op">.</tt><tt id="link-284" class="py-name" targets="Method pyspark.context.SparkContext.stop()=pyspark.context.SparkContext-class.html#stop"><a title="pyspark.context.SparkContext.stop" class="py-name" href="#" onclick="return doclink('link-284', 'stop', 'link-284');">stop</a></tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L2044"></a><tt class="py-lineno">2044</tt>  <tt class="py-line">    <tt class="py-keyword">if</tt> <tt class="py-name">failure_count</tt><tt class="py-op">:</tt> </tt>
<a name="L2045"></a><tt class="py-lineno">2045</tt>  <tt class="py-line">        <tt class="py-name">exit</tt><tt class="py-op">(</tt><tt class="py-op">-</tt><tt class="py-number">1</tt><tt class="py-op">)</tt> </tt>
</div><a name="L2046"></a><tt class="py-lineno">2046</tt>  <tt class="py-line"> </tt>
<a name="L2047"></a><tt class="py-lineno">2047</tt>  <tt class="py-line"> </tt>
<a name="L2048"></a><tt class="py-lineno">2048</tt>  <tt class="py-line"><tt class="py-keyword">if</tt> <tt class="py-name">__name__</tt> <tt class="py-op">==</tt> <tt class="py-string">"__main__"</tt><tt class="py-op">:</tt> </tt>
<a name="L2049"></a><tt class="py-lineno">2049</tt>  <tt class="py-line">    <tt class="py-name">_test</tt><tt class="py-op">(</tt><tt class="py-op">)</tt> </tt>
<a name="L2050"></a><tt class="py-lineno">2050</tt>  <tt class="py-line"> </tt><script type="text/javascript">
<!--
expandto(location.href);
// -->
</script>
</pre>
<br />
<!-- ==================== NAVIGATION BAR ==================== -->
<table class="navbar" border="0" width="100%" cellpadding="0"
       bgcolor="#a0c0ff" cellspacing="0">
  <tr valign="middle">
  <!-- Home link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="pyspark-module.html">Home</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Tree link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="module-tree.html">Trees</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Index link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="identifier-index.html">Indices</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Help link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="help.html">Help</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Project homepage -->
      <th class="navbar" align="right" width="100%">
        <table border="0" cellpadding="0" cellspacing="0">
          <tr><th class="navbar" align="center"
            ><a class="navbar" target="_top" href="http://spark.apache.org">Spark 1.1.0 Python API Docs</a></th>
          </tr></table></th>
  </tr>
</table>
<table border="0" cellpadding="0" cellspacing="0" width="100%%">
  <tr>
    <td align="left" class="footer">
    Generated by Epydoc 3.0.1 on Thu Sep 11 01:19:41 2014
    </td>
    <td align="right" class="footer">
      <a target="mainFrame" href="http://epydoc.sourceforge.net"
        >http://epydoc.sourceforge.net</a>
    </td>
  </tr>
</table>

<script type="text/javascript">
  <!--
  // Private objects are initially displayed (because if
  // javascript is turned off then we want them to be
  // visible); but by default, we want to hide them.  So hide
  // them unless we have a cookie that says to show them.
  checkCookie();
  // -->
</script>
</body>
</html>
