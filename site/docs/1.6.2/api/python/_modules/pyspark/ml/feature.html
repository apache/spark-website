<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>pyspark.ml.feature &mdash; PySpark 1.6.2 documentation</title>
    
    <link rel="stylesheet" href="../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pyspark.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.6.2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../_static/pyspark.js"></script>
    <link rel="top" title="PySpark 1.6.2 documentation" href="../../../index.html" />
    <link rel="up" title="pyspark.ml" href="../ml.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
    
        <li><a href="../../../index.html">PySpark 1.6.2 documentation</a> &raquo;</li>

          <li><a href="../../index.html" >Module code</a> &raquo;</li>
          <li><a href="../../pyspark.html" >pyspark</a> &raquo;</li>
          <li><a href="../ml.html" accesskey="U">pyspark.ml</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <h1>Source code for pyspark.ml.feature</h1><div class="highlight"><pre>
<span class="c">#</span>
<span class="c"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c"># this work for additional information regarding copyright ownership.</span>
<span class="c"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c"># the License.  You may obtain a copy of the License at</span>
<span class="c">#</span>
<span class="c">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c">#</span>
<span class="c"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c"># See the License for the specific language governing permissions and</span>
<span class="c"># limitations under the License.</span>
<span class="c">#</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span> <span class="o">&gt;</span> <span class="s">&#39;3&#39;</span><span class="p">:</span>
    <span class="nb">basestring</span> <span class="o">=</span> <span class="nb">str</span>

<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">since</span>
<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="kn">import</span> <span class="n">ignore_unicode_prefix</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.param.shared</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.util</span> <span class="kn">import</span> <span class="n">keyword_only</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.wrapper</span> <span class="kn">import</span> <span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">JavaModel</span><span class="p">,</span> <span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">_jvm</span>
<span class="kn">from</span> <span class="nn">pyspark.mllib.common</span> <span class="kn">import</span> <span class="n">inherit_doc</span>
<span class="kn">from</span> <span class="nn">pyspark.mllib.linalg</span> <span class="kn">import</span> <span class="n">_convert_to_vector</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;Binarizer&#39;</span><span class="p">,</span> <span class="s">&#39;Bucketizer&#39;</span><span class="p">,</span> <span class="s">&#39;CountVectorizer&#39;</span><span class="p">,</span> <span class="s">&#39;CountVectorizerModel&#39;</span><span class="p">,</span> <span class="s">&#39;DCT&#39;</span><span class="p">,</span>
           <span class="s">&#39;ElementwiseProduct&#39;</span><span class="p">,</span> <span class="s">&#39;HashingTF&#39;</span><span class="p">,</span> <span class="s">&#39;IDF&#39;</span><span class="p">,</span> <span class="s">&#39;IDFModel&#39;</span><span class="p">,</span> <span class="s">&#39;IndexToString&#39;</span><span class="p">,</span> <span class="s">&#39;MinMaxScaler&#39;</span><span class="p">,</span>
           <span class="s">&#39;MinMaxScalerModel&#39;</span><span class="p">,</span> <span class="s">&#39;NGram&#39;</span><span class="p">,</span> <span class="s">&#39;Normalizer&#39;</span><span class="p">,</span> <span class="s">&#39;OneHotEncoder&#39;</span><span class="p">,</span> <span class="s">&#39;PCA&#39;</span><span class="p">,</span> <span class="s">&#39;PCAModel&#39;</span><span class="p">,</span>
           <span class="s">&#39;PolynomialExpansion&#39;</span><span class="p">,</span> <span class="s">&#39;RegexTokenizer&#39;</span><span class="p">,</span> <span class="s">&#39;RFormula&#39;</span><span class="p">,</span> <span class="s">&#39;RFormulaModel&#39;</span><span class="p">,</span> <span class="s">&#39;SQLTransformer&#39;</span><span class="p">,</span>
           <span class="s">&#39;StandardScaler&#39;</span><span class="p">,</span> <span class="s">&#39;StandardScalerModel&#39;</span><span class="p">,</span> <span class="s">&#39;StopWordsRemover&#39;</span><span class="p">,</span> <span class="s">&#39;StringIndexer&#39;</span><span class="p">,</span>
           <span class="s">&#39;StringIndexerModel&#39;</span><span class="p">,</span> <span class="s">&#39;Tokenizer&#39;</span><span class="p">,</span> <span class="s">&#39;VectorAssembler&#39;</span><span class="p">,</span> <span class="s">&#39;VectorIndexer&#39;</span><span class="p">,</span> <span class="s">&#39;VectorSlicer&#39;</span><span class="p">,</span>
           <span class="s">&#39;Word2Vec&#39;</span><span class="p">,</span> <span class="s">&#39;Word2VecModel&#39;</span><span class="p">]</span>


<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="Binarizer"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Binarizer">[docs]</a><span class="k">class</span> <span class="nc">Binarizer</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Binarize a column of continuous features given a threshold.</span>

<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(0.5,)], [&quot;values&quot;])</span>
<span class="sd">    &gt;&gt;&gt; binarizer = Binarizer(threshold=1.0, inputCol=&quot;values&quot;, outputCol=&quot;features&quot;)</span>
<span class="sd">    &gt;&gt;&gt; binarizer.transform(df).head().features</span>
<span class="sd">    0.0</span>
<span class="sd">    &gt;&gt;&gt; binarizer.setParams(outputCol=&quot;freqs&quot;).transform(df).head().freqs</span>
<span class="sd">    0.0</span>
<span class="sd">    &gt;&gt;&gt; params = {binarizer.threshold: -0.5, binarizer.outputCol: &quot;vector&quot;}</span>
<span class="sd">    &gt;&gt;&gt; binarizer.transform(df, params).head().vector</span>
<span class="sd">    1.0</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;threshold&quot;</span><span class="p">,</span>
                      <span class="s">&quot;threshold in binary classification prediction, in range [0, 1]&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, threshold=0.0, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Binarizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.Binarizer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;threshold&quot;</span><span class="p">,</span>
                               <span class="s">&quot;threshold in binary classification prediction, in range [0, 1]&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Binarizer.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Binarizer.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, threshold=0.0, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this Binarizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Binarizer.setThreshold"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Binarizer.setThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`threshold`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Binarizer.getThreshold"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Binarizer.getThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">getThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of threshold or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="Bucketizer"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Bucketizer">[docs]</a><span class="k">class</span> <span class="nc">Bucketizer</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Maps a column of continuous features to a column of feature buckets.</span>

<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(0.1,), (0.4,), (1.2,), (1.5,)], [&quot;values&quot;])</span>
<span class="sd">    &gt;&gt;&gt; bucketizer = Bucketizer(splits=[-float(&quot;inf&quot;), 0.5, 1.4, float(&quot;inf&quot;)],</span>
<span class="sd">    ...     inputCol=&quot;values&quot;, outputCol=&quot;buckets&quot;)</span>
<span class="sd">    &gt;&gt;&gt; bucketed = bucketizer.transform(df).collect()</span>
<span class="sd">    &gt;&gt;&gt; bucketed[0].buckets</span>
<span class="sd">    0.0</span>
<span class="sd">    &gt;&gt;&gt; bucketed[1].buckets</span>
<span class="sd">    0.0</span>
<span class="sd">    &gt;&gt;&gt; bucketed[2].buckets</span>
<span class="sd">    1.0</span>
<span class="sd">    &gt;&gt;&gt; bucketed[3].buckets</span>
<span class="sd">    2.0</span>
<span class="sd">    &gt;&gt;&gt; bucketizer.setParams(outputCol=&quot;b&quot;).transform(df).head().b</span>
<span class="sd">    0.0</span>

<span class="sd">    .. versionadded:: 1.3.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">splits</span> <span class="o">=</span> \
        <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;splits&quot;</span><span class="p">,</span>
              <span class="s">&quot;Split points for mapping continuous features into buckets. With n+1 splits, &quot;</span> <span class="o">+</span>
              <span class="s">&quot;there are n buckets. A bucket defined by splits x,y holds values in the &quot;</span> <span class="o">+</span>
              <span class="s">&quot;range [x,y) except the last bucket, which also includes y. The splits &quot;</span> <span class="o">+</span>
              <span class="s">&quot;should be strictly increasing. Values at -inf, inf must be explicitly &quot;</span> <span class="o">+</span>
              <span class="s">&quot;provided to cover all Double values; otherwise, values outside the splits &quot;</span> <span class="o">+</span>
              <span class="s">&quot;specified will be treated as errors.&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, splits=None, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bucketizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.Bucketizer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="c">#: param for Splitting points for mapping continuous features into buckets. With n+1 splits,</span>
        <span class="c">#  there are n buckets. A bucket defined by splits x,y holds values in the range [x,y)</span>
        <span class="c">#  except the last bucket, which also includes y. The splits should be strictly increasing.</span>
        <span class="c">#  Values at -inf, inf must be explicitly provided to cover all Double values; otherwise,</span>
        <span class="c">#  values outside the splits specified will be treated as errors.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">splits</span> <span class="o">=</span> \
            <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;splits&quot;</span><span class="p">,</span>
                  <span class="s">&quot;Split points for mapping continuous features into buckets. With n+1 splits, &quot;</span> <span class="o">+</span>
                  <span class="s">&quot;there are n buckets. A bucket defined by splits x,y holds values in the &quot;</span> <span class="o">+</span>
                  <span class="s">&quot;range [x,y) except the last bucket, which also includes y. The splits &quot;</span> <span class="o">+</span>
                  <span class="s">&quot;should be strictly increasing. Values at -inf, inf must be explicitly &quot;</span> <span class="o">+</span>
                  <span class="s">&quot;provided to cover all Double values; otherwise, values outside the splits &quot;</span> <span class="o">+</span>
                  <span class="s">&quot;specified will be treated as errors.&quot;</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Bucketizer.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Bucketizer.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, splits=None, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this Bucketizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Bucketizer.setSplits"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Bucketizer.setSplits">[docs]</a>    <span class="k">def</span> <span class="nf">setSplits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`splits`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">splits</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Bucketizer.getSplits"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Bucketizer.getSplits">[docs]</a>    <span class="k">def</span> <span class="nf">getSplits</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of threshold or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">splits</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="CountVectorizer"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.CountVectorizer">[docs]</a><span class="k">class</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Extracts a vocabulary from document collections and generates a :py:attr:`CountVectorizerModel`.</span>

<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame(</span>
<span class="sd">    ...    [(0, [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]), (1, [&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;])],</span>
<span class="sd">    ...    [&quot;label&quot;, &quot;raw&quot;])</span>
<span class="sd">    &gt;&gt;&gt; cv = CountVectorizer(inputCol=&quot;raw&quot;, outputCol=&quot;vectors&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = cv.fit(df)</span>
<span class="sd">    &gt;&gt;&gt; model.transform(df).show(truncate=False)</span>
<span class="sd">    +-----+---------------+-------------------------+</span>
<span class="sd">    |label|raw            |vectors                  |</span>
<span class="sd">    +-----+---------------+-------------------------+</span>
<span class="sd">    |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|</span>
<span class="sd">    |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|</span>
<span class="sd">    +-----+---------------+-------------------------+</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; sorted(map(str, model.vocabulary))</span>
<span class="sd">    [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]</span>

<span class="sd">    .. versionadded:: 1.6.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">minTF</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span>
        <span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;minTF&quot;</span><span class="p">,</span> <span class="s">&quot;Filter to ignore rare words in&quot;</span> <span class="o">+</span>
        <span class="s">&quot; a document. For each document, terms with frequency/count less than the given&quot;</span> <span class="o">+</span>
        <span class="s">&quot; threshold are ignored. If this is an integer &gt;= 1, then this specifies a count (of&quot;</span> <span class="o">+</span>
        <span class="s">&quot; times the term must appear in the document); if this is a double in [0,1), then this &quot;</span> <span class="o">+</span>
        <span class="s">&quot;specifies a fraction (out of the document&#39;s token count). Note that the parameter is &quot;</span> <span class="o">+</span>
        <span class="s">&quot;only used in transform of CountVectorizerModel and does not affect fitting. Default 1.0&quot;</span><span class="p">)</span>
    <span class="n">minDF</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span>
        <span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;minDF&quot;</span><span class="p">,</span> <span class="s">&quot;Specifies the minimum number of&quot;</span> <span class="o">+</span>
        <span class="s">&quot; different documents a term must appear in to be included in the vocabulary.&quot;</span> <span class="o">+</span>
        <span class="s">&quot; If this is an integer &gt;= 1, this specifies the number of documents the term must&quot;</span> <span class="o">+</span>
        <span class="s">&quot; appear in; if this is a double in [0,1), then this specifies the fraction of documents.&quot;</span> <span class="o">+</span>
        <span class="s">&quot; Default 1.0&quot;</span><span class="p">)</span>
    <span class="n">vocabSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span>
        <span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;vocabSize&quot;</span><span class="p">,</span> <span class="s">&quot;max size of the vocabulary. Default 1 &lt;&lt; 18.&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">minTF</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">minDF</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">vocabSize</span><span class="o">=</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">18</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, minTF=1.0, minDF=1.0, vocabSize=1 &lt;&lt; 18, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.CountVectorizer&quot;</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minTF</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s">&quot;minTF&quot;</span><span class="p">,</span> <span class="s">&quot;Filter to ignore rare words in&quot;</span> <span class="o">+</span>
            <span class="s">&quot; a document. For each document, terms with frequency/count less than the given&quot;</span> <span class="o">+</span>
            <span class="s">&quot; threshold are ignored. If this is an integer &gt;= 1, then this specifies a count (of&quot;</span> <span class="o">+</span>
            <span class="s">&quot; times the term must appear in the document); if this is a double in [0,1), then &quot;</span> <span class="o">+</span>
            <span class="s">&quot;this specifies a fraction (out of the document&#39;s token count). Note that the &quot;</span> <span class="o">+</span>
            <span class="s">&quot;parameter is only used in transform of CountVectorizerModel and does not affect&quot;</span> <span class="o">+</span>
            <span class="s">&quot;fitting. Default 1.0&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minDF</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s">&quot;minDF&quot;</span><span class="p">,</span> <span class="s">&quot;Specifies the minimum number of&quot;</span> <span class="o">+</span>
            <span class="s">&quot; different documents a term must appear in to be included in the vocabulary.&quot;</span> <span class="o">+</span>
            <span class="s">&quot; If this is an integer &gt;= 1, this specifies the number of documents the term must&quot;</span> <span class="o">+</span>
            <span class="s">&quot; appear in; if this is a double in [0,1), then this specifies the fraction of &quot;</span> <span class="o">+</span>
            <span class="s">&quot;documents. Default 1.0&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocabSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s">&quot;vocabSize&quot;</span><span class="p">,</span> <span class="s">&quot;max size of the vocabulary. Default 1 &lt;&lt; 18.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">minTF</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">minDF</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">vocabSize</span><span class="o">=</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">18</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="CountVectorizer.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.CountVectorizer.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">minTF</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">minDF</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">vocabSize</span><span class="o">=</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">18</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, minTF=1.0, minDF=1.0, vocabSize=1 &lt;&lt; 18, inputCol=None, outputCol=None)</span>
<span class="sd">        Set the params for the CountVectorizer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="CountVectorizer.setMinTF"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.CountVectorizer.setMinTF">[docs]</a>    <span class="k">def</span> <span class="nf">setMinTF</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`minTF`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">minTF</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="CountVectorizer.getMinTF"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.CountVectorizer.getMinTF">[docs]</a>    <span class="k">def</span> <span class="nf">getMinTF</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of minTF or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">minTF</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="CountVectorizer.setMinDF"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.CountVectorizer.setMinDF">[docs]</a>    <span class="k">def</span> <span class="nf">setMinDF</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`minDF`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">minDF</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="CountVectorizer.getMinDF"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.CountVectorizer.getMinDF">[docs]</a>    <span class="k">def</span> <span class="nf">getMinDF</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of minDF or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">minDF</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="CountVectorizer.setVocabSize"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.CountVectorizer.setVocabSize">[docs]</a>    <span class="k">def</span> <span class="nf">setVocabSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`vocabSize`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabSize</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="CountVectorizer.getVocabSize"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.CountVectorizer.getVocabSize">[docs]</a>    <span class="k">def</span> <span class="nf">getVocabSize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of vocabSize or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabSize</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">CountVectorizerModel</span><span class="p">(</span><span class="n">java_model</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="CountVectorizerModel"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.CountVectorizerModel">[docs]</a><span class="k">class</span> <span class="nc">CountVectorizerModel</span><span class="p">(</span><span class="n">JavaModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Model fitted by CountVectorizer.</span>

<span class="sd">    .. versionadded:: 1.6.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="CountVectorizerModel.vocabulary"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.CountVectorizerModel.vocabulary">[docs]</a>    <span class="k">def</span> <span class="nf">vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        An array of terms in the vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s">&quot;vocabulary&quot;</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="DCT"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.DCT">[docs]</a><span class="k">class</span> <span class="nc">DCT</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    A feature transformer that takes the 1D discrete cosine transform</span>
<span class="sd">    of a real vector. No zero padding is performed on the input vector.</span>
<span class="sd">    It returns a real vector of the same length representing the DCT.</span>
<span class="sd">    The return vector is scaled such that the transform matrix is</span>
<span class="sd">    unitary (aka scaled DCT-II).</span>

<span class="sd">    More information on</span>
<span class="sd">    `https://en.wikipedia.org/wiki/Discrete_cosine_transform#DCT-II Wikipedia`.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.mllib.linalg import Vectors</span>
<span class="sd">    &gt;&gt;&gt; df1 = sqlContext.createDataFrame([(Vectors.dense([5.0, 8.0, 6.0]),)], [&quot;vec&quot;])</span>
<span class="sd">    &gt;&gt;&gt; dct = DCT(inverse=False, inputCol=&quot;vec&quot;, outputCol=&quot;resultVec&quot;)</span>
<span class="sd">    &gt;&gt;&gt; df2 = dct.transform(df1)</span>
<span class="sd">    &gt;&gt;&gt; df2.head().resultVec</span>
<span class="sd">    DenseVector([10.969..., -0.707..., -2.041...])</span>
<span class="sd">    &gt;&gt;&gt; df3 = DCT(inverse=True, inputCol=&quot;resultVec&quot;, outputCol=&quot;origVec&quot;).transform(df2)</span>
<span class="sd">    &gt;&gt;&gt; df3.head().origVec</span>
<span class="sd">    DenseVector([5.0, 8.0, 6.0])</span>

<span class="sd">    .. versionadded:: 1.6.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">inverse</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;inverse&quot;</span><span class="p">,</span> <span class="s">&quot;Set transformer to perform inverse DCT, &quot;</span> <span class="o">+</span>
                    <span class="s">&quot;default False.&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inverse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, inverse=False, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DCT</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.DCT&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inverse</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;inverse&quot;</span><span class="p">,</span> <span class="s">&quot;Set transformer to perform inverse DCT, &quot;</span> <span class="o">+</span>
                             <span class="s">&quot;default False.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">inverse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="DCT.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.DCT.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inverse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, inverse=False, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this DCT.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="DCT.setInverse"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.DCT.setInverse">[docs]</a>    <span class="k">def</span> <span class="nf">setInverse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`inverse`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">inverse</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="DCT.getInverse"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.DCT.getInverse">[docs]</a>    <span class="k">def</span> <span class="nf">getInverse</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of inverse or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inverse</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="ElementwiseProduct"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.ElementwiseProduct">[docs]</a><span class="k">class</span> <span class="nc">ElementwiseProduct</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Outputs the Hadamard product (i.e., the element-wise product) of each input vector</span>
<span class="sd">    with a provided &quot;weight&quot; vector. In other words, it scales each column of the dataset</span>
<span class="sd">    by a scalar multiplier.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.mllib.linalg import Vectors</span>
<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(Vectors.dense([2.0, 1.0, 3.0]),)], [&quot;values&quot;])</span>
<span class="sd">    &gt;&gt;&gt; ep = ElementwiseProduct(scalingVec=Vectors.dense([1.0, 2.0, 3.0]),</span>
<span class="sd">    ...     inputCol=&quot;values&quot;, outputCol=&quot;eprod&quot;)</span>
<span class="sd">    &gt;&gt;&gt; ep.transform(df).head().eprod</span>
<span class="sd">    DenseVector([2.0, 2.0, 9.0])</span>
<span class="sd">    &gt;&gt;&gt; ep.setParams(scalingVec=Vectors.dense([2.0, 3.0, 5.0])).transform(df).head().eprod</span>
<span class="sd">    DenseVector([4.0, 3.0, 15.0])</span>

<span class="sd">    .. versionadded:: 1.5.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">scalingVec</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;scalingVec&quot;</span><span class="p">,</span> <span class="s">&quot;vector for hadamard product, &quot;</span> <span class="o">+</span>
                       <span class="s">&quot;it must be MLlib Vector type.&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scalingVec</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, scalingVec=None, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ElementwiseProduct</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.ElementwiseProduct&quot;</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scalingVec</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;scalingVec&quot;</span><span class="p">,</span> <span class="s">&quot;vector for hadamard product, &quot;</span> <span class="o">+</span>
                                <span class="s">&quot;it must be MLlib Vector type.&quot;</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="ElementwiseProduct.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.ElementwiseProduct.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scalingVec</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, scalingVec=None, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this ElementwiseProduct.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="ElementwiseProduct.setScalingVec"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.ElementwiseProduct.setScalingVec">[docs]</a>    <span class="k">def</span> <span class="nf">setScalingVec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`scalingVec`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">scalingVec</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="ElementwiseProduct.getScalingVec"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.ElementwiseProduct.getScalingVec">[docs]</a>    <span class="k">def</span> <span class="nf">getScalingVec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of scalingVec or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scalingVec</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="HashingTF"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.HashingTF">[docs]</a><span class="k">class</span> <span class="nc">HashingTF</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">,</span> <span class="n">HasNumFeatures</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Maps a sequence of terms to their term frequencies using the</span>
<span class="sd">    hashing trick.</span>

<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],)], [&quot;words&quot;])</span>
<span class="sd">    &gt;&gt;&gt; hashingTF = HashingTF(numFeatures=10, inputCol=&quot;words&quot;, outputCol=&quot;features&quot;)</span>
<span class="sd">    &gt;&gt;&gt; hashingTF.transform(df).head().features</span>
<span class="sd">    SparseVector(10, {7: 1.0, 8: 1.0, 9: 1.0})</span>
<span class="sd">    &gt;&gt;&gt; hashingTF.setParams(outputCol=&quot;freqs&quot;).transform(df).head().freqs</span>
<span class="sd">    SparseVector(10, {7: 1.0, 8: 1.0, 9: 1.0})</span>
<span class="sd">    &gt;&gt;&gt; params = {hashingTF.numFeatures: 5, hashingTF.outputCol: &quot;vector&quot;}</span>
<span class="sd">    &gt;&gt;&gt; hashingTF.transform(df, params).head().vector</span>
<span class="sd">    SparseVector(5, {2: 1.0, 3: 1.0, 4: 1.0})</span>

<span class="sd">    .. versionadded:: 1.3.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">numFeatures</span><span class="o">=</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">18</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, numFeatures=1 &lt;&lt; 18, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HashingTF</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.HashingTF&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">numFeatures</span><span class="o">=</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">18</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.3.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="HashingTF.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.HashingTF.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">numFeatures</span><span class="o">=</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">18</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, numFeatures=1 &lt;&lt; 18, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this HashingTF.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="IDF"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.IDF">[docs]</a><span class="k">class</span> <span class="nc">IDF</span><span class="p">(</span><span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Compute the Inverse Document Frequency (IDF) given a collection of documents.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.mllib.linalg import DenseVector</span>
<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(DenseVector([1.0, 2.0]),),</span>
<span class="sd">    ...     (DenseVector([0.0, 1.0]),), (DenseVector([3.0, 0.2]),)], [&quot;tf&quot;])</span>
<span class="sd">    &gt;&gt;&gt; idf = IDF(minDocFreq=3, inputCol=&quot;tf&quot;, outputCol=&quot;idf&quot;)</span>
<span class="sd">    &gt;&gt;&gt; idf.fit(df).transform(df).head().idf</span>
<span class="sd">    DenseVector([0.0, 0.0])</span>
<span class="sd">    &gt;&gt;&gt; idf.setParams(outputCol=&quot;freqs&quot;).fit(df).transform(df).collect()[1].freqs</span>
<span class="sd">    DenseVector([0.0, 0.0])</span>
<span class="sd">    &gt;&gt;&gt; params = {idf.minDocFreq: 1, idf.outputCol: &quot;vector&quot;}</span>
<span class="sd">    &gt;&gt;&gt; idf.fit(df, params).transform(df).head().vector</span>
<span class="sd">    DenseVector([0.2877, 0.0])</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">minDocFreq</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;minDocFreq&quot;</span><span class="p">,</span>
                       <span class="s">&quot;minimum of documents in which a term should appear for filtering&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">minDocFreq</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, minDocFreq=0, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">IDF</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.IDF&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minDocFreq</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;minDocFreq&quot;</span><span class="p">,</span>
                                <span class="s">&quot;minimum of documents in which a term should appear for filtering&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">minDocFreq</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="IDF.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.IDF.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">minDocFreq</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, minDocFreq=0, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this IDF.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="IDF.setMinDocFreq"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.IDF.setMinDocFreq">[docs]</a>    <span class="k">def</span> <span class="nf">setMinDocFreq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`minDocFreq`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">minDocFreq</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="IDF.getMinDocFreq"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.IDF.getMinDocFreq">[docs]</a>    <span class="k">def</span> <span class="nf">getMinDocFreq</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of minDocFreq or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">minDocFreq</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">IDFModel</span><span class="p">(</span><span class="n">java_model</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="IDFModel"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.IDFModel">[docs]</a><span class="k">class</span> <span class="nc">IDFModel</span><span class="p">(</span><span class="n">JavaModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Model fitted by IDF.</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

</div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="MinMaxScaler"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.MinMaxScaler">[docs]</a><span class="k">class</span> <span class="nc">MinMaxScaler</span><span class="p">(</span><span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Rescale each feature individually to a common range [min, max] linearly using column summary</span>
<span class="sd">    statistics, which is also known as min-max normalization or Rescaling. The rescaled value for</span>
<span class="sd">    feature E is calculated as,</span>

<span class="sd">    Rescaled(e_i) = (e_i - E_min) / (E_max - E_min) * (max - min) + min</span>

<span class="sd">    For the case E_max == E_min, Rescaled(e_i) = 0.5 * (max + min)</span>

<span class="sd">    Note that since zero values will probably be transformed to non-zero values, output of the</span>
<span class="sd">    transformer will be DenseVector even for sparse input.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.mllib.linalg import Vectors</span>
<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(Vectors.dense([0.0]),), (Vectors.dense([2.0]),)], [&quot;a&quot;])</span>
<span class="sd">    &gt;&gt;&gt; mmScaler = MinMaxScaler(inputCol=&quot;a&quot;, outputCol=&quot;scaled&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = mmScaler.fit(df)</span>
<span class="sd">    &gt;&gt;&gt; model.transform(df).show()</span>
<span class="sd">    +-----+------+</span>
<span class="sd">    |    a|scaled|</span>
<span class="sd">    +-----+------+</span>
<span class="sd">    |[0.0]| [0.0]|</span>
<span class="sd">    |[2.0]| [1.0]|</span>
<span class="sd">    +-----+------+</span>
<span class="sd">    ...</span>

<span class="sd">    .. versionadded:: 1.6.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="nb">min</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;min&quot;</span><span class="p">,</span> <span class="s">&quot;Lower bound of the output feature range&quot;</span><span class="p">)</span>
    <span class="nb">max</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;max&quot;</span><span class="p">,</span> <span class="s">&quot;Upper bound of the output feature range&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, min=0.0, max=1.0, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MinMaxScaler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.MinMaxScaler&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;min&quot;</span><span class="p">,</span> <span class="s">&quot;Lower bound of the output feature range&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;max&quot;</span><span class="p">,</span> <span class="s">&quot;Upper bound of the output feature range&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="MinMaxScaler.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.MinMaxScaler.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, min=0.0, max=1.0, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this MinMaxScaler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="MinMaxScaler.setMin"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.MinMaxScaler.setMin">[docs]</a>    <span class="k">def</span> <span class="nf">setMin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`min`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">min</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="MinMaxScaler.getMin"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.MinMaxScaler.getMin">[docs]</a>    <span class="k">def</span> <span class="nf">getMin</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of min or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="MinMaxScaler.setMax"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.MinMaxScaler.setMax">[docs]</a>    <span class="k">def</span> <span class="nf">setMax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`max`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">max</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="MinMaxScaler.getMax"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.MinMaxScaler.getMax">[docs]</a>    <span class="k">def</span> <span class="nf">getMax</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of max or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">MinMaxScalerModel</span><span class="p">(</span><span class="n">java_model</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="MinMaxScalerModel"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.MinMaxScalerModel">[docs]</a><span class="k">class</span> <span class="nc">MinMaxScalerModel</span><span class="p">(</span><span class="n">JavaModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Model fitted by :py:class:`MinMaxScaler`.</span>

<span class="sd">    .. versionadded:: 1.6.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

</div>
<span class="nd">@inherit_doc</span>
<span class="nd">@ignore_unicode_prefix</span>
<div class="viewcode-block" id="NGram"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.NGram">[docs]</a><span class="k">class</span> <span class="nc">NGram</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    A feature transformer that converts the input array of strings into an array of n-grams. Null</span>
<span class="sd">    values in the input array are ignored.</span>
<span class="sd">    It returns an array of n-grams where each n-gram is represented by a space-separated string of</span>
<span class="sd">    words.</span>
<span class="sd">    When the input is empty, an empty array is returned.</span>
<span class="sd">    When the input array length is less than n (number of elements per n-gram), no n-grams are</span>
<span class="sd">    returned.</span>

<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([Row(inputTokens=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;])])</span>
<span class="sd">    &gt;&gt;&gt; ngram = NGram(n=2, inputCol=&quot;inputTokens&quot;, outputCol=&quot;nGrams&quot;)</span>
<span class="sd">    &gt;&gt;&gt; ngram.transform(df).head()</span>
<span class="sd">    Row(inputTokens=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;, u&#39;d&#39;, u&#39;e&#39;], nGrams=[u&#39;a b&#39;, u&#39;b c&#39;, u&#39;c d&#39;, u&#39;d e&#39;])</span>
<span class="sd">    &gt;&gt;&gt; # Change n-gram length</span>
<span class="sd">    &gt;&gt;&gt; ngram.setParams(n=4).transform(df).head()</span>
<span class="sd">    Row(inputTokens=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;, u&#39;d&#39;, u&#39;e&#39;], nGrams=[u&#39;a b c d&#39;, u&#39;b c d e&#39;])</span>
<span class="sd">    &gt;&gt;&gt; # Temporarily modify output column.</span>
<span class="sd">    &gt;&gt;&gt; ngram.transform(df, {ngram.outputCol: &quot;output&quot;}).head()</span>
<span class="sd">    Row(inputTokens=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;, u&#39;d&#39;, u&#39;e&#39;], output=[u&#39;a b c d&#39;, u&#39;b c d e&#39;])</span>
<span class="sd">    &gt;&gt;&gt; ngram.transform(df).head()</span>
<span class="sd">    Row(inputTokens=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;, u&#39;d&#39;, u&#39;e&#39;], nGrams=[u&#39;a b c d&#39;, u&#39;b c d e&#39;])</span>
<span class="sd">    &gt;&gt;&gt; # Must use keyword arguments to specify params.</span>
<span class="sd">    &gt;&gt;&gt; ngram.setParams(&quot;text&quot;)</span>
<span class="sd">    Traceback (most recent call last):</span>
<span class="sd">        ...</span>
<span class="sd">    TypeError: Method setParams forces keyword arguments.</span>

<span class="sd">    .. versionadded:: 1.5.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;n&quot;</span><span class="p">,</span> <span class="s">&quot;number of elements per n-gram (&gt;=1)&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, n=2, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NGram</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.NGram&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;n&quot;</span><span class="p">,</span> <span class="s">&quot;number of elements per n-gram (&gt;=1)&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="NGram.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.NGram.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, n=2, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this NGram.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="NGram.setN"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.NGram.setN">[docs]</a>    <span class="k">def</span> <span class="nf">setN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`n`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="NGram.getN"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.NGram.getN">[docs]</a>    <span class="k">def</span> <span class="nf">getN</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of n or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="Normalizer"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Normalizer">[docs]</a><span class="k">class</span> <span class="nc">Normalizer</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">     Normalize a vector to have unit norm using the given p-norm.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.mllib.linalg import Vectors</span>
<span class="sd">    &gt;&gt;&gt; svec = Vectors.sparse(4, {1: 4.0, 3: 3.0})</span>
<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(Vectors.dense([3.0, -4.0]), svec)], [&quot;dense&quot;, &quot;sparse&quot;])</span>
<span class="sd">    &gt;&gt;&gt; normalizer = Normalizer(p=2.0, inputCol=&quot;dense&quot;, outputCol=&quot;features&quot;)</span>
<span class="sd">    &gt;&gt;&gt; normalizer.transform(df).head().features</span>
<span class="sd">    DenseVector([0.6, -0.8])</span>
<span class="sd">    &gt;&gt;&gt; normalizer.setParams(inputCol=&quot;sparse&quot;, outputCol=&quot;freqs&quot;).transform(df).head().freqs</span>
<span class="sd">    SparseVector(4, {1: 0.8, 3: 0.6})</span>
<span class="sd">    &gt;&gt;&gt; params = {normalizer.p: 1.0, normalizer.inputCol: &quot;dense&quot;, normalizer.outputCol: &quot;vector&quot;}</span>
<span class="sd">    &gt;&gt;&gt; normalizer.transform(df, params).head().vector</span>
<span class="sd">    DenseVector([0.4286, -0.5714])</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;p&quot;</span><span class="p">,</span> <span class="s">&quot;the p norm value.&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, p=2.0, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Normalizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.Normalizer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;p&quot;</span><span class="p">,</span> <span class="s">&quot;the p norm value.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Normalizer.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Normalizer.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, p=2.0, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this Normalizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Normalizer.setP"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Normalizer.setP">[docs]</a>    <span class="k">def</span> <span class="nf">setP</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`p`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Normalizer.getP"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Normalizer.getP">[docs]</a>    <span class="k">def</span> <span class="nf">getP</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of p or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="OneHotEncoder"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.OneHotEncoder">[docs]</a><span class="k">class</span> <span class="nc">OneHotEncoder</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    A one-hot encoder that maps a column of category indices to a</span>
<span class="sd">    column of binary vectors, with at most a single one-value per row</span>
<span class="sd">    that indicates the input category index.</span>
<span class="sd">    For example with 5 categories, an input value of 2.0 would map to</span>
<span class="sd">    an output vector of `[0.0, 0.0, 1.0, 0.0]`.</span>
<span class="sd">    The last category is not included by default (configurable via</span>
<span class="sd">    :py:attr:`dropLast`) because it makes the vector entries sum up to</span>
<span class="sd">    one, and hence linearly dependent.</span>
<span class="sd">    So an input value of 4.0 maps to `[0.0, 0.0, 0.0, 0.0]`.</span>
<span class="sd">    Note that this is different from scikit-learn&#39;s OneHotEncoder,</span>
<span class="sd">    which keeps all categories.</span>
<span class="sd">    The output vectors are sparse.</span>

<span class="sd">    .. seealso::</span>

<span class="sd">       :py:class:`StringIndexer` for converting categorical values into</span>
<span class="sd">       category indices</span>

<span class="sd">    &gt;&gt;&gt; stringIndexer = StringIndexer(inputCol=&quot;label&quot;, outputCol=&quot;indexed&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = stringIndexer.fit(stringIndDf)</span>
<span class="sd">    &gt;&gt;&gt; td = model.transform(stringIndDf)</span>
<span class="sd">    &gt;&gt;&gt; encoder = OneHotEncoder(inputCol=&quot;indexed&quot;, outputCol=&quot;features&quot;)</span>
<span class="sd">    &gt;&gt;&gt; encoder.transform(td).head().features</span>
<span class="sd">    SparseVector(2, {0: 1.0})</span>
<span class="sd">    &gt;&gt;&gt; encoder.setParams(outputCol=&quot;freqs&quot;).transform(td).head().freqs</span>
<span class="sd">    SparseVector(2, {0: 1.0})</span>
<span class="sd">    &gt;&gt;&gt; params = {encoder.dropLast: False, encoder.outputCol: &quot;test&quot;}</span>
<span class="sd">    &gt;&gt;&gt; encoder.transform(td, params).head().test</span>
<span class="sd">    SparseVector(3, {0: 1.0})</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">dropLast</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;dropLast&quot;</span><span class="p">,</span> <span class="s">&quot;whether to drop the last category&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropLast</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, includeFirst=True, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">OneHotEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.OneHotEncoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropLast</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;dropLast&quot;</span><span class="p">,</span> <span class="s">&quot;whether to drop the last category&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">dropLast</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="OneHotEncoder.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.OneHotEncoder.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropLast</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, dropLast=True, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this OneHotEncoder.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="OneHotEncoder.setDropLast"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.OneHotEncoder.setDropLast">[docs]</a>    <span class="k">def</span> <span class="nf">setDropLast</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`dropLast`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dropLast</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="OneHotEncoder.getDropLast"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.OneHotEncoder.getDropLast">[docs]</a>    <span class="k">def</span> <span class="nf">getDropLast</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of dropLast or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropLast</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="PolynomialExpansion"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.PolynomialExpansion">[docs]</a><span class="k">class</span> <span class="nc">PolynomialExpansion</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Perform feature expansion in a polynomial space. As said in wikipedia of Polynomial Expansion,</span>
<span class="sd">    which is available at `http://en.wikipedia.org/wiki/Polynomial_expansion`, &quot;In mathematics, an</span>
<span class="sd">    expansion of a product of sums expresses it as a sum of products by using the fact that</span>
<span class="sd">    multiplication distributes over addition&quot;. Take a 2-variable feature vector as an example:</span>
<span class="sd">    `(x, y)`, if we want to expand it with degree 2, then we get `(x, x * x, y, x * y, y * y)`.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.mllib.linalg import Vectors</span>
<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(Vectors.dense([0.5, 2.0]),)], [&quot;dense&quot;])</span>
<span class="sd">    &gt;&gt;&gt; px = PolynomialExpansion(degree=2, inputCol=&quot;dense&quot;, outputCol=&quot;expanded&quot;)</span>
<span class="sd">    &gt;&gt;&gt; px.transform(df).head().expanded</span>
<span class="sd">    DenseVector([0.5, 0.25, 2.0, 1.0, 4.0])</span>
<span class="sd">    &gt;&gt;&gt; px.setParams(outputCol=&quot;test&quot;).transform(df).head().test</span>
<span class="sd">    DenseVector([0.5, 0.25, 2.0, 1.0, 4.0])</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">degree</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;degree&quot;</span><span class="p">,</span> <span class="s">&quot;the polynomial degree to expand (&gt;= 1)&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, degree=2, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PolynomialExpansion</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span>
            <span class="s">&quot;org.apache.spark.ml.feature.PolynomialExpansion&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;degree&quot;</span><span class="p">,</span> <span class="s">&quot;the polynomial degree to expand (&gt;= 1)&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="PolynomialExpansion.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.PolynomialExpansion.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, degree=2, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this PolynomialExpansion.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="PolynomialExpansion.setDegree"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.PolynomialExpansion.setDegree">[docs]</a>    <span class="k">def</span> <span class="nf">setDegree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`degree`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="PolynomialExpansion.getDegree"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.PolynomialExpansion.getDegree">[docs]</a>    <span class="k">def</span> <span class="nf">getDegree</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of degree or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<span class="nd">@ignore_unicode_prefix</span>
<div class="viewcode-block" id="RegexTokenizer"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RegexTokenizer">[docs]</a><span class="k">class</span> <span class="nc">RegexTokenizer</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    A regex based tokenizer that extracts tokens either by using the</span>
<span class="sd">    provided regex pattern (in Java dialect) to split the text</span>
<span class="sd">    (default) or repeatedly matching the regex (if gaps is false).</span>
<span class="sd">    Optional parameters also allow filtering tokens using a minimal</span>
<span class="sd">    length.</span>
<span class="sd">    It returns an array of strings that can be empty.</span>

<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(&quot;a b  c&quot;,)], [&quot;text&quot;])</span>
<span class="sd">    &gt;&gt;&gt; reTokenizer = RegexTokenizer(inputCol=&quot;text&quot;, outputCol=&quot;words&quot;)</span>
<span class="sd">    &gt;&gt;&gt; reTokenizer.transform(df).head()</span>
<span class="sd">    Row(text=u&#39;a b  c&#39;, words=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; # Change a parameter.</span>
<span class="sd">    &gt;&gt;&gt; reTokenizer.setParams(outputCol=&quot;tokens&quot;).transform(df).head()</span>
<span class="sd">    Row(text=u&#39;a b  c&#39;, tokens=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; # Temporarily modify a parameter.</span>
<span class="sd">    &gt;&gt;&gt; reTokenizer.transform(df, {reTokenizer.outputCol: &quot;words&quot;}).head()</span>
<span class="sd">    Row(text=u&#39;a b  c&#39;, words=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; reTokenizer.transform(df).head()</span>
<span class="sd">    Row(text=u&#39;a b  c&#39;, tokens=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; # Must use keyword arguments to specify params.</span>
<span class="sd">    &gt;&gt;&gt; reTokenizer.setParams(&quot;text&quot;)</span>
<span class="sd">    Traceback (most recent call last):</span>
<span class="sd">        ...</span>
<span class="sd">    TypeError: Method setParams forces keyword arguments.</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">minTokenLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;minTokenLength&quot;</span><span class="p">,</span> <span class="s">&quot;minimum token length (&gt;= 0)&quot;</span><span class="p">)</span>
    <span class="n">gaps</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;gaps&quot;</span><span class="p">,</span> <span class="s">&quot;whether regex splits on gaps (True) or matches tokens&quot;</span><span class="p">)</span>
    <span class="n">pattern</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;pattern&quot;</span><span class="p">,</span> <span class="s">&quot;regex pattern (Java dialect) used for tokenizing&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">minTokenLength</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">pattern</span><span class="o">=</span><span class="s">&quot;</span><span class="se">\\</span><span class="s">s+&quot;</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, minTokenLength=1, gaps=True, pattern=&quot;\\s+&quot;, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RegexTokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.RegexTokenizer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minTokenLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;minTokenLength&quot;</span><span class="p">,</span> <span class="s">&quot;minimum token length (&gt;= 0)&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gaps</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;gaps&quot;</span><span class="p">,</span> <span class="s">&quot;whether regex splits on gaps (True) or matches tokens&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pattern</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;pattern&quot;</span><span class="p">,</span> <span class="s">&quot;regex pattern (Java dialect) used for tokenizing&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">minTokenLength</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">pattern</span><span class="o">=</span><span class="s">&quot;</span><span class="se">\\</span><span class="s">s+&quot;</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="RegexTokenizer.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RegexTokenizer.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">minTokenLength</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">pattern</span><span class="o">=</span><span class="s">&quot;</span><span class="se">\\</span><span class="s">s+&quot;</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, minTokenLength=1, gaps=True, pattern=&quot;\\s+&quot;, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this RegexTokenizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="RegexTokenizer.setMinTokenLength"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RegexTokenizer.setMinTokenLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMinTokenLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`minTokenLength`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">minTokenLength</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="RegexTokenizer.getMinTokenLength"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RegexTokenizer.getMinTokenLength">[docs]</a>    <span class="k">def</span> <span class="nf">getMinTokenLength</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of minTokenLength or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">minTokenLength</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="RegexTokenizer.setGaps"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RegexTokenizer.setGaps">[docs]</a>    <span class="k">def</span> <span class="nf">setGaps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`gaps`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">gaps</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="RegexTokenizer.getGaps"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RegexTokenizer.getGaps">[docs]</a>    <span class="k">def</span> <span class="nf">getGaps</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of gaps or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gaps</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="RegexTokenizer.setPattern"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RegexTokenizer.setPattern">[docs]</a>    <span class="k">def</span> <span class="nf">setPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`pattern`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pattern</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="RegexTokenizer.getPattern"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RegexTokenizer.getPattern">[docs]</a>    <span class="k">def</span> <span class="nf">getPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of pattern or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pattern</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="SQLTransformer"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.SQLTransformer">[docs]</a><span class="k">class</span> <span class="nc">SQLTransformer</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Implements the transforms which are defined by SQL statement.</span>
<span class="sd">    Currently we only support SQL syntax like &#39;SELECT ... FROM __THIS__&#39;</span>
<span class="sd">    where &#39;__THIS__&#39; represents the underlying table of the input dataset.</span>

<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(0, 1.0, 3.0), (2, 2.0, 5.0)], [&quot;id&quot;, &quot;v1&quot;, &quot;v2&quot;])</span>
<span class="sd">    &gt;&gt;&gt; sqlTrans = SQLTransformer(</span>
<span class="sd">    ...     statement=&quot;SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sqlTrans.transform(df).head()</span>
<span class="sd">    Row(id=0, v1=1.0, v2=3.0, v3=4.0, v4=3.0)</span>

<span class="sd">    .. versionadded:: 1.6.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">statement</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;statement&quot;</span><span class="p">,</span> <span class="s">&quot;SQL statement&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">statement</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, statement=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SQLTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.SQLTransformer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">statement</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;statement&quot;</span><span class="p">,</span> <span class="s">&quot;SQL statement&quot;</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="SQLTransformer.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.SQLTransformer.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">statement</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, statement=None)</span>
<span class="sd">        Sets params for this SQLTransformer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="SQLTransformer.setStatement"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.SQLTransformer.setStatement">[docs]</a>    <span class="k">def</span> <span class="nf">setStatement</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`statement`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">statement</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="SQLTransformer.getStatement"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.SQLTransformer.getStatement">[docs]</a>    <span class="k">def</span> <span class="nf">getStatement</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of statement or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">statement</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="StandardScaler"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StandardScaler">[docs]</a><span class="k">class</span> <span class="nc">StandardScaler</span><span class="p">(</span><span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Standardizes features by removing the mean and scaling to unit variance using column summary</span>
<span class="sd">    statistics on the samples in the training set.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.mllib.linalg import Vectors</span>
<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(Vectors.dense([0.0]),), (Vectors.dense([2.0]),)], [&quot;a&quot;])</span>
<span class="sd">    &gt;&gt;&gt; standardScaler = StandardScaler(inputCol=&quot;a&quot;, outputCol=&quot;scaled&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = standardScaler.fit(df)</span>
<span class="sd">    &gt;&gt;&gt; model.mean</span>
<span class="sd">    DenseVector([1.0])</span>
<span class="sd">    &gt;&gt;&gt; model.std</span>
<span class="sd">    DenseVector([1.4142])</span>
<span class="sd">    &gt;&gt;&gt; model.transform(df).collect()[1].scaled</span>
<span class="sd">    DenseVector([1.4142])</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">withMean</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;withMean&quot;</span><span class="p">,</span> <span class="s">&quot;Center data with mean&quot;</span><span class="p">)</span>
    <span class="n">withStd</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;withStd&quot;</span><span class="p">,</span> <span class="s">&quot;Scale to unit standard deviation&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">withMean</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">withStd</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, withMean=False, withStd=True, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.StandardScaler&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">withMean</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;withMean&quot;</span><span class="p">,</span> <span class="s">&quot;Center data with mean&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">withStd</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;withStd&quot;</span><span class="p">,</span> <span class="s">&quot;Scale to unit standard deviation&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">withMean</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">withStd</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StandardScaler.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StandardScaler.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">withMean</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">withStd</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, withMean=False, withStd=True, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this StandardScaler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StandardScaler.setWithMean"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StandardScaler.setWithMean">[docs]</a>    <span class="k">def</span> <span class="nf">setWithMean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`withMean`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">withMean</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StandardScaler.getWithMean"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StandardScaler.getWithMean">[docs]</a>    <span class="k">def</span> <span class="nf">getWithMean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of withMean or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">withMean</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StandardScaler.setWithStd"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StandardScaler.setWithStd">[docs]</a>    <span class="k">def</span> <span class="nf">setWithStd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`withStd`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">withStd</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StandardScaler.getWithStd"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StandardScaler.getWithStd">[docs]</a>    <span class="k">def</span> <span class="nf">getWithStd</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of withStd or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">withStd</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">StandardScalerModel</span><span class="p">(</span><span class="n">java_model</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="StandardScalerModel"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StandardScalerModel">[docs]</a><span class="k">class</span> <span class="nc">StandardScalerModel</span><span class="p">(</span><span class="n">JavaModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Model fitted by StandardScaler.</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StandardScalerModel.std"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StandardScalerModel.std">[docs]</a>    <span class="k">def</span> <span class="nf">std</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Standard deviation of the StandardScalerModel.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s">&quot;std&quot;</span><span class="p">)</span>
</div>
    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StandardScalerModel.mean"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StandardScalerModel.mean">[docs]</a>    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Mean of the StandardScalerModel.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s">&quot;mean&quot;</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="StringIndexer"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StringIndexer">[docs]</a><span class="k">class</span> <span class="nc">StringIndexer</span><span class="p">(</span><span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">,</span> <span class="n">HasHandleInvalid</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    A label indexer that maps a string column of labels to an ML column of label indices.</span>
<span class="sd">    If the input column is numeric, we cast it to string and index the string values.</span>
<span class="sd">    The indices are in [0, numLabels), ordered by label frequencies.</span>
<span class="sd">    So the most frequent label gets index 0.</span>

<span class="sd">    &gt;&gt;&gt; stringIndexer = StringIndexer(inputCol=&quot;label&quot;, outputCol=&quot;indexed&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = stringIndexer.fit(stringIndDf)</span>
<span class="sd">    &gt;&gt;&gt; td = model.transform(stringIndDf)</span>
<span class="sd">    &gt;&gt;&gt; sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),</span>
<span class="sd">    ...     key=lambda x: x[0])</span>
<span class="sd">    [(0, 0.0), (1, 2.0), (2, 1.0), (3, 0.0), (4, 0.0), (5, 1.0)]</span>
<span class="sd">    &gt;&gt;&gt; inverter = IndexToString(inputCol=&quot;indexed&quot;, outputCol=&quot;label2&quot;, labels=model.labels)</span>
<span class="sd">    &gt;&gt;&gt; itd = inverter.transform(td)</span>
<span class="sd">    &gt;&gt;&gt; sorted(set([(i[0], str(i[1])) for i in itd.select(itd.id, itd.label2).collect()]),</span>
<span class="sd">    ...     key=lambda x: x[0])</span>
<span class="sd">    [(0, &#39;a&#39;), (1, &#39;b&#39;), (2, &#39;c&#39;), (3, &#39;a&#39;), (4, &#39;a&#39;), (5, &#39;c&#39;)]</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">handleInvalid</span><span class="o">=</span><span class="s">&quot;error&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, inputCol=None, outputCol=None, handleInvalid=&quot;error&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StringIndexer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.StringIndexer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">handleInvalid</span><span class="o">=</span><span class="s">&quot;error&quot;</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StringIndexer.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StringIndexer.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">handleInvalid</span><span class="o">=</span><span class="s">&quot;error&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, inputCol=None, outputCol=None, handleInvalid=&quot;error&quot;)</span>
<span class="sd">        Sets params for this StringIndexer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">StringIndexerModel</span><span class="p">(</span><span class="n">java_model</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="StringIndexerModel"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StringIndexerModel">[docs]</a><span class="k">class</span> <span class="nc">StringIndexerModel</span><span class="p">(</span><span class="n">JavaModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Model fitted by StringIndexer.</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StringIndexerModel.labels"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StringIndexerModel.labels">[docs]</a>    <span class="k">def</span> <span class="nf">labels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Ordered list of labels, corresponding to indices to be assigned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s">&quot;labels&quot;</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="IndexToString"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.IndexToString">[docs]</a><span class="k">class</span> <span class="nc">IndexToString</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    A :py:class:`Transformer` that maps a column of indices back to a new column of</span>
<span class="sd">    corresponding string values.</span>
<span class="sd">    The index-string mapping is either from the ML attributes of the input column,</span>
<span class="sd">    or from user-supplied labels (which take precedence over ML attributes).</span>
<span class="sd">    See L{StringIndexer} for converting strings into indices.</span>

<span class="sd">    .. versionadded:: 1.6.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make the labels show up in generated doc</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;labels&quot;</span><span class="p">,</span>
                   <span class="s">&quot;Optional array of labels specifying index-string mapping.&quot;</span> <span class="o">+</span>
                   <span class="s">&quot; If not provided or if empty, then metadata from inputCol is used instead.&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, inputCol=None, outputCol=None, labels=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">IndexToString</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.IndexToString&quot;</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;labels&quot;</span><span class="p">,</span>
                            <span class="s">&quot;Optional array of labels specifying index-string mapping. If not&quot;</span> <span class="o">+</span>
                            <span class="s">&quot; provided or if empty, then metadata from inputCol is used instead.&quot;</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="IndexToString.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.IndexToString.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, inputCol=None, outputCol=None, labels=None)</span>
<span class="sd">        Sets params for this IndexToString.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="IndexToString.setLabels"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.IndexToString.setLabels">[docs]</a>    <span class="k">def</span> <span class="nf">setLabels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`labels`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="IndexToString.getLabels"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.IndexToString.getLabels">[docs]</a>    <span class="k">def</span> <span class="nf">getLabels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of :py:attr:`labels` or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

</div></div>
<div class="viewcode-block" id="StopWordsRemover"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StopWordsRemover">[docs]</a><span class="k">class</span> <span class="nc">StopWordsRemover</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    A feature transformer that filters out stop words from input.</span>
<span class="sd">    Note: null values from input array are preserved unless adding null to stopWords explicitly.</span>

<span class="sd">    .. versionadded:: 1.6.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c"># a placeholder to make the stopwords show up in generated doc</span>
    <span class="n">stopWords</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;stopWords&quot;</span><span class="p">,</span> <span class="s">&quot;The words to be filtered out&quot;</span><span class="p">)</span>
    <span class="n">caseSensitive</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;caseSensitive&quot;</span><span class="p">,</span> <span class="s">&quot;whether to do a case sensitive &quot;</span> <span class="o">+</span>
                          <span class="s">&quot;comparison over the stop words&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">stopWords</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">caseSensitive</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, inputCol=None, outputCol=None, stopWords=None,\</span>
<span class="sd">                 caseSensitive=false)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StopWordsRemover</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.StopWordsRemover&quot;</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stopWords</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;stopWords&quot;</span><span class="p">,</span> <span class="s">&quot;The words to be filtered out&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">caseSensitive</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;caseSensitive&quot;</span><span class="p">,</span> <span class="s">&quot;whether to do a case &quot;</span> <span class="o">+</span>
                                   <span class="s">&quot;sensitive comparison over the stop words&quot;</span><span class="p">)</span>
        <span class="n">stopWordsObj</span> <span class="o">=</span> <span class="n">_jvm</span><span class="p">()</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">ml</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">StopWords</span>
        <span class="n">defaultStopWords</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">stopWordsObj</span><span class="o">.</span><span class="n">English</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">stopWords</span><span class="o">=</span><span class="n">defaultStopWords</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StopWordsRemover.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StopWordsRemover.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">stopWords</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                  <span class="n">caseSensitive</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, inputCol=&quot;input&quot;, outputCol=&quot;output&quot;, stopWords=None,\</span>
<span class="sd">                  caseSensitive=false)</span>
<span class="sd">        Sets params for this StopWordRemover.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StopWordsRemover.setStopWords"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StopWordsRemover.setStopWords">[docs]</a>    <span class="k">def</span> <span class="nf">setStopWords</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Specify the stopwords to be filtered.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stopWords</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StopWordsRemover.getStopWords"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StopWordsRemover.getStopWords">[docs]</a>    <span class="k">def</span> <span class="nf">getStopWords</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the stopwords.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stopWords</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StopWordsRemover.setCaseSensitive"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StopWordsRemover.setCaseSensitive">[docs]</a>    <span class="k">def</span> <span class="nf">setCaseSensitive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set whether to do a case sensitive comparison over the stop words</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">caseSensitive</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="StopWordsRemover.getCaseSensitive"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.StopWordsRemover.getCaseSensitive">[docs]</a>    <span class="k">def</span> <span class="nf">getCaseSensitive</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get whether to do a case sensitive comparison over the stop words.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">caseSensitive</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<span class="nd">@ignore_unicode_prefix</span>
<div class="viewcode-block" id="Tokenizer"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Tokenizer">[docs]</a><span class="k">class</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    A tokenizer that converts the input string to lowercase and then</span>
<span class="sd">    splits it by white spaces.</span>

<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(&quot;a b c&quot;,)], [&quot;text&quot;])</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer(inputCol=&quot;text&quot;, outputCol=&quot;words&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.transform(df).head()</span>
<span class="sd">    Row(text=u&#39;a b c&#39;, words=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; # Change a parameter.</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.setParams(outputCol=&quot;tokens&quot;).transform(df).head()</span>
<span class="sd">    Row(text=u&#39;a b c&#39;, tokens=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; # Temporarily modify a parameter.</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.transform(df, {tokenizer.outputCol: &quot;words&quot;}).head()</span>
<span class="sd">    Row(text=u&#39;a b c&#39;, words=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.transform(df).head()</span>
<span class="sd">    Row(text=u&#39;a b c&#39;, tokens=[u&#39;a&#39;, u&#39;b&#39;, u&#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; # Must use keyword arguments to specify params.</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.setParams(&quot;text&quot;)</span>
<span class="sd">    Traceback (most recent call last):</span>
<span class="sd">        ...</span>
<span class="sd">    TypeError: Method setParams forces keyword arguments.</span>

<span class="sd">    .. versionadded:: 1.3.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Tokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.Tokenizer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.3.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Tokenizer.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Tokenizer.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, inputCol=&quot;input&quot;, outputCol=&quot;output&quot;)</span>
<span class="sd">        Sets params for this Tokenizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="VectorAssembler"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorAssembler">[docs]</a><span class="k">class</span> <span class="nc">VectorAssembler</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCols</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    A feature transformer that merges multiple columns into a vector column.</span>

<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(1, 0, 3)], [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])</span>
<span class="sd">    &gt;&gt;&gt; vecAssembler = VectorAssembler(inputCols=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], outputCol=&quot;features&quot;)</span>
<span class="sd">    &gt;&gt;&gt; vecAssembler.transform(df).head().features</span>
<span class="sd">    DenseVector([1.0, 0.0, 3.0])</span>
<span class="sd">    &gt;&gt;&gt; vecAssembler.setParams(outputCol=&quot;freqs&quot;).transform(df).head().freqs</span>
<span class="sd">    DenseVector([1.0, 0.0, 3.0])</span>
<span class="sd">    &gt;&gt;&gt; params = {vecAssembler.inputCols: [&quot;b&quot;, &quot;a&quot;], vecAssembler.outputCol: &quot;vector&quot;}</span>
<span class="sd">    &gt;&gt;&gt; vecAssembler.transform(df, params).head().vector</span>
<span class="sd">    DenseVector([0.0, 1.0])</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCols</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, inputCols=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VectorAssembler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.VectorAssembler&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="VectorAssembler.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorAssembler.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCols</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, inputCols=None, outputCol=None)</span>
<span class="sd">        Sets params for this VectorAssembler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="VectorIndexer"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorIndexer">[docs]</a><span class="k">class</span> <span class="nc">VectorIndexer</span><span class="p">(</span><span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Class for indexing categorical feature columns in a dataset of [[Vector]].</span>

<span class="sd">    This has 2 usage modes:</span>
<span class="sd">      - Automatically identify categorical features (default behavior)</span>
<span class="sd">         - This helps process a dataset of unknown vectors into a dataset with some continuous</span>
<span class="sd">           features and some categorical features. The choice between continuous and categorical</span>
<span class="sd">           is based upon a maxCategories parameter.</span>
<span class="sd">         - Set maxCategories to the maximum number of categorical any categorical feature should</span>
<span class="sd">           have.</span>
<span class="sd">         - E.g.: Feature 0 has unique values {-1.0, 0.0}, and feature 1 values {1.0, 3.0, 5.0}.</span>
<span class="sd">           If maxCategories = 2, then feature 0 will be declared categorical and use indices {0, 1},</span>
<span class="sd">           and feature 1 will be declared continuous.</span>
<span class="sd">      - Index all features, if all features are categorical</span>
<span class="sd">         - If maxCategories is set to be very large, then this will build an index of unique</span>
<span class="sd">           values for all features.</span>
<span class="sd">         - Warning: This can cause problems if features are continuous since this will collect ALL</span>
<span class="sd">           unique values to the driver.</span>
<span class="sd">         - E.g.: Feature 0 has unique values {-1.0, 0.0}, and feature 1 values {1.0, 3.0, 5.0}.</span>
<span class="sd">           If maxCategories &gt;= 3, then both features will be declared categorical.</span>

<span class="sd">     This returns a model which can transform categorical features to use 0-based indices.</span>

<span class="sd">    Index stability:</span>
<span class="sd">      - This is not guaranteed to choose the same category index across multiple runs.</span>
<span class="sd">      - If a categorical feature includes value 0, then this is guaranteed to map value 0 to</span>
<span class="sd">        index 0. This maintains vector sparsity.</span>
<span class="sd">      - More stability may be added in the future.</span>

<span class="sd">     TODO: Future extensions: The following functionality is planned for the future:</span>
<span class="sd">      - Preserve metadata in transform; if a feature&#39;s metadata is already present,</span>
<span class="sd">        do not recompute.</span>
<span class="sd">      - Specify certain features to not index, either via a parameter or via existing metadata.</span>
<span class="sd">      - Add warning if a categorical feature has only 1 category.</span>
<span class="sd">      - Add option for allowing unknown categories.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.mllib.linalg import Vectors</span>
<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([(Vectors.dense([-1.0, 0.0]),),</span>
<span class="sd">    ...     (Vectors.dense([0.0, 1.0]),), (Vectors.dense([0.0, 2.0]),)], [&quot;a&quot;])</span>
<span class="sd">    &gt;&gt;&gt; indexer = VectorIndexer(maxCategories=2, inputCol=&quot;a&quot;, outputCol=&quot;indexed&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = indexer.fit(df)</span>
<span class="sd">    &gt;&gt;&gt; model.transform(df).head().indexed</span>
<span class="sd">    DenseVector([1.0, 0.0])</span>
<span class="sd">    &gt;&gt;&gt; model.numFeatures</span>
<span class="sd">    2</span>
<span class="sd">    &gt;&gt;&gt; model.categoryMaps</span>
<span class="sd">    {0: {0.0: 0, -1.0: 1}}</span>
<span class="sd">    &gt;&gt;&gt; indexer.setParams(outputCol=&quot;test&quot;).fit(df).transform(df).collect()[1].test</span>
<span class="sd">    DenseVector([0.0, 1.0])</span>
<span class="sd">    &gt;&gt;&gt; params = {indexer.maxCategories: 3, indexer.outputCol: &quot;vector&quot;}</span>
<span class="sd">    &gt;&gt;&gt; model2 = indexer.fit(df, params)</span>
<span class="sd">    &gt;&gt;&gt; model2.transform(df).head().vector</span>
<span class="sd">    DenseVector([1.0, 0.0])</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">maxCategories</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;maxCategories&quot;</span><span class="p">,</span>
                          <span class="s">&quot;Threshold for the number of values a categorical feature can take &quot;</span> <span class="o">+</span>
                          <span class="s">&quot;(&gt;= 2). If a feature is found to have &gt; maxCategories values, then &quot;</span> <span class="o">+</span>
                          <span class="s">&quot;it is declared continuous.&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxCategories</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, maxCategories=20, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VectorIndexer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.VectorIndexer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxCategories</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;maxCategories&quot;</span><span class="p">,</span>
                                   <span class="s">&quot;Threshold for the number of values a categorical feature &quot;</span> <span class="o">+</span>
                                   <span class="s">&quot;can take (&gt;= 2). If a feature is found to have &quot;</span> <span class="o">+</span>
                                   <span class="s">&quot;&gt; maxCategories values, then it is declared continuous.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">maxCategories</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="VectorIndexer.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorIndexer.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxCategories</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, maxCategories=20, inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this VectorIndexer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="VectorIndexer.setMaxCategories"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorIndexer.setMaxCategories">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxCategories</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`maxCategories`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">maxCategories</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="VectorIndexer.getMaxCategories"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorIndexer.getMaxCategories">[docs]</a>    <span class="k">def</span> <span class="nf">getMaxCategories</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of maxCategories or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">maxCategories</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">VectorIndexerModel</span><span class="p">(</span><span class="n">java_model</span><span class="p">)</span>

</div>
<span class="k">class</span> <span class="nc">VectorIndexerModel</span><span class="p">(</span><span class="n">JavaModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Model fitted by VectorIndexer.</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">numFeatures</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Number of features, i.e., length of Vectors which this transforms.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s">&quot;numFeatures&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">categoryMaps</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Feature value index.  Keys are categorical feature indices (column indices).</span>
<span class="sd">        Values are maps from original features values to 0-based category indices.</span>
<span class="sd">        If a feature is not in this map, it is treated as continuous.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s">&quot;javaCategoryMaps&quot;</span><span class="p">)</span>


<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="VectorSlicer"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorSlicer">[docs]</a><span class="k">class</span> <span class="nc">VectorSlicer</span><span class="p">(</span><span class="n">JavaTransformer</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    This class takes a feature vector and outputs a new feature vector with a subarray</span>
<span class="sd">    of the original features.</span>

<span class="sd">    The subset of features can be specified with either indices (`setIndices()`)</span>
<span class="sd">    or names (`setNames()`).  At least one feature must be selected. Duplicate features</span>
<span class="sd">    are not allowed, so there can be no overlap between selected indices and names.</span>

<span class="sd">    The output vector will order features with the selected indices first (in the order given),</span>
<span class="sd">    followed by the selected names (in the order given).</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.mllib.linalg import Vectors</span>
<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([</span>
<span class="sd">    ...     (Vectors.dense([-2.0, 2.3, 0.0, 0.0, 1.0]),),</span>
<span class="sd">    ...     (Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0]),),</span>
<span class="sd">    ...     (Vectors.dense([0.6, -1.1, -3.0, 4.5, 3.3]),)], [&quot;features&quot;])</span>
<span class="sd">    &gt;&gt;&gt; vs = VectorSlicer(inputCol=&quot;features&quot;, outputCol=&quot;sliced&quot;, indices=[1, 4])</span>
<span class="sd">    &gt;&gt;&gt; vs.transform(df).head().sliced</span>
<span class="sd">    DenseVector([2.3, 1.0])</span>

<span class="sd">    .. versionadded:: 1.6.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;indices&quot;</span><span class="p">,</span> <span class="s">&quot;An array of indices to select features from &quot;</span> <span class="o">+</span>
                    <span class="s">&quot;a vector column. There can be no overlap with names.&quot;</span><span class="p">)</span>
    <span class="n">names</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;names&quot;</span><span class="p">,</span> <span class="s">&quot;An array of feature names to select features from &quot;</span> <span class="o">+</span>
                  <span class="s">&quot;a vector column. These names must be specified by ML &quot;</span> <span class="o">+</span>
                  <span class="s">&quot;org.apache.spark.ml.attribute.Attribute. There can be no overlap with &quot;</span> <span class="o">+</span>
                  <span class="s">&quot;indices.&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, inputCol=None, outputCol=None, indices=None, names=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VectorSlicer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.VectorSlicer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;indices&quot;</span><span class="p">,</span> <span class="s">&quot;An array of indices to select features from &quot;</span> <span class="o">+</span>
                             <span class="s">&quot;a vector column. There can be no overlap with names.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">names</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;names&quot;</span><span class="p">,</span> <span class="s">&quot;An array of feature names to select features from &quot;</span> <span class="o">+</span>
                           <span class="s">&quot;a vector column. These names must be specified by ML &quot;</span> <span class="o">+</span>
                           <span class="s">&quot;org.apache.spark.ml.attribute.Attribute. There can be no overlap &quot;</span> <span class="o">+</span>
                           <span class="s">&quot;with indices.&quot;</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="VectorSlicer.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorSlicer.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, inputCol=None, outputCol=None, indices=None, names=None):</span>
<span class="sd">        Sets params for this VectorSlicer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="VectorSlicer.setIndices"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorSlicer.setIndices">[docs]</a>    <span class="k">def</span> <span class="nf">setIndices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`indices`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="VectorSlicer.getIndices"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorSlicer.getIndices">[docs]</a>    <span class="k">def</span> <span class="nf">getIndices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of indices or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="VectorSlicer.setNames"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorSlicer.setNames">[docs]</a>    <span class="k">def</span> <span class="nf">setNames</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`names`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">names</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.6.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="VectorSlicer.getNames"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.VectorSlicer.getNames">[docs]</a>    <span class="k">def</span> <span class="nf">getNames</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of names or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">names</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<span class="nd">@ignore_unicode_prefix</span>
<div class="viewcode-block" id="Word2Vec"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2Vec">[docs]</a><span class="k">class</span> <span class="nc">Word2Vec</span><span class="p">(</span><span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">HasStepSize</span><span class="p">,</span> <span class="n">HasMaxIter</span><span class="p">,</span> <span class="n">HasSeed</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Word2Vec trains a model of `Map(String, Vector)`, i.e. transforms a word into a code for further</span>
<span class="sd">    natural language processing or machine learning process.</span>

<span class="sd">    &gt;&gt;&gt; sent = (&quot;a b &quot; * 100 + &quot;a c &quot; * 10).split(&quot; &quot;)</span>
<span class="sd">    &gt;&gt;&gt; doc = sqlContext.createDataFrame([(sent,), (sent,)], [&quot;sentence&quot;])</span>
<span class="sd">    &gt;&gt;&gt; model = Word2Vec(vectorSize=5, seed=42, inputCol=&quot;sentence&quot;, outputCol=&quot;model&quot;).fit(doc)</span>
<span class="sd">    &gt;&gt;&gt; model.getVectors().show()</span>
<span class="sd">    +----+--------------------+</span>
<span class="sd">    |word|              vector|</span>
<span class="sd">    +----+--------------------+</span>
<span class="sd">    |   a|[0.09461779892444...|</span>
<span class="sd">    |   b|[1.15474212169647...|</span>
<span class="sd">    |   c|[-0.3794820010662...|</span>
<span class="sd">    +----+--------------------+</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; model.findSynonyms(&quot;a&quot;, 2).show()</span>
<span class="sd">    +----+--------------------+</span>
<span class="sd">    |word|          similarity|</span>
<span class="sd">    +----+--------------------+</span>
<span class="sd">    |   b| 0.16782984556103436|</span>
<span class="sd">    |   c|-0.46761559092107646|</span>
<span class="sd">    +----+--------------------+</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; model.transform(doc).head().model</span>
<span class="sd">    DenseVector([0.5524, -0.4995, -0.3599, 0.0241, 0.3461])</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">vectorSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;vectorSize&quot;</span><span class="p">,</span>
                       <span class="s">&quot;the dimension of codes after transforming from words&quot;</span><span class="p">)</span>
    <span class="n">numPartitions</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;numPartitions&quot;</span><span class="p">,</span>
                          <span class="s">&quot;number of partitions for sentences of words&quot;</span><span class="p">)</span>
    <span class="n">minCount</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;minCount&quot;</span><span class="p">,</span>
                     <span class="s">&quot;the minimum number of times a token must appear to be included in the &quot;</span> <span class="o">+</span>
                     <span class="s">&quot;word2vec model&#39;s vocabulary&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectorSize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">minCount</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">numPartitions</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stepSize</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">maxIter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, vectorSize=100, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, \</span>
<span class="sd">                 seed=None, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Word2Vec</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.Word2Vec&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vectorSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;vectorSize&quot;</span><span class="p">,</span>
                                <span class="s">&quot;the dimension of codes after transforming from words&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">numPartitions</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;numPartitions&quot;</span><span class="p">,</span>
                                   <span class="s">&quot;number of partitions for sentences of words&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minCount</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;minCount&quot;</span><span class="p">,</span>
                              <span class="s">&quot;the minimum number of times a token must appear to be included &quot;</span> <span class="o">+</span>
                              <span class="s">&quot;in the word2vec model&#39;s vocabulary&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">vectorSize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">minCount</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">numPartitions</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stepSize</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">maxIter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                         <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Word2Vec.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2Vec.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectorSize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">minCount</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">numPartitions</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stepSize</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">maxIter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, \</span>
<span class="sd">                 inputCol=None, outputCol=None)</span>
<span class="sd">        Sets params for this Word2Vec.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Word2Vec.setVectorSize"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2Vec.setVectorSize">[docs]</a>    <span class="k">def</span> <span class="nf">setVectorSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`vectorSize`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vectorSize</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Word2Vec.getVectorSize"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2Vec.getVectorSize">[docs]</a>    <span class="k">def</span> <span class="nf">getVectorSize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of vectorSize or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vectorSize</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Word2Vec.setNumPartitions"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2Vec.setNumPartitions">[docs]</a>    <span class="k">def</span> <span class="nf">setNumPartitions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`numPartitions`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">numPartitions</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Word2Vec.getNumPartitions"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2Vec.getNumPartitions">[docs]</a>    <span class="k">def</span> <span class="nf">getNumPartitions</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of numPartitions or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numPartitions</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Word2Vec.setMinCount"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2Vec.setMinCount">[docs]</a>    <span class="k">def</span> <span class="nf">setMinCount</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`minCount`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">minCount</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.4.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Word2Vec.getMinCount"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2Vec.getMinCount">[docs]</a>    <span class="k">def</span> <span class="nf">getMinCount</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of minCount or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">minCount</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Word2VecModel</span><span class="p">(</span><span class="n">java_model</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="Word2VecModel"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2VecModel">[docs]</a><span class="k">class</span> <span class="nc">Word2VecModel</span><span class="p">(</span><span class="n">JavaModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Model fitted by Word2Vec.</span>

<span class="sd">    .. versionadded:: 1.4.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Word2VecModel.getVectors"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2VecModel.getVectors">[docs]</a>    <span class="k">def</span> <span class="nf">getVectors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the vector representation of the words as a dataframe</span>
<span class="sd">        with two fields, word and vector.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s">&quot;getVectors&quot;</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="Word2VecModel.findSynonyms"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.Word2VecModel.findSynonyms">[docs]</a>    <span class="k">def</span> <span class="nf">findSynonyms</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">num</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find &quot;num&quot; number of words closest in similarity to &quot;word&quot;.</span>
<span class="sd">        word can be a string or vector representation.</span>
<span class="sd">        Returns a dataframe with two fields word and similarity (which</span>
<span class="sd">        gives the cosine similarity).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="nb">basestring</span><span class="p">):</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">_convert_to_vector</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s">&quot;findSynonyms&quot;</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>

</div></div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="PCA"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.PCA">[docs]</a><span class="k">class</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">HasInputCol</span><span class="p">,</span> <span class="n">HasOutputCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    PCA trains a model to project vectors to a low-dimensional space using PCA.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.mllib.linalg import Vectors</span>
<span class="sd">    &gt;&gt;&gt; data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),</span>
<span class="sd">    ...     (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),</span>
<span class="sd">    ...     (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]</span>
<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame(data,[&quot;features&quot;])</span>
<span class="sd">    &gt;&gt;&gt; pca = PCA(k=2, inputCol=&quot;features&quot;, outputCol=&quot;pca_features&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = pca.fit(df)</span>
<span class="sd">    &gt;&gt;&gt; model.transform(df).collect()[0].pca_features</span>
<span class="sd">    DenseVector([1.648..., -4.013...])</span>

<span class="sd">    .. versionadded:: 1.5.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="s">&quot;the number of principal components&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, k=None, inputCol=None, outputCol=None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PCA</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.PCA&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="s">&quot;the number of principal components&quot;</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="PCA.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.PCA.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, k=None, inputCol=None, outputCol=None)</span>
<span class="sd">        Set params for this PCA.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="PCA.setK"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.PCA.setK">[docs]</a>    <span class="k">def</span> <span class="nf">setK</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`k`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="PCA.getK"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.PCA.getK">[docs]</a>    <span class="k">def</span> <span class="nf">getK</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of k or its default value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">PCAModel</span><span class="p">(</span><span class="n">java_model</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="PCAModel"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.PCAModel">[docs]</a><span class="k">class</span> <span class="nc">PCAModel</span><span class="p">(</span><span class="n">JavaModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Model fitted by PCA.</span>

<span class="sd">    .. versionadded:: 1.5.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

</div>
<span class="nd">@inherit_doc</span>
<div class="viewcode-block" id="RFormula"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RFormula">[docs]</a><span class="k">class</span> <span class="nc">RFormula</span><span class="p">(</span><span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">HasFeaturesCol</span><span class="p">,</span> <span class="n">HasLabelCol</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Implements the transforms required for fitting a dataset against an</span>
<span class="sd">    R model formula. Currently we support a limited subset of the R</span>
<span class="sd">    operators, including &#39;~&#39;, &#39;.&#39;, &#39;:&#39;, &#39;+&#39;, and &#39;-&#39;. Also see the R formula</span>
<span class="sd">    docs:</span>
<span class="sd">    http://stat.ethz.ch/R-manual/R-patched/library/stats/html/formula.html</span>

<span class="sd">    &gt;&gt;&gt; df = sqlContext.createDataFrame([</span>
<span class="sd">    ...     (1.0, 1.0, &quot;a&quot;),</span>
<span class="sd">    ...     (0.0, 2.0, &quot;b&quot;),</span>
<span class="sd">    ...     (0.0, 0.0, &quot;a&quot;)</span>
<span class="sd">    ... ], [&quot;y&quot;, &quot;x&quot;, &quot;s&quot;])</span>
<span class="sd">    &gt;&gt;&gt; rf = RFormula(formula=&quot;y ~ x + s&quot;)</span>
<span class="sd">    &gt;&gt;&gt; rf.fit(df).transform(df).show()</span>
<span class="sd">    +---+---+---+---------+-----+</span>
<span class="sd">    |  y|  x|  s| features|label|</span>
<span class="sd">    +---+---+---+---------+-----+</span>
<span class="sd">    |1.0|1.0|  a|[1.0,1.0]|  1.0|</span>
<span class="sd">    |0.0|2.0|  b|[2.0,0.0]|  0.0|</span>
<span class="sd">    |0.0|0.0|  a|[0.0,1.0]|  0.0|</span>
<span class="sd">    +---+---+---+---------+-----+</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; rf.fit(df, {rf.formula: &quot;y ~ . - s&quot;}).transform(df).show()</span>
<span class="sd">    +---+---+---+--------+-----+</span>
<span class="sd">    |  y|  x|  s|features|label|</span>
<span class="sd">    +---+---+---+--------+-----+</span>
<span class="sd">    |1.0|1.0|  a|   [1.0]|  1.0|</span>
<span class="sd">    |0.0|2.0|  b|   [2.0]|  0.0|</span>
<span class="sd">    |0.0|0.0|  a|   [0.0]|  0.0|</span>
<span class="sd">    +---+---+---+--------+-----+</span>
<span class="sd">    ...</span>

<span class="sd">    .. versionadded:: 1.5.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c"># a placeholder to make it appear in the generated doc</span>
    <span class="n">formula</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s">&quot;formula&quot;</span><span class="p">,</span> <span class="s">&quot;R model formula&quot;</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">formula</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">featuresCol</span><span class="o">=</span><span class="s">&quot;features&quot;</span><span class="p">,</span> <span class="n">labelCol</span><span class="o">=</span><span class="s">&quot;label&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__(self, formula=None, featuresCol=&quot;features&quot;, labelCol=&quot;label&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RFormula</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_java_obj</span><span class="p">(</span><span class="s">&quot;org.apache.spark.ml.feature.RFormula&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">uid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">formula</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&quot;formula&quot;</span><span class="p">,</span> <span class="s">&quot;R model formula&quot;</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__init__</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="RFormula.setParams"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RFormula.setParams">[docs]</a>    <span class="k">def</span> <span class="nf">setParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">formula</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">featuresCol</span><span class="o">=</span><span class="s">&quot;features&quot;</span><span class="p">,</span> <span class="n">labelCol</span><span class="o">=</span><span class="s">&quot;label&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        setParams(self, formula=None, featuresCol=&quot;features&quot;, labelCol=&quot;label&quot;)</span>
<span class="sd">        Sets params for RFormula.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setParams</span><span class="o">.</span><span class="n">_input_kwargs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="RFormula.setFormula"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RFormula.setFormula">[docs]</a>    <span class="k">def</span> <span class="nf">setFormula</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the value of :py:attr:`formula`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paramMap</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">formula</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="bp">self</span>
</div>
    <span class="nd">@since</span><span class="p">(</span><span class="s">&quot;1.5.0&quot;</span><span class="p">)</span>
<div class="viewcode-block" id="RFormula.getFormula"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RFormula.getFormula">[docs]</a>    <span class="k">def</span> <span class="nf">getFormula</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the value of :py:attr:`formula`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">formula</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">RFormulaModel</span><span class="p">(</span><span class="n">java_model</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="RFormulaModel"><a class="viewcode-back" href="../../../pyspark.ml.html#pyspark.ml.feature.RFormulaModel">[docs]</a><span class="k">class</span> <span class="nc">RFormulaModel</span><span class="p">(</span><span class="n">JavaModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Experimental</span>

<span class="sd">    Model fitted by :py:class:`RFormula`.</span>

<span class="sd">    .. versionadded:: 1.5.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

</div>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark.context</span> <span class="kn">import</span> <span class="n">SparkContext</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span><span class="p">,</span> <span class="n">SQLContext</span>
    <span class="n">globs</span> <span class="o">=</span> <span class="nb">globals</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="c"># The small batch size here ensures that we see multiple batches,</span>
    <span class="c"># even in these small test examples:</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="s">&quot;local[2]&quot;</span><span class="p">,</span> <span class="s">&quot;ml.feature tests&quot;</span><span class="p">)</span>
    <span class="n">sqlContext</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
    <span class="n">globs</span><span class="p">[</span><span class="s">&#39;sc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sc</span>
    <span class="n">globs</span><span class="p">[</span><span class="s">&#39;sqlContext&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sqlContext</span>
    <span class="n">testData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;a&quot;</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;b&quot;</span><span class="p">),</span>
                               <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;c&quot;</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;a&quot;</span><span class="p">),</span>
                               <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;a&quot;</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;c&quot;</span><span class="p">)],</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">globs</span><span class="p">[</span><span class="s">&#39;stringIndDf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">testData</span><span class="p">)</span>
    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span><span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span> <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span><span class="p">)</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="nb">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/spark-logo-hd.png" alt="Logo"/>
            </a></p>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
    
        <li><a href="../../../index.html">PySpark 1.6.2 documentation</a> &raquo;</li>

          <li><a href="../../index.html" >Module code</a> &raquo;</li>
          <li><a href="../../pyspark.html" >pyspark</a> &raquo;</li>
          <li><a href="../ml.html" >pyspark.ml</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>