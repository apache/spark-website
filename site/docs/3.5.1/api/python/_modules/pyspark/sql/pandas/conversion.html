
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>pyspark.sql.pandas.conversion &#8212; PySpark master documentation</title>
    
    <link href="../../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" href="../../../../_static/styles/pydata-sphinx-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/pyspark.css" />
    
    <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <link rel="canonical" href="https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/pandas/conversion.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../../index.html">
  <img src="../../../../_static/spark-logo-reverse.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../index.html">
  Overview
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../getting_started/index.html">
  Getting Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../user_guide/index.html">
  User Guides
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../reference/index.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../development/index.html">
  Development
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../migration_guide/index.html">
  Migration Guides
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <!--
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<div id="version-button" class="dropdown">
    <button type="button" class="btn btn-secondary btn-sm navbar-btn dropdown-toggle" id="version_switcher_button" data-toggle="dropdown">
        master
        <span class="caret"></span>
    </button>
    <div id="version_switcher" class="dropdown-menu list-group-flush py-0" aria-labelledby="version_switcher_button">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
</div>

<script type="text/javascript">
// Function to construct the target URL from the JSON components
function buildURL(entry) {
    var template = "https://spark.apache.org/docs/{version}/api/python/index.html";  // supplied by jinja
    template = template.replace("{version}", entry.version);
    return template;
}

// Function to check if corresponding page path exists in other version of docs
// and, if so, go there instead of the homepage of the other docs version
function checkPageExistsAndRedirect(event) {
    const currentFilePath = "_modules/pyspark/sql/pandas/conversion.html",
          otherDocsHomepage = event.target.getAttribute("href");
    let tryUrl = `${otherDocsHomepage}${currentFilePath}`;
    $.ajax({
        type: 'HEAD',
        url: tryUrl,
        // if the page exists, go there
        success: function() {
            location.href = tryUrl;
        }
    }).fail(function() {
        location.href = otherDocsHomepage;
    });
    return false;
}

// Function to populate the version switcher
(function () {
    // get JSON config
    $.getJSON("https://spark.apache.org/static/versions.json", function(data, textStatus, jqXHR) {
        // create the nodes first (before AJAX calls) to ensure the order is
        // correct (for now, links will go to doc version homepage)
        $.each(data, function(index, entry) {
            // if no custom name specified (e.g., "latest"), use version string
            if (!("name" in entry)) {
                entry.name = entry.version;
            }
            // construct the appropriate URL, and add it to the dropdown
            entry.url = buildURL(entry);
            const node = document.createElement("a");
            node.setAttribute("class", "list-group-item list-group-item-action py-1");
            node.setAttribute("href", `${entry.url}`);
            node.textContent = `${entry.name}`;
            node.onclick = checkPageExistsAndRedirect;
            $("#version_switcher").append(node);
        });
    });
})();
</script>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for pyspark.sql.pandas.conversion</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="n">no_type_check</span><span class="p">,</span>
    <span class="n">overload</span><span class="p">,</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">warn</span>

<span class="kn">from</span> <span class="nn">pyspark.errors.exceptions.captured</span> <span class="kn">import</span> <span class="n">unwrap_spark_exception</span>
<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="kn">import</span> <span class="n">_load_from_socket</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.pandas.serializers</span> <span class="kn">import</span> <span class="n">ArrowCollectSerializer</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">_dedup_names</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">ArrayType</span><span class="p">,</span> <span class="n">MapType</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">,</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">DataType</span><span class="p">,</span> <span class="n">_create_row</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.utils</span> <span class="kn">import</span> <span class="n">is_timestamp_ntz_preferred</span>
<span class="kn">from</span> <span class="nn">pyspark.traceback_utils</span> <span class="kn">import</span> <span class="n">SCCallSiteSync</span>
<span class="kn">from</span> <span class="nn">pyspark.errors</span> <span class="kn">import</span> <span class="n">PySparkTypeError</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">pyarrow</span> <span class="k">as</span> <span class="nn">pa</span>
    <span class="kn">from</span> <span class="nn">py4j.java_gateway</span> <span class="kn">import</span> <span class="n">JavaObject</span>

    <span class="kn">from</span> <span class="nn">pyspark.sql.pandas._typing</span> <span class="kn">import</span> <span class="n">DataFrameLike</span> <span class="k">as</span> <span class="n">PandasDataFrameLike</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">DataFrame</span>


<span class="k">class</span> <span class="nc">PandasConversionMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mix-in for the conversion from Spark to pandas. Currently, only :class:`DataFrame`</span>
<span class="sd">    can use this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">toPandas</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PandasDataFrameLike&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.</span>

<span class="sd">        This is only available if Pandas is installed and available.</span>

<span class="sd">        .. versionadded:: 1.3.0</span>

<span class="sd">        .. versionchanged:: 3.4.0</span>
<span class="sd">            Supports Spark Connect.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This method should only be used if the resulting Pandas ``pandas.DataFrame`` is</span>
<span class="sd">        expected to be small, as all the data is loaded into the driver&#39;s memory.</span>

<span class="sd">        Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; df.toPandas()  # doctest: +SKIP</span>
<span class="sd">           age   name</span>
<span class="sd">        0    2  Alice</span>
<span class="sd">        1    5    Bob</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">_create_converter_to_pandas</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pandas_version</span>

        <span class="n">require_minimum_pandas_version</span><span class="p">()</span>

        <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

        <span class="n">jconf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparkSession</span><span class="o">.</span><span class="n">_jconf</span>

        <span class="k">if</span> <span class="n">jconf</span><span class="o">.</span><span class="n">arrowPySparkEnabled</span><span class="p">():</span>
            <span class="n">use_arrow</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">to_arrow_schema</span>
                <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pyarrow_version</span>

                <span class="n">require_minimum_pyarrow_version</span><span class="p">()</span>
                <span class="n">to_arrow_schema</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>

                <span class="k">if</span> <span class="n">jconf</span><span class="o">.</span><span class="n">arrowPySparkFallbackEnabled</span><span class="p">():</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;toPandas attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true; however, &quot;</span>
                        <span class="s2">&quot;failed by the reason below:</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="s2">&quot;Attempting non-optimization as &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; is set to &quot;</span>
                        <span class="s2">&quot;true.&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="n">use_arrow</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;toPandas attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has &quot;</span>
                        <span class="s2">&quot;reached the error below and will not continue because automatic fallback &quot;</span>
                        <span class="s2">&quot;with &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; has been set to &quot;</span>
                        <span class="s2">&quot;false.</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="k">raise</span>

            <span class="c1"># Try to use Arrow optimization when the schema is supported and the required version</span>
            <span class="c1"># of PyArrow is found, if &#39;spark.sql.execution.arrow.pyspark.enabled&#39; is enabled.</span>
            <span class="k">if</span> <span class="n">use_arrow</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">import</span> <span class="nn">pyarrow</span>

                    <span class="n">self_destruct</span> <span class="o">=</span> <span class="n">jconf</span><span class="o">.</span><span class="n">arrowPySparkSelfDestructEnabled</span><span class="p">()</span>
                    <span class="n">batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collect_as_arrow</span><span class="p">(</span><span class="n">split_batches</span><span class="o">=</span><span class="n">self_destruct</span><span class="p">)</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">table</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">Table</span><span class="o">.</span><span class="n">from_batches</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
                        <span class="c1"># Ensure only the table has a reference to the batches, so that</span>
                        <span class="c1"># self_destruct (if enabled) is effective</span>
                        <span class="k">del</span> <span class="n">batches</span>
                        <span class="c1"># Pandas DataFrame created from PyArrow uses datetime64[ns] for date type</span>
                        <span class="c1"># values, but we should use datetime.date to match the behavior with when</span>
                        <span class="c1"># Arrow optimization is disabled.</span>
                        <span class="n">pandas_options</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;date_as_object&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
                        <span class="k">if</span> <span class="n">self_destruct</span><span class="p">:</span>
                            <span class="c1"># Configure PyArrow to use as little memory as possible:</span>
                            <span class="c1"># self_destruct - free columns as they are converted</span>
                            <span class="c1"># split_blocks - create a separate Pandas block for each column</span>
                            <span class="c1"># use_threads - convert one column at a time</span>
                            <span class="n">pandas_options</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                                <span class="p">{</span>
                                    <span class="s2">&quot;self_destruct&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                                    <span class="s2">&quot;split_blocks&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                                    <span class="s2">&quot;use_threads&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                                <span class="p">}</span>
                            <span class="p">)</span>
                        <span class="c1"># Rename columns to avoid duplicated column names.</span>
                        <span class="n">pdf</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">rename_columns</span><span class="p">(</span>
                            <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;col_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">num_columns</span><span class="p">)]</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">(</span><span class="o">**</span><span class="n">pandas_options</span><span class="p">)</span>

                        <span class="c1"># Rename back to the original column names.</span>
                        <span class="n">pdf</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">columns</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">pdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">timezone</span> <span class="o">=</span> <span class="n">jconf</span><span class="o">.</span><span class="n">sessionLocalTimeZone</span><span class="p">()</span>
                        <span class="n">struct_in_pandas</span> <span class="o">=</span> <span class="n">jconf</span><span class="o">.</span><span class="n">pandasStructHandlingMode</span><span class="p">()</span>

                        <span class="n">error_on_duplicated_field_names</span> <span class="o">=</span> <span class="kc">False</span>
                        <span class="k">if</span> <span class="n">struct_in_pandas</span> <span class="o">==</span> <span class="s2">&quot;legacy&quot;</span><span class="p">:</span>
                            <span class="n">error_on_duplicated_field_names</span> <span class="o">=</span> <span class="kc">True</span>
                            <span class="n">struct_in_pandas</span> <span class="o">=</span> <span class="s2">&quot;dict&quot;</span>

                        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                            <span class="p">[</span>
                                <span class="n">_create_converter_to_pandas</span><span class="p">(</span>
                                    <span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span>
                                    <span class="n">field</span><span class="o">.</span><span class="n">nullable</span><span class="p">,</span>
                                    <span class="n">timezone</span><span class="o">=</span><span class="n">timezone</span><span class="p">,</span>
                                    <span class="n">struct_in_pandas</span><span class="o">=</span><span class="n">struct_in_pandas</span><span class="p">,</span>
                                    <span class="n">error_on_duplicated_field_names</span><span class="o">=</span><span class="n">error_on_duplicated_field_names</span><span class="p">,</span>
                                <span class="p">)(</span><span class="n">pser</span><span class="p">)</span>
                                <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">pser</span><span class="p">),</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="o">.</span><span class="n">fields</span><span class="p">)</span>
                            <span class="p">],</span>
                            <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">pdf</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="c1"># We might have to allow fallback here as well but multiple Spark jobs can</span>
                    <span class="c1"># be executed. So, simply fail in this case for now.</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;toPandas attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has &quot;</span>
                        <span class="s2">&quot;reached the error below and can not continue. Note that &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; does not have an &quot;</span>
                        <span class="s2">&quot;effect on failures in the middle of &quot;</span>
                        <span class="s2">&quot;computation.</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="k">raise</span>

        <span class="c1"># Below is toPandas without Arrow optimization.</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">(</span>
                <span class="n">rows</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)),</span> <span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">timezone</span> <span class="o">=</span> <span class="n">jconf</span><span class="o">.</span><span class="n">sessionLocalTimeZone</span><span class="p">()</span>
            <span class="n">struct_in_pandas</span> <span class="o">=</span> <span class="n">jconf</span><span class="o">.</span><span class="n">pandasStructHandlingMode</span><span class="p">()</span>

            <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">_create_converter_to_pandas</span><span class="p">(</span>
                        <span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span>
                        <span class="n">field</span><span class="o">.</span><span class="n">nullable</span><span class="p">,</span>
                        <span class="n">timezone</span><span class="o">=</span><span class="n">timezone</span><span class="p">,</span>
                        <span class="n">struct_in_pandas</span><span class="o">=</span><span class="p">(</span>
                            <span class="s2">&quot;row&quot;</span> <span class="k">if</span> <span class="n">struct_in_pandas</span> <span class="o">==</span> <span class="s2">&quot;legacy&quot;</span> <span class="k">else</span> <span class="n">struct_in_pandas</span>
                        <span class="p">),</span>
                        <span class="n">error_on_duplicated_field_names</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">timestamp_utc_localized</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">)(</span><span class="n">pser</span><span class="p">)</span>
                    <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">pser</span><span class="p">),</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="o">.</span><span class="n">fields</span><span class="p">)</span>
                <span class="p">],</span>
                <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pdf</span>

    <span class="k">def</span> <span class="nf">_collect_as_arrow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_batches</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;pa.RecordBatch&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns all records as a list of ArrowRecordBatches, pyarrow must be installed</span>
<span class="sd">        and available on driver and worker Python environments.</span>
<span class="sd">        This is an experimental feature.</span>

<span class="sd">        :param split_batches: split batches such that each column is in its own allocation, so</span>
<span class="sd">            that the selfDestruct optimization is effective; default False.</span>

<span class="sd">        .. note:: Experimental.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">SCCallSiteSync</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sc</span><span class="p">):</span>
            <span class="p">(</span>
                <span class="n">port</span><span class="p">,</span>
                <span class="n">auth_secret</span><span class="p">,</span>
                <span class="n">jsocket_auth_server</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jdf</span><span class="o">.</span><span class="n">collectAsArrowToPython</span><span class="p">()</span>

        <span class="c1"># Collect list of un-ordered batches where last element is a list of correct order indices</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">batch_stream</span> <span class="o">=</span> <span class="n">_load_from_socket</span><span class="p">((</span><span class="n">port</span><span class="p">,</span> <span class="n">auth_secret</span><span class="p">),</span> <span class="n">ArrowCollectSerializer</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">split_batches</span><span class="p">:</span>
                <span class="c1"># When spark.sql.execution.arrow.pyspark.selfDestruct.enabled, ensure</span>
                <span class="c1"># each column in each record batch is contained in its own allocation.</span>
                <span class="c1"># Otherwise, selfDestruct does nothing; it frees each column as its</span>
                <span class="c1"># converted, but each column will actually be a list of slices of record</span>
                <span class="c1"># batches, and so no memory is actually freed until all columns are</span>
                <span class="c1"># converted.</span>
                <span class="kn">import</span> <span class="nn">pyarrow</span> <span class="k">as</span> <span class="nn">pa</span>

                <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">batch_or_indices</span> <span class="ow">in</span> <span class="n">batch_stream</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_or_indices</span><span class="p">,</span> <span class="n">pa</span><span class="o">.</span><span class="n">RecordBatch</span><span class="p">):</span>
                        <span class="n">batch_or_indices</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">RecordBatch</span><span class="o">.</span><span class="n">from_arrays</span><span class="p">(</span>
                            <span class="p">[</span>
                                <span class="c1"># This call actually reallocates the array</span>
                                <span class="n">pa</span><span class="o">.</span><span class="n">concat_arrays</span><span class="p">([</span><span class="n">array</span><span class="p">])</span>
                                <span class="k">for</span> <span class="n">array</span> <span class="ow">in</span> <span class="n">batch_or_indices</span>
                            <span class="p">],</span>
                            <span class="n">schema</span><span class="o">=</span><span class="n">batch_or_indices</span><span class="o">.</span><span class="n">schema</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_or_indices</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">batch_stream</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">unwrap_spark_exception</span><span class="p">():</span>
                <span class="c1"># Join serving thread and raise any exceptions from collectAsArrowToPython</span>
                <span class="n">jsocket_auth_server</span><span class="o">.</span><span class="n">getResult</span><span class="p">()</span>

        <span class="c1"># Separate RecordBatches from batch order indices in results</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="n">results</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">batch_order</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Re-order the batch list using the correct order</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">batches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">batch_order</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">SparkConversionMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Min-in for the conversion from pandas to Spark. Currently, only :class:`SparkSession`</span>
<span class="sd">    can use this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_jsparkSession</span><span class="p">:</span> <span class="s2">&quot;JavaObject&quot;</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">createDataFrame</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="s2">&quot;PandasDataFrameLike&quot;</span><span class="p">,</span> <span class="n">samplingRatio</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DataFrame&quot;</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">createDataFrame</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="s2">&quot;PandasDataFrameLike&quot;</span><span class="p">,</span>
        <span class="n">schema</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">verifySchema</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DataFrame&quot;</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">createDataFrame</span><span class="p">(</span>  <span class="c1"># type: ignore[misc]</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="s2">&quot;PandasDataFrameLike&quot;</span><span class="p">,</span>
        <span class="n">schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">samplingRatio</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verifySchema</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DataFrame&quot;</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pandas_version</span>

        <span class="n">require_minimum_pandas_version</span><span class="p">()</span>

        <span class="n">timezone</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jconf</span><span class="o">.</span><span class="n">sessionLocalTimeZone</span><span class="p">()</span>

        <span class="c1"># If no schema supplied by user then get the names of columns only</span>
        <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">schema</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jconf</span><span class="o">.</span><span class="n">arrowPySparkEnabled</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_from_pandas_with_arrow</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">timezone</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jconf</span><span class="o">.</span><span class="n">arrowPySparkFallbackEnabled</span><span class="p">():</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;createDataFrame attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true; however, &quot;</span>
                        <span class="s2">&quot;failed by the reason below:</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="s2">&quot;Attempting non-optimization as &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; is set to &quot;</span>
                        <span class="s2">&quot;true.&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;createDataFrame attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has &quot;</span>
                        <span class="s2">&quot;reached the error below and will not continue because automatic &quot;</span>
                        <span class="s2">&quot;fallback with &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; &quot;</span>
                        <span class="s2">&quot;has been set to false.</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="k">raise</span>
        <span class="n">converted_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_from_pandas</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">timezone</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_dataframe</span><span class="p">(</span><span class="n">converted_data</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">samplingRatio</span><span class="p">,</span> <span class="n">verifySchema</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert_from_pandas</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">pdf</span><span class="p">:</span> <span class="s2">&quot;PandasDataFrameLike&quot;</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">timezone</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert a pandas.DataFrame to list of records that can be used to make a DataFrame</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        list</span>
<span class="sd">            list of records</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">timezone</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="p">(</span>
                <span class="n">_check_series_convert_timestamps_tz_local</span><span class="p">,</span>
                <span class="n">_get_local_timezone</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="kn">from</span> <span class="nn">pandas.core.dtypes.common</span> <span class="kn">import</span> <span class="n">is_datetime64tz_dtype</span><span class="p">,</span> <span class="n">is_timedelta64_dtype</span>

            <span class="n">copied</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">StructType</span><span class="p">):</span>

                <span class="k">def</span> <span class="nf">_create_converter</span><span class="p">(</span><span class="n">data_type</span><span class="p">:</span> <span class="n">DataType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">],</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_type</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">):</span>

                        <span class="k">def</span> <span class="nf">correct_timestamp</span><span class="p">(</span><span class="n">pser</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">:</span>
                            <span class="k">return</span> <span class="n">_check_series_convert_timestamps_tz_local</span><span class="p">(</span><span class="n">pser</span><span class="p">,</span> <span class="n">timezone</span><span class="p">)</span>

                        <span class="k">return</span> <span class="n">correct_timestamp</span>

                    <span class="k">def</span> <span class="nf">_converter</span><span class="p">(</span><span class="n">dt</span><span class="p">:</span> <span class="n">DataType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Any</span><span class="p">]]:</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">ArrayType</span><span class="p">):</span>
                            <span class="n">element_conv</span> <span class="o">=</span> <span class="n">_converter</span><span class="p">(</span><span class="n">dt</span><span class="o">.</span><span class="n">elementType</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>

                            <span class="k">def</span> <span class="nf">convert_array</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                                    <span class="k">return</span> <span class="kc">None</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="k">return</span> <span class="p">[</span><span class="n">element_conv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">value</span><span class="p">]</span>

                            <span class="k">return</span> <span class="n">convert_array</span>

                        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">MapType</span><span class="p">):</span>
                            <span class="n">key_conv</span> <span class="o">=</span> <span class="n">_converter</span><span class="p">(</span><span class="n">dt</span><span class="o">.</span><span class="n">keyType</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
                            <span class="n">value_conv</span> <span class="o">=</span> <span class="n">_converter</span><span class="p">(</span><span class="n">dt</span><span class="o">.</span><span class="n">valueType</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>

                            <span class="k">def</span> <span class="nf">convert_map</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                                    <span class="k">return</span> <span class="kc">None</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="k">return</span> <span class="p">{</span><span class="n">key_conv</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="n">value_conv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">value</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

                            <span class="k">return</span> <span class="n">convert_map</span>

                        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">StructType</span><span class="p">):</span>
                            <span class="n">field_names</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">names</span>
                            <span class="n">dedup_field_names</span> <span class="o">=</span> <span class="n">_dedup_names</span><span class="p">(</span><span class="n">field_names</span><span class="p">)</span>
                            <span class="n">field_convs</span> <span class="o">=</span> <span class="p">[</span>
                                <span class="n">_converter</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">dataType</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">dt</span><span class="o">.</span><span class="n">fields</span>
                            <span class="p">]</span>

                            <span class="k">def</span> <span class="nf">convert_struct</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                                    <span class="k">return</span> <span class="kc">None</span>
                                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                                    <span class="n">_values</span> <span class="o">=</span> <span class="p">[</span>
                                        <span class="n">field_convs</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">value</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
                                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dedup_field_names</span><span class="p">)</span>
                                    <span class="p">]</span>
                                    <span class="k">return</span> <span class="n">_create_row</span><span class="p">(</span><span class="n">field_names</span><span class="p">,</span> <span class="n">_values</span><span class="p">)</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="n">_values</span> <span class="o">=</span> <span class="p">[</span>
                                        <span class="n">field_convs</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                                    <span class="p">]</span>
                                    <span class="k">return</span> <span class="n">_create_row</span><span class="p">(</span><span class="n">field_names</span><span class="p">,</span> <span class="n">_values</span><span class="p">)</span>

                            <span class="k">return</span> <span class="n">convert_struct</span>

                        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">):</span>

                            <span class="k">def</span> <span class="nf">convert_timestamp</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                                    <span class="k">return</span> <span class="kc">None</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="k">return</span> <span class="p">(</span>
                                        <span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                                        <span class="o">.</span><span class="n">tz_localize</span><span class="p">(</span><span class="n">timezone</span><span class="p">,</span> <span class="n">ambiguous</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                                        <span class="o">.</span><span class="n">tz_convert</span><span class="p">(</span><span class="n">_get_local_timezone</span><span class="p">())</span>
                                        <span class="o">.</span><span class="n">tz_localize</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                                        <span class="o">.</span><span class="n">to_pydatetime</span><span class="p">()</span>
                                    <span class="p">)</span>

                            <span class="k">return</span> <span class="n">convert_timestamp</span>

                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">return</span> <span class="kc">None</span>

                    <span class="n">conv</span> <span class="o">=</span> <span class="n">_converter</span><span class="p">(</span><span class="n">data_type</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">conv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">return</span> <span class="k">lambda</span> <span class="n">pser</span><span class="p">:</span> <span class="n">pser</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span>  <span class="c1"># type: ignore[return-value]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="k">lambda</span> <span class="n">pser</span><span class="p">:</span> <span class="n">pser</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">pdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                        <span class="p">[</span>
                            <span class="n">_create_converter</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">)(</span><span class="n">pser</span><span class="p">)</span>
                            <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">pser</span><span class="p">),</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">schema</span><span class="o">.</span><span class="n">fields</span><span class="p">)</span>
                        <span class="p">],</span>
                        <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">copied</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">should_localize</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">is_timestamp_ntz_preferred</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">series</span> <span class="ow">in</span> <span class="n">pdf</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">s</span> <span class="o">=</span> <span class="n">series</span>
                    <span class="k">if</span> <span class="n">should_localize</span> <span class="ow">and</span> <span class="n">is_datetime64tz_dtype</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="ow">and</span> <span class="n">s</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">tz</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">s</span> <span class="o">=</span> <span class="n">_check_series_convert_timestamps_tz_local</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">timezone</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">series</span><span class="p">:</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">copied</span><span class="p">:</span>
                            <span class="c1"># Copy once if the series is modified to prevent the original</span>
                            <span class="c1"># Pandas DataFrame from being updated</span>
                            <span class="n">pdf</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                            <span class="n">copied</span> <span class="o">=</span> <span class="kc">True</span>
                        <span class="n">pdf</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

            <span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">series</span> <span class="ow">in</span> <span class="n">pdf</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">is_timedelta64_dtype</span><span class="p">(</span><span class="n">series</span><span class="p">):</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">copied</span><span class="p">:</span>
                        <span class="n">pdf</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                        <span class="n">copied</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="c1"># Explicitly set the timedelta as object so the output of numpy records can</span>
                    <span class="c1"># hold the timedelta instances as are. Otherwise, it converts to the internal</span>
                    <span class="c1"># numeric values.</span>
                    <span class="n">ser</span> <span class="o">=</span> <span class="n">pdf</span><span class="p">[</span><span class="n">column</span><span class="p">]</span>
                    <span class="n">pdf</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
                        <span class="n">ser</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">to_pytimedelta</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">ser</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;object&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">ser</span><span class="o">.</span><span class="n">name</span>
                    <span class="p">)</span>

        <span class="c1"># Convert pandas.DataFrame to list of numpy records</span>
        <span class="n">np_records</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">set_axis</span><span class="p">(</span>
            <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;col_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">columns</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to_records</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Check if any columns need to be fixed for Spark to infer properly</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">np_records</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">record_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_numpy_record_dtype</span><span class="p">(</span><span class="n">np_records</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">record_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">record_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">np_records</span><span class="p">]</span>

        <span class="c1"># Convert list of numpy records to python lists</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">np_records</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_get_numpy_record_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rec</span><span class="p">:</span> <span class="s2">&quot;np.recarray&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;np.dtype&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Used when converting a pandas.DataFrame to Spark using to_records(), this will correct</span>
<span class="sd">        the dtypes of fields in a record so they can be properly loaded into Spark.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        rec : numpy.record</span>
<span class="sd">            a numpy record to check field dtypes</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        numpy.dtype</span>
<span class="sd">            corrected dtype for a numpy.record or None if no correction needed</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

        <span class="n">cur_dtypes</span> <span class="o">=</span> <span class="n">rec</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">col_names</span> <span class="o">=</span> <span class="n">cur_dtypes</span><span class="o">.</span><span class="n">names</span>
        <span class="n">record_type_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">has_rec_fix</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cur_dtypes</span><span class="p">)):</span>
            <span class="n">curr_type</span> <span class="o">=</span> <span class="n">cur_dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># If type is a datetime64 timestamp, convert to microseconds</span>
            <span class="c1"># NOTE: if dtype is datetime[ns] then np.record.tolist() will output values as longs,</span>
            <span class="c1"># conversion from [us] or lower will lead to py datetime objects, see SPARK-22417</span>
            <span class="k">if</span> <span class="n">curr_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="s2">&quot;datetime64[ns]&quot;</span><span class="p">):</span>
                <span class="n">curr_type</span> <span class="o">=</span> <span class="s2">&quot;datetime64[us]&quot;</span>
                <span class="n">has_rec_fix</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">record_type_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">str</span><span class="p">(</span><span class="n">col_names</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">curr_type</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">record_type_list</span><span class="p">)</span> <span class="k">if</span> <span class="n">has_rec_fix</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_create_from_pandas_with_arrow</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">pdf</span><span class="p">:</span> <span class="s2">&quot;PandasDataFrameLike&quot;</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">timezone</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DataFrame&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a DataFrame from a given pandas.DataFrame by slicing it into partitions, converting</span>
<span class="sd">        to Arrow data, then sending to the JVM to parallelize. If a schema is passed in, the</span>
<span class="sd">        data types will be used to coerce the data in Pandas to Arrow conversion.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.serializers</span> <span class="kn">import</span> <span class="n">ArrowStreamPandasSerializer</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">TimestampType</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">from_arrow_type</span><span class="p">,</span>
            <span class="n">to_arrow_type</span><span class="p">,</span>
            <span class="n">_deduplicate_field_names</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">require_minimum_pandas_version</span><span class="p">,</span>
            <span class="n">require_minimum_pyarrow_version</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">require_minimum_pandas_version</span><span class="p">()</span>
        <span class="n">require_minimum_pyarrow_version</span><span class="p">()</span>

        <span class="kn">from</span> <span class="nn">pandas.api.types</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="n">is_datetime64_dtype</span><span class="p">,</span>
            <span class="n">is_datetime64tz_dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="kn">import</span> <span class="nn">pyarrow</span> <span class="k">as</span> <span class="nn">pa</span>

        <span class="c1"># Create the Spark schema from list of names passed in with Arrow types</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">arrow_schema</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">Schema</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="n">preserve_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">struct</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span>
            <span class="n">prefer_timestamp_ntz</span> <span class="o">=</span> <span class="n">is_timestamp_ntz_preferred</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">arrow_schema</span><span class="p">):</span>
                <span class="n">struct</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
                    <span class="n">name</span><span class="p">,</span> <span class="n">from_arrow_type</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="n">prefer_timestamp_ntz</span><span class="p">),</span> <span class="n">nullable</span><span class="o">=</span><span class="n">field</span><span class="o">.</span><span class="n">nullable</span>
                <span class="p">)</span>
            <span class="n">schema</span> <span class="o">=</span> <span class="n">struct</span>

        <span class="c1"># Determine arrow types to coerce data when creating batches</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">StructType</span><span class="p">):</span>
            <span class="n">spark_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">_deduplicate_field_names</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">dataType</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">schema</span><span class="o">.</span><span class="n">fields</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">DataType</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">PySparkTypeError</span><span class="p">(</span>
                <span class="n">error_class</span><span class="o">=</span><span class="s2">&quot;UNSUPPORTED_DATA_TYPE_FOR_ARROW&quot;</span><span class="p">,</span>
                <span class="n">message_parameters</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;data_type&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">schema</span><span class="p">)},</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Any timestamps must be coerced to be compatible with Spark</span>
            <span class="n">spark_types</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">TimestampType</span><span class="p">()</span> <span class="k">if</span> <span class="n">is_datetime64_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_datetime64tz_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">pdf</span><span class="o">.</span><span class="n">dtypes</span>
            <span class="p">]</span>

        <span class="c1"># Slice the DataFrame to be batched</span>
        <span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jconf</span><span class="o">.</span><span class="n">arrowMaxRecordsPerBatch</span><span class="p">()</span>
        <span class="n">pdf_slices</span> <span class="o">=</span> <span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">step</span><span class="p">]</span> <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pdf</span><span class="p">),</span> <span class="n">step</span><span class="p">))</span>

        <span class="c1"># Create list of Arrow (columns, arrow_type, spark_type) for serializer dump_stream</span>
        <span class="n">arrow_data</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">[</span>
                <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">to_arrow_type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">c</span><span class="p">),</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pdf_slice</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">spark_types</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="k">for</span> <span class="n">pdf_slice</span> <span class="ow">in</span> <span class="n">pdf_slices</span>
        <span class="p">]</span>

        <span class="n">jsparkSession</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsparkSession</span>

        <span class="n">safecheck</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jconf</span><span class="o">.</span><span class="n">arrowSafeTypeConversion</span><span class="p">()</span>
        <span class="n">ser</span> <span class="o">=</span> <span class="n">ArrowStreamPandasSerializer</span><span class="p">(</span><span class="n">timezone</span><span class="p">,</span> <span class="n">safecheck</span><span class="p">)</span>

        <span class="nd">@no_type_check</span>
        <span class="k">def</span> <span class="nf">reader_func</span><span class="p">(</span><span class="n">temp_filename</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">PythonSQLUtils</span><span class="o">.</span><span class="n">readArrowStreamFromFile</span><span class="p">(</span><span class="n">temp_filename</span><span class="p">)</span>

        <span class="nd">@no_type_check</span>
        <span class="k">def</span> <span class="nf">create_iter_server</span><span class="p">():</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">ArrowIteratorServer</span><span class="p">()</span>

        <span class="c1"># Create Spark DataFrame from Arrow stream file, using one batch per partition</span>
        <span class="n">jiter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_serialize_to_jvm</span><span class="p">(</span><span class="n">arrow_data</span><span class="p">,</span> <span class="n">ser</span><span class="p">,</span> <span class="n">reader_func</span><span class="p">,</span> <span class="n">create_iter_server</span><span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">PythonSQLUtils</span><span class="o">.</span><span class="n">toDataFrame</span><span class="p">(</span><span class="n">jiter</span><span class="p">,</span> <span class="n">schema</span><span class="o">.</span><span class="n">json</span><span class="p">(),</span> <span class="n">jsparkSession</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">df</span><span class="o">.</span><span class="n">_schema</span> <span class="o">=</span> <span class="n">schema</span>
        <span class="k">return</span> <span class="n">df</span>


<span class="k">def</span> <span class="nf">_test</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
    <span class="kn">import</span> <span class="nn">pyspark.sql.pandas.conversion</span>

    <span class="n">globs</span> <span class="o">=</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">pandas</span><span class="o">.</span><span class="n">conversion</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[4]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;sql.pandas.conversion tests&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">globs</span><span class="p">[</span><span class="s2">&quot;spark&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spark</span>
    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span>
        <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">pandas</span><span class="o">.</span><span class="n">conversion</span><span class="p">,</span>
        <span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
        <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">NORMALIZE_WHITESPACE</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">REPORT_NDIFF</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright .<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>