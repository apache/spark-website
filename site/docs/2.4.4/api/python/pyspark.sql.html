
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>pyspark.sql module &#8212; PySpark 2.4.4 documentation</title>
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/pyspark.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/pyspark.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="pyspark.streaming module" href="pyspark.streaming.html" />
    <link rel="prev" title="pyspark package" href="pyspark.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="pyspark.streaming.html" title="pyspark.streaming module"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="pyspark.html" title="pyspark package"
             accesskey="P">previous</a> |</li>
    
        <li class="nav-item nav-item-0"><a href="index.html">PySpark 2.4.4 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="pyspark.html" accesskey="U">pyspark package</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="pyspark-sql-module">
<h1>pyspark.sql module<a class="headerlink" href="#pyspark-sql-module" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-pyspark.sql">
<span id="module-context"></span><h2>Module Context<a class="headerlink" href="#module-pyspark.sql" title="Permalink to this headline">¶</a></h2>
<p>Important classes of Spark SQL and DataFrames:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.SparkSession</span></code></a>
Main entry point for <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> and SQL functionality.</p></li>
<li><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.DataFrame</span></code></a>
A distributed collection of data grouped into named columns.</p></li>
<li><p><a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.Column</span></code></a>
A column expression in a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p></li>
<li><p><a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.Row</span></code></a>
A row of data in a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p></li>
<li><p><a class="reference internal" href="#pyspark.sql.GroupedData" title="pyspark.sql.GroupedData"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.GroupedData</span></code></a>
Aggregation methods, returned by <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.groupBy()</span></code></a>.</p></li>
<li><p><a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions" title="pyspark.sql.DataFrameNaFunctions"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.DataFrameNaFunctions</span></code></a>
Methods for handling missing data (null values).</p></li>
<li><p><a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions" title="pyspark.sql.DataFrameStatFunctions"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.DataFrameStatFunctions</span></code></a>
Methods for statistics functionality.</p></li>
<li><p><a class="reference internal" href="#module-pyspark.sql.functions" title="pyspark.sql.functions"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.functions</span></code></a>
List of built-in functions available for <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p></li>
<li><p><a class="reference internal" href="#module-pyspark.sql.types" title="pyspark.sql.types"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types</span></code></a>
List of data types available.</p></li>
<li><p><a class="reference internal" href="#pyspark.sql.Window" title="pyspark.sql.Window"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.Window</span></code></a>
For working with window functions.</p></li>
</ul>
</div></blockquote>
<dl class="class">
<dt id="pyspark.sql.SparkSession">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">SparkSession</code><span class="sig-paren">(</span><em class="sig-param">sparkContext</em>, <em class="sig-param">jsparkSession=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession" title="Permalink to this definition">¶</a></dt>
<dd><p>The entry point to programming Spark with the Dataset and DataFrame API.</p>
<p>A SparkSession can be used create <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>, register <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> as
tables, execute SQL over tables, cache tables, and read parquet files.
To create a SparkSession, use the following builder pattern:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Word Count&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.some.config.option&quot;</span><span class="p">,</span> <span class="s2">&quot;some-value&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>
</div>
<dl class="attribute">
<dt id="pyspark.sql.SparkSession.builder">
<code class="sig-name descname">builder</code><a class="headerlink" href="#pyspark.sql.SparkSession.builder" title="Permalink to this definition">¶</a></dt>
<dd><p>A class attribute having a <a class="reference internal" href="#pyspark.sql.SparkSession.Builder" title="pyspark.sql.SparkSession.Builder"><code class="xref py py-class docutils literal notranslate"><span class="pre">Builder</span></code></a> to construct <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code></a> instances</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.SparkSession.Builder">
<em class="property">class </em><code class="sig-name descname">Builder</code><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.Builder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.Builder" title="Permalink to this definition">¶</a></dt>
<dd><p>Builder for <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code></a>.</p>
<dl class="method">
<dt id="pyspark.sql.SparkSession.Builder.appName">
<code class="sig-name descname">appName</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.Builder.appName"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.Builder.appName" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets a name for the application, which will be shown in the Spark web UI.</p>
<p>If no application name is set, a randomly generated name will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> – an application name</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.Builder.config">
<code class="sig-name descname">config</code><span class="sig-paren">(</span><em class="sig-param">key=None</em>, <em class="sig-param">value=None</em>, <em class="sig-param">conf=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.Builder.config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.Builder.config" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets a config option. Options set using this method are automatically propagated to
both <code class="xref py py-class docutils literal notranslate"><span class="pre">SparkConf</span></code> and <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code></a>’s own configuration.</p>
<p>For an existing SparkConf, use <cite>conf</cite> parameter.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.conf</span> <span class="k">import</span> <span class="n">SparkConf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">SparkConf</span><span class="p">())</span>
<span class="go">&lt;pyspark.sql.session...</span>
</pre></div>
</div>
<p>For a (key, value) pair, you can omit parameter names.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.some.config.option&quot;</span><span class="p">,</span> <span class="s2">&quot;some-value&quot;</span><span class="p">)</span>
<span class="go">&lt;pyspark.sql.session...</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>key</strong> – a key name string for configuration property</p></li>
<li><p><strong>value</strong> – a value for configuration property</p></li>
<li><p><strong>conf</strong> – an instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">SparkConf</span></code></p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.Builder.enableHiveSupport">
<code class="sig-name descname">enableHiveSupport</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.Builder.enableHiveSupport"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.Builder.enableHiveSupport" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables Hive support, including connectivity to a persistent Hive metastore, support
for Hive serdes, and Hive user-defined functions.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.Builder.getOrCreate">
<code class="sig-name descname">getOrCreate</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.Builder.getOrCreate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.Builder.getOrCreate" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets an existing <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code></a> or, if there is no existing one, creates a
new one based on the options set in this builder.</p>
<p>This method first checks whether there is a valid global default SparkSession, and if
yes, return that one. If no valid global default SparkSession exists, the method
creates a new SparkSession and assigns the newly created SparkSession as the global
default.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;k1&quot;</span><span class="p">,</span> <span class="s2">&quot;v1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;k1&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">s1</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;k1&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;v1&quot;</span>
<span class="go">True</span>
</pre></div>
</div>
<p>In case an existing SparkSession is returned, the config options specified
in this builder will be applied to the existing SparkSession.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s2</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;k2&quot;</span><span class="p">,</span> <span class="s2">&quot;v2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;k1&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">s2</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;k1&quot;</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;k2&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">s2</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;k2&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.Builder.master">
<code class="sig-name descname">master</code><span class="sig-paren">(</span><em class="sig-param">master</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.Builder.master"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.Builder.master" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the Spark master URL to connect to, such as “local” to run locally, “local[4]”
to run locally with 4 cores, or “spark://master:7077” to run on a Spark standalone
cluster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>master</strong> – a url for spark master</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.catalog">
<em class="property">property </em><code class="sig-name descname">catalog</code><a class="headerlink" href="#pyspark.sql.SparkSession.catalog" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface through which the user may create, drop, alter or query underlying
databases, tables, functions etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.Catalog" title="pyspark.sql.Catalog"><code class="xref py py-class docutils literal notranslate"><span class="pre">Catalog</span></code></a></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.conf">
<em class="property">property </em><code class="sig-name descname">conf</code><a class="headerlink" href="#pyspark.sql.SparkSession.conf" title="Permalink to this definition">¶</a></dt>
<dd><p>Runtime configuration interface for Spark.</p>
<p>This is the interface through which the user can get and set all Spark and Hadoop
configurations that are relevant to Spark SQL. When getting the value of a config,
this defaults to the value set in the underlying <code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code>, if any.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.createDataFrame">
<code class="sig-name descname">createDataFrame</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">samplingRatio=None</em>, <em class="sig-param">verifySchema=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.createDataFrame"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.createDataFrame" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> from an <code class="xref py py-class docutils literal notranslate"><span class="pre">RDD</span></code>, a list or a <code class="xref py py-class docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code>.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">schema</span></code> is a list of column names, the type of each column
will be inferred from <code class="docutils literal notranslate"><span class="pre">data</span></code>.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">schema</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>, it will try to infer the schema (column names and types)
from <code class="docutils literal notranslate"><span class="pre">data</span></code>, which should be an RDD of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code></a>,
or <code class="xref py py-class docutils literal notranslate"><span class="pre">namedtuple</span></code>, or <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">schema</span></code> is <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType</span></code></a> or a datatype string, it must match
the real data, or an exception will be thrown at runtime. If the given schema is not
<a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a>, it will be wrapped into a
<a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> as its only field, and the field name will be “value”,
each record will also be wrapped into a tuple, which can be converted to row later.</p>
<p>If schema inference is needed, <code class="docutils literal notranslate"><span class="pre">samplingRatio</span></code> is used to determined the ratio of
rows used for schema inference. The first row will be used if <code class="docutils literal notranslate"><span class="pre">samplingRatio</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – an RDD of any kind of SQL data representation(e.g. row, tuple, int, boolean,
etc.), or <code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code>, or <code class="xref py py-class docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code>.</p></li>
<li><p><strong>schema</strong> – a <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType</span></code></a> or a datatype string or a list of
column names, default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.  The data type string format equals to
<a class="reference internal" href="#pyspark.sql.types.DataType.simpleString" title="pyspark.sql.types.DataType.simpleString"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType.simpleString</span></code></a>, except that top level struct type can
omit the <code class="docutils literal notranslate"><span class="pre">struct&lt;&gt;</span></code> and atomic types use <code class="docutils literal notranslate"><span class="pre">typeName()</span></code> as their format, e.g. use
<code class="docutils literal notranslate"><span class="pre">byte</span></code> instead of <code class="docutils literal notranslate"><span class="pre">tinyint</span></code> for <a class="reference internal" href="#pyspark.sql.types.ByteType" title="pyspark.sql.types.ByteType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.ByteType</span></code></a>. We can also use
<code class="docutils literal notranslate"><span class="pre">int</span></code> as a short name for <code class="docutils literal notranslate"><span class="pre">IntegerType</span></code>.</p></li>
<li><p><strong>samplingRatio</strong> – the sample ratio of rows used for inferring</p></li>
<li><p><strong>verifySchema</strong> – verify data types of every row against schema.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.1: </span>Added verifySchema.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Usage with spark.sql.execution.arrow.enabled=True is experimental.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=&#39;Alice&#39;, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=1, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=&#39;Alice&#39;, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">person</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">Person</span><span class="p">(</span><span class="o">*</span><span class="n">r</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">person</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>   <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
<span class="gp">... </span>   <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(name=&#39;Alice&#39;, age=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(0=1, 1=2)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s2">&quot;a: string, b: int&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(a=&#39;Alice&#39;, b=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s2">&quot;int&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(value=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s2">&quot;boolean&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span> 
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">Py4JJavaError</span>: <span class="n">...</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.newSession">
<code class="sig-name descname">newSession</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.newSession"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.newSession" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new SparkSession as new session, that has separate SQLConf,
registered temporary views and UDFs, but shared SparkContext and
table cache.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.range">
<code class="sig-name descname">range</code><span class="sig-paren">(</span><em class="sig-param">start</em>, <em class="sig-param">end=None</em>, <em class="sig-param">step=1</em>, <em class="sig-param">numPartitions=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.range"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.range" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> with single <a class="reference internal" href="#pyspark.sql.types.LongType" title="pyspark.sql.types.LongType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.LongType</span></code></a> column named
<code class="docutils literal notranslate"><span class="pre">id</span></code>, containing elements in a range from <code class="docutils literal notranslate"><span class="pre">start</span></code> to <code class="docutils literal notranslate"><span class="pre">end</span></code> (exclusive) with
step value <code class="docutils literal notranslate"><span class="pre">step</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> – the start value</p></li>
<li><p><strong>end</strong> – the end value (exclusive)</p></li>
<li><p><strong>step</strong> – the incremental step (default: 1)</p></li>
<li><p><strong>numPartitions</strong> – the number of partitions of the DataFrame</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=1), Row(id=3), Row(id=5)]</span>
</pre></div>
</div>
<p>If only one argument is specified, it will be used as the end value.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=0), Row(id=1), Row(id=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.read">
<em class="property">property </em><code class="sig-name descname">read</code><a class="headerlink" href="#pyspark.sql.SparkSession.read" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameReader" title="pyspark.sql.DataFrameReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrameReader</span></code></a> that can be used to read data
in as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.DataFrameReader" title="pyspark.sql.DataFrameReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrameReader</span></code></a></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.readStream">
<em class="property">property </em><code class="sig-name descname">readStream</code><a class="headerlink" href="#pyspark.sql.SparkSession.readStream" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataStreamReader</span></code> that can be used to read data streams
as a streaming <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataStreamReader</span></code></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.sparkContext">
<em class="property">property </em><code class="sig-name descname">sparkContext</code><a class="headerlink" href="#pyspark.sql.SparkSession.sparkContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the underlying <code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.sql">
<code class="sig-name descname">sql</code><span class="sig-paren">(</span><em class="sig-param">sqlQuery</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.sql"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.sql" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> representing the result of the given query.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT field1 AS f1, field2 as f2 from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=1, f2=&#39;row1&#39;), Row(f1=2, f2=&#39;row2&#39;), Row(f1=3, f2=&#39;row3&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.stop">
<code class="sig-name descname">stop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.stop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stop the underlying <code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.streams">
<em class="property">property </em><code class="sig-name descname">streams</code><a class="headerlink" href="#pyspark.sql.SparkSession.streams" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQueryManager</span></code> that allows managing all the
<code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQuery</span></code> StreamingQueries active on <cite>this</cite> context.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQueryManager</span></code></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.table">
<code class="sig-name descname">table</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/session.html#SparkSession.table"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SparkSession.table" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the specified table as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.udf">
<em class="property">property </em><code class="sig-name descname">udf</code><a class="headerlink" href="#pyspark.sql.SparkSession.udf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.UDFRegistration" title="pyspark.sql.UDFRegistration"><code class="xref py py-class docutils literal notranslate"><span class="pre">UDFRegistration</span></code></a> for UDF registration.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.UDFRegistration" title="pyspark.sql.UDFRegistration"><code class="xref py py-class docutils literal notranslate"><span class="pre">UDFRegistration</span></code></a></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.version">
<em class="property">property </em><code class="sig-name descname">version</code><a class="headerlink" href="#pyspark.sql.SparkSession.version" title="Permalink to this definition">¶</a></dt>
<dd><p>The version of Spark on which this application is running.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.SQLContext">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">SQLContext</code><span class="sig-paren">(</span><em class="sig-param">sparkContext</em>, <em class="sig-param">sparkSession=None</em>, <em class="sig-param">jsqlContext=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext" title="Permalink to this definition">¶</a></dt>
<dd><p>The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.</p>
<p>As of Spark 2.0, this is replaced by <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code></a>. However, we are keeping the class
here for backward compatibility.</p>
<p>A SQLContext can be used create <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>, register <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> as
tables, execute SQL over tables, cache tables, and read parquet files.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparkContext</strong> – The <code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code> backing this SQLContext.</p></li>
<li><p><strong>sparkSession</strong> – The <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code></a> around which this SQLContext wraps.</p></li>
<li><p><strong>jsqlContext</strong> – An optional JVM Scala SQLContext. If set, we do not instantiate a new
SQLContext in the JVM, instead we make all calls to this object.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="pyspark.sql.SQLContext.cacheTable">
<code class="sig-name descname">cacheTable</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.cacheTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.cacheTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Caches the specified table in-memory.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.clearCache">
<code class="sig-name descname">clearCache</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.clearCache"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.clearCache" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes all cached tables from the in-memory cache.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.createDataFrame">
<code class="sig-name descname">createDataFrame</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">samplingRatio=None</em>, <em class="sig-param">verifySchema=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.createDataFrame"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.createDataFrame" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> from an <code class="xref py py-class docutils literal notranslate"><span class="pre">RDD</span></code>, a list or a <code class="xref py py-class docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code>.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">schema</span></code> is a list of column names, the type of each column
will be inferred from <code class="docutils literal notranslate"><span class="pre">data</span></code>.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">schema</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>, it will try to infer the schema (column names and types)
from <code class="docutils literal notranslate"><span class="pre">data</span></code>, which should be an RDD of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code></a>,
or <code class="xref py py-class docutils literal notranslate"><span class="pre">namedtuple</span></code>, or <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">schema</span></code> is <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType</span></code></a> or a datatype string it must match
the real data, or an exception will be thrown at runtime. If the given schema is not
<a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a>, it will be wrapped into a
<a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> as its only field, and the field name will be “value”,
each record will also be wrapped into a tuple, which can be converted to row later.</p>
<p>If schema inference is needed, <code class="docutils literal notranslate"><span class="pre">samplingRatio</span></code> is used to determined the ratio of
rows used for schema inference. The first row will be used if <code class="docutils literal notranslate"><span class="pre">samplingRatio</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – an RDD of any kind of SQL data representation(e.g. <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code></a>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>, <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">boolean</span></code>, etc.), or <code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code>, or
<code class="xref py py-class docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code>.</p></li>
<li><p><strong>schema</strong> – a <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType</span></code></a> or a datatype string or a list of
column names, default is None.  The data type string format equals to
<a class="reference internal" href="#pyspark.sql.types.DataType.simpleString" title="pyspark.sql.types.DataType.simpleString"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType.simpleString</span></code></a>, except that top level struct type can
omit the <code class="docutils literal notranslate"><span class="pre">struct&lt;&gt;</span></code> and atomic types use <code class="docutils literal notranslate"><span class="pre">typeName()</span></code> as their format, e.g. use
<code class="docutils literal notranslate"><span class="pre">byte</span></code> instead of <code class="docutils literal notranslate"><span class="pre">tinyint</span></code> for <a class="reference internal" href="#pyspark.sql.types.ByteType" title="pyspark.sql.types.ByteType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.ByteType</span></code></a>.
We can also use <code class="docutils literal notranslate"><span class="pre">int</span></code> as a short name for <a class="reference internal" href="#pyspark.sql.types.IntegerType" title="pyspark.sql.types.IntegerType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.IntegerType</span></code></a>.</p></li>
<li><p><strong>samplingRatio</strong> – the sample ratio of rows used for inferring</p></li>
<li><p><strong>verifySchema</strong> – verify data types of every row against schema.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.0: </span>The <code class="docutils literal notranslate"><span class="pre">schema</span></code> parameter can be a <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType</span></code></a> or a
datatype string after 2.0.
If it’s not a <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a>, it will be wrapped into a
<a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> and each record will also be wrapped into a tuple.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.1: </span>Added verifySchema.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=&#39;Alice&#39;, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=1, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=&#39;Alice&#39;, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">person</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">Person</span><span class="p">(</span><span class="o">*</span><span class="n">r</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">person</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>   <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
<span class="gp">... </span>   <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(name=&#39;Alice&#39;, age=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(0=1, 1=2)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s2">&quot;a: string, b: int&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(a=&#39;Alice&#39;, b=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s2">&quot;int&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(value=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s2">&quot;boolean&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span> 
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">Py4JJavaError</span>: <span class="n">...</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.createExternalTable">
<code class="sig-name descname">createExternalTable</code><span class="sig-paren">(</span><em class="sig-param">tableName</em>, <em class="sig-param">path=None</em>, <em class="sig-param">source=None</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.createExternalTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.createExternalTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates an external table based on the dataset in a data source.</p>
<p>It returns the DataFrame associated with the external table.</p>
<p>The data source is specified by the <code class="docutils literal notranslate"><span class="pre">source</span></code> and a set of <code class="docutils literal notranslate"><span class="pre">options</span></code>.
If <code class="docutils literal notranslate"><span class="pre">source</span></code> is not specified, the default data source configured by
<code class="docutils literal notranslate"><span class="pre">spark.sql.sources.default</span></code> will be used.</p>
<p>Optionally, a schema can be provided as the schema of the returned <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> and
created external table.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.dropTempTable">
<code class="sig-name descname">dropTempTable</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.dropTempTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.dropTempTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove the temp table from catalog.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">dropTempTable</span><span class="p">(</span><span class="s2">&quot;table1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.getConf">
<code class="sig-name descname">getConf</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">defaultValue=&lt;no value&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.getConf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.getConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of Spark SQL configuration property for the given key.</p>
<p>If the key is not set and defaultValue is set, return
defaultValue. If the key is not set and defaultValue is not set, return
the system default value.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">)</span>
<span class="go">&#39;200&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="sa">u</span><span class="s2">&quot;10&quot;</span><span class="p">)</span>
<span class="go">&#39;10&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="sa">u</span><span class="s2">&quot;50&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="sa">u</span><span class="s2">&quot;10&quot;</span><span class="p">)</span>
<span class="go">&#39;50&#39;</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.getOrCreate">
<em class="property">classmethod </em><code class="sig-name descname">getOrCreate</code><span class="sig-paren">(</span><em class="sig-param">sc</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.getOrCreate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.getOrCreate" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the existing SQLContext or create a new one with given SparkContext.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sc</strong> – SparkContext</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.newSession">
<code class="sig-name descname">newSession</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.newSession"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.newSession" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new SQLContext as new session, that has separate SQLConf,
registered temporary views and UDFs, but shared SparkContext and
table cache.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.range">
<code class="sig-name descname">range</code><span class="sig-paren">(</span><em class="sig-param">start</em>, <em class="sig-param">end=None</em>, <em class="sig-param">step=1</em>, <em class="sig-param">numPartitions=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.range"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.range" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> with single <a class="reference internal" href="#pyspark.sql.types.LongType" title="pyspark.sql.types.LongType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.LongType</span></code></a> column named
<code class="docutils literal notranslate"><span class="pre">id</span></code>, containing elements in a range from <code class="docutils literal notranslate"><span class="pre">start</span></code> to <code class="docutils literal notranslate"><span class="pre">end</span></code> (exclusive) with
step value <code class="docutils literal notranslate"><span class="pre">step</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> – the start value</p></li>
<li><p><strong>end</strong> – the end value (exclusive)</p></li>
<li><p><strong>step</strong> – the incremental step (default: 1)</p></li>
<li><p><strong>numPartitions</strong> – the number of partitions of the DataFrame</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=1), Row(id=3), Row(id=5)]</span>
</pre></div>
</div>
<p>If only one argument is specified, it will be used as the end value.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=0), Row(id=1), Row(id=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.read">
<em class="property">property </em><code class="sig-name descname">read</code><a class="headerlink" href="#pyspark.sql.SQLContext.read" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameReader" title="pyspark.sql.DataFrameReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrameReader</span></code></a> that can be used to read data
in as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.DataFrameReader" title="pyspark.sql.DataFrameReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrameReader</span></code></a></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.readStream">
<em class="property">property </em><code class="sig-name descname">readStream</code><a class="headerlink" href="#pyspark.sql.SQLContext.readStream" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataStreamReader</span></code> that can be used to read data streams
as a streaming <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataStreamReader</span></code></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text_sdf</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.registerDataFrameAsTable">
<code class="sig-name descname">registerDataFrameAsTable</code><span class="sig-paren">(</span><em class="sig-param">df</em>, <em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.registerDataFrameAsTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.registerDataFrameAsTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers the given <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> as a temporary table in the catalog.</p>
<p>Temporary tables exist only during the lifetime of this instance of <a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">SQLContext</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;table1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.registerFunction">
<code class="sig-name descname">registerFunction</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">f</em>, <em class="sig-param">returnType=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.registerFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.registerFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <code class="xref py py-func docutils literal notranslate"><span class="pre">spark.udf.register()</span></code>.
See <a class="reference internal" href="#pyspark.sql.UDFRegistration.register" title="pyspark.sql.UDFRegistration.register"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.UDFRegistration.register()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deprecated in 2.3.0. Use <code class="xref py py-func docutils literal notranslate"><span class="pre">spark.udf.register()</span></code> instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.2.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.registerJavaFunction">
<code class="sig-name descname">registerJavaFunction</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">javaClassName</em>, <em class="sig-param">returnType=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.registerJavaFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.registerJavaFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <code class="xref py py-func docutils literal notranslate"><span class="pre">spark.udf.registerJavaFunction()</span></code>.
See <a class="reference internal" href="#pyspark.sql.UDFRegistration.registerJavaFunction" title="pyspark.sql.UDFRegistration.registerJavaFunction"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.UDFRegistration.registerJavaFunction()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deprecated in 2.3.0. Use <code class="xref py py-func docutils literal notranslate"><span class="pre">spark.udf.registerJavaFunction()</span></code> instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.setConf">
<code class="sig-name descname">setConf</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.setConf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.setConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the given Spark SQL configuration property.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.sql">
<code class="sig-name descname">sql</code><span class="sig-paren">(</span><em class="sig-param">sqlQuery</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.sql"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.sql" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> representing the result of the given query.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT field1 AS f1, field2 as f2 from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=1, f2=&#39;row1&#39;), Row(f1=2, f2=&#39;row2&#39;), Row(f1=3, f2=&#39;row3&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.streams">
<em class="property">property </em><code class="sig-name descname">streams</code><a class="headerlink" href="#pyspark.sql.SQLContext.streams" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQueryManager</span></code> that allows managing all the
<code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQuery</span></code> StreamingQueries active on <cite>this</cite> context.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.table">
<code class="sig-name descname">table</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.table"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.table" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the specified table or view as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.tableNames">
<code class="sig-name descname">tableNames</code><span class="sig-paren">(</span><em class="sig-param">dbName=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.tableNames"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.tableNames" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of names of tables in the database <code class="docutils literal notranslate"><span class="pre">dbName</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dbName</strong> – string, name of the database to use. Default to the current database.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of table names, in string</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;table1&quot;</span> <span class="ow">in</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">tableNames</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;table1&quot;</span> <span class="ow">in</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">tableNames</span><span class="p">(</span><span class="s2">&quot;default&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.tables">
<code class="sig-name descname">tables</code><span class="sig-paren">(</span><em class="sig-param">dbName=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.tables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.tables" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> containing names of tables in the given database.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">dbName</span></code> is not specified, the current database will be used.</p>
<p>The returned DataFrame has two columns: <code class="docutils literal notranslate"><span class="pre">tableName</span></code> and <code class="docutils literal notranslate"><span class="pre">isTemporary</span></code>
(a column with <code class="xref py py-class docutils literal notranslate"><span class="pre">BooleanType</span></code> indicating if a table is a temporary one or not).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dbName</strong> – string, name of the database to use.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">tables</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s2">&quot;tableName = &#39;table1&#39;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">Row(database=&#39;&#39;, tableName=&#39;table1&#39;, isTemporary=True)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.udf">
<em class="property">property </em><code class="sig-name descname">udf</code><a class="headerlink" href="#pyspark.sql.SQLContext.udf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.UDFRegistration" title="pyspark.sql.UDFRegistration"><code class="xref py py-class docutils literal notranslate"><span class="pre">UDFRegistration</span></code></a> for UDF registration.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.UDFRegistration" title="pyspark.sql.UDFRegistration"><code class="xref py py-class docutils literal notranslate"><span class="pre">UDFRegistration</span></code></a></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.uncacheTable">
<code class="sig-name descname">uncacheTable</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#SQLContext.uncacheTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.uncacheTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the specified table from the in-memory cache.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.HiveContext">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">HiveContext</code><span class="sig-paren">(</span><em class="sig-param">sparkContext</em>, <em class="sig-param">jhiveContext=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#HiveContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.HiveContext" title="Permalink to this definition">¶</a></dt>
<dd><p>A variant of Spark SQL that integrates with data stored in Hive.</p>
<p>Configuration for Hive is read from <code class="docutils literal notranslate"><span class="pre">hive-site.xml</span></code> on the classpath.
It supports running both SQL and HiveQL commands.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sparkContext</strong> – The SparkContext to wrap.</p></li>
<li><p><strong>jhiveContext</strong> – An optional JVM Scala HiveContext. If set, we do not instantiate a new
<a class="reference internal" href="#pyspark.sql.HiveContext" title="pyspark.sql.HiveContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">HiveContext</span></code></a> in the JVM, instead we make all calls to this object.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deprecated in 2.0.0. Use SparkSession.builder.enableHiveSupport().getOrCreate().</p>
</div>
<dl class="method">
<dt id="pyspark.sql.HiveContext.refreshTable">
<code class="sig-name descname">refreshTable</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/context.html#HiveContext.refreshTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.HiveContext.refreshTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Invalidate and refresh all the cached the metadata of the given
table. For performance reasons, Spark SQL or the external data source
library it uses might cache certain metadata about a table, such as the
location of blocks. When those change outside of Spark SQL, users should
call this function to invalidate the cache.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.UDFRegistration">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">UDFRegistration</code><span class="sig-paren">(</span><em class="sig-param">sparkSession</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/udf.html#UDFRegistration"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.UDFRegistration" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper for user-defined function registration. This instance can be accessed by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">spark.udf</span></code> or <code class="xref py py-attr docutils literal notranslate"><span class="pre">sqlContext.udf</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.1.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.UDFRegistration.register">
<code class="sig-name descname">register</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">f</em>, <em class="sig-param">returnType=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/udf.html#UDFRegistration.register"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.UDFRegistration.register" title="Permalink to this definition">¶</a></dt>
<dd><p>Register a Python function (including lambda function) or a user-defined function
as a SQL function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – name of the user-defined function in SQL statements.</p></li>
<li><p><strong>f</strong> – a Python function, or a user-defined function. The user-defined function can
be either row-at-a-time or vectorized. See <a class="reference internal" href="#pyspark.sql.functions.udf" title="pyspark.sql.functions.udf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.functions.udf()</span></code></a> and
<a class="reference internal" href="#pyspark.sql.functions.pandas_udf" title="pyspark.sql.functions.pandas_udf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.functions.pandas_udf()</span></code></a>.</p></li>
<li><p><strong>returnType</strong> – the return type of the registered user-defined function. The value can
be either a <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType</span></code></a> object or a DDL-formatted type string.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a user-defined function.</p>
</dd>
</dl>
<p>To register a nondeterministic Python function, users need to first build
a nondeterministic user-defined function for the Python function and then register it
as a SQL function.</p>
<p><cite>returnType</cite> can be optionally specified when <cite>f</cite> is a Python function but not
when <cite>f</cite> is a user-defined function. Please see below.</p>
<ol class="arabic">
<li><p>When <cite>f</cite> is a Python function:</p>
<blockquote>
<div><p><cite>returnType</cite> defaults to string type and can be optionally specified. The produced
object must match the specified type. In this case, this API works as if
<cite>register(name, f, returnType=StringType())</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">strlen</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;stringLengthString&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT stringLengthString(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(stringLengthString(test)=&#39;4&#39;)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT &#39;foo&#39; AS text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">strlen</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(stringLengthString(text)=&#39;3&#39;)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;stringLengthInt&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT stringLengthInt(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(stringLengthInt(test)=4)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;stringLengthInt&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT stringLengthInt(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(stringLengthInt(test)=4)]</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>When <cite>f</cite> is a user-defined function:</p>
<blockquote>
<div><p>Spark uses the return type of the given user-defined function as the return type of
the registered user-defined function. <cite>returnType</cite> should not be specified.
In this case, this API works as if <cite>register(name, f)</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">udf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slen</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;slen&quot;</span><span class="p">,</span> <span class="n">slen</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT slen(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(slen(test)=4)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">random</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">udf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_udf</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span><span class="o">.</span><span class="n">asNondeterministic</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_random_udf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;random_udf&quot;</span><span class="p">,</span> <span class="n">random_udf</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT random_udf()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(random_udf()=82)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s2">&quot;integer&quot;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">SCALAR</span><span class="p">)</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">add_one</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;add_one&quot;</span><span class="p">,</span> <span class="n">add_one</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT add_one(id) FROM range(3)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s2">&quot;integer&quot;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_AGG</span><span class="p">)</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">sum_udf</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;sum_udf&quot;</span><span class="p">,</span> <span class="n">sum_udf</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="s2">&quot;SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="n">q</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Registration for a user-defined function (case 2.) was added from
Spark 2.3.0.</p>
</div>
</div></blockquote>
</li>
</ol>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.UDFRegistration.registerJavaFunction">
<code class="sig-name descname">registerJavaFunction</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">javaClassName</em>, <em class="sig-param">returnType=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/udf.html#UDFRegistration.registerJavaFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.UDFRegistration.registerJavaFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Register a Java user-defined function as a SQL function.</p>
<p>In addition to a name and the function itself, the return type can be optionally specified.
When the return type is not specified we would infer it via reflection.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – name of the user-defined function</p></li>
<li><p><strong>javaClassName</strong> – fully qualified name of java class</p></li>
<li><p><strong>returnType</strong> – the return type of the registered Java function. The value can be either
a <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType</span></code></a> object or a DDL-formatted type string.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">registerJavaFunction</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;javaStringLength&quot;</span><span class="p">,</span> <span class="s2">&quot;test.org.apache.spark.sql.JavaStringLength&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT javaStringLength(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(UDF:javaStringLength(test)=4)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">registerJavaFunction</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;javaStringLength2&quot;</span><span class="p">,</span> <span class="s2">&quot;test.org.apache.spark.sql.JavaStringLength&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT javaStringLength2(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(UDF:javaStringLength2(test)=4)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">registerJavaFunction</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;javaStringLength3&quot;</span><span class="p">,</span> <span class="s2">&quot;test.org.apache.spark.sql.JavaStringLength&quot;</span><span class="p">,</span> <span class="s2">&quot;integer&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT javaStringLength3(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(UDF:javaStringLength3(test)=4)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.UDFRegistration.registerJavaUDAF">
<code class="sig-name descname">registerJavaUDAF</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">javaClassName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/udf.html#UDFRegistration.registerJavaUDAF"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.UDFRegistration.registerJavaUDAF" title="Permalink to this definition">¶</a></dt>
<dd><p>Register a Java user-defined aggregate function as a SQL function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – name of the user-defined aggregate function</p></li>
<li><p><strong>javaClassName</strong> – fully qualified name of java class</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">registerJavaUDAF</span><span class="p">(</span><span class="s2">&quot;javaUDAF&quot;</span><span class="p">,</span> <span class="s2">&quot;test.org.apache.spark.sql.MyDoubleAvg&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">),(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)],[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;df&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name, javaUDAF(id) as avg from df group by name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;b&#39;, avg=102.0), Row(name=&#39;a&#39;, avg=102.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrame">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">DataFrame</code><span class="sig-paren">(</span><em class="sig-param">jdf</em>, <em class="sig-param">sql_ctx</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame" title="Permalink to this definition">¶</a></dt>
<dd><p>A distributed collection of data grouped into named columns.</p>
<p>A <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> is equivalent to a relational table in Spark SQL,
and can be created using various functions in <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">people</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;...&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Once created, it can be manipulated using the various domain-specific-language
(DSL) functions defined in: <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>, <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a>.</p>
<p>To select a column from the data frame, use the apply method:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ageCol</span> <span class="o">=</span> <span class="n">people</span><span class="o">.</span><span class="n">age</span>
</pre></div>
</div>
<p>A more concrete example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># To create DataFrame using SparkSession</span>
<span class="n">people</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;...&quot;</span><span class="p">)</span>
<span class="n">department</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;...&quot;</span><span class="p">)</span>

<span class="n">people</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">people</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">)</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">department</span><span class="p">,</span> <span class="n">people</span><span class="o">.</span><span class="n">deptId</span> <span class="o">==</span> <span class="n">department</span><span class="o">.</span><span class="n">id</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">department</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s2">&quot;salary&quot;</span><span class="p">:</span> <span class="s2">&quot;avg&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">:</span> <span class="s2">&quot;max&quot;</span><span class="p">})</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrame.agg">
<code class="sig-name descname">agg</code><span class="sig-paren">(</span><em class="sig-param">*exprs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.agg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.agg" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate on the entire <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> without groups
(shorthand for <code class="docutils literal notranslate"><span class="pre">df.groupBy.agg()</span></code>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s2">&quot;age&quot;</span><span class="p">:</span> <span class="s2">&quot;max&quot;</span><span class="p">})</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(max(age)=5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(min(age)=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.alias">
<code class="sig-name descname">alias</code><span class="sig-paren">(</span><em class="sig-param">alias</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.alias"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.alias" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> with an alias set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alias</strong> – string, an alias name to be set for the DataFrame.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_as1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;df_as1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_as2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;df_as2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">joined_df</span> <span class="o">=</span> <span class="n">df_as1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df_as2</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s2">&quot;df_as1.name&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">col</span><span class="p">(</span><span class="s2">&quot;df_as2.name&quot;</span><span class="p">),</span> <span class="s1">&#39;inner&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">joined_df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;df_as1.name&quot;</span><span class="p">,</span> <span class="s2">&quot;df_as2.name&quot;</span><span class="p">,</span> <span class="s2">&quot;df_as2.age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Bob&#39;, name=&#39;Bob&#39;, age=5), Row(name=&#39;Alice&#39;, name=&#39;Alice&#39;, age=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.approxQuantile">
<code class="sig-name descname">approxQuantile</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">probabilities</em>, <em class="sig-param">relativeError</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.approxQuantile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.approxQuantile" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the approximate quantiles of numerical columns of a
DataFrame.</p>
<p>The result of this algorithm has the following deterministic bound:
If the DataFrame has N elements and if we request the quantile at
probability <cite>p</cite> up to error <cite>err</cite>, then the algorithm will return
a sample <cite>x</cite> from the DataFrame so that the <em>exact</em> rank of <cite>x</cite> is
close to (p * N). More precisely,</p>
<blockquote>
<div><p>floor((p - err) * N) &lt;= rank(x) &lt;= ceil((p + err) * N).</p>
</div></blockquote>
<p>This method implements a variation of the Greenwald-Khanna
algorithm (with some speed optimizations). The algorithm was first
present in [[<a class="reference external" href="http://dx.doi.org/10.1145/375663.375670">http://dx.doi.org/10.1145/375663.375670</a>
Space-efficient Online Computation of Quantile Summaries]]
by Greenwald and Khanna.</p>
<p>Note that null values will be ignored in numerical columns before calculation.
For columns only containing null values, an empty list is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – str, list.
Can be a single column name, or a list of names for multiple columns.</p></li>
<li><p><strong>probabilities</strong> – a list of quantile probabilities
Each number must belong to [0, 1].
For example 0 is the minimum, 0.5 is the median, 1 is the maximum.</p></li>
<li><p><strong>relativeError</strong> – The relative target precision to achieve
(&gt;= 0). If set to zero, the exact quantiles are computed, which
could be very expensive. Note that values greater than 1 are
accepted but give the same result as 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the approximate quantiles at the given probabilities. If
the input <cite>col</cite> is a string, the output is a list of floats. If the
input <cite>col</cite> is a list or tuple of strings, the output is also a
list, but each element in it is a list of floats, i.e., the output
is a list of list of floats.</p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.2: </span>Added support for multiple columns.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.cache">
<code class="sig-name descname">cache</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.cache"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.cache" title="Permalink to this definition">¶</a></dt>
<dd><p>Persists the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> with the default storage level (<code class="xref py py-class docutils literal notranslate"><span class="pre">MEMORY_AND_DISK</span></code>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The default storage level has changed to <code class="xref py py-class docutils literal notranslate"><span class="pre">MEMORY_AND_DISK</span></code> to match Scala in 2.0.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.checkpoint">
<code class="sig-name descname">checkpoint</code><span class="sig-paren">(</span><em class="sig-param">eager=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.checkpoint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
logical plan of this DataFrame, which is especially useful in iterative algorithms where the
plan may grow exponentially. It will be saved to files inside the checkpoint
directory set with <code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext.setCheckpointDir()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>eager</strong> – Whether to checkpoint this DataFrame immediately</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.coalesce">
<code class="sig-name descname">coalesce</code><span class="sig-paren">(</span><em class="sig-param">numPartitions</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.coalesce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.coalesce" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> that has exactly <cite>numPartitions</cite> partitions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>numPartitions</strong> – int, to specify the target number of partitions</p>
</dd>
</dl>
<p>Similar to coalesce defined on an <code class="xref py py-class docutils literal notranslate"><span class="pre">RDD</span></code>, this operation results in a
narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,
there will not be a shuffle, instead each of the 100 new partitions will
claim 10 of the current partitions. If a larger number of partitions is requested,
it will stay at the current number of partitions.</p>
<p>However, if you’re doing a drastic coalesce, e.g. to numPartitions = 1,
this may result in your computation taking place on fewer nodes than
you like (e.g. one node in the case of numPartitions = 1). To avoid this,
you can call repartition(). This will add a shuffle step, but means the
current upstream partitions will be executed in parallel (per whatever
the current partitioning is).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.colRegex">
<code class="sig-name descname">colRegex</code><span class="sig-paren">(</span><em class="sig-param">colName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.colRegex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.colRegex" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects column based on the column name specified as a regex and returns it
as <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>colName</strong> – string, column name specified as a regex.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;c&quot;</span><span class="p">,</span>  <span class="mi">3</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;Col1&quot;</span><span class="p">,</span> <span class="s2">&quot;Col2&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">colRegex</span><span class="p">(</span><span class="s2">&quot;`(Col1)?+.+`&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+</span>
<span class="go">|Col2|</span>
<span class="go">+----+</span>
<span class="go">|   1|</span>
<span class="go">|   2|</span>
<span class="go">|   3|</span>
<span class="go">+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.collect">
<code class="sig-name descname">collect</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.collect"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.collect" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all the records as a list of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.columns">
<em class="property">property </em><code class="sig-name descname">columns</code><a class="headerlink" href="#pyspark.sql.DataFrame.columns" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all column names as a list.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">columns</span>
<span class="go">[&#39;age&#39;, &#39;name&#39;]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.corr">
<code class="sig-name descname">corr</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em>, <em class="sig-param">method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.corr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.corr" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the correlation of two columns of a DataFrame as a double value.
Currently only supports the Pearson Correlation Coefficient.
<a class="reference internal" href="#pyspark.sql.DataFrame.corr" title="pyspark.sql.DataFrame.corr"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.corr()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.corr" title="pyspark.sql.DataFrameStatFunctions.corr"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameStatFunctions.corr()</span></code></a> are aliases of each other.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – The name of the first column</p></li>
<li><p><strong>col2</strong> – The name of the second column</p></li>
<li><p><strong>method</strong> – The correlation method. Currently only supports “pearson”</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.count">
<code class="sig-name descname">count</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.count"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of rows in this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">2</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.cov">
<code class="sig-name descname">cov</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.cov"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.cov" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the sample covariance for the given columns, specified by their names, as a
double value. <a class="reference internal" href="#pyspark.sql.DataFrame.cov" title="pyspark.sql.DataFrame.cov"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.cov()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.cov" title="pyspark.sql.DataFrameStatFunctions.cov"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameStatFunctions.cov()</span></code></a> are aliases.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – The name of the first column</p></li>
<li><p><strong>col2</strong> – The name of the second column</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.createGlobalTempView">
<code class="sig-name descname">createGlobalTempView</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.createGlobalTempView"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.createGlobalTempView" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a global temporary view with this DataFrame.</p>
<p>The lifetime of this temporary view is tied to this Spark application.
throws <code class="xref py py-class docutils literal notranslate"><span class="pre">TempTableAlreadyExistsException</span></code>, if the view name already exists in the
catalog.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createGlobalTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;select * from global_temp.people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createGlobalTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>  
<span class="gt">Traceback (most recent call last):</span>
<span class="c">...</span>
<span class="gr">AnalysisException</span>: <span class="n">u&quot;Temporary table &#39;people&#39; already exists;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">dropGlobalTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.createOrReplaceGlobalTempView">
<code class="sig-name descname">createOrReplaceGlobalTempView</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.createOrReplaceGlobalTempView"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.createOrReplaceGlobalTempView" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates or replaces a global temporary view using the given name.</p>
<p>The lifetime of this temporary view is tied to this Spark application.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceGlobalTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">createOrReplaceGlobalTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;select * from global_temp.people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df3</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">dropGlobalTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.createOrReplaceTempView">
<code class="sig-name descname">createOrReplaceTempView</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.createOrReplaceTempView"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.createOrReplaceTempView" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates or replaces a local temporary view with this DataFrame.</p>
<p>The lifetime of this temporary table is tied to the <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code></a>
that was used to create this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;select * from people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df3</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">dropTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.createTempView">
<code class="sig-name descname">createTempView</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.createTempView"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.createTempView" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a local temporary view with this DataFrame.</p>
<p>The lifetime of this temporary table is tied to the <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code></a>
that was used to create this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.
throws <code class="xref py py-class docutils literal notranslate"><span class="pre">TempTableAlreadyExistsException</span></code>, if the view name already exists in the
catalog.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;select * from people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>  
<span class="gt">Traceback (most recent call last):</span>
<span class="c">...</span>
<span class="gr">AnalysisException</span>: <span class="n">u&quot;Temporary table &#39;people&#39; already exists;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">dropTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.crossJoin">
<code class="sig-name descname">crossJoin</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.crossJoin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.crossJoin" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the cartesian product with another <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – Right side of the cartesian product.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Tom&#39;, height=80), Row(name=&#39;Bob&#39;, height=85)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">crossJoin</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;, height=80), Row(age=2, name=&#39;Alice&#39;, height=85),</span>
<span class="go"> Row(age=5, name=&#39;Bob&#39;, height=80), Row(age=5, name=&#39;Bob&#39;, height=85)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.crosstab">
<code class="sig-name descname">crosstab</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.crosstab"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.crosstab" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a pair-wise frequency table of the given columns. Also known as a contingency
table. The number of distinct values for each column should be less than 1e4. At most 1e6
non-zero pair frequencies will be returned.
The first column of each row will be the distinct values of <cite>col1</cite> and the column names
will be the distinct values of <cite>col2</cite>. The name of the first column will be <cite>$col1_$col2</cite>.
Pairs that have no occurrences will have zero as their counts.
<a class="reference internal" href="#pyspark.sql.DataFrame.crosstab" title="pyspark.sql.DataFrame.crosstab"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.crosstab()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.crosstab" title="pyspark.sql.DataFrameStatFunctions.crosstab"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameStatFunctions.crosstab()</span></code></a> are aliases.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – The name of the first column. Distinct items will make the first item of
each row.</p></li>
<li><p><strong>col2</strong> – The name of the second column. Distinct items will make the column names
of the DataFrame.</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.cube">
<code class="sig-name descname">cube</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.cube"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.cube" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a multi-dimensional cube for the current <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> using
the specified columns, so we can run aggregation on them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">cube</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| name| age|count|</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| null|null|    2|</span>
<span class="go">| null|   2|    1|</span>
<span class="go">| null|   5|    1|</span>
<span class="go">|Alice|null|    1|</span>
<span class="go">|Alice|   2|    1|</span>
<span class="go">|  Bob|null|    1|</span>
<span class="go">|  Bob|   5|    1|</span>
<span class="go">+-----+----+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.describe">
<code class="sig-name descname">describe</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.describe"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.describe" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes basic statistics for numeric and string columns.</p>
<p>This include count, mean, stddev, min, and max. If no columns are
given, this function computes statistics for all numerical or string columns.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is meant for exploratory data analysis, as we make no
guarantee about the backward compatibility of the schema of the resulting DataFrame.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">([</span><span class="s1">&#39;age&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------+------------------+</span>
<span class="go">|summary|               age|</span>
<span class="go">+-------+------------------+</span>
<span class="go">|  count|                 2|</span>
<span class="go">|   mean|               3.5|</span>
<span class="go">| stddev|2.1213203435596424|</span>
<span class="go">|    min|                 2|</span>
<span class="go">|    max|                 5|</span>
<span class="go">+-------+------------------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------+------------------+-----+</span>
<span class="go">|summary|               age| name|</span>
<span class="go">+-------+------------------+-----+</span>
<span class="go">|  count|                 2|    2|</span>
<span class="go">|   mean|               3.5| null|</span>
<span class="go">| stddev|2.1213203435596424| null|</span>
<span class="go">|    min|                 2|Alice|</span>
<span class="go">|    max|                 5|  Bob|</span>
<span class="go">+-------+------------------+-----+</span>
</pre></div>
</div>
<p>Use summary for expanded statistics and control over which statistics to compute.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.distinct">
<code class="sig-name descname">distinct</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.distinct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.distinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> containing the distinct rows in this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">2</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.drop">
<code class="sig-name descname">drop</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.drop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.drop" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> that drops the specified column.
This is a no-op if schema doesn’t contain the given column name(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – a string name of the column to drop, or a
<a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> to drop, or a list of string name of the columns to drop.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;), Row(name=&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;), Row(name=&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;inner&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, height=85, name=&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;inner&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;, height=85)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;inner&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.dropDuplicates">
<code class="sig-name descname">dropDuplicates</code><span class="sig-paren">(</span><em class="sig-param">subset=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.dropDuplicates"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.dropDuplicates" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> with duplicate rows removed,
optionally only considering certain columns.</p>
<p>For a static batch <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>, it just drops duplicate rows. For a streaming
<a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>, it will keep all data across triggers as intermediate state to drop
duplicates rows. You can use <a class="reference internal" href="#pyspark.sql.DataFrame.withWatermark" title="pyspark.sql.DataFrame.withWatermark"><code class="xref py py-func docutils literal notranslate"><span class="pre">withWatermark()</span></code></a> to limit how late the duplicate data can
be and system will accordingly limit the state. In addition, too late data older than
watermark will be dropped to avoid any possibility of duplicates.</p>
<p><a class="reference internal" href="#pyspark.sql.DataFrame.drop_duplicates" title="pyspark.sql.DataFrame.drop_duplicates"><code class="xref py py-func docutils literal notranslate"><span class="pre">drop_duplicates()</span></code></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.dropDuplicates" title="pyspark.sql.DataFrame.dropDuplicates"><code class="xref py py-func docutils literal notranslate"><span class="pre">dropDuplicates()</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span> \
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">),</span> \
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">),</span> \
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">)])</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">|  5|    80|Alice|</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">([</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">|  5|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.drop_duplicates">
<code class="sig-name descname">drop_duplicates</code><span class="sig-paren">(</span><em class="sig-param">subset=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.DataFrame.drop_duplicates" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#pyspark.sql.DataFrame.drop_duplicates" title="pyspark.sql.DataFrame.drop_duplicates"><code class="xref py py-func docutils literal notranslate"><span class="pre">drop_duplicates()</span></code></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.dropDuplicates" title="pyspark.sql.DataFrame.dropDuplicates"><code class="xref py py-func docutils literal notranslate"><span class="pre">dropDuplicates()</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.dropna">
<code class="sig-name descname">dropna</code><span class="sig-paren">(</span><em class="sig-param">how='any'</em>, <em class="sig-param">thresh=None</em>, <em class="sig-param">subset=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.dropna"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.dropna" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> omitting rows with null values.
<a class="reference internal" href="#pyspark.sql.DataFrame.dropna" title="pyspark.sql.DataFrame.dropna"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.dropna()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.drop" title="pyspark.sql.DataFrameNaFunctions.drop"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameNaFunctions.drop()</span></code></a> are aliases of each other.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>how</strong> – ‘any’ or ‘all’.
If ‘any’, drop a row if it contains any nulls.
If ‘all’, drop a row only if all its values are null.</p></li>
<li><p><strong>thresh</strong> – int, default None
If specified, drop rows that have less than <cite>thresh</cite> non-null values.
This overwrites the <cite>how</cite> parameter.</p></li>
<li><p><strong>subset</strong> – optional list of column names to consider.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">drop</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.dtypes">
<em class="property">property </em><code class="sig-name descname">dtypes</code><a class="headerlink" href="#pyspark.sql.DataFrame.dtypes" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all column names and their data types as a list.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;int&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.exceptAll">
<code class="sig-name descname">exceptAll</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.exceptAll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.exceptAll" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> containing rows in this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> but
not in another <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> while preserving duplicates.</p>
<p>This is equivalent to <cite>EXCEPT ALL</cite> in SQL.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
<span class="gp">... </span>        <span class="p">[(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>  <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="s2">&quot;C2&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="s2">&quot;C2&quot;</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span><span class="o">.</span><span class="n">exceptAll</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+---+</span>
<span class="go">| C1| C2|</span>
<span class="go">+---+---+</span>
<span class="go">|  a|  1|</span>
<span class="go">|  a|  1|</span>
<span class="go">|  a|  2|</span>
<span class="go">|  c|  4|</span>
<span class="go">+---+---+</span>
</pre></div>
</div>
<p>Also as standard in SQL, this function resolves columns by position (not by name).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.explain">
<code class="sig-name descname">explain</code><span class="sig-paren">(</span><em class="sig-param">extended=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.explain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.explain" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the (logical and physical) plans to the console for debugging purpose.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>extended</strong> – boolean, default <code class="docutils literal notranslate"><span class="pre">False</span></code>. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, prints only the physical plan.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">explain</span><span class="p">()</span>
<span class="go">== Physical Plan ==</span>
<span class="go">Scan ExistingRDD[age#0,name#1]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">== Parsed Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Analyzed Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Optimized Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Physical Plan ==</span>
<span class="gp">...</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.fillna">
<code class="sig-name descname">fillna</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">subset=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.fillna"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.fillna" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace null values, alias for <code class="docutils literal notranslate"><span class="pre">na.fill()</span></code>.
<a class="reference internal" href="#pyspark.sql.DataFrame.fillna" title="pyspark.sql.DataFrame.fillna"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.fillna()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.fill" title="pyspark.sql.DataFrameNaFunctions.fill"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameNaFunctions.fill()</span></code></a> are aliases of each other.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> – int, long, float, string, bool or dict.
Value to replace null values with.
If the value is a dict, then <cite>subset</cite> is ignored and <cite>value</cite> must be a mapping
from column name (string) to replacement value. The replacement value must be
an int, long, float, boolean, or string.</p></li>
<li><p><strong>subset</strong> – optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">|  5|    50|  Bob|</span>
<span class="go">| 50|    50|  Tom|</span>
<span class="go">| 50|    50| null|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df5</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+-------+-----+</span>
<span class="go">| age|   name|  spy|</span>
<span class="go">+----+-------+-----+</span>
<span class="go">|  10|  Alice|false|</span>
<span class="go">|   5|    Bob|false|</span>
<span class="go">|null|Mallory| true|</span>
<span class="go">+----+-------+-----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">({</span><span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;unknown&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-------+</span>
<span class="go">|age|height|   name|</span>
<span class="go">+---+------+-------+</span>
<span class="go">| 10|    80|  Alice|</span>
<span class="go">|  5|  null|    Bob|</span>
<span class="go">| 50|  null|    Tom|</span>
<span class="go">| 50|  null|unknown|</span>
<span class="go">+---+------+-------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.filter">
<code class="sig-name descname">filter</code><span class="sig-paren">(</span><em class="sig-param">condition</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.filter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.filter" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters rows using the given condition.</p>
<p><a class="reference internal" href="#pyspark.sql.DataFrame.where" title="pyspark.sql.DataFrame.where"><code class="xref py py-func docutils literal notranslate"><span class="pre">where()</span></code></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.filter" title="pyspark.sql.DataFrame.filter"><code class="xref py py-func docutils literal notranslate"><span class="pre">filter()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>condition</strong> – a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> of <a class="reference internal" href="#pyspark.sql.types.BooleanType" title="pyspark.sql.types.BooleanType"><code class="xref py py-class docutils literal notranslate"><span class="pre">types.BooleanType</span></code></a>
or a string of SQL expression.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s2">&quot;age &gt; 3&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s2">&quot;age = 2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.first">
<code class="sig-name descname">first</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.first"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.first" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first row as a <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">Row(age=2, name=&#39;Alice&#39;)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.foreach">
<code class="sig-name descname">foreach</code><span class="sig-paren">(</span><em class="sig-param">f</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.foreach"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.foreach" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the <code class="docutils literal notranslate"><span class="pre">f</span></code> function to all <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code></a> of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<p>This is a shorthand for <code class="docutils literal notranslate"><span class="pre">df.rdd.foreach()</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">person</span><span class="p">):</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">person</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.foreachPartition">
<code class="sig-name descname">foreachPartition</code><span class="sig-paren">(</span><em class="sig-param">f</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.foreachPartition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.foreachPartition" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the <code class="docutils literal notranslate"><span class="pre">f</span></code> function to each partition of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<p>This a shorthand for <code class="docutils literal notranslate"><span class="pre">df.rdd.foreachPartition()</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">people</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">person</span> <span class="ow">in</span> <span class="n">people</span><span class="p">:</span>
<span class="gp">... </span>        <span class="nb">print</span><span class="p">(</span><span class="n">person</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">foreachPartition</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.freqItems">
<code class="sig-name descname">freqItems</code><span class="sig-paren">(</span><em class="sig-param">cols</em>, <em class="sig-param">support=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.freqItems"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.freqItems" title="Permalink to this definition">¶</a></dt>
<dd><p>Finding frequent items for columns, possibly with false positives. Using the
frequent element count algorithm described in
“<a class="reference external" href="http://dx.doi.org/10.1145/762471.762473">http://dx.doi.org/10.1145/762471.762473</a>, proposed by Karp, Schenker, and Papadimitriou”.
<a class="reference internal" href="#pyspark.sql.DataFrame.freqItems" title="pyspark.sql.DataFrame.freqItems"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.freqItems()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.freqItems" title="pyspark.sql.DataFrameStatFunctions.freqItems"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameStatFunctions.freqItems()</span></code></a> are aliases.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is meant for exploratory data analysis, as we make no
guarantee about the backward compatibility of the schema of the resulting DataFrame.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cols</strong> – Names of the columns to calculate frequent items for as a list or tuple of
strings.</p></li>
<li><p><strong>support</strong> – The frequency with which to consider an item ‘frequent’. Default is 1%.
The support must be greater than 1e-4.</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.groupBy">
<code class="sig-name descname">groupBy</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.groupBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.groupBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Groups the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> using the specified columns,
so we can run aggregation on them. See <a class="reference internal" href="#pyspark.sql.GroupedData" title="pyspark.sql.GroupedData"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupedData</span></code></a>
for all the available aggregate functions.</p>
<p><a class="reference internal" href="#pyspark.sql.DataFrame.groupby" title="pyspark.sql.DataFrame.groupby"><code class="xref py py-func docutils literal notranslate"><span class="pre">groupby()</span></code></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><code class="xref py py-func docutils literal notranslate"><span class="pre">groupBy()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of columns to group by.
Each element should be a column name (string) or an expression (<a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a>).</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">avg</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(avg(age)=3.5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;mean&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(name=&#39;Alice&#39;, avg(age)=2.0), Row(name=&#39;Bob&#39;, avg(age)=5.0)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">avg</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(name=&#39;Alice&#39;, avg(age)=2.0), Row(name=&#39;Bob&#39;, avg(age)=5.0)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">([</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">])</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=2, count=1), Row(name=&#39;Bob&#39;, age=5, count=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.groupby">
<code class="sig-name descname">groupby</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.DataFrame.groupby" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#pyspark.sql.DataFrame.groupby" title="pyspark.sql.DataFrame.groupby"><code class="xref py py-func docutils literal notranslate"><span class="pre">groupby()</span></code></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><code class="xref py py-func docutils literal notranslate"><span class="pre">groupBy()</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.head">
<code class="sig-name descname">head</code><span class="sig-paren">(</span><em class="sig-param">n=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.head"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.head" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first <code class="docutils literal notranslate"><span class="pre">n</span></code> rows.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method should only be used if the resulting array is expected
to be small, as all the data is loaded into the driver’s memory.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>n</strong> – int, default 1. Number of rows to return.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If n is greater than 1, return a list of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code></a>.
If n is 1, return a single Row.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="go">Row(age=2, name=&#39;Alice&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.hint">
<code class="sig-name descname">hint</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">*parameters</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.hint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.hint" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies some hint on the current DataFrame.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – A name of the hint.</p></li>
<li><p><strong>parameters</strong> – Optional parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">hint</span><span class="p">(</span><span class="s2">&quot;broadcast&quot;</span><span class="p">),</span> <span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+---+------+</span>
<span class="go">|name|age|height|</span>
<span class="go">+----+---+------+</span>
<span class="go">| Bob|  5|    85|</span>
<span class="go">+----+---+------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.intersect">
<code class="sig-name descname">intersect</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.intersect"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.intersect" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> containing rows only in
both this frame and another frame.</p>
<p>This is equivalent to <cite>INTERSECT</cite> in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.intersectAll">
<code class="sig-name descname">intersectAll</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.intersectAll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.intersectAll" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> containing rows in both this dataframe and other
dataframe while preserving duplicates.</p>
<p>This is equivalent to <cite>INTERSECT ALL</cite> in SQL.
&gt;&gt;&gt; df1 = spark.createDataFrame([(“a”, 1), (“a”, 1), (“b”, 3), (“c”, 4)], [“C1”, “C2”])
&gt;&gt;&gt; df2 = spark.createDataFrame([(“a”, 1), (“a”, 1), (“b”, 3)], [“C1”, “C2”])</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span><span class="o">.</span><span class="n">intersectAll</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="s2">&quot;C2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+---+</span>
<span class="go">| C1| C2|</span>
<span class="go">+---+---+</span>
<span class="go">|  a|  1|</span>
<span class="go">|  a|  1|</span>
<span class="go">|  b|  3|</span>
<span class="go">+---+---+</span>
</pre></div>
</div>
<p>Also as standard in SQL, this function resolves columns by position (not by name).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.isLocal">
<code class="sig-name descname">isLocal</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.isLocal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.isLocal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if the <a class="reference internal" href="#pyspark.sql.DataFrame.collect" title="pyspark.sql.DataFrame.collect"><code class="xref py py-func docutils literal notranslate"><span class="pre">collect()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrame.take" title="pyspark.sql.DataFrame.take"><code class="xref py py-func docutils literal notranslate"><span class="pre">take()</span></code></a> methods can be run locally
(without any Spark executors).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.isStreaming">
<em class="property">property </em><code class="sig-name descname">isStreaming</code><a class="headerlink" href="#pyspark.sql.DataFrame.isStreaming" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns true if this <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code> contains one or more sources that continuously
return data as it arrives. A <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code> that reads data from a streaming source
must be executed as a <code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQuery</span></code> using the <code class="xref py py-func docutils literal notranslate"><span class="pre">start()</span></code> method in
<code class="xref py py-class docutils literal notranslate"><span class="pre">DataStreamWriter</span></code>.  Methods that return a single answer, (e.g., <a class="reference internal" href="#pyspark.sql.DataFrame.count" title="pyspark.sql.DataFrame.count"><code class="xref py py-func docutils literal notranslate"><span class="pre">count()</span></code></a> or
<a class="reference internal" href="#pyspark.sql.DataFrame.collect" title="pyspark.sql.DataFrame.collect"><code class="xref py py-func docutils literal notranslate"><span class="pre">collect()</span></code></a>) will throw an <code class="xref py py-class docutils literal notranslate"><span class="pre">AnalysisException</span></code> when there is a streaming
source present.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.join">
<code class="sig-name descname">join</code><span class="sig-paren">(</span><em class="sig-param">other</em>, <em class="sig-param">on=None</em>, <em class="sig-param">how=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.join"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Joins with another <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>, using the given join expression.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>other</strong> – Right side of the join</p></li>
<li><p><strong>on</strong> – a string for the join column name, a list of column names,
a join expression (Column), or a list of Columns.
If <cite>on</cite> is a string or a list of strings indicating the name of the join column(s),
the column(s) must exist on both sides, and this performs an equi-join.</p></li>
<li><p><strong>how</strong> – str, default <code class="docutils literal notranslate"><span class="pre">inner</span></code>. Must be one of: <code class="docutils literal notranslate"><span class="pre">inner</span></code>, <code class="docutils literal notranslate"><span class="pre">cross</span></code>, <code class="docutils literal notranslate"><span class="pre">outer</span></code>,
<code class="docutils literal notranslate"><span class="pre">full</span></code>, <code class="docutils literal notranslate"><span class="pre">full_outer</span></code>, <code class="docutils literal notranslate"><span class="pre">left</span></code>, <code class="docutils literal notranslate"><span class="pre">left_outer</span></code>, <code class="docutils literal notranslate"><span class="pre">right</span></code>, <code class="docutils literal notranslate"><span class="pre">right_outer</span></code>,
<code class="docutils literal notranslate"><span class="pre">left_semi</span></code>, and <code class="docutils literal notranslate"><span class="pre">left_anti</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>The following performs a full outer join between <code class="docutils literal notranslate"><span class="pre">df1</span></code> and <code class="docutils literal notranslate"><span class="pre">df2</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;outer&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">height</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=None, height=80), Row(name=&#39;Bob&#39;, height=85), Row(name=&#39;Alice&#39;, height=None)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;outer&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Tom&#39;, height=80), Row(name=&#39;Bob&#39;, height=85), Row(name=&#39;Alice&#39;, height=None)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cond</span> <span class="o">=</span> <span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df3</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">==</span> <span class="n">df3</span><span class="o">.</span><span class="n">age</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df3</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="s1">&#39;outer&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df3</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=2), Row(name=&#39;Bob&#39;, age=5)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">height</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Bob&#39;, height=85)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df4</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Bob&#39;, age=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.limit">
<code class="sig-name descname">limit</code><span class="sig-paren">(</span><em class="sig-param">num</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.limit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.limit" title="Permalink to this definition">¶</a></dt>
<dd><p>Limits the result count to the number specified.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.localCheckpoint">
<code class="sig-name descname">localCheckpoint</code><span class="sig-paren">(</span><em class="sig-param">eager=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.localCheckpoint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.localCheckpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a locally checkpointed version of this Dataset. Checkpointing can be used to
truncate the logical plan of this DataFrame, which is especially useful in iterative
algorithms where the plan may grow exponentially. Local checkpoints are stored in the
executors using the caching subsystem and therefore they are not reliable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>eager</strong> – Whether to checkpoint this DataFrame immediately</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.na">
<em class="property">property </em><code class="sig-name descname">na</code><a class="headerlink" href="#pyspark.sql.DataFrame.na" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions" title="pyspark.sql.DataFrameNaFunctions"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrameNaFunctions</span></code></a> for handling missing values.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.orderBy">
<code class="sig-name descname">orderBy</code><span class="sig-paren">(</span><em class="sig-param">*cols</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.DataFrame.orderBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> sorted by the specified column(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cols</strong> – list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> or column names to sort by.</p></li>
<li><p><strong>ascending</strong> – boolean or list of boolean (default True).
Sort ascending vs. descending. Specify list for multiple sort orders.
If a list is specified, length of the list must equal length of the <cite>cols</cite>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">asc</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">desc</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">),</span> <span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">([</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.persist">
<code class="sig-name descname">persist</code><span class="sig-paren">(</span><em class="sig-param">storageLevel=StorageLevel(True</em>, <em class="sig-param">True</em>, <em class="sig-param">False</em>, <em class="sig-param">False</em>, <em class="sig-param">1)</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.persist"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.persist" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the storage level to persist the contents of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> across
operations after the first time it is computed. This can only be used to assign
a new storage level if the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> does not have a storage level set yet.
If no storage level is specified defaults to (<code class="xref py py-class docutils literal notranslate"><span class="pre">MEMORY_AND_DISK</span></code>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The default storage level has changed to <code class="xref py py-class docutils literal notranslate"><span class="pre">MEMORY_AND_DISK</span></code> to match Scala in 2.0.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.printSchema">
<code class="sig-name descname">printSchema</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.printSchema"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.printSchema" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints out the schema in the tree format.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="go">root</span>
<span class="go"> |-- age: integer (nullable = true)</span>
<span class="go"> |-- name: string (nullable = true)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.randomSplit">
<code class="sig-name descname">randomSplit</code><span class="sig-paren">(</span><em class="sig-param">weights</em>, <em class="sig-param">seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.randomSplit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.randomSplit" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly splits this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> with the provided weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weights</strong> – list of doubles as weights with which to split the DataFrame. Weights will
be normalized if they don’t sum up to 1.0.</p></li>
<li><p><strong>seed</strong> – The seed for sampling.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span> <span class="o">=</span> <span class="n">df4</span><span class="o">.</span><span class="n">randomSplit</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="mi">24</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.rdd">
<em class="property">property </em><code class="sig-name descname">rdd</code><a class="headerlink" href="#pyspark.sql.DataFrame.rdd" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the content as an <a class="reference internal" href="pyspark.html#pyspark.RDD" title="pyspark.RDD"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.RDD</span></code></a> of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.registerTempTable">
<code class="sig-name descname">registerTempTable</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.registerTempTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.registerTempTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers this DataFrame as a temporary table using the given name.</p>
<p>The lifetime of this temporary table is tied to the <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code></a>
that was used to create this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;select * from people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">dropTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deprecated in 2.0, use createOrReplaceTempView instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.repartition">
<code class="sig-name descname">repartition</code><span class="sig-paren">(</span><em class="sig-param">numPartitions</em>, <em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.repartition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.repartition" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> partitioned by the given partitioning expressions. The
resulting DataFrame is hash partitioned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>numPartitions</strong> – can be an int to specify the target number of partitions or a Column.
If it is a Column, it will be used as the first partitioning column. If not specified,
the default number of partitions is used.</p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 1.6: </span>Added optional arguments to specify the partitioning columns. Also made numPartitions
optional if partitioning columns are specified.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
<span class="go">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  2|Alice|</span>
<span class="go">+---+-----+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">+---+-----+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
<span class="go">7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  2|Alice|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.repartitionByRange">
<code class="sig-name descname">repartitionByRange</code><span class="sig-paren">(</span><em class="sig-param">numPartitions</em>, <em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.repartitionByRange"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.repartitionByRange" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> partitioned by the given partitioning expressions. The
resulting DataFrame is range partitioned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>numPartitions</strong> – can be an int to specify the target number of partitions or a Column.
If it is a Column, it will be used as the first partitioning column. If not specified,
the default number of partitions is used.</p>
</dd>
</dl>
<p>At least one partition-by expression must be specified.
When no explicit sort order is specified, “ascending nulls first” is assumed.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">repartitionByRange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">+---+-----+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">repartitionByRange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">repartitionByRange</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.replace">
<code class="sig-name descname">replace</code><span class="sig-paren">(</span><em class="sig-param">to_replace</em>, <em class="sig-param">value=&lt;no value&gt;</em>, <em class="sig-param">subset=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.replace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> replacing a value with another value.
<a class="reference internal" href="#pyspark.sql.DataFrame.replace" title="pyspark.sql.DataFrame.replace"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.replace()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.replace" title="pyspark.sql.DataFrameNaFunctions.replace"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameNaFunctions.replace()</span></code></a> are
aliases of each other.
Values to_replace and value must have the same type and can only be numerics, booleans,
or strings. Value can have None. When replacing, the new value will be cast
to the type of the existing column.
For numeric replacements all values to be replaced should have unique
floating point representation. In case of conflicts (for example with <cite>{42: -1, 42.0: 1}</cite>)
and arbitrary replacement will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>to_replace</strong> – bool, int, long, float, string, list or dict.
Value to be replaced.
If the value is a dict, then <cite>value</cite> is ignored or can be omitted, and <cite>to_replace</cite>
must be a mapping between a value and a replacement.</p></li>
<li><p><strong>value</strong> – bool, int, long, float, string, list or None.
The replacement value must be a bool, int, long, float, string or None. If <cite>value</cite> is a
list, <cite>value</cite> should be of the same length and type as <cite>to_replace</cite>.
If <cite>value</cite> is a scalar and <cite>to_replace</cite> is a sequence, then <cite>value</cite> is
used as a replacement for each item in <cite>to_replace</cite>.</p></li>
<li><p><strong>subset</strong> – optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+-----+</span>
<span class="go">| age|height| name|</span>
<span class="go">+----+------+-----+</span>
<span class="go">|  20|    80|Alice|</span>
<span class="go">|   5|  null|  Bob|</span>
<span class="go">|null|  null|  Tom|</span>
<span class="go">|null|  null| null|</span>
<span class="go">+----+------+-----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+----+</span>
<span class="go">| age|height|name|</span>
<span class="go">+----+------+----+</span>
<span class="go">|  10|    80|null|</span>
<span class="go">|   5|  null| Bob|</span>
<span class="go">|null|  null| Tom|</span>
<span class="go">|null|  null|null|</span>
<span class="go">+----+------+----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="s1">&#39;Alice&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">})</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+----+</span>
<span class="go">| age|height|name|</span>
<span class="go">+----+------+----+</span>
<span class="go">|  10|    80|null|</span>
<span class="go">|   5|  null| Bob|</span>
<span class="go">|null|  null| Tom|</span>
<span class="go">|null|  null|null|</span>
<span class="go">+----+------+----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="s1">&#39;Bob&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">],</span> <span class="s1">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+----+</span>
<span class="go">| age|height|name|</span>
<span class="go">+----+------+----+</span>
<span class="go">|  10|    80|   A|</span>
<span class="go">|   5|  null|   B|</span>
<span class="go">|null|  null| Tom|</span>
<span class="go">|null|  null|null|</span>
<span class="go">+----+------+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.rollup">
<code class="sig-name descname">rollup</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.rollup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.rollup" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a multi-dimensional rollup for the current <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> using
the specified columns, so we can run aggregation on them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">rollup</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| name| age|count|</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| null|null|    2|</span>
<span class="go">|Alice|null|    1|</span>
<span class="go">|Alice|   2|    1|</span>
<span class="go">|  Bob|null|    1|</span>
<span class="go">|  Bob|   5|    1|</span>
<span class="go">+-----+----+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.sample">
<code class="sig-name descname">sample</code><span class="sig-paren">(</span><em class="sig-param">withReplacement=None</em>, <em class="sig-param">fraction=None</em>, <em class="sig-param">seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sampled subset of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>withReplacement</strong> – Sample with replacement or not (default False).</p></li>
<li><p><strong>fraction</strong> – Fraction of rows to generate, range [0.0, 1.0].</p></li>
<li><p><strong>seed</strong> – Seed for sampling (default a random seed).</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is not guaranteed to provide exactly the fraction specified of the total
count of the given <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><cite>fraction</cite> is required and, <cite>withReplacement</cite> and <cite>seed</cite> are optional.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">fraction</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">withReplacement</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">fraction</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">10</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.sampleBy">
<code class="sig-name descname">sampleBy</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">fractions</em>, <em class="sig-param">seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.sampleBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.sampleBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a stratified sample without replacement based on the
fraction given on each stratum.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – column that defines strata</p></li>
<li><p><strong>fractions</strong> – sampling fraction for each stratum. If a stratum is not
specified, we treat its fraction as zero.</p></li>
<li><p><strong>seed</strong> – random seed</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a new DataFrame that represents the stratified sample</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">col</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">((</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sampled</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampleBy</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="n">fractions</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">},</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sampled</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|key|count|</span>
<span class="go">+---+-----+</span>
<span class="go">|  0|    5|</span>
<span class="go">|  1|    9|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.schema">
<em class="property">property </em><code class="sig-name descname">schema</code><a class="headerlink" href="#pyspark.sql.DataFrame.schema" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the schema of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> as a <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">schema</span>
<span class="go">StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Projects a set of expressions and returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of column names (string) or expressions (<a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a>).
If one of the column names is ‘*’, that column is expanded to include all columns
in the current DataFrame.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=2), Row(name=&#39;Bob&#39;, age=5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">+</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;, age=12), Row(name=&#39;Bob&#39;, age=15)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.selectExpr">
<code class="sig-name descname">selectExpr</code><span class="sig-paren">(</span><em class="sig-param">*expr</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.selectExpr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.selectExpr" title="Permalink to this definition">¶</a></dt>
<dd><p>Projects a set of SQL expressions and returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<p>This is a variant of <a class="reference internal" href="#pyspark.sql.DataFrame.select" title="pyspark.sql.DataFrame.select"><code class="xref py py-func docutils literal notranslate"><span class="pre">select()</span></code></a> that accepts SQL expressions.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s2">&quot;age * 2&quot;</span><span class="p">,</span> <span class="s2">&quot;abs(age)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.show">
<code class="sig-name descname">show</code><span class="sig-paren">(</span><em class="sig-param">n=20</em>, <em class="sig-param">truncate=True</em>, <em class="sig-param">vertical=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.show"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.show" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the first <code class="docutils literal notranslate"><span class="pre">n</span></code> rows to the console.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n</strong> – Number of rows to show.</p></li>
<li><p><strong>truncate</strong> – If set to True, truncate strings longer than 20 chars by default.
If set to a number greater than one, truncates long strings to length <code class="docutils literal notranslate"><span class="pre">truncate</span></code>
and align cells right.</p></li>
<li><p><strong>vertical</strong> – If set to True, print output rows vertically (one line
per column value).</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span>
<span class="go">DataFrame[age: int, name: string]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">+---+-----+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">+---+----+</span>
<span class="go">|age|name|</span>
<span class="go">+---+----+</span>
<span class="go">|  2| Ali|</span>
<span class="go">|  5| Bob|</span>
<span class="go">+---+----+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">vertical</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">-RECORD 0-----</span>
<span class="go"> age  | 2</span>
<span class="go"> name | Alice</span>
<span class="go">-RECORD 1-----</span>
<span class="go"> age  | 5</span>
<span class="go"> name | Bob</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.sort">
<code class="sig-name descname">sort</code><span class="sig-paren">(</span><em class="sig-param">*cols</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.sort"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.sort" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> sorted by the specified column(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cols</strong> – list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> or column names to sort by.</p></li>
<li><p><strong>ascending</strong> – boolean or list of boolean (default True).
Sort ascending vs. descending. Specify list for multiple sort orders.
If a list is specified, length of the list must equal length of the <cite>cols</cite>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">asc</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">desc</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">),</span> <span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">([</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;), Row(age=2, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.sortWithinPartitions">
<code class="sig-name descname">sortWithinPartitions</code><span class="sig-paren">(</span><em class="sig-param">*cols</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.sortWithinPartitions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.sortWithinPartitions" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> with each partition sorted by the specified column(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cols</strong> – list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> or column names to sort by.</p></li>
<li><p><strong>ascending</strong> – boolean or list of boolean (default True).
Sort ascending vs. descending. Specify list for multiple sort orders.
If a list is specified, length of the list must equal length of the <cite>cols</cite>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sortWithinPartitions</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.stat">
<em class="property">property </em><code class="sig-name descname">stat</code><a class="headerlink" href="#pyspark.sql.DataFrame.stat" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions" title="pyspark.sql.DataFrameStatFunctions"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrameStatFunctions</span></code></a> for statistic functions.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.storageLevel">
<em class="property">property </em><code class="sig-name descname">storageLevel</code><a class="headerlink" href="#pyspark.sql.DataFrame.storageLevel" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>’s current storage level.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">storageLevel</span>
<span class="go">StorageLevel(False, False, False, False, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span><span class="o">.</span><span class="n">storageLevel</span>
<span class="go">StorageLevel(True, True, False, True, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">persist</span><span class="p">(</span><span class="n">StorageLevel</span><span class="o">.</span><span class="n">DISK_ONLY_2</span><span class="p">)</span><span class="o">.</span><span class="n">storageLevel</span>
<span class="go">StorageLevel(True, False, False, False, 2)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.subtract">
<code class="sig-name descname">subtract</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.subtract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.subtract" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> containing rows in this frame
but not in another frame.</p>
<p>This is equivalent to <cite>EXCEPT DISTINCT</cite> in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.summary">
<code class="sig-name descname">summary</code><span class="sig-paren">(</span><em class="sig-param">*statistics</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.summary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes specified statistics for numeric and string columns. Available statistics are:
- count
- mean
- stddev
- min
- max
- arbitrary approximate percentiles specified as a percentage (eg, 75%)</p>
<p>If no statistics are given, this function computes count, mean, stddev, min,
approximate quartiles (percentiles at 25%, 50%, and 75%), and max.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is meant for exploratory data analysis, as we make no
guarantee about the backward compatibility of the schema of the resulting DataFrame.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------+------------------+-----+</span>
<span class="go">|summary|               age| name|</span>
<span class="go">+-------+------------------+-----+</span>
<span class="go">|  count|                 2|    2|</span>
<span class="go">|   mean|               3.5| null|</span>
<span class="go">| stddev|2.1213203435596424| null|</span>
<span class="go">|    min|                 2|Alice|</span>
<span class="go">|    25%|                 2| null|</span>
<span class="go">|    50%|                 2| null|</span>
<span class="go">|    75%|                 5| null|</span>
<span class="go">|    max|                 5|  Bob|</span>
<span class="go">+-------+------------------+-----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;25%&quot;</span><span class="p">,</span> <span class="s2">&quot;75%&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------+---+-----+</span>
<span class="go">|summary|age| name|</span>
<span class="go">+-------+---+-----+</span>
<span class="go">|  count|  2|    2|</span>
<span class="go">|    min|  2|Alice|</span>
<span class="go">|    25%|  2| null|</span>
<span class="go">|    75%|  5| null|</span>
<span class="go">|    max|  5|  Bob|</span>
<span class="go">+-------+---+-----+</span>
</pre></div>
</div>
<p>To do a summary for specific columns first select them:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="s2">&quot;count&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------+---+----+</span>
<span class="go">|summary|age|name|</span>
<span class="go">+-------+---+----+</span>
<span class="go">|  count|  2|   2|</span>
<span class="go">+-------+---+----+</span>
</pre></div>
</div>
<p>See also describe for basic statistics.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.take">
<code class="sig-name descname">take</code><span class="sig-paren">(</span><em class="sig-param">num</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.take"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.take" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first <code class="docutils literal notranslate"><span class="pre">num</span></code> rows as a <code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code> of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.toDF">
<code class="sig-name descname">toDF</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.toDF"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.toDF" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new class:<cite>DataFrame</cite> that with new specified column names</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of new column names (string)</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="s1">&#39;f2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=2, f2=&#39;Alice&#39;), Row(f1=5, f2=&#39;Bob&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.toJSON">
<code class="sig-name descname">toJSON</code><span class="sig-paren">(</span><em class="sig-param">use_unicode=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.toJSON"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.toJSON" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> into a <code class="xref py py-class docutils literal notranslate"><span class="pre">RDD</span></code> of string.</p>
<p>Each row is turned into a JSON document as one element in the returned RDD.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">toJSON</span><span class="p">()</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">&#39;{&quot;age&quot;:2,&quot;name&quot;:&quot;Alice&quot;}&#39;</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.toLocalIterator">
<code class="sig-name descname">toLocalIterator</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.toLocalIterator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.toLocalIterator" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator that contains all of the rows in this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.
The iterator will consume as much memory as the largest partition in this DataFrame.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">toLocalIterator</span><span class="p">())</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;), Row(age=5, name=&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.toPandas">
<code class="sig-name descname">toPandas</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.toPandas"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.toPandas" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> as Pandas <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code>.</p>
<p>This is only available if Pandas is installed and available.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method should only be used if the resulting Pandas’s DataFrame is expected
to be small, as all the data is loaded into the driver’s memory.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Usage with spark.sql.execution.arrow.enabled=True is experimental.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>  
<span class="go">   age   name</span>
<span class="go">0    2  Alice</span>
<span class="go">1    5    Bob</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.union">
<code class="sig-name descname">union</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.union"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.union" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> containing union of rows in this and another frame.</p>
<p>This is equivalent to <cite>UNION ALL</cite> in SQL. To do a SQL-style set union
(that does deduplication of elements), use this function followed by <a class="reference internal" href="#pyspark.sql.DataFrame.distinct" title="pyspark.sql.DataFrame.distinct"><code class="xref py py-func docutils literal notranslate"><span class="pre">distinct()</span></code></a>.</p>
<p>Also as standard in SQL, this function resolves columns by position (not by name).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.unionAll">
<code class="sig-name descname">unionAll</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.unionAll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.unionAll" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> containing union of rows in this and another frame.</p>
<p>This is equivalent to <cite>UNION ALL</cite> in SQL. To do a SQL-style set union
(that does deduplication of elements), use this function followed by <a class="reference internal" href="#pyspark.sql.DataFrame.distinct" title="pyspark.sql.DataFrame.distinct"><code class="xref py py-func docutils literal notranslate"><span class="pre">distinct()</span></code></a>.</p>
<p>Also as standard in SQL, this function resolves columns by position (not by name).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deprecated in 2.0, use <a class="reference internal" href="#pyspark.sql.DataFrame.union" title="pyspark.sql.DataFrame.union"><code class="xref py py-func docutils literal notranslate"><span class="pre">union()</span></code></a> instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.unionByName">
<code class="sig-name descname">unionByName</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.unionByName"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.unionByName" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> containing union of rows in this and another frame.</p>
<p>This is different from both <cite>UNION ALL</cite> and <cite>UNION DISTINCT</cite> in SQL. To do a SQL-style set
union (that does deduplication of elements), use this function followed by <a class="reference internal" href="#pyspark.sql.DataFrame.distinct" title="pyspark.sql.DataFrame.distinct"><code class="xref py py-func docutils literal notranslate"><span class="pre">distinct()</span></code></a>.</p>
<p>The difference between this function and <a class="reference internal" href="#pyspark.sql.DataFrame.union" title="pyspark.sql.DataFrame.union"><code class="xref py py-func docutils literal notranslate"><span class="pre">union()</span></code></a> is that this function
resolves columns by name (not by position):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[</span><span class="s2">&quot;col0&quot;</span><span class="p">,</span> <span class="s2">&quot;col1&quot;</span><span class="p">,</span> <span class="s2">&quot;col2&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="p">[</span><span class="s2">&quot;col1&quot;</span><span class="p">,</span> <span class="s2">&quot;col2&quot;</span><span class="p">,</span> <span class="s2">&quot;col0&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span><span class="o">.</span><span class="n">unionByName</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+----+----+</span>
<span class="go">|col0|col1|col2|</span>
<span class="go">+----+----+----+</span>
<span class="go">|   1|   2|   3|</span>
<span class="go">|   6|   4|   5|</span>
<span class="go">+----+----+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.unpersist">
<code class="sig-name descname">unpersist</code><span class="sig-paren">(</span><em class="sig-param">blocking=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.unpersist"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.unpersist" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> as non-persistent, and remove all blocks for it from
memory and disk.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><cite>blocking</cite> default has changed to False to match Scala in 2.0.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.where">
<code class="sig-name descname">where</code><span class="sig-paren">(</span><em class="sig-param">condition</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.DataFrame.where" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#pyspark.sql.DataFrame.where" title="pyspark.sql.DataFrame.where"><code class="xref py py-func docutils literal notranslate"><span class="pre">where()</span></code></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.filter" title="pyspark.sql.DataFrame.filter"><code class="xref py py-func docutils literal notranslate"><span class="pre">filter()</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.withColumn">
<code class="sig-name descname">withColumn</code><span class="sig-paren">(</span><em class="sig-param">colName</em>, <em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.withColumn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.withColumn" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> by adding a column or replacing the
existing column that has the same name.</p>
<p>The column expression must be an expression over this DataFrame; attempting to add
a column from some other dataframe will raise an error.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>colName</strong> – string, name of the new column.</p></li>
<li><p><strong>col</strong> – a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> expression for the new column.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;age2&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;, age2=4), Row(age=5, name=&#39;Bob&#39;, age2=7)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.withColumnRenamed">
<code class="sig-name descname">withColumnRenamed</code><span class="sig-paren">(</span><em class="sig-param">existing</em>, <em class="sig-param">new</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.withColumnRenamed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.withColumnRenamed" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> by renaming an existing column.
This is a no-op if schema doesn’t contain the given column name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>existing</strong> – string, name of the existing column to rename.</p></li>
<li><p><strong>new</strong> – string, new name of the column.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;age2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age2=2, name=&#39;Alice&#39;), Row(age2=5, name=&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.withWatermark">
<code class="sig-name descname">withWatermark</code><span class="sig-paren">(</span><em class="sig-param">eventTime</em>, <em class="sig-param">delayThreshold</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrame.withWatermark"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrame.withWatermark" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines an event time watermark for this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>. A watermark tracks a point
in time before which we assume no more late data is going to arrive.</p>
<dl class="simple">
<dt>Spark will use this watermark for several purposes:</dt><dd><ul class="simple">
<li><p>To know when a given time window aggregation can be finalized and thus can be emitted
when using output modes that do not allow updates.</p></li>
<li><p>To minimize the amount of state that we need to keep for on-going aggregations.</p></li>
</ul>
</dd>
</dl>
<p>The current watermark is computed by looking at the <cite>MAX(eventTime)</cite> seen across
all of the partitions in the query minus a user specified <cite>delayThreshold</cite>.  Due to the cost
of coordinating this value across partitions, the actual watermark used is only guaranteed
to be at least <cite>delayThreshold</cite> behind the actual event time.  In some cases we may still
process records that arrive more than <cite>delayThreshold</cite> late.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eventTime</strong> – the name of the column that contains the event time of the row.</p></li>
<li><p><strong>delayThreshold</strong> – the minimum delay to wait to data to arrive late, relative to the
latest record that has been processed in the form of an interval
(e.g. “1 minute” or “5 hours”).</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sdf</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">sdf</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s1">&#39;timestamp&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">withWatermark</span><span class="p">(</span><span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="s1">&#39;10 minutes&#39;</span><span class="p">)</span>
<span class="go">DataFrame[name: string, time: timestamp]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.write">
<em class="property">property </em><code class="sig-name descname">write</code><a class="headerlink" href="#pyspark.sql.DataFrame.write" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface for saving the content of the non-streaming <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> out into external
storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.DataFrameWriter" title="pyspark.sql.DataFrameWriter"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrameWriter</span></code></a></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.writeStream">
<em class="property">property </em><code class="sig-name descname">writeStream</code><a class="headerlink" href="#pyspark.sql.DataFrame.writeStream" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface for saving the content of the streaming <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> out into external
storage.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataStreamWriter</span></code></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.GroupedData">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">GroupedData</code><span class="sig-paren">(</span><em class="sig-param">jgd</em>, <em class="sig-param">df</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/group.html#GroupedData"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.GroupedData" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of methods for aggregations on a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>,
created by <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.groupBy()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.GroupedData.agg">
<code class="sig-name descname">agg</code><span class="sig-paren">(</span><em class="sig-param">*exprs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/group.html#GroupedData.agg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.GroupedData.agg" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute aggregates and returns the result as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<p>The available aggregate functions can be:</p>
<ol class="arabic">
<li><p>built-in aggregation functions, such as <cite>avg</cite>, <cite>max</cite>, <cite>min</cite>, <cite>sum</cite>, <cite>count</cite></p></li>
<li><p>group aggregate pandas UDFs, created with <a class="reference internal" href="#pyspark.sql.functions.pandas_udf" title="pyspark.sql.functions.pandas_udf"><code class="xref py py-func docutils literal notranslate"><span class="pre">pyspark.sql.functions.pandas_udf()</span></code></a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is no partial aggregation with group aggregate UDFs, i.e.,
a full shuffle is required. Also, all the data of a group will be loaded into
memory, so the user should be aware of the potential OOM risk if data is skewed
and certain groups are too large to fit in memory.</p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#pyspark.sql.functions.pandas_udf" title="pyspark.sql.functions.pandas_udf"><code class="xref py py-func docutils literal notranslate"><span class="pre">pyspark.sql.functions.pandas_udf()</span></code></a></p>
</div>
</li>
</ol>
<p>If <code class="docutils literal notranslate"><span class="pre">exprs</span></code> is a single <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code> mapping from string to string, then the key
is the column to perform aggregation on, and the value is the aggregate function.</p>
<p>Alternatively, <code class="docutils literal notranslate"><span class="pre">exprs</span></code> can also be a list of aggregate <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> expressions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Built-in aggregation functions and group aggregate pandas UDFs cannot be mixed
in a single call to this function.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>exprs</strong> – a dict mapping from column name (string) to aggregate functions (string),
or a list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a>.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gdf</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">gdf</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s2">&quot;*&quot;</span><span class="p">:</span> <span class="s2">&quot;count&quot;</span><span class="p">})</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(name=&#39;Alice&#39;, count(1)=1), Row(name=&#39;Bob&#39;, count(1)=1)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">gdf</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(name=&#39;Alice&#39;, min(age)=2), Row(name=&#39;Bob&#39;, min(age)=5)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_AGG</span><span class="p">)</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">min_udf</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">gdf</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">min_udf</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>  
<span class="go">[Row(name=&#39;Alice&#39;, min_udf(age)=2), Row(name=&#39;Bob&#39;, min_udf(age)=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param">udf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/group.html#GroupedData.apply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.GroupedData.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Maps each group of the current <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> using a pandas udf and returns the result
as a <cite>DataFrame</cite>.</p>
<p>The user-defined function should take a <cite>pandas.DataFrame</cite> and return another
<cite>pandas.DataFrame</cite>. For each group, all columns are passed together as a <cite>pandas.DataFrame</cite>
to the user-function and the returned <cite>pandas.DataFrame</cite> are combined as a
<a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<p>The returned <cite>pandas.DataFrame</cite> can be of arbitrary length and its schema must match the
returnType of the pandas udf.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function requires a full shuffle. all the data of a group will be loaded
into memory, so the user should be aware of the potential OOM risk if data is skewed
and certain groups are too large to fit in memory.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Experimental</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>udf</strong> – a grouped map user-defined function returned by
<a class="reference internal" href="#pyspark.sql.functions.pandas_udf" title="pyspark.sql.functions.pandas_udf"><code class="xref py py-func docutils literal notranslate"><span class="pre">pyspark.sql.functions.pandas_udf()</span></code></a>.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s2">&quot;id long, v double&quot;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_MAP</span><span class="p">)</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">pdf</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">v</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">v</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">pdf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">v</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">normalize</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  
<span class="go">+---+-------------------+</span>
<span class="go">| id|                  v|</span>
<span class="go">+---+-------------------+</span>
<span class="go">|  1|-0.7071067811865475|</span>
<span class="go">|  1| 0.7071067811865475|</span>
<span class="go">|  2|-0.8320502943378437|</span>
<span class="go">|  2|-0.2773500981126146|</span>
<span class="go">|  2| 1.1094003924504583|</span>
<span class="go">+---+-------------------+</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#pyspark.sql.functions.pandas_udf" title="pyspark.sql.functions.pandas_udf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.functions.pandas_udf()</span></code></a></p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.avg">
<code class="sig-name descname">avg</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/group.html#GroupedData.avg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.GroupedData.avg" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes average values for each numeric columns for each group.</p>
<p><a class="reference internal" href="#pyspark.sql.GroupedData.mean" title="pyspark.sql.GroupedData.mean"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean()</span></code></a> is an alias for <a class="reference internal" href="#pyspark.sql.GroupedData.avg" title="pyspark.sql.GroupedData.avg"><code class="xref py py-func docutils literal notranslate"><span class="pre">avg()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of column names (string). Non-numeric columns are ignored.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">avg</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(avg(age)=3.5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">avg</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(avg(age)=3.5, avg(height)=82.5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.count">
<code class="sig-name descname">count</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/group.html#GroupedData.count"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.GroupedData.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Counts the number of records for each group.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(age=2, count=1), Row(age=5, count=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.max">
<code class="sig-name descname">max</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/group.html#GroupedData.max"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.GroupedData.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the max value for each numeric columns for each group.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(max(age)=5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(max(age)=5, max(height)=85)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.mean">
<code class="sig-name descname">mean</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/group.html#GroupedData.mean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.GroupedData.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes average values for each numeric columns for each group.</p>
<p><a class="reference internal" href="#pyspark.sql.GroupedData.mean" title="pyspark.sql.GroupedData.mean"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean()</span></code></a> is an alias for <a class="reference internal" href="#pyspark.sql.GroupedData.avg" title="pyspark.sql.GroupedData.avg"><code class="xref py py-func docutils literal notranslate"><span class="pre">avg()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of column names (string). Non-numeric columns are ignored.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(avg(age)=3.5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(avg(age)=3.5, avg(height)=82.5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.min">
<code class="sig-name descname">min</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/group.html#GroupedData.min"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.GroupedData.min" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the min value for each numeric column for each group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of column names (string). Non-numeric columns are ignored.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(min(age)=2)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(min(age)=2, min(height)=80)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.pivot">
<code class="sig-name descname">pivot</code><span class="sig-paren">(</span><em class="sig-param">pivot_col</em>, <em class="sig-param">values=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/group.html#GroupedData.pivot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.GroupedData.pivot" title="Permalink to this definition">¶</a></dt>
<dd><p>Pivots a column of the current <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> and perform the specified aggregation.
There are two versions of pivot function: one that requires the caller to specify the list
of distinct values to pivot on, and one that does not. The latter is more concise but less
efficient, because Spark needs to first compute the list of distinct values internally.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pivot_col</strong> – Name of the column to pivot.</p></li>
<li><p><strong>values</strong> – List of values that will be translated to columns in the output DataFrame.</p></li>
</ul>
</dd>
</dl>
<p># Compute the sum of earnings for each year by course with each course as a separate column</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;year&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="s2">&quot;course&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;dotNET&quot;</span><span class="p">,</span> <span class="s2">&quot;Java&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;earnings&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(year=2012, dotNET=15000, Java=20000), Row(year=2013, dotNET=48000, Java=30000)]</span>
</pre></div>
</div>
<p># Or without specifying column values (less efficient)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;year&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="s2">&quot;course&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;earnings&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df5</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;sales.year&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="s2">&quot;sales.course&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;sales.earnings&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.sum">
<code class="sig-name descname">sum</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/group.html#GroupedData.sum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.GroupedData.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the sum for each numeric columns for each group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of column names (string). Non-numeric columns are ignored.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(sum(age)=7)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(sum(age)=7, sum(height)=165)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Column">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">Column</code><span class="sig-paren">(</span><em class="sig-param">jc</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column" title="Permalink to this definition">¶</a></dt>
<dd><p>A column in a DataFrame.</p>
<p><a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> instances can be created by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Select a column out of a DataFrame</span>

<span class="n">df</span><span class="o">.</span><span class="n">colName</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;colName&quot;</span><span class="p">]</span>

<span class="c1"># 2. Create from an expression</span>
<span class="n">df</span><span class="o">.</span><span class="n">colName</span> <span class="o">+</span> <span class="mi">1</span>
<span class="mi">1</span> <span class="o">/</span> <span class="n">df</span><span class="o">.</span><span class="n">colName</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.Column.alias">
<code class="sig-name descname">alias</code><span class="sig-paren">(</span><em class="sig-param">*alias</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column.alias"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column.alias" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this column aliased with a new name or names (in the case of expressions that
return more than one column, such as explode).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alias</strong> – strings of desired column names (collects all positional arguments passed)</p></li>
<li><p><strong>metadata</strong> – a dict of information to be stored in <code class="docutils literal notranslate"><span class="pre">metadata</span></code> attribute of the
corresponding :class: <cite>StructField</cite> (optional, keyword only argument)</p></li>
</ul>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.2: </span>Added optional <code class="docutils literal notranslate"><span class="pre">metadata</span></code> argument.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;age2&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age2=2), Row(age2=5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;age3&quot;</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;max&#39;</span><span class="p">:</span> <span class="mi">99</span><span class="p">}))</span><span class="o">.</span><span class="n">schema</span><span class="p">[</span><span class="s1">&#39;age3&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s1">&#39;max&#39;</span><span class="p">]</span>
<span class="go">99</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.asc">
<code class="sig-name descname">asc</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.asc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on ascending order of the column.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;Tom&#39;</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">asc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;), Row(name=&#39;Tom&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.asc_nulls_first">
<code class="sig-name descname">asc_nulls_first</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.asc_nulls_first" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on ascending order of the column, and null values
return before non-null values.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;Tom&#39;</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">asc_nulls_first</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=None), Row(name=&#39;Alice&#39;), Row(name=&#39;Tom&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.asc_nulls_last">
<code class="sig-name descname">asc_nulls_last</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.asc_nulls_last" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on ascending order of the column, and null values
appear after non-null values.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;Tom&#39;</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">asc_nulls_last</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Alice&#39;), Row(name=&#39;Tom&#39;), Row(name=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.astype">
<code class="sig-name descname">astype</code><span class="sig-paren">(</span><em class="sig-param">dataType</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.astype" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#pyspark.sql.Column.astype" title="pyspark.sql.Column.astype"><code class="xref py py-func docutils literal notranslate"><span class="pre">astype()</span></code></a> is an alias for <a class="reference internal" href="#pyspark.sql.Column.cast" title="pyspark.sql.Column.cast"><code class="xref py py-func docutils literal notranslate"><span class="pre">cast()</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.between">
<code class="sig-name descname">between</code><span class="sig-paren">(</span><em class="sig-param">lowerBound</em>, <em class="sig-param">upperBound</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column.between"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column.between" title="Permalink to this definition">¶</a></dt>
<dd><p>A boolean expression that is evaluated to true if the value of this
expression is between the given columns.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+---------------------------+</span>
<span class="go">| name|((age &gt;= 2) AND (age &lt;= 4))|</span>
<span class="go">+-----+---------------------------+</span>
<span class="go">|Alice|                       true|</span>
<span class="go">|  Bob|                      false|</span>
<span class="go">+-----+---------------------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.bitwiseAND">
<code class="sig-name descname">bitwiseAND</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.bitwiseAND" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute bitwise AND of this expression with another expression.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – a value or <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> to calculate bitwise and(&amp;) against
this <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a>.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">170</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">75</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">bitwiseAND</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row((a &amp; b)=10)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.bitwiseOR">
<code class="sig-name descname">bitwiseOR</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.bitwiseOR" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute bitwise OR of this expression with another expression.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – a value or <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> to calculate bitwise or(|) against
this <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a>.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">170</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">75</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">bitwiseOR</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row((a | b)=235)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.bitwiseXOR">
<code class="sig-name descname">bitwiseXOR</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.bitwiseXOR" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute bitwise XOR of this expression with another expression.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – a value or <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> to calculate bitwise xor(^) against
this <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a>.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">170</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">75</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">bitwiseXOR</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row((a ^ b)=225)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.cast">
<code class="sig-name descname">cast</code><span class="sig-paren">(</span><em class="sig-param">dataType</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column.cast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column.cast" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the column into type <code class="docutils literal notranslate"><span class="pre">dataType</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&quot;string&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;ages&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(ages=&#39;2&#39;), Row(ages=&#39;5&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">StringType</span><span class="p">())</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;ages&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(ages=&#39;2&#39;), Row(ages=&#39;5&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.contains">
<code class="sig-name descname">contains</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.contains" title="Permalink to this definition">¶</a></dt>
<dd><p>Contains the other element. Returns a boolean <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> based on a string match.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – string in line</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;o&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.desc">
<code class="sig-name descname">desc</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.desc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the descending order of the column.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;Tom&#39;</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Tom&#39;), Row(name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.desc_nulls_first">
<code class="sig-name descname">desc_nulls_first</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.desc_nulls_first" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the descending order of the column, and null values
appear before non-null values.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;Tom&#39;</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">desc_nulls_first</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=None), Row(name=&#39;Tom&#39;), Row(name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.desc_nulls_last">
<code class="sig-name descname">desc_nulls_last</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.desc_nulls_last" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the descending order of the column, and null values
appear after non-null values.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;Tom&#39;</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">desc_nulls_last</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=&#39;Tom&#39;), Row(name=&#39;Alice&#39;), Row(name=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.endswith">
<code class="sig-name descname">endswith</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.endswith" title="Permalink to this definition">¶</a></dt>
<dd><p>String ends with. Returns a boolean <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> based on a string match.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – string at end of line (do not use a regex <cite>$</cite>)</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;ice&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;ice$&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.eqNullSafe">
<code class="sig-name descname">eqNullSafe</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.eqNullSafe" title="Permalink to this definition">¶</a></dt>
<dd><p>Equality test that is safe for null values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – a value or <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="s1">&#39;foo&#39;</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">df1</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;foo&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">df1</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">eqNullSafe</span><span class="p">(</span><span class="s1">&#39;foo&#39;</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">df1</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">eqNullSafe</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------------+---------------+----------------+</span>
<span class="go">|(value = foo)|(value &lt;=&gt; foo)|(value &lt;=&gt; NULL)|</span>
<span class="go">+-------------+---------------+----------------+</span>
<span class="go">|         true|           true|           false|</span>
<span class="go">|         null|          false|            true|</span>
<span class="go">+-------------+---------------+----------------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="n">value</span> <span class="o">=</span> <span class="s1">&#39;bar&#39;</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="n">value</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">df2</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">eqNullSafe</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]))</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;NaN&#39;</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">42.0</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">eqNullSafe</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">eqNullSafe</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;NaN&#39;</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">eqNullSafe</span><span class="p">(</span><span class="mf">42.0</span><span class="p">)</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----------------+---------------+----------------+</span>
<span class="go">|(value &lt;=&gt; NULL)|(value &lt;=&gt; NaN)|(value &lt;=&gt; 42.0)|</span>
<span class="go">+----------------+---------------+----------------+</span>
<span class="go">|           false|           true|           false|</span>
<span class="go">|           false|          false|            true|</span>
<span class="go">|            true|          false|           false|</span>
<span class="go">+----------------+---------------+----------------+</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike Pandas, PySpark doesn’t consider NaN values to be NULL.
See the <a class="reference external" href="https://spark.apache.org/docs/latest/sql-programming-guide.html#nan-semantics">NaN Semantics</a> for details.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.getField">
<code class="sig-name descname">getField</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column.getField"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column.getField" title="Permalink to this definition">¶</a></dt>
<dd><p>An expression that gets a field by name in a StructField.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">))])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">getField</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+</span>
<span class="go">|r.b|</span>
<span class="go">+---+</span>
<span class="go">|  b|</span>
<span class="go">+---+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+</span>
<span class="go">|r.a|</span>
<span class="go">+---+</span>
<span class="go">|  1|</span>
<span class="go">+---+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.getItem">
<code class="sig-name descname">getItem</code><span class="sig-paren">(</span><em class="sig-param">key</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column.getItem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column.getItem" title="Permalink to this definition">¶</a></dt>
<dd><p>An expression that gets an item at position <code class="docutils literal notranslate"><span class="pre">ordinal</span></code> out of a list,
or gets an item by key out of a dict.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;value&quot;</span><span class="p">})],</span> <span class="p">[</span><span class="s2">&quot;l&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">l</span><span class="o">.</span><span class="n">getItem</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">getItem</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+</span>
<span class="go">|l[0]|d[key]|</span>
<span class="go">+----+------+</span>
<span class="go">|   1| value|</span>
<span class="go">+----+------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="s2">&quot;key&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+</span>
<span class="go">|l[0]|d[key]|</span>
<span class="go">+----+------+</span>
<span class="go">|   1| value|</span>
<span class="go">+----+------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.isNotNull">
<code class="sig-name descname">isNotNull</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.isNotNull" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the current expression is NOT null.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Tom&#39;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="kc">None</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">height</span><span class="o">.</span><span class="n">isNotNull</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(height=80, name=&#39;Tom&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.isNull">
<code class="sig-name descname">isNull</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.isNull" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the current expression is null.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Tom&#39;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="kc">None</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">height</span><span class="o">.</span><span class="n">isNull</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(height=None, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.isin">
<code class="sig-name descname">isin</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column.isin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column.isin" title="Permalink to this definition">¶</a></dt>
<dd><p>A boolean expression that is evaluated to true if the value of this
expression is contained by the evaluated values of the arguments.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="s2">&quot;Bob&quot;</span><span class="p">,</span> <span class="s2">&quot;Mike&quot;</span><span class="p">)]</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])]</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.like">
<code class="sig-name descname">like</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.like" title="Permalink to this definition">¶</a></dt>
<dd><p>SQL like expression. Returns a boolean <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> based on a SQL LIKE match.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – a SQL LIKE pattern</p>
</dd>
</dl>
<p>See <a class="reference internal" href="#pyspark.sql.Column.rlike" title="pyspark.sql.Column.rlike"><code class="xref py py-func docutils literal notranslate"><span class="pre">rlike()</span></code></a> for a regex version</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">like</span><span class="p">(</span><span class="s1">&#39;Al%&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.name">
<code class="sig-name descname">name</code><span class="sig-paren">(</span><em class="sig-param">*alias</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.name" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#pyspark.sql.Column.name" title="pyspark.sql.Column.name"><code class="xref py py-func docutils literal notranslate"><span class="pre">name()</span></code></a> is an alias for <a class="reference internal" href="#pyspark.sql.Column.alias" title="pyspark.sql.Column.alias"><code class="xref py py-func docutils literal notranslate"><span class="pre">alias()</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.otherwise">
<code class="sig-name descname">otherwise</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column.otherwise"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column.otherwise" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions.
If <a class="reference internal" href="#pyspark.sql.Column.otherwise" title="pyspark.sql.Column.otherwise"><code class="xref py py-func docutils literal notranslate"><span class="pre">Column.otherwise()</span></code></a> is not invoked, None is returned for unmatched conditions.</p>
<p>See <a class="reference internal" href="#pyspark.sql.functions.when" title="pyspark.sql.functions.when"><code class="xref py py-func docutils literal notranslate"><span class="pre">pyspark.sql.functions.when()</span></code></a> for example usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> – a literal value, or a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> expression.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+-------------------------------------+</span>
<span class="go">| name|CASE WHEN (age &gt; 3) THEN 1 ELSE 0 END|</span>
<span class="go">+-----+-------------------------------------+</span>
<span class="go">|Alice|                                    0|</span>
<span class="go">|  Bob|                                    1|</span>
<span class="go">+-----+-------------------------------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.over">
<code class="sig-name descname">over</code><span class="sig-paren">(</span><em class="sig-param">window</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column.over"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column.over" title="Permalink to this definition">¶</a></dt>
<dd><p>Define a windowing column.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>window</strong> – a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><code class="xref py py-class docutils literal notranslate"><span class="pre">WindowSpec</span></code></a></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Column</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">)</span>                 <span class="o">.</span><span class="n">rowsBetween</span><span class="p">(</span><span class="n">Window</span><span class="o">.</span><span class="n">unboundedPreceding</span><span class="p">,</span> <span class="n">Window</span><span class="o">.</span><span class="n">currentRow</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">rank</span><span class="p">,</span> <span class="nb">min</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window</span><span class="p">))</span>                 <span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+----+---+</span>
<span class="go">|age| name|rank|min|</span>
<span class="go">+---+-----+----+---+</span>
<span class="go">|  5|  Bob|   1|  5|</span>
<span class="go">|  2|Alice|   1|  2|</span>
<span class="go">+---+-----+----+---+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.rlike">
<code class="sig-name descname">rlike</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.rlike" title="Permalink to this definition">¶</a></dt>
<dd><p>SQL RLIKE expression (LIKE with Regex). Returns a boolean <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> based on a regex
match.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – an extended regex expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">rlike</span><span class="p">(</span><span class="s1">&#39;ice$&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.startswith">
<code class="sig-name descname">startswith</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.Column.startswith" title="Permalink to this definition">¶</a></dt>
<dd><p>String starts with. Returns a boolean <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> based on a string match.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – string at start of line (do not use a regex <cite>^</cite>)</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;Al&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;^Al&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.substr">
<code class="sig-name descname">substr</code><span class="sig-paren">(</span><em class="sig-param">startPos</em>, <em class="sig-param">length</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column.substr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column.substr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> which is a substring of the column.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>startPos</strong> – start position (int or Column)</p></li>
<li><p><strong>length</strong> – length of the substring (int or Column)</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">substr</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;col&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(col=&#39;Ali&#39;), Row(col=&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.when">
<code class="sig-name descname">when</code><span class="sig-paren">(</span><em class="sig-param">condition</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/column.html#Column.when"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Column.when" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions.
If <a class="reference internal" href="#pyspark.sql.Column.otherwise" title="pyspark.sql.Column.otherwise"><code class="xref py py-func docutils literal notranslate"><span class="pre">Column.otherwise()</span></code></a> is not invoked, None is returned for unmatched conditions.</p>
<p>See <a class="reference internal" href="#pyspark.sql.functions.when" title="pyspark.sql.functions.when"><code class="xref py py-func docutils literal notranslate"><span class="pre">pyspark.sql.functions.when()</span></code></a> for example usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>condition</strong> – a boolean <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> expression.</p></li>
<li><p><strong>value</strong> – a literal value, or a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code></a> expression.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+------------------------------------------------------------+</span>
<span class="go">| name|CASE WHEN (age &gt; 4) THEN 1 WHEN (age &lt; 3) THEN -1 ELSE 0 END|</span>
<span class="go">+-----+------------------------------------------------------------+</span>
<span class="go">|Alice|                                                          -1|</span>
<span class="go">|  Bob|                                                           1|</span>
<span class="go">+-----+------------------------------------------------------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Catalog">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">Catalog</code><span class="sig-paren">(</span><em class="sig-param">sparkSession</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog" title="Permalink to this definition">¶</a></dt>
<dd><p>User-facing catalog API, accessible through <cite>SparkSession.catalog</cite>.</p>
<p>This is a thin wrapper around its Scala implementation org.apache.spark.sql.catalog.Catalog.</p>
<dl class="method">
<dt id="pyspark.sql.Catalog.cacheTable">
<code class="sig-name descname">cacheTable</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.cacheTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.cacheTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Caches the specified table in-memory.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.clearCache">
<code class="sig-name descname">clearCache</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.clearCache"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.clearCache" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes all cached tables from the in-memory cache.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.createExternalTable">
<code class="sig-name descname">createExternalTable</code><span class="sig-paren">(</span><em class="sig-param">tableName</em>, <em class="sig-param">path=None</em>, <em class="sig-param">source=None</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.createExternalTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.createExternalTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a table based on the dataset in a data source.</p>
<p>It returns the DataFrame associated with the external table.</p>
<p>The data source is specified by the <code class="docutils literal notranslate"><span class="pre">source</span></code> and a set of <code class="docutils literal notranslate"><span class="pre">options</span></code>.
If <code class="docutils literal notranslate"><span class="pre">source</span></code> is not specified, the default data source configured by
<code class="docutils literal notranslate"><span class="pre">spark.sql.sources.default</span></code> will be used.</p>
<p>Optionally, a schema can be provided as the schema of the returned <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> and
created external table.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.createTable">
<code class="sig-name descname">createTable</code><span class="sig-paren">(</span><em class="sig-param">tableName</em>, <em class="sig-param">path=None</em>, <em class="sig-param">source=None</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.createTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.createTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a table based on the dataset in a data source.</p>
<p>It returns the DataFrame associated with the table.</p>
<p>The data source is specified by the <code class="docutils literal notranslate"><span class="pre">source</span></code> and a set of <code class="docutils literal notranslate"><span class="pre">options</span></code>.
If <code class="docutils literal notranslate"><span class="pre">source</span></code> is not specified, the default data source configured by
<code class="docutils literal notranslate"><span class="pre">spark.sql.sources.default</span></code> will be used. When <code class="docutils literal notranslate"><span class="pre">path</span></code> is specified, an external table is
created from the data at the given path. Otherwise a managed table is created.</p>
<p>Optionally, a schema can be provided as the schema of the returned <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> and
created table.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.currentDatabase">
<code class="sig-name descname">currentDatabase</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.currentDatabase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.currentDatabase" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current default database in this session.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.dropGlobalTempView">
<code class="sig-name descname">dropGlobalTempView</code><span class="sig-paren">(</span><em class="sig-param">viewName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.dropGlobalTempView"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.dropGlobalTempView" title="Permalink to this definition">¶</a></dt>
<dd><p>Drops the global temporary view with the given view name in the catalog.
If the view has been cached before, then it will also be uncached.
Returns true if this view is dropped successfully, false otherwise.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span><span class="o">.</span><span class="n">createGlobalTempView</span><span class="p">(</span><span class="s2">&quot;my_table&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;global_temp.my_table&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=1, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">dropGlobalTempView</span><span class="p">(</span><span class="s2">&quot;my_table&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;global_temp.my_table&quot;</span><span class="p">)</span> 
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">AnalysisException</span>: <span class="n">...</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.dropTempView">
<code class="sig-name descname">dropTempView</code><span class="sig-paren">(</span><em class="sig-param">viewName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.dropTempView"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.dropTempView" title="Permalink to this definition">¶</a></dt>
<dd><p>Drops the local temporary view with the given view name in the catalog.
If the view has been cached before, then it will also be uncached.
Returns true if this view is dropped successfully, false otherwise.</p>
<p>Note that, the return type of this method was None in Spark 2.0, but changed to Boolean
in Spark 2.1.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span><span class="o">.</span><span class="n">createTempView</span><span class="p">(</span><span class="s2">&quot;my_table&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;my_table&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=1, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">dropTempView</span><span class="p">(</span><span class="s2">&quot;my_table&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;my_table&quot;</span><span class="p">)</span> 
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">AnalysisException</span>: <span class="n">...</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.isCached">
<code class="sig-name descname">isCached</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.isCached"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.isCached" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns true if the table is currently cached in-memory.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.listColumns">
<code class="sig-name descname">listColumns</code><span class="sig-paren">(</span><em class="sig-param">tableName</em>, <em class="sig-param">dbName=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.listColumns"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.listColumns" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of columns for the given table/view in the specified database.</p>
<p>If no database is specified, the current database is used.</p>
<p>Note: the order of arguments here is different from that of its JVM counterpart
because Python does not support method overloading.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.listDatabases">
<code class="sig-name descname">listDatabases</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.listDatabases"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.listDatabases" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of databases available across all sessions.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.listFunctions">
<code class="sig-name descname">listFunctions</code><span class="sig-paren">(</span><em class="sig-param">dbName=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.listFunctions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.listFunctions" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of functions registered in the specified database.</p>
<p>If no database is specified, the current database is used.
This includes all temporary functions.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.listTables">
<code class="sig-name descname">listTables</code><span class="sig-paren">(</span><em class="sig-param">dbName=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.listTables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.listTables" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of tables/views in the specified database.</p>
<p>If no database is specified, the current database is used.
This includes all temporary views.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.recoverPartitions">
<code class="sig-name descname">recoverPartitions</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.recoverPartitions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.recoverPartitions" title="Permalink to this definition">¶</a></dt>
<dd><p>Recovers all the partitions of the given table and update the catalog.</p>
<p>Only works with a partitioned table, and not a view.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.refreshByPath">
<code class="sig-name descname">refreshByPath</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.refreshByPath"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.refreshByPath" title="Permalink to this definition">¶</a></dt>
<dd><p>Invalidates and refreshes all the cached data (and the associated metadata) for any
DataFrame that contains the given data source path.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.refreshTable">
<code class="sig-name descname">refreshTable</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.refreshTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.refreshTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Invalidates and refreshes all the cached data and metadata of the given table.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.registerFunction">
<code class="sig-name descname">registerFunction</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">f</em>, <em class="sig-param">returnType=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.registerFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.registerFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <code class="xref py py-func docutils literal notranslate"><span class="pre">spark.udf.register()</span></code>.
See <a class="reference internal" href="#pyspark.sql.UDFRegistration.register" title="pyspark.sql.UDFRegistration.register"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.UDFRegistration.register()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deprecated in 2.3.0. Use <code class="xref py py-func docutils literal notranslate"><span class="pre">spark.udf.register()</span></code> instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.setCurrentDatabase">
<code class="sig-name descname">setCurrentDatabase</code><span class="sig-paren">(</span><em class="sig-param">dbName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.setCurrentDatabase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.setCurrentDatabase" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the current default database in this session.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Catalog.uncacheTable">
<code class="sig-name descname">uncacheTable</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/catalog.html#Catalog.uncacheTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Catalog.uncacheTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the specified table from the in-memory cache.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Row">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">Row</code><a class="reference internal" href="_modules/pyspark/sql/types.html#Row"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Row" title="Permalink to this definition">¶</a></dt>
<dd><p>A row in <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.
The fields in it can be accessed:</p>
<ul class="simple">
<li><p>like attributes (<code class="docutils literal notranslate"><span class="pre">row.key</span></code>)</p></li>
<li><p>like dictionary values (<code class="docutils literal notranslate"><span class="pre">row[key]</span></code>)</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">key</span> <span class="pre">in</span> <span class="pre">row</span></code> will search through row keys.</p>
<p>Row can be used to create a row object by using named arguments,
the fields will be sorted by names. It is not allowed to omit
a named argument to represent the value is None or missing. This should be
explicitly set to None in this case.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">row</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;Alice&quot;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span>
<span class="go">Row(age=11, name=&#39;Alice&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span>
<span class="go">(&#39;Alice&#39;, 11)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">row</span><span class="o">.</span><span class="n">age</span>
<span class="go">(&#39;Alice&#39;, 11)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;name&#39;</span> <span class="ow">in</span> <span class="n">row</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;wrong_key&#39;</span> <span class="ow">in</span> <span class="n">row</span>
<span class="go">False</span>
</pre></div>
</div>
<p>Row also can be used to create another Row like class, then it
could be used to create Row objects, such as</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span>
<span class="go">&lt;Row(name, age)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;name&#39;</span> <span class="ow">in</span> <span class="n">Person</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;wrong_key&#39;</span> <span class="ow">in</span> <span class="n">Person</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span><span class="p">(</span><span class="s2">&quot;Alice&quot;</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="go">Row(name=&#39;Alice&#39;, age=11)</span>
</pre></div>
</div>
<dl class="method">
<dt id="pyspark.sql.Row.asDict">
<code class="sig-name descname">asDict</code><span class="sig-paren">(</span><em class="sig-param">recursive=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#Row.asDict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Row.asDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return as an dict</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recursive</strong> – turns the nested Row as dict (default: False).</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;Alice&quot;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span><span class="o">.</span><span class="n">asDict</span><span class="p">()</span> <span class="o">==</span> <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="mi">11</span><span class="p">}</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span><span class="o">.</span><span class="n">asDict</span><span class="p">()</span> <span class="o">==</span> <span class="p">{</span><span class="s1">&#39;key&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">age</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">)}</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span><span class="o">.</span><span class="n">asDict</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span> <span class="o">==</span> <span class="p">{</span><span class="s1">&#39;key&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}}</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameNaFunctions">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">DataFrameNaFunctions</code><span class="sig-paren">(</span><em class="sig-param">df</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameNaFunctions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions" title="Permalink to this definition">¶</a></dt>
<dd><p>Functionality for working with missing data in <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameNaFunctions.drop">
<code class="sig-name descname">drop</code><span class="sig-paren">(</span><em class="sig-param">how='any'</em>, <em class="sig-param">thresh=None</em>, <em class="sig-param">subset=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameNaFunctions.drop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions.drop" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> omitting rows with null values.
<a class="reference internal" href="#pyspark.sql.DataFrame.dropna" title="pyspark.sql.DataFrame.dropna"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.dropna()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.drop" title="pyspark.sql.DataFrameNaFunctions.drop"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameNaFunctions.drop()</span></code></a> are aliases of each other.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>how</strong> – ‘any’ or ‘all’.
If ‘any’, drop a row if it contains any nulls.
If ‘all’, drop a row only if all its values are null.</p></li>
<li><p><strong>thresh</strong> – int, default None
If specified, drop rows that have less than <cite>thresh</cite> non-null values.
This overwrites the <cite>how</cite> parameter.</p></li>
<li><p><strong>subset</strong> – optional list of column names to consider.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">drop</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameNaFunctions.fill">
<code class="sig-name descname">fill</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">subset=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameNaFunctions.fill"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions.fill" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace null values, alias for <code class="docutils literal notranslate"><span class="pre">na.fill()</span></code>.
<a class="reference internal" href="#pyspark.sql.DataFrame.fillna" title="pyspark.sql.DataFrame.fillna"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.fillna()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.fill" title="pyspark.sql.DataFrameNaFunctions.fill"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameNaFunctions.fill()</span></code></a> are aliases of each other.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> – int, long, float, string, bool or dict.
Value to replace null values with.
If the value is a dict, then <cite>subset</cite> is ignored and <cite>value</cite> must be a mapping
from column name (string) to replacement value. The replacement value must be
an int, long, float, boolean, or string.</p></li>
<li><p><strong>subset</strong> – optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">|  5|    50|  Bob|</span>
<span class="go">| 50|    50|  Tom|</span>
<span class="go">| 50|    50| null|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df5</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+-------+-----+</span>
<span class="go">| age|   name|  spy|</span>
<span class="go">+----+-------+-----+</span>
<span class="go">|  10|  Alice|false|</span>
<span class="go">|   5|    Bob|false|</span>
<span class="go">|null|Mallory| true|</span>
<span class="go">+----+-------+-----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">({</span><span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;unknown&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-------+</span>
<span class="go">|age|height|   name|</span>
<span class="go">+---+------+-------+</span>
<span class="go">| 10|    80|  Alice|</span>
<span class="go">|  5|  null|    Bob|</span>
<span class="go">| 50|  null|    Tom|</span>
<span class="go">| 50|  null|unknown|</span>
<span class="go">+---+------+-------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameNaFunctions.replace">
<code class="sig-name descname">replace</code><span class="sig-paren">(</span><em class="sig-param">to_replace</em>, <em class="sig-param">value=&lt;no value&gt;</em>, <em class="sig-param">subset=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameNaFunctions.replace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> replacing a value with another value.
<a class="reference internal" href="#pyspark.sql.DataFrame.replace" title="pyspark.sql.DataFrame.replace"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.replace()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.replace" title="pyspark.sql.DataFrameNaFunctions.replace"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameNaFunctions.replace()</span></code></a> are
aliases of each other.
Values to_replace and value must have the same type and can only be numerics, booleans,
or strings. Value can have None. When replacing, the new value will be cast
to the type of the existing column.
For numeric replacements all values to be replaced should have unique
floating point representation. In case of conflicts (for example with <cite>{42: -1, 42.0: 1}</cite>)
and arbitrary replacement will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>to_replace</strong> – bool, int, long, float, string, list or dict.
Value to be replaced.
If the value is a dict, then <cite>value</cite> is ignored or can be omitted, and <cite>to_replace</cite>
must be a mapping between a value and a replacement.</p></li>
<li><p><strong>value</strong> – bool, int, long, float, string, list or None.
The replacement value must be a bool, int, long, float, string or None. If <cite>value</cite> is a
list, <cite>value</cite> should be of the same length and type as <cite>to_replace</cite>.
If <cite>value</cite> is a scalar and <cite>to_replace</cite> is a sequence, then <cite>value</cite> is
used as a replacement for each item in <cite>to_replace</cite>.</p></li>
<li><p><strong>subset</strong> – optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+-----+</span>
<span class="go">| age|height| name|</span>
<span class="go">+----+------+-----+</span>
<span class="go">|  20|    80|Alice|</span>
<span class="go">|   5|  null|  Bob|</span>
<span class="go">|null|  null|  Tom|</span>
<span class="go">|null|  null| null|</span>
<span class="go">+----+------+-----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+----+</span>
<span class="go">| age|height|name|</span>
<span class="go">+----+------+----+</span>
<span class="go">|  10|    80|null|</span>
<span class="go">|   5|  null| Bob|</span>
<span class="go">|null|  null| Tom|</span>
<span class="go">|null|  null|null|</span>
<span class="go">+----+------+----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="s1">&#39;Alice&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">})</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+----+</span>
<span class="go">| age|height|name|</span>
<span class="go">+----+------+----+</span>
<span class="go">|  10|    80|null|</span>
<span class="go">|   5|  null| Bob|</span>
<span class="go">|null|  null| Tom|</span>
<span class="go">|null|  null|null|</span>
<span class="go">+----+------+----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="s1">&#39;Bob&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">],</span> <span class="s1">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+----+</span>
<span class="go">| age|height|name|</span>
<span class="go">+----+------+----+</span>
<span class="go">|  10|    80|   A|</span>
<span class="go">|   5|  null|   B|</span>
<span class="go">|null|  null| Tom|</span>
<span class="go">|null|  null|null|</span>
<span class="go">+----+------+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameStatFunctions">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">DataFrameStatFunctions</code><span class="sig-paren">(</span><em class="sig-param">df</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameStatFunctions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions" title="Permalink to this definition">¶</a></dt>
<dd><p>Functionality for statistic functions with <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.approxQuantile">
<code class="sig-name descname">approxQuantile</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">probabilities</em>, <em class="sig-param">relativeError</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameStatFunctions.approxQuantile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.approxQuantile" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the approximate quantiles of numerical columns of a
DataFrame.</p>
<p>The result of this algorithm has the following deterministic bound:
If the DataFrame has N elements and if we request the quantile at
probability <cite>p</cite> up to error <cite>err</cite>, then the algorithm will return
a sample <cite>x</cite> from the DataFrame so that the <em>exact</em> rank of <cite>x</cite> is
close to (p * N). More precisely,</p>
<blockquote>
<div><p>floor((p - err) * N) &lt;= rank(x) &lt;= ceil((p + err) * N).</p>
</div></blockquote>
<p>This method implements a variation of the Greenwald-Khanna
algorithm (with some speed optimizations). The algorithm was first
present in [[<a class="reference external" href="http://dx.doi.org/10.1145/375663.375670">http://dx.doi.org/10.1145/375663.375670</a>
Space-efficient Online Computation of Quantile Summaries]]
by Greenwald and Khanna.</p>
<p>Note that null values will be ignored in numerical columns before calculation.
For columns only containing null values, an empty list is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – str, list.
Can be a single column name, or a list of names for multiple columns.</p></li>
<li><p><strong>probabilities</strong> – a list of quantile probabilities
Each number must belong to [0, 1].
For example 0 is the minimum, 0.5 is the median, 1 is the maximum.</p></li>
<li><p><strong>relativeError</strong> – The relative target precision to achieve
(&gt;= 0). If set to zero, the exact quantiles are computed, which
could be very expensive. Note that values greater than 1 are
accepted but give the same result as 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the approximate quantiles at the given probabilities. If
the input <cite>col</cite> is a string, the output is a list of floats. If the
input <cite>col</cite> is a list or tuple of strings, the output is also a
list, but each element in it is a list of floats, i.e., the output
is a list of list of floats.</p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.2: </span>Added support for multiple columns.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.corr">
<code class="sig-name descname">corr</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em>, <em class="sig-param">method=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameStatFunctions.corr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.corr" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the correlation of two columns of a DataFrame as a double value.
Currently only supports the Pearson Correlation Coefficient.
<a class="reference internal" href="#pyspark.sql.DataFrame.corr" title="pyspark.sql.DataFrame.corr"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.corr()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.corr" title="pyspark.sql.DataFrameStatFunctions.corr"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameStatFunctions.corr()</span></code></a> are aliases of each other.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – The name of the first column</p></li>
<li><p><strong>col2</strong> – The name of the second column</p></li>
<li><p><strong>method</strong> – The correlation method. Currently only supports “pearson”</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.cov">
<code class="sig-name descname">cov</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameStatFunctions.cov"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.cov" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the sample covariance for the given columns, specified by their names, as a
double value. <a class="reference internal" href="#pyspark.sql.DataFrame.cov" title="pyspark.sql.DataFrame.cov"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.cov()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.cov" title="pyspark.sql.DataFrameStatFunctions.cov"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameStatFunctions.cov()</span></code></a> are aliases.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – The name of the first column</p></li>
<li><p><strong>col2</strong> – The name of the second column</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.crosstab">
<code class="sig-name descname">crosstab</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameStatFunctions.crosstab"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.crosstab" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a pair-wise frequency table of the given columns. Also known as a contingency
table. The number of distinct values for each column should be less than 1e4. At most 1e6
non-zero pair frequencies will be returned.
The first column of each row will be the distinct values of <cite>col1</cite> and the column names
will be the distinct values of <cite>col2</cite>. The name of the first column will be <cite>$col1_$col2</cite>.
Pairs that have no occurrences will have zero as their counts.
<a class="reference internal" href="#pyspark.sql.DataFrame.crosstab" title="pyspark.sql.DataFrame.crosstab"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.crosstab()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.crosstab" title="pyspark.sql.DataFrameStatFunctions.crosstab"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameStatFunctions.crosstab()</span></code></a> are aliases.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – The name of the first column. Distinct items will make the first item of
each row.</p></li>
<li><p><strong>col2</strong> – The name of the second column. Distinct items will make the column names
of the DataFrame.</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.freqItems">
<code class="sig-name descname">freqItems</code><span class="sig-paren">(</span><em class="sig-param">cols</em>, <em class="sig-param">support=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameStatFunctions.freqItems"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.freqItems" title="Permalink to this definition">¶</a></dt>
<dd><p>Finding frequent items for columns, possibly with false positives. Using the
frequent element count algorithm described in
“<a class="reference external" href="http://dx.doi.org/10.1145/762471.762473">http://dx.doi.org/10.1145/762471.762473</a>, proposed by Karp, Schenker, and Papadimitriou”.
<a class="reference internal" href="#pyspark.sql.DataFrame.freqItems" title="pyspark.sql.DataFrame.freqItems"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.freqItems()</span></code></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.freqItems" title="pyspark.sql.DataFrameStatFunctions.freqItems"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrameStatFunctions.freqItems()</span></code></a> are aliases.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is meant for exploratory data analysis, as we make no
guarantee about the backward compatibility of the schema of the resulting DataFrame.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cols</strong> – Names of the columns to calculate frequent items for as a list or tuple of
strings.</p></li>
<li><p><strong>support</strong> – The frequency with which to consider an item ‘frequent’. Default is 1%.
The support must be greater than 1e-4.</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.sampleBy">
<code class="sig-name descname">sampleBy</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">fractions</em>, <em class="sig-param">seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/dataframe.html#DataFrameStatFunctions.sampleBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.sampleBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a stratified sample without replacement based on the
fraction given on each stratum.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – column that defines strata</p></li>
<li><p><strong>fractions</strong> – sampling fraction for each stratum. If a stratum is not
specified, we treat its fraction as zero.</p></li>
<li><p><strong>seed</strong> – random seed</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a new DataFrame that represents the stratified sample</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">col</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">((</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sampled</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampleBy</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="n">fractions</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">},</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sampled</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|key|count|</span>
<span class="go">+---+-----+</span>
<span class="go">|  0|    5|</span>
<span class="go">|  1|    9|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Window">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">Window</code><a class="reference internal" href="_modules/pyspark/sql/window.html#Window"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Window" title="Permalink to this definition">¶</a></dt>
<dd><p>Utility functions for defining window in DataFrames.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rowsBetween</span><span class="p">(</span><span class="n">Window</span><span class="o">.</span><span class="n">unboundedPreceding</span><span class="p">,</span> <span class="n">Window</span><span class="o">.</span><span class="n">currentRow</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># PARTITION BY country ORDER BY date RANGE BETWEEN 3 PRECEDING AND 3 FOLLOWING</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&quot;country&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rangeBetween</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When ordering is not defined, an unbounded window frame (rowFrame,
unboundedPreceding, unboundedFollowing) is used by default. When ordering is defined,
a growing window frame (rangeFrame, unboundedPreceding, currentRow) is used by default.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
<dl class="attribute">
<dt id="pyspark.sql.Window.currentRow">
<code class="sig-name descname">currentRow</code><em class="property"> = 0</em><a class="headerlink" href="#pyspark.sql.Window.currentRow" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.Window.orderBy">
<em class="property">static </em><code class="sig-name descname">orderBy</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/window.html#Window.orderBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Window.orderBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><code class="xref py py-class docutils literal notranslate"><span class="pre">WindowSpec</span></code></a> with the ordering defined.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Window.partitionBy">
<em class="property">static </em><code class="sig-name descname">partitionBy</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/window.html#Window.partitionBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Window.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><code class="xref py py-class docutils literal notranslate"><span class="pre">WindowSpec</span></code></a> with the partitioning defined.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Window.rangeBetween">
<em class="property">static </em><code class="sig-name descname">rangeBetween</code><span class="sig-paren">(</span><em class="sig-param">start</em>, <em class="sig-param">end</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/window.html#Window.rangeBetween"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Window.rangeBetween" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><code class="xref py py-class docutils literal notranslate"><span class="pre">WindowSpec</span></code></a> with the frame boundaries defined,
from <cite>start</cite> (inclusive) to <cite>end</cite> (inclusive).</p>
<p>Both <cite>start</cite> and <cite>end</cite> are relative from the current row. For example,
“0” means “current row”, while “-1” means one off before the current row,
and “5” means the five off after the current row.</p>
<p>We recommend users use <code class="docutils literal notranslate"><span class="pre">Window.unboundedPreceding</span></code>, <code class="docutils literal notranslate"><span class="pre">Window.unboundedFollowing</span></code>,
and <code class="docutils literal notranslate"><span class="pre">Window.currentRow</span></code> to specify special boundary values, rather than using integral
values directly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> – boundary start, inclusive.
The frame is unbounded if this is <code class="docutils literal notranslate"><span class="pre">Window.unboundedPreceding</span></code>, or
any value less than or equal to max(-sys.maxsize, -9223372036854775808).</p></li>
<li><p><strong>end</strong> – boundary end, inclusive.
The frame is unbounded if this is <code class="docutils literal notranslate"><span class="pre">Window.unboundedFollowing</span></code>, or
any value greater than or equal to min(sys.maxsize, 9223372036854775807).</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Window.rowsBetween">
<em class="property">static </em><code class="sig-name descname">rowsBetween</code><span class="sig-paren">(</span><em class="sig-param">start</em>, <em class="sig-param">end</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/window.html#Window.rowsBetween"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Window.rowsBetween" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><code class="xref py py-class docutils literal notranslate"><span class="pre">WindowSpec</span></code></a> with the frame boundaries defined,
from <cite>start</cite> (inclusive) to <cite>end</cite> (inclusive).</p>
<p>Both <cite>start</cite> and <cite>end</cite> are relative positions from the current row.
For example, “0” means “current row”, while “-1” means the row before
the current row, and “5” means the fifth row after the current row.</p>
<p>We recommend users use <code class="docutils literal notranslate"><span class="pre">Window.unboundedPreceding</span></code>, <code class="docutils literal notranslate"><span class="pre">Window.unboundedFollowing</span></code>,
and <code class="docutils literal notranslate"><span class="pre">Window.currentRow</span></code> to specify special boundary values, rather than using integral
values directly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> – boundary start, inclusive.
The frame is unbounded if this is <code class="docutils literal notranslate"><span class="pre">Window.unboundedPreceding</span></code>, or
any value less than or equal to -9223372036854775808.</p></li>
<li><p><strong>end</strong> – boundary end, inclusive.
The frame is unbounded if this is <code class="docutils literal notranslate"><span class="pre">Window.unboundedFollowing</span></code>, or
any value greater than or equal to 9223372036854775807.</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.Window.unboundedFollowing">
<code class="sig-name descname">unboundedFollowing</code><em class="property"> = 9223372036854775807</em><a class="headerlink" href="#pyspark.sql.Window.unboundedFollowing" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.Window.unboundedPreceding">
<code class="sig-name descname">unboundedPreceding</code><em class="property"> = -9223372036854775808</em><a class="headerlink" href="#pyspark.sql.Window.unboundedPreceding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.WindowSpec">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">WindowSpec</code><span class="sig-paren">(</span><em class="sig-param">jspec</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/window.html#WindowSpec"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.WindowSpec" title="Permalink to this definition">¶</a></dt>
<dd><p>A window specification that defines the partitioning, ordering,
and frame boundaries.</p>
<p>Use the static methods in <a class="reference internal" href="#pyspark.sql.Window" title="pyspark.sql.Window"><code class="xref py py-class docutils literal notranslate"><span class="pre">Window</span></code></a> to create a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><code class="xref py py-class docutils literal notranslate"><span class="pre">WindowSpec</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.WindowSpec.orderBy">
<code class="sig-name descname">orderBy</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/window.html#WindowSpec.orderBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.WindowSpec.orderBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the ordering columns in a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><code class="xref py py-class docutils literal notranslate"><span class="pre">WindowSpec</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – names of columns or expressions</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.WindowSpec.partitionBy">
<code class="sig-name descname">partitionBy</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/window.html#WindowSpec.partitionBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.WindowSpec.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the partitioning columns in a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><code class="xref py py-class docutils literal notranslate"><span class="pre">WindowSpec</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – names of columns or expressions</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.WindowSpec.rangeBetween">
<code class="sig-name descname">rangeBetween</code><span class="sig-paren">(</span><em class="sig-param">start</em>, <em class="sig-param">end</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/window.html#WindowSpec.rangeBetween"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.WindowSpec.rangeBetween" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the frame boundaries, from <cite>start</cite> (inclusive) to <cite>end</cite> (inclusive).</p>
<p>Both <cite>start</cite> and <cite>end</cite> are relative from the current row. For example,
“0” means “current row”, while “-1” means one off before the current row,
and “5” means the five off after the current row.</p>
<p>We recommend users use <code class="docutils literal notranslate"><span class="pre">Window.unboundedPreceding</span></code>, <code class="docutils literal notranslate"><span class="pre">Window.unboundedFollowing</span></code>,
and <code class="docutils literal notranslate"><span class="pre">Window.currentRow</span></code> to specify special boundary values, rather than using integral
values directly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> – boundary start, inclusive.
The frame is unbounded if this is <code class="docutils literal notranslate"><span class="pre">Window.unboundedPreceding</span></code>, or
any value less than or equal to max(-sys.maxsize, -9223372036854775808).</p></li>
<li><p><strong>end</strong> – boundary end, inclusive.
The frame is unbounded if this is <code class="docutils literal notranslate"><span class="pre">Window.unboundedFollowing</span></code>, or
any value greater than or equal to min(sys.maxsize, 9223372036854775807).</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.WindowSpec.rowsBetween">
<code class="sig-name descname">rowsBetween</code><span class="sig-paren">(</span><em class="sig-param">start</em>, <em class="sig-param">end</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/window.html#WindowSpec.rowsBetween"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.WindowSpec.rowsBetween" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the frame boundaries, from <cite>start</cite> (inclusive) to <cite>end</cite> (inclusive).</p>
<p>Both <cite>start</cite> and <cite>end</cite> are relative positions from the current row.
For example, “0” means “current row”, while “-1” means the row before
the current row, and “5” means the fifth row after the current row.</p>
<p>We recommend users use <code class="docutils literal notranslate"><span class="pre">Window.unboundedPreceding</span></code>, <code class="docutils literal notranslate"><span class="pre">Window.unboundedFollowing</span></code>,
and <code class="docutils literal notranslate"><span class="pre">Window.currentRow</span></code> to specify special boundary values, rather than using integral
values directly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> – boundary start, inclusive.
The frame is unbounded if this is <code class="docutils literal notranslate"><span class="pre">Window.unboundedPreceding</span></code>, or
any value less than or equal to max(-sys.maxsize, -9223372036854775808).</p></li>
<li><p><strong>end</strong> – boundary end, inclusive.
The frame is unbounded if this is <code class="docutils literal notranslate"><span class="pre">Window.unboundedFollowing</span></code>, or
any value greater than or equal to min(sys.maxsize, 9223372036854775807).</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameReader">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">DataFrameReader</code><span class="sig-paren">(</span><em class="sig-param">spark</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface used to load a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> from external storage systems
(e.g. file systems, key-value stores, etc). Use <code class="xref py py-func docutils literal notranslate"><span class="pre">spark.read()</span></code>
to access this.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameReader.csv">
<code class="sig-name descname">csv</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">sep=None</em>, <em class="sig-param">encoding=None</em>, <em class="sig-param">quote=None</em>, <em class="sig-param">escape=None</em>, <em class="sig-param">comment=None</em>, <em class="sig-param">header=None</em>, <em class="sig-param">inferSchema=None</em>, <em class="sig-param">ignoreLeadingWhiteSpace=None</em>, <em class="sig-param">ignoreTrailingWhiteSpace=None</em>, <em class="sig-param">nullValue=None</em>, <em class="sig-param">nanValue=None</em>, <em class="sig-param">positiveInf=None</em>, <em class="sig-param">negativeInf=None</em>, <em class="sig-param">dateFormat=None</em>, <em class="sig-param">timestampFormat=None</em>, <em class="sig-param">maxColumns=None</em>, <em class="sig-param">maxCharsPerColumn=None</em>, <em class="sig-param">maxMalformedLogPerPartition=None</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">columnNameOfCorruptRecord=None</em>, <em class="sig-param">multiLine=None</em>, <em class="sig-param">charToEscapeQuoteEscaping=None</em>, <em class="sig-param">samplingRatio=None</em>, <em class="sig-param">enforceSchema=None</em>, <em class="sig-param">emptyValue=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.csv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.csv" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a CSV file and returns the result as a  <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<p>This function will go through the input once to determine the input schema if
<code class="docutils literal notranslate"><span class="pre">inferSchema</span></code> is enabled. To avoid going through the entire data once, disable
<code class="docutils literal notranslate"><span class="pre">inferSchema</span></code> option or specify the schema explicitly using <code class="docutils literal notranslate"><span class="pre">schema</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – string, or list of strings, for input path(s),
or RDD of Strings storing CSV rows.</p></li>
<li><p><strong>schema</strong> – an optional <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> for the input schema
or a DDL-formatted string (For example <code class="docutils literal notranslate"><span class="pre">col0</span> <span class="pre">INT,</span> <span class="pre">col1</span> <span class="pre">DOUBLE</span></code>).</p></li>
<li><p><strong>sep</strong> – sets a single character as a separator for each field and value.
If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">,</span></code>.</p></li>
<li><p><strong>encoding</strong> – decodes the CSV files by the given encoding type. If None is set,
it uses the default value, <code class="docutils literal notranslate"><span class="pre">UTF-8</span></code>.</p></li>
<li><p><strong>quote</strong> – sets a single character used for escaping quoted values where the
separator can be part of the value. If None is set, it uses the default
value, <code class="docutils literal notranslate"><span class="pre">&quot;</span></code>. If you would like to turn off quotations, you need to set an
empty string.</p></li>
<li><p><strong>escape</strong> – sets a single character used for escaping quotes inside an already
quoted value. If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">\</span></code>.</p></li>
<li><p><strong>comment</strong> – sets a single character used for skipping lines beginning with this
character. By default (None), it is disabled.</p></li>
<li><p><strong>header</strong> – uses the first line as names of columns. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>inferSchema</strong> – infers the input schema automatically from data. It requires one extra
pass over the data. If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>enforceSchema</strong> – If it is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the specified or inferred schema will be
forcibly applied to datasource files, and headers in CSV files will be
ignored. If the option is set to <code class="docutils literal notranslate"><span class="pre">false</span></code>, the schema will be
validated against all headers in CSV files or the first header in RDD
if the <code class="docutils literal notranslate"><span class="pre">header</span></code> option is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>. Field names in the schema
and column names in CSV headers are checked by their positions
taking into account <code class="docutils literal notranslate"><span class="pre">spark.sql.caseSensitive</span></code>. If None is set,
<code class="docutils literal notranslate"><span class="pre">true</span></code> is used by default. Though the default value is <code class="docutils literal notranslate"><span class="pre">true</span></code>,
it is recommended to disable the <code class="docutils literal notranslate"><span class="pre">enforceSchema</span></code> option
to avoid incorrect results.</p></li>
<li><p><strong>ignoreLeadingWhiteSpace</strong> – A flag indicating whether or not leading whitespaces from
values being read should be skipped. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>ignoreTrailingWhiteSpace</strong> – A flag indicating whether or not trailing whitespaces from
values being read should be skipped. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>nullValue</strong> – sets the string representation of a null value. If None is set, it uses
the default value, empty string. Since 2.0.1, this <code class="docutils literal notranslate"><span class="pre">nullValue</span></code> param
applies to all supported types including the string type.</p></li>
<li><p><strong>nanValue</strong> – sets the string representation of a non-number value. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">NaN</span></code>.</p></li>
<li><p><strong>positiveInf</strong> – sets the string representation of a positive infinity value. If None
is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">Inf</span></code>.</p></li>
<li><p><strong>negativeInf</strong> – sets the string representation of a negative infinity value. If None
is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">Inf</span></code>.</p></li>
<li><p><strong>dateFormat</strong> – sets the string that indicates a date format. Custom date formats
follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>. This
applies to date type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd</span></code>.</p></li>
<li><p><strong>timestampFormat</strong> – sets the string that indicates a timestamp format. Custom date
formats follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>.
This applies to timestamp type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd'T'HH:mm:ss.SSSXXX</span></code>.</p></li>
<li><p><strong>maxColumns</strong> – defines a hard limit of how many columns a record can have. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">20480</span></code>.</p></li>
<li><p><strong>maxCharsPerColumn</strong> – defines the maximum number of characters allowed for any given
value being read. If None is set, it uses the default value,
<code class="docutils literal notranslate"><span class="pre">-1</span></code> meaning unlimited length.</p></li>
<li><p><strong>maxMalformedLogPerPartition</strong> – this parameter is no longer used since Spark 2.2.0.
If specified, it is ignored.</p></li>
<li><p><strong>mode</strong> – <dl class="simple">
<dt>allows a mode for dealing with corrupt records during parsing. If None is</dt><dd><p>set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code>. Note that Spark tries to
parse only required columns in CSV under column pruning. Therefore, corrupt
records can be different based on required set of fields. This behavior can
be controlled by <code class="docutils literal notranslate"><span class="pre">spark.sql.csv.parser.columnPruning.enabled</span></code>
(enabled by default).</p>
</dd>
</dl>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> : when it meets a corrupted record, puts the malformed string into a field configured by <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code>, and sets other fields to <code class="docutils literal notranslate"><span class="pre">null</span></code>. To keep corrupt records, an user can set a string type field named <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code> in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. A record with less/more tokens than schema is not a corrupted record to CSV. When it meets a record having fewer tokens than the length of the schema, sets <code class="docutils literal notranslate"><span class="pre">null</span></code> to extra fields. When the record has more tokens than the length of the schema, it drops extra tokens.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DROPMALFORMED</span></code> : ignores the whole corrupted records.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FAILFAST</span></code> : throws an exception when it meets corrupted records.</p></li>
</ul>
</p></li>
<li><p><strong>columnNameOfCorruptRecord</strong> – allows renaming the new field having malformed string
created by <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> mode. This overrides
<code class="docutils literal notranslate"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></code>. If None is set,
it uses the value specified in
<code class="docutils literal notranslate"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></code>.</p></li>
<li><p><strong>multiLine</strong> – parse records, which may span multiple lines. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>charToEscapeQuoteEscaping</strong> – sets a single character used for escaping the escape for
the quote character. If None is set, the default value is
escape character when escape and quote characters are
different, <code class="docutils literal notranslate"><span class="pre">\0</span></code> otherwise.</p></li>
<li><p><strong>samplingRatio</strong> – defines fraction of rows used for schema inferring.
If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><strong>emptyValue</strong> – sets the string representation of an empty value. If None is set, it uses
the default value, empty string.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/ages.csv&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;_c0&#39;, &#39;string&#39;), (&#39;_c1&#39;, &#39;string&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/ages.csv&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;_c0&#39;, &#39;string&#39;), (&#39;_c1&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.format">
<code class="sig-name descname">format</code><span class="sig-paren">(</span><em class="sig-param">source</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.format"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the input data source format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>source</strong> – string, name of the data source, e.g. ‘json’, ‘parquet’.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.jdbc">
<code class="sig-name descname">jdbc</code><span class="sig-paren">(</span><em class="sig-param">url</em>, <em class="sig-param">table</em>, <em class="sig-param">column=None</em>, <em class="sig-param">lowerBound=None</em>, <em class="sig-param">upperBound=None</em>, <em class="sig-param">numPartitions=None</em>, <em class="sig-param">predicates=None</em>, <em class="sig-param">properties=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.jdbc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.jdbc" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> representing the database table named <code class="docutils literal notranslate"><span class="pre">table</span></code>
accessible via JDBC URL <code class="docutils literal notranslate"><span class="pre">url</span></code> and connection <code class="docutils literal notranslate"><span class="pre">properties</span></code>.</p>
<p>Partitions of the table will be retrieved in parallel if either <code class="docutils literal notranslate"><span class="pre">column</span></code> or
<code class="docutils literal notranslate"><span class="pre">predicates</span></code> is specified. <code class="docutils literal notranslate"><span class="pre">lowerBound`,</span> <span class="pre">``upperBound</span></code> and <code class="docutils literal notranslate"><span class="pre">numPartitions</span></code>
is needed when <code class="docutils literal notranslate"><span class="pre">column</span></code> is specified.</p>
<p>If both <code class="docutils literal notranslate"><span class="pre">column</span></code> and <code class="docutils literal notranslate"><span class="pre">predicates</span></code> are specified, <code class="docutils literal notranslate"><span class="pre">column</span></code> will be used.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Don’t create too many partitions in parallel on a large cluster;
otherwise Spark might crash your external database systems.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>url</strong> – a JDBC URL of the form <code class="docutils literal notranslate"><span class="pre">jdbc:subprotocol:subname</span></code></p></li>
<li><p><strong>table</strong> – the name of the table</p></li>
<li><p><strong>column</strong> – the name of an integer column that will be used for partitioning;
if this parameter is specified, then <code class="docutils literal notranslate"><span class="pre">numPartitions</span></code>, <code class="docutils literal notranslate"><span class="pre">lowerBound</span></code>
(inclusive), and <code class="docutils literal notranslate"><span class="pre">upperBound</span></code> (exclusive) will form partition strides
for generated WHERE clause expressions used to split the column
<code class="docutils literal notranslate"><span class="pre">column</span></code> evenly</p></li>
<li><p><strong>lowerBound</strong> – the minimum value of <code class="docutils literal notranslate"><span class="pre">column</span></code> used to decide partition stride</p></li>
<li><p><strong>upperBound</strong> – the maximum value of <code class="docutils literal notranslate"><span class="pre">column</span></code> used to decide partition stride</p></li>
<li><p><strong>numPartitions</strong> – the number of partitions</p></li>
<li><p><strong>predicates</strong> – a list of expressions suitable for inclusion in WHERE clauses;
each one defines one partition of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a></p></li>
<li><p><strong>properties</strong> – a dictionary of JDBC database connection arguments. Normally at
least properties “user” and “password” with their corresponding values.
For example { ‘user’ : ‘SYSTEM’, ‘password’ : ‘mypassword’ }</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a DataFrame</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.json">
<code class="sig-name descname">json</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">primitivesAsString=None</em>, <em class="sig-param">prefersDecimal=None</em>, <em class="sig-param">allowComments=None</em>, <em class="sig-param">allowUnquotedFieldNames=None</em>, <em class="sig-param">allowSingleQuotes=None</em>, <em class="sig-param">allowNumericLeadingZero=None</em>, <em class="sig-param">allowBackslashEscapingAnyCharacter=None</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">columnNameOfCorruptRecord=None</em>, <em class="sig-param">dateFormat=None</em>, <em class="sig-param">timestampFormat=None</em>, <em class="sig-param">multiLine=None</em>, <em class="sig-param">allowUnquotedControlChars=None</em>, <em class="sig-param">lineSep=None</em>, <em class="sig-param">samplingRatio=None</em>, <em class="sig-param">dropFieldIfAllNull=None</em>, <em class="sig-param">encoding=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.json" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads JSON files and returns the results as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<p><a class="reference external" href="http://jsonlines.org/">JSON Lines</a> (newline-delimited JSON) is supported by default.
For JSON (one record per file), set the <code class="docutils literal notranslate"><span class="pre">multiLine</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">schema</span></code> parameter is not specified, this function goes
through the input once to determine the input schema.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – string represents path to the JSON dataset, or a list of paths,
or RDD of Strings storing JSON objects.</p></li>
<li><p><strong>schema</strong> – an optional <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> for the input schema or
a DDL-formatted string (For example <code class="docutils literal notranslate"><span class="pre">col0</span> <span class="pre">INT,</span> <span class="pre">col1</span> <span class="pre">DOUBLE</span></code>).</p></li>
<li><p><strong>primitivesAsString</strong> – infers all primitive values as a string type. If None is set,
it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>prefersDecimal</strong> – infers all floating-point values as a decimal type. If the values
do not fit in decimal, then it infers them as doubles. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>allowComments</strong> – ignores Java/C++ style comment in JSON records. If None is set,
it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>allowUnquotedFieldNames</strong> – allows unquoted JSON field names. If None is set,
it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>allowSingleQuotes</strong> – allows single quotes in addition to double quotes. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
<li><p><strong>allowNumericLeadingZero</strong> – allows leading zeros in numbers (e.g. 00012). If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>allowBackslashEscapingAnyCharacter</strong> – allows accepting quoting of all character
using backslash quoting mechanism. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>mode</strong> – <dl class="simple">
<dt>allows a mode for dealing with corrupt records during parsing. If None is</dt><dd><p>set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code>.</p>
</dd>
</dl>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> : when it meets a corrupted record, puts the malformed string                   into a field configured by <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code>, and sets other                   fields to <code class="docutils literal notranslate"><span class="pre">null</span></code>. To keep corrupt records, an user can set a string type                   field named <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code> in an user-defined schema. If a                   schema does not have the field, it drops corrupt records during parsing.                   When inferring a schema, it implicitly adds a <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code>                   field in an output schema.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DROPMALFORMED</span></code> : ignores the whole corrupted records.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FAILFAST</span></code> : throws an exception when it meets corrupted records.</p></li>
</ul>
</p></li>
<li><p><strong>columnNameOfCorruptRecord</strong> – allows renaming the new field having malformed string
created by <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> mode. This overrides
<code class="docutils literal notranslate"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></code>. If None is set,
it uses the value specified in
<code class="docutils literal notranslate"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></code>.</p></li>
<li><p><strong>dateFormat</strong> – sets the string that indicates a date format. Custom date formats
follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>. This
applies to date type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd</span></code>.</p></li>
<li><p><strong>timestampFormat</strong> – sets the string that indicates a timestamp format. Custom date
formats follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>.
This applies to timestamp type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd'T'HH:mm:ss.SSSXXX</span></code>.</p></li>
<li><p><strong>multiLine</strong> – parse one record, which may span multiple lines, per file. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>allowUnquotedControlChars</strong> – allows JSON Strings to contain unquoted control
characters (ASCII characters with value less than 32,
including tab and line feed characters) or not.</p></li>
<li><p><strong>encoding</strong> – allows to forcibly set one of standard basic or extended encoding for
the JSON files. For example UTF-16BE, UTF-32LE. If None is set,
the encoding of input JSON will be detected automatically
when the multiLine option is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
<li><p><strong>lineSep</strong> – defines the line separator that should be used for parsing. If None is
set, it covers all <code class="docutils literal notranslate"><span class="pre">\r</span></code>, <code class="docutils literal notranslate"><span class="pre">\r\n</span></code> and <code class="docutils literal notranslate"><span class="pre">\n</span></code>.</p></li>
<li><p><strong>samplingRatio</strong> – defines fraction of input JSON objects used for schema inferring.
If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><strong>dropFieldIfAllNull</strong> – whether to ignore column of all null values or empty
array/struct during schema inference. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.load">
<code class="sig-name descname">load</code><span class="sig-paren">(</span><em class="sig-param">path=None</em>, <em class="sig-param">format=None</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.load"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads data from a data source and returns it as a :class`DataFrame`.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – optional string or a list of string for file-system backed data sources.</p></li>
<li><p><strong>format</strong> – optional string for format of the data source. Default to ‘parquet’.</p></li>
<li><p><strong>schema</strong> – optional <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> for the input schema
or a DDL-formatted string (For example <code class="docutils literal notranslate"><span class="pre">col0</span> <span class="pre">INT,</span> <span class="pre">col1</span> <span class="pre">DOUBLE</span></code>).</p></li>
<li><p><strong>options</strong> – all other string options</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;parquet&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">opt1</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">opt2</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">opt3</span><span class="o">=</span><span class="s1">&#39;str&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;name&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;month&#39;, &#39;int&#39;), (&#39;day&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">([</span><span class="s1">&#39;python/test_support/sql/people.json&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;python/test_support/sql/people1.json&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;aka&#39;, &#39;string&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.option">
<code class="sig-name descname">option</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.option"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.option" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds an input option for the underlying data source.</p>
<dl class="simple">
<dt>You can set the following option(s) for reading files:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">timeZone</span></code>: sets the string that indicates a timezone to be used to parse timestamps</dt><dd><p>in the JSON/CSV datasources or partition values.
If it isn’t set, it uses the default value, session local timezone.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.options">
<code class="sig-name descname">options</code><span class="sig-paren">(</span><em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.options"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.options" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds input options for the underlying data source.</p>
<dl class="simple">
<dt>You can set the following option(s) for reading files:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">timeZone</span></code>: sets the string that indicates a timezone to be used to parse timestamps</dt><dd><p>in the JSON/CSV datasources or partition values.
If it isn’t set, it uses the default value, session local timezone.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.orc">
<code class="sig-name descname">orc</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.orc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.orc" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads ORC files, returning the result as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/orc_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;a&#39;, &#39;bigint&#39;), (&#39;b&#39;, &#39;int&#39;), (&#39;c&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.parquet">
<code class="sig-name descname">parquet</code><span class="sig-paren">(</span><em class="sig-param">*paths</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.parquet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.parquet" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads Parquet files, returning the result as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<dl class="simple">
<dt>You can set the following Parquet-specific option(s) for reading Parquet files:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mergeSchema</span></code>: sets whether we should merge schemas collected from all                 Parquet part-files. This will override <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.mergeSchema</span></code>.                 The default value is specified in <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.mergeSchema</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;name&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;month&#39;, &#39;int&#39;), (&#39;day&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.schema">
<code class="sig-name descname">schema</code><span class="sig-paren">(</span><em class="sig-param">schema</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.schema"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.schema" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the input schema.</p>
<p>Some data sources (e.g. JSON) can infer the input schema automatically from data.
By specifying the schema here, the underlying data source can skip the schema
inference step, and thus speed up data loading.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>schema</strong> – a <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> object or a DDL-formatted string
(For example <code class="docutils literal notranslate"><span class="pre">col0</span> <span class="pre">INT,</span> <span class="pre">col1</span> <span class="pre">DOUBLE</span></code>).</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="s2">&quot;col0 INT, col1 DOUBLE&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.table">
<code class="sig-name descname">table</code><span class="sig-paren">(</span><em class="sig-param">tableName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.table"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.table" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the specified table as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tableName</strong> – string, name of the table.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s1">&#39;tmpTable&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s1">&#39;tmpTable&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;name&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;month&#39;, &#39;int&#39;), (&#39;day&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.text">
<code class="sig-name descname">text</code><span class="sig-paren">(</span><em class="sig-param">paths</em>, <em class="sig-param">wholetext=False</em>, <em class="sig-param">lineSep=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameReader.text"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameReader.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads text files and returns a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> whose schema starts with a
string column named “value”, and followed by partitioned columns if there
are any.</p>
<p>By default, each line in the text file is a new row in the resulting DataFrame.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>paths</strong> – string, or list of strings, for input path(s).</p></li>
<li><p><strong>wholetext</strong> – if true, read each file from input path(s) as a single row.</p></li>
<li><p><strong>lineSep</strong> – defines the line separator that should be used for parsing. If None is
set, it covers all <code class="docutils literal notranslate"><span class="pre">\r</span></code>, <code class="docutils literal notranslate"><span class="pre">\r\n</span></code> and <code class="docutils literal notranslate"><span class="pre">\n</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/text-test.txt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(value=&#39;hello&#39;), Row(value=&#39;this&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/text-test.txt&#39;</span><span class="p">,</span> <span class="n">wholetext</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(value=&#39;hello\nthis&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameWriter">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.</code><code class="sig-name descname">DataFrameWriter</code><span class="sig-paren">(</span><em class="sig-param">df</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface used to write a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to external storage systems
(e.g. file systems, key-value stores, etc). Use <a class="reference internal" href="#pyspark.sql.DataFrame.write" title="pyspark.sql.DataFrame.write"><code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.write()</span></code></a>
to access this.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.bucketBy">
<code class="sig-name descname">bucketBy</code><span class="sig-paren">(</span><em class="sig-param">numBuckets</em>, <em class="sig-param">col</em>, <em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.bucketBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.bucketBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Buckets the output by the given columns.If specified,
the output is laid out on the file system similar to Hive’s bucketing scheme.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>numBuckets</strong> – the number of buckets to save</p></li>
<li><p><strong>col</strong> – a name of a column, or a list of names.</p></li>
<li><p><strong>cols</strong> – additional names (optional). If <cite>col</cite> is a list it should be empty.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Applicable for file-based data sources in combination with
<a class="reference internal" href="#pyspark.sql.DataFrameWriter.saveAsTable" title="pyspark.sql.DataFrameWriter.saveAsTable"><code class="xref py py-meth docutils literal notranslate"><span class="pre">DataFrameWriter.saveAsTable()</span></code></a>.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;parquet&#39;</span><span class="p">)</span>  
<span class="gp">... </span>    <span class="o">.</span><span class="n">bucketBy</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;month&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s1">&#39;bucketed_table&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.csv">
<code class="sig-name descname">csv</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">compression=None</em>, <em class="sig-param">sep=None</em>, <em class="sig-param">quote=None</em>, <em class="sig-param">escape=None</em>, <em class="sig-param">header=None</em>, <em class="sig-param">nullValue=None</em>, <em class="sig-param">escapeQuotes=None</em>, <em class="sig-param">quoteAll=None</em>, <em class="sig-param">dateFormat=None</em>, <em class="sig-param">timestampFormat=None</em>, <em class="sig-param">ignoreLeadingWhiteSpace=None</em>, <em class="sig-param">ignoreTrailingWhiteSpace=None</em>, <em class="sig-param">charToEscapeQuoteEscaping=None</em>, <em class="sig-param">encoding=None</em>, <em class="sig-param">emptyValue=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.csv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.csv" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> in CSV format at the specified path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path in any Hadoop supported file system</p></li>
<li><p><strong>mode</strong> – <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">append</span></code>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">overwrite</span></code>: Overwrite existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ignore</span></code>: Silently ignore this operation if data already exists.</p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">error</span></code> or <code class="docutils literal notranslate"><span class="pre">errorifexists</span></code> (default case): Throw an exception if data already </dt><dd><p>exists.</p>
</dd>
</dl>
</li>
</ul>
</p></li>
<li><p><strong>compression</strong> – compression codec to use when saving to file. This can be one of the
known case-insensitive shorten names (none, bzip2, gzip, lz4,
snappy and deflate).</p></li>
<li><p><strong>sep</strong> – sets a single character as a separator for each field and value. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">,</span></code>.</p></li>
<li><p><strong>quote</strong> – sets a single character used for escaping quoted values where the
separator can be part of the value. If None is set, it uses the default
value, <code class="docutils literal notranslate"><span class="pre">&quot;</span></code>. If an empty string is set, it uses <code class="docutils literal notranslate"><span class="pre">u0000</span></code> (null character).</p></li>
<li><p><strong>escape</strong> – sets a single character used for escaping quotes inside an already
quoted value. If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">\</span></code></p></li>
<li><p><strong>escapeQuotes</strong> – a flag indicating whether values containing quotes should always
be enclosed in quotes. If None is set, it uses the default value
<code class="docutils literal notranslate"><span class="pre">true</span></code>, escaping all values containing a quote character.</p></li>
<li><p><strong>quoteAll</strong> – a flag indicating whether all values should always be enclosed in
quotes. If None is set, it uses the default value <code class="docutils literal notranslate"><span class="pre">false</span></code>,
only escaping values containing a quote character.</p></li>
<li><p><strong>header</strong> – writes the names of columns as the first line. If None is set, it uses
the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>nullValue</strong> – sets the string representation of a null value. If None is set, it uses
the default value, empty string.</p></li>
<li><p><strong>dateFormat</strong> – sets the string that indicates a date format. Custom date formats
follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>. This
applies to date type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd</span></code>.</p></li>
<li><p><strong>timestampFormat</strong> – sets the string that indicates a timestamp format. Custom date
formats follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>.
This applies to timestamp type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd'T'HH:mm:ss.SSSXXX</span></code>.</p></li>
<li><p><strong>ignoreLeadingWhiteSpace</strong> – a flag indicating whether or not leading whitespaces from
values being written should be skipped. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
<li><p><strong>ignoreTrailingWhiteSpace</strong> – a flag indicating whether or not trailing whitespaces from
values being written should be skipped. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
<li><p><strong>charToEscapeQuoteEscaping</strong> – sets a single character used for escaping the escape for
the quote character. If None is set, the default value is
escape character when escape and quote characters are
different, <code class="docutils literal notranslate"><span class="pre">\0</span></code> otherwise..</p></li>
<li><p><strong>encoding</strong> – sets the encoding (charset) of saved csv files. If None is set,
the default UTF-8 charset will be used.</p></li>
<li><p><strong>emptyValue</strong> – sets the string representation of an empty value. If None is set, it uses
the default value, <code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.format">
<code class="sig-name descname">format</code><span class="sig-paren">(</span><em class="sig-param">source</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.format"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the underlying output data source.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>source</strong> – string, name of the data source, e.g. ‘json’, ‘parquet’.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.insertInto">
<code class="sig-name descname">insertInto</code><span class="sig-paren">(</span><em class="sig-param">tableName</em>, <em class="sig-param">overwrite=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.insertInto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.insertInto" title="Permalink to this definition">¶</a></dt>
<dd><p>Inserts the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to the specified table.</p>
<p>It requires that the schema of the class:<cite>DataFrame</cite> is the same as the
schema of the table.</p>
<p>Optionally overwriting any existing data.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.jdbc">
<code class="sig-name descname">jdbc</code><span class="sig-paren">(</span><em class="sig-param">url</em>, <em class="sig-param">table</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">properties=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.jdbc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.jdbc" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to an external database table via JDBC.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Don’t create too many partitions in parallel on a large cluster;
otherwise Spark might crash your external database systems.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>url</strong> – a JDBC URL of the form <code class="docutils literal notranslate"><span class="pre">jdbc:subprotocol:subname</span></code></p></li>
<li><p><strong>table</strong> – Name of the table in the external database.</p></li>
<li><p><strong>mode</strong> – <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">append</span></code>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">overwrite</span></code>: Overwrite existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ignore</span></code>: Silently ignore this operation if data already exists.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">error</span></code> or <code class="docutils literal notranslate"><span class="pre">errorifexists</span></code> (default case): Throw an exception if data already                 exists.</p></li>
</ul>
</p></li>
<li><p><strong>properties</strong> – a dictionary of JDBC database connection arguments. Normally at
least properties “user” and “password” with their corresponding values.
For example { ‘user’ : ‘SYSTEM’, ‘password’ : ‘mypassword’ }</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.json">
<code class="sig-name descname">json</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">compression=None</em>, <em class="sig-param">dateFormat=None</em>, <em class="sig-param">timestampFormat=None</em>, <em class="sig-param">lineSep=None</em>, <em class="sig-param">encoding=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.json" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> in JSON format
(<a class="reference external" href="http://jsonlines.org/">JSON Lines text format or newline-delimited JSON</a>) at the
specified path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path in any Hadoop supported file system</p></li>
<li><p><strong>mode</strong> – <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">append</span></code>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">overwrite</span></code>: Overwrite existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ignore</span></code>: Silently ignore this operation if data already exists.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">error</span></code> or <code class="docutils literal notranslate"><span class="pre">errorifexists</span></code> (default case): Throw an exception if data already                 exists.</p></li>
</ul>
</p></li>
<li><p><strong>compression</strong> – compression codec to use when saving to file. This can be one of the
known case-insensitive shorten names (none, bzip2, gzip, lz4,
snappy and deflate).</p></li>
<li><p><strong>dateFormat</strong> – sets the string that indicates a date format. Custom date formats
follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>. This
applies to date type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd</span></code>.</p></li>
<li><p><strong>timestampFormat</strong> – sets the string that indicates a timestamp format. Custom date
formats follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>.
This applies to timestamp type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd'T'HH:mm:ss.SSSXXX</span></code>.</p></li>
<li><p><strong>encoding</strong> – specifies encoding (charset) of saved json files. If None is set,
the default UTF-8 charset will be used.</p></li>
<li><p><strong>lineSep</strong> – defines the line separator that should be used for writing. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">\n</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.mode">
<code class="sig-name descname">mode</code><span class="sig-paren">(</span><em class="sig-param">saveMode</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.mode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.mode" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the behavior when data or table already exists.</p>
<p>Options include:</p>
<ul class="simple">
<li><p><cite>append</cite>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to existing data.</p></li>
<li><p><cite>overwrite</cite>: Overwrite existing data.</p></li>
<li><p><cite>error</cite> or <cite>errorifexists</cite>: Throw an exception if data already exists.</p></li>
<li><p><cite>ignore</cite>: Silently ignore this operation if data already exists.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s1">&#39;append&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.option">
<code class="sig-name descname">option</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.option"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.option" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds an output option for the underlying data source.</p>
<dl class="simple">
<dt>You can set the following option(s) for writing files:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">timeZone</span></code>: sets the string that indicates a timezone to be used to format</dt><dd><p>timestamps in the JSON/CSV datasources or partition values.
If it isn’t set, it uses the default value, session local timezone.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.options">
<code class="sig-name descname">options</code><span class="sig-paren">(</span><em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.options"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.options" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds output options for the underlying data source.</p>
<dl class="simple">
<dt>You can set the following option(s) for writing files:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">timeZone</span></code>: sets the string that indicates a timezone to be used to format</dt><dd><p>timestamps in the JSON/CSV datasources or partition values.
If it isn’t set, it uses the default value, session local timezone.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.orc">
<code class="sig-name descname">orc</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">partitionBy=None</em>, <em class="sig-param">compression=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.orc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.orc" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> in ORC format at the specified path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path in any Hadoop supported file system</p></li>
<li><p><strong>mode</strong> – <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">append</span></code>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">overwrite</span></code>: Overwrite existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ignore</span></code>: Silently ignore this operation if data already exists.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">error</span></code> or <code class="docutils literal notranslate"><span class="pre">errorifexists</span></code> (default case): Throw an exception if data already                 exists.</p></li>
</ul>
</p></li>
<li><p><strong>partitionBy</strong> – names of partitioning columns</p></li>
<li><p><strong>compression</strong> – compression codec to use when saving to file. This can be one of the
known case-insensitive shorten names (none, snappy, zlib, and lzo).
This will override <code class="docutils literal notranslate"><span class="pre">orc.compress</span></code> and
<code class="docutils literal notranslate"><span class="pre">spark.sql.orc.compression.codec</span></code>. If None is set, it uses the value
specified in <code class="docutils literal notranslate"><span class="pre">spark.sql.orc.compression.codec</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">orc_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/orc_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">orc_df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.parquet">
<code class="sig-name descname">parquet</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">partitionBy=None</em>, <em class="sig-param">compression=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.parquet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.parquet" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> in Parquet format at the specified path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path in any Hadoop supported file system</p></li>
<li><p><strong>mode</strong> – <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">append</span></code>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">overwrite</span></code>: Overwrite existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ignore</span></code>: Silently ignore this operation if data already exists.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">error</span></code> or <code class="docutils literal notranslate"><span class="pre">errorifexists</span></code> (default case): Throw an exception if data already                 exists.</p></li>
</ul>
</p></li>
<li><p><strong>partitionBy</strong> – names of partitioning columns</p></li>
<li><p><strong>compression</strong> – compression codec to use when saving to file. This can be one of the
known case-insensitive shorten names (none, uncompressed, snappy, gzip,
lzo, brotli, lz4, and zstd). This will override
<code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.compression.codec</span></code>. If None is set, it uses the
value specified in <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.compression.codec</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.partitionBy">
<code class="sig-name descname">partitionBy</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.partitionBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Partitions the output by the given columns on the file system.</p>
<p>If specified, the output is laid out on the file system similar
to Hive’s partitioning scheme.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – name of columns</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;month&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.save">
<code class="sig-name descname">save</code><span class="sig-paren">(</span><em class="sig-param">path=None</em>, <em class="sig-param">format=None</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">partitionBy=None</em>, <em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the contents of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to a data source.</p>
<p>The data source is specified by the <code class="docutils literal notranslate"><span class="pre">format</span></code> and a set of <code class="docutils literal notranslate"><span class="pre">options</span></code>.
If <code class="docutils literal notranslate"><span class="pre">format</span></code> is not specified, the default data source configured by
<code class="docutils literal notranslate"><span class="pre">spark.sql.sources.default</span></code> will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path in a Hadoop supported file system</p></li>
<li><p><strong>format</strong> – the format used to save</p></li>
<li><p><strong>mode</strong> – <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">append</span></code>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">overwrite</span></code>: Overwrite existing data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ignore</span></code>: Silently ignore this operation if data already exists.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">error</span></code> or <code class="docutils literal notranslate"><span class="pre">errorifexists</span></code> (default case): Throw an exception if data already                 exists.</p></li>
</ul>
</p></li>
<li><p><strong>partitionBy</strong> – names of partitioning columns</p></li>
<li><p><strong>options</strong> – all other string options</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s1">&#39;append&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s1">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.saveAsTable">
<code class="sig-name descname">saveAsTable</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">format=None</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">partitionBy=None</em>, <em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.saveAsTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.saveAsTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> as the specified table.</p>
<p>In the case the table already exists, behavior of this function depends on the
save mode, specified by the <cite>mode</cite> function (default to throwing an exception).
When <cite>mode</cite> is <cite>Overwrite</cite>, the schema of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> does not need to be
the same as that of the existing table.</p>
<ul class="simple">
<li><p><cite>append</cite>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></a> to existing data.</p></li>
<li><p><cite>overwrite</cite>: Overwrite existing data.</p></li>
<li><p><cite>error</cite> or <cite>errorifexists</cite>: Throw an exception if data already exists.</p></li>
<li><p><cite>ignore</cite>: Silently ignore this operation if data already exists.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – the table name</p></li>
<li><p><strong>format</strong> – the format used to save</p></li>
<li><p><strong>mode</strong> – one of <cite>append</cite>, <cite>overwrite</cite>, <cite>error</cite>, <cite>errorifexists</cite>, <cite>ignore</cite>                      (default: error)</p></li>
<li><p><strong>partitionBy</strong> – names of partitioning columns</p></li>
<li><p><strong>options</strong> – all other string options</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.sortBy">
<code class="sig-name descname">sortBy</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.sortBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.sortBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Sorts the output in each bucket by the given columns on the file system.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – a name of a column, or a list of names.</p></li>
<li><p><strong>cols</strong> – additional names (optional). If <cite>col</cite> is a list it should be empty.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;parquet&#39;</span><span class="p">)</span>  
<span class="gp">... </span>    <span class="o">.</span><span class="n">bucketBy</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;month&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="o">.</span><span class="n">sortBy</span><span class="p">(</span><span class="s1">&#39;day&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s1">&#39;sorted_bucketed_table&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.text">
<code class="sig-name descname">text</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">compression=None</em>, <em class="sig-param">lineSep=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/readwriter.html#DataFrameWriter.text"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DataFrameWriter.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the DataFrame in a text file at the specified path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path in any Hadoop supported file system</p></li>
<li><p><strong>compression</strong> – compression codec to use when saving to file. This can be one of the
known case-insensitive shorten names (none, bzip2, gzip, lz4,
snappy and deflate).</p></li>
<li><p><strong>lineSep</strong> – defines the line separator that should be used for writing. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">\n</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>The DataFrame must have only one column that is of string type.
Each row becomes a new line in the output file.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pyspark.sql.types">
<span id="pyspark-sql-types-module"></span><h2>pyspark.sql.types module<a class="headerlink" href="#module-pyspark.sql.types" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pyspark.sql.types.DataType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">DataType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for data types.</p>
<dl class="method">
<dt id="pyspark.sql.types.DataType.fromInternal">
<code class="sig-name descname">fromInternal</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an internal SQL object into a native Python object.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.json">
<code class="sig-name descname">json</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.json" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.jsonValue">
<code class="sig-name descname">jsonValue</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.needConversion">
<code class="sig-name descname">needConversion</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Does this type need to conversion between Python object and internal SQL object.</p>
<p>This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.simpleString">
<code class="sig-name descname">simpleString</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.toInternal">
<code class="sig-name descname">toInternal</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a Python object into an internal SQL object.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.typeName">
<em class="property">classmethod </em><code class="sig-name descname">typeName</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.typeName"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.typeName" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.NullType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">NullType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#NullType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.NullType" title="Permalink to this definition">¶</a></dt>
<dd><p>Null type.</p>
<p>The data type representing None, used for the types that cannot be inferred.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.StringType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">StringType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#StringType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StringType" title="Permalink to this definition">¶</a></dt>
<dd><p>String data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.BinaryType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">BinaryType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#BinaryType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.BinaryType" title="Permalink to this definition">¶</a></dt>
<dd><p>Binary (byte array) data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.BooleanType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">BooleanType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#BooleanType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.BooleanType" title="Permalink to this definition">¶</a></dt>
<dd><p>Boolean data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.DateType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">DateType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#DateType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DateType" title="Permalink to this definition">¶</a></dt>
<dd><p>Date (datetime.date) data type.</p>
<dl class="attribute">
<dt id="pyspark.sql.types.DateType.EPOCH_ORDINAL">
<code class="sig-name descname">EPOCH_ORDINAL</code><em class="property"> = 719163</em><a class="headerlink" href="#pyspark.sql.types.DateType.EPOCH_ORDINAL" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DateType.fromInternal">
<code class="sig-name descname">fromInternal</code><span class="sig-paren">(</span><em class="sig-param">v</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DateType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DateType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an internal SQL object into a native Python object.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DateType.needConversion">
<code class="sig-name descname">needConversion</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DateType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DateType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Does this type need to conversion between Python object and internal SQL object.</p>
<p>This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DateType.toInternal">
<code class="sig-name descname">toInternal</code><span class="sig-paren">(</span><em class="sig-param">d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DateType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DateType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a Python object into an internal SQL object.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.TimestampType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">TimestampType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#TimestampType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.TimestampType" title="Permalink to this definition">¶</a></dt>
<dd><p>Timestamp (datetime.datetime) data type.</p>
<dl class="method">
<dt id="pyspark.sql.types.TimestampType.fromInternal">
<code class="sig-name descname">fromInternal</code><span class="sig-paren">(</span><em class="sig-param">ts</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#TimestampType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.TimestampType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an internal SQL object into a native Python object.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.TimestampType.needConversion">
<code class="sig-name descname">needConversion</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#TimestampType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.TimestampType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Does this type need to conversion between Python object and internal SQL object.</p>
<p>This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.TimestampType.toInternal">
<code class="sig-name descname">toInternal</code><span class="sig-paren">(</span><em class="sig-param">dt</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#TimestampType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.TimestampType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a Python object into an internal SQL object.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.DecimalType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">DecimalType</code><span class="sig-paren">(</span><em class="sig-param">precision=10</em>, <em class="sig-param">scale=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DecimalType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DecimalType" title="Permalink to this definition">¶</a></dt>
<dd><p>Decimal (decimal.Decimal) data type.</p>
<p>The DecimalType must have fixed precision (the maximum total number of digits)
and scale (the number of digits on the right of dot). For example, (5, 2) can
support the value from [-999.99 to 999.99].</p>
<p>The precision can be up to 38, the scale must be less or equal to precision.</p>
<p>When create a DecimalType, the default precision and scale is (10, 0). When infer
schema from decimal.Decimal objects, it will be DecimalType(38, 18).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>precision</strong> – the maximum total number of digits (default: 10)</p></li>
<li><p><strong>scale</strong> – the number of digits on right side of dot. (default: 0)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="pyspark.sql.types.DecimalType.jsonValue">
<code class="sig-name descname">jsonValue</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DecimalType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DecimalType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DecimalType.simpleString">
<code class="sig-name descname">simpleString</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#DecimalType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DecimalType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.DoubleType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">DoubleType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#DoubleType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DoubleType" title="Permalink to this definition">¶</a></dt>
<dd><p>Double data type, representing double precision floats.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.FloatType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">FloatType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#FloatType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.FloatType" title="Permalink to this definition">¶</a></dt>
<dd><p>Float data type, representing single precision floats.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.ByteType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">ByteType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#ByteType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ByteType" title="Permalink to this definition">¶</a></dt>
<dd><p>Byte data type, i.e. a signed integer in a single byte.</p>
<dl class="method">
<dt id="pyspark.sql.types.ByteType.simpleString">
<code class="sig-name descname">simpleString</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#ByteType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ByteType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.IntegerType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">IntegerType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#IntegerType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.IntegerType" title="Permalink to this definition">¶</a></dt>
<dd><p>Int data type, i.e. a signed 32-bit integer.</p>
<dl class="method">
<dt id="pyspark.sql.types.IntegerType.simpleString">
<code class="sig-name descname">simpleString</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#IntegerType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.IntegerType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.LongType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">LongType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#LongType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.LongType" title="Permalink to this definition">¶</a></dt>
<dd><p>Long data type, i.e. a signed 64-bit integer.</p>
<p>If the values are beyond the range of [-9223372036854775808, 9223372036854775807],
please use <a class="reference internal" href="#pyspark.sql.types.DecimalType" title="pyspark.sql.types.DecimalType"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecimalType</span></code></a>.</p>
<dl class="method">
<dt id="pyspark.sql.types.LongType.simpleString">
<code class="sig-name descname">simpleString</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#LongType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.LongType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.ShortType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">ShortType</code><a class="reference internal" href="_modules/pyspark/sql/types.html#ShortType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ShortType" title="Permalink to this definition">¶</a></dt>
<dd><p>Short data type, i.e. a signed 16-bit integer.</p>
<dl class="method">
<dt id="pyspark.sql.types.ShortType.simpleString">
<code class="sig-name descname">simpleString</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#ShortType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ShortType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.ArrayType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">ArrayType</code><span class="sig-paren">(</span><em class="sig-param">elementType</em>, <em class="sig-param">containsNull=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType" title="Permalink to this definition">¶</a></dt>
<dd><p>Array data type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>elementType</strong> – <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataType</span></code></a> of each element in the array.</p></li>
<li><p><strong>containsNull</strong> – boolean, whether the array can contain null (None) values.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="pyspark.sql.types.ArrayType.fromInternal">
<code class="sig-name descname">fromInternal</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an internal SQL object into a native Python object.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.fromJson">
<em class="property">classmethod </em><code class="sig-name descname">fromJson</code><span class="sig-paren">(</span><em class="sig-param">json</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.jsonValue">
<code class="sig-name descname">jsonValue</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.needConversion">
<code class="sig-name descname">needConversion</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Does this type need to conversion between Python object and internal SQL object.</p>
<p>This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.simpleString">
<code class="sig-name descname">simpleString</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.toInternal">
<code class="sig-name descname">toInternal</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a Python object into an internal SQL object.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.MapType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">MapType</code><span class="sig-paren">(</span><em class="sig-param">keyType</em>, <em class="sig-param">valueType</em>, <em class="sig-param">valueContainsNull=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType" title="Permalink to this definition">¶</a></dt>
<dd><p>Map data type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keyType</strong> – <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataType</span></code></a> of the keys in the map.</p></li>
<li><p><strong>valueType</strong> – <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataType</span></code></a> of the values in the map.</p></li>
<li><p><strong>valueContainsNull</strong> – indicates whether values can contain null (None) values.</p></li>
</ul>
</dd>
</dl>
<p>Keys in a map data type are not allowed to be null (None).</p>
<dl class="method">
<dt id="pyspark.sql.types.MapType.fromInternal">
<code class="sig-name descname">fromInternal</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an internal SQL object into a native Python object.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.fromJson">
<em class="property">classmethod </em><code class="sig-name descname">fromJson</code><span class="sig-paren">(</span><em class="sig-param">json</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.jsonValue">
<code class="sig-name descname">jsonValue</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.needConversion">
<code class="sig-name descname">needConversion</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Does this type need to conversion between Python object and internal SQL object.</p>
<p>This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.simpleString">
<code class="sig-name descname">simpleString</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.toInternal">
<code class="sig-name descname">toInternal</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a Python object into an internal SQL object.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.StructField">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">StructField</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">dataType</em>, <em class="sig-param">nullable=True</em>, <em class="sig-param">metadata=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField" title="Permalink to this definition">¶</a></dt>
<dd><p>A field in <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">StructType</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – string, name of the field.</p></li>
<li><p><strong>dataType</strong> – <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataType</span></code></a> of the field.</p></li>
<li><p><strong>nullable</strong> – boolean, whether the field can be null (None) or not.</p></li>
<li><p><strong>metadata</strong> – a dict from string to simple type that can be toInternald to JSON automatically</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="pyspark.sql.types.StructField.fromInternal">
<code class="sig-name descname">fromInternal</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an internal SQL object into a native Python object.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.fromJson">
<em class="property">classmethod </em><code class="sig-name descname">fromJson</code><span class="sig-paren">(</span><em class="sig-param">json</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.jsonValue">
<code class="sig-name descname">jsonValue</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.needConversion">
<code class="sig-name descname">needConversion</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.needConversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Does this type need to conversion between Python object and internal SQL object.</p>
<p>This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.simpleString">
<code class="sig-name descname">simpleString</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.toInternal">
<code class="sig-name descname">toInternal</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.toInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a Python object into an internal SQL object.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.typeName">
<code class="sig-name descname">typeName</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.typeName"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.typeName" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.StructType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.types.</code><code class="sig-name descname">StructType</code><span class="sig-paren">(</span><em class="sig-param">fields=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType" title="Permalink to this definition">¶</a></dt>
<dd><p>Struct type, consisting of a list of <a class="reference internal" href="#pyspark.sql.types.StructField" title="pyspark.sql.types.StructField"><code class="xref py py-class docutils literal notranslate"><span class="pre">StructField</span></code></a>.</p>
<p>This is the data type representing a <code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code>.</p>
<p>Iterating a <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">StructType</span></code></a> will iterate its <a class="reference internal" href="#pyspark.sql.types.StructField" title="pyspark.sql.types.StructField"><code class="xref py py-class docutils literal notranslate"><span class="pre">StructField</span></code></a>s.
A contained <a class="reference internal" href="#pyspark.sql.types.StructField" title="pyspark.sql.types.StructField"><code class="xref py py-class docutils literal notranslate"><span class="pre">StructField</span></code></a> can be accessed by name or position.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span><span class="p">[</span><span class="s2">&quot;f1&quot;</span><span class="p">]</span>
<span class="go">StructField(f1,StringType,true)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">StructField(f1,StringType,true)</span>
</pre></div>
</div>
<dl class="method">
<dt id="pyspark.sql.types.StructType.add">
<code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param">field</em>, <em class="sig-param">data_type=None</em>, <em class="sig-param">nullable=True</em>, <em class="sig-param">metadata=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a StructType by adding new elements to it to define the schema. The method accepts
either:</p>
<blockquote>
<div><ol class="loweralpha simple">
<li><p>A single parameter which is a StructField object.</p></li>
<li><p>Between 2 and 4 parameters as (name, data_type, nullable (optional),
metadata(optional). The data_type parameter may be either a String or a
DataType object.</p></li>
</ol>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">&quot;f2&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct2</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span> \
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;f2&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">==</span> <span class="n">struct2</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct2</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">==</span> <span class="n">struct2</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct2</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">==</span> <span class="n">struct2</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>field</strong> – Either the name of the field or a StructField object</p></li>
<li><p><strong>data_type</strong> – If present, the DataType of the StructField to create</p></li>
<li><p><strong>nullable</strong> – Whether the field to add should be nullable (default True)</p></li>
<li><p><strong>metadata</strong> – Any additional metadata (default None)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a new updated StructType</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.fieldNames">
<code class="sig-name descname">fieldNames</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.fieldNames"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.fieldNames" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all field names in a list.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">struct</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct</span><span class="o">.</span><span class="n">fieldNames</span><span class="p">()</span>
<span class="go">[&#39;f1&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.fromInternal">
<code class="sig-name descname">fromInternal</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an internal SQL object into a native Python object.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.fromJson">
<em class="property">classmethod </em><code class="sig-name descname">fromJson</code><span class="sig-paren">(</span><em class="sig-param">json</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.jsonValue">
<code class="sig-name descname">jsonValue</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.needConversion">
<code class="sig-name descname">needConversion</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Does this type need to conversion between Python object and internal SQL object.</p>
<p>This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.simpleString">
<code class="sig-name descname">simpleString</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.toInternal">
<code class="sig-name descname">toInternal</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a Python object into an internal SQL object.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pyspark.sql.functions">
<span id="pyspark-sql-functions-module"></span><h2>pyspark.sql.functions module<a class="headerlink" href="#module-pyspark.sql.functions" title="Permalink to this headline">¶</a></h2>
<p>A collections of builtin functions</p>
<dl class="class">
<dt id="pyspark.sql.functions.PandasUDFType">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">PandasUDFType</code><a class="reference internal" href="_modules/pyspark/sql/functions.html#PandasUDFType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.PandasUDFType" title="Permalink to this definition">¶</a></dt>
<dd><p>Pandas UDF Types. See <a class="reference internal" href="#pyspark.sql.functions.pandas_udf" title="pyspark.sql.functions.pandas_udf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.functions.pandas_udf()</span></code></a>.</p>
<dl class="attribute">
<dt id="pyspark.sql.functions.PandasUDFType.GROUPED_AGG">
<code class="sig-name descname">GROUPED_AGG</code><em class="property"> = 202</em><a class="headerlink" href="#pyspark.sql.functions.PandasUDFType.GROUPED_AGG" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.functions.PandasUDFType.GROUPED_MAP">
<code class="sig-name descname">GROUPED_MAP</code><em class="property"> = 201</em><a class="headerlink" href="#pyspark.sql.functions.PandasUDFType.GROUPED_MAP" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.functions.PandasUDFType.SCALAR">
<code class="sig-name descname">SCALAR</code><em class="property"> = 200</em><a class="headerlink" href="#pyspark.sql.functions.PandasUDFType.SCALAR" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.abs">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">abs</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the absolute value.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.acos">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">acos</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.acos" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>inverse cosine of <cite>col</cite>, as if computed by <cite>java.lang.Math.acos()</cite></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.add_months">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">add_months</code><span class="sig-paren">(</span><em class="sig-param">start</em>, <em class="sig-param">months</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#add_months"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.add_months" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the date that is <cite>months</cite> months after <cite>start</cite></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">add_months</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">dt</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;next_month&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(next_month=datetime.date(2015, 5, 8))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.approxCountDistinct">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">approxCountDistinct</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">rsd=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#approxCountDistinct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.approxCountDistinct" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deprecated in 2.1, use <a class="reference internal" href="#pyspark.sql.functions.approx_count_distinct" title="pyspark.sql.functions.approx_count_distinct"><code class="xref py py-func docutils literal notranslate"><span class="pre">approx_count_distinct()</span></code></a> instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.approx_count_distinct">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">approx_count_distinct</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">rsd=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#approx_count_distinct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.approx_count_distinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns a new <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> for approximate distinct count of
column <cite>col</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>rsd</strong> – maximum estimation error allowed (default = 0.05). For rsd &lt; 0.01, it is more
efficient to use <a class="reference internal" href="#pyspark.sql.functions.countDistinct" title="pyspark.sql.functions.countDistinct"><code class="xref py py-func docutils literal notranslate"><span class="pre">countDistinct()</span></code></a></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">approx_count_distinct</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;distinct_ages&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(distinct_ages=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new array column.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of column names (string) or list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> expressions that have
the same data type.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;arr&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(arr=[2, 2]), Row(arr=[5, 5])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">])</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;arr&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(arr=[2, 2]), Row(arr=[5, 5])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_contains">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_contains</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_contains"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_contains" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns null if the array is null, true if the array contains the
given value, and false otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – name of column containing array</p></li>
<li><p><strong>value</strong> – value to check for in array</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">],),</span> <span class="p">([],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_contains</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_distinct">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_distinct</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_distinct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_distinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: removes duplicate values from the array.
:param col: name of column or expression</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],),</span> <span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_distinct</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_except">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_except</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_except"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_except" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns an array of the elements in col1 but not in col2,
without duplicates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – name of column containing array</p></li>
<li><p><strong>col2</strong> – name of column containing array</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">c1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">],</span> <span class="n">c2</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">])])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_except</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">c1</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">c2</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(array_except(c1, c2)=[&#39;b&#39;])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_intersect">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_intersect</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_intersect"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_intersect" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns an array of the elements in the intersection of col1 and col2,
without duplicates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – name of column containing array</p></li>
<li><p><strong>col2</strong> – name of column containing array</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">c1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">],</span> <span class="n">c2</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">])])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_intersect</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">c1</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">c2</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(array_intersect(c1, c2)=[&#39;a&#39;, &#39;c&#39;])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_join">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_join</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">delimiter</em>, <em class="sig-param">null_replacement=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_join"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_join" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates the elements of <cite>column</cite> using the <cite>delimiter</cite>. Null values are replaced with
<cite>null_replacement</cite> if set, otherwise they are ignored.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">],),</span> <span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_join</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;joined&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(joined=&#39;a,b,c&#39;), Row(joined=&#39;a&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_join</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">,</span> <span class="s2">&quot;NULL&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;joined&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(joined=&#39;a,b,c&#39;), Row(joined=&#39;a,NULL&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_max">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_max</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_max"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_max" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns the maximum value of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],),</span> <span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_max</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;max&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(max=3), Row(max=10)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_min">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_min</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_min"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_min" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns the minimum value of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],),</span> <span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_min</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;min&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(min=1), Row(min=-1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_position">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_position</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_position"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_position" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: Locates the position of the first occurrence of the given value
in the given array. Returns null if either of the arguments are null.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The position is not zero based, but 1 based index. Returns 0 if the given
value could not be found in the array.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">],),</span> <span class="p">([],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_position</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(array_position(data, a)=3), Row(array_position(data, a)=0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_remove">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_remove</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">element</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_remove"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_remove" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: Remove all elements that equal to element from the given array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – name of column containing array</p></li>
<li><p><strong>element</strong> – element to be removed from the array</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],),</span> <span class="p">([],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_remove</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_repeat">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_repeat</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">count</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_repeat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_repeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: creates an array containing a column repeated count times.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;ab&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_repeat</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=[&#39;ab&#39;, &#39;ab&#39;, &#39;ab&#39;])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_sort">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_sort</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_sort"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_sort" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: sorts the input array in ascending order. The elements of the input array
must be orderable. Null elements will be placed at the end of the returned array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">],),([</span><span class="mi">1</span><span class="p">],),([],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_sort</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_union">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">array_union</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_union"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_union" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns an array of the elements in the union of col1 and col2,
without duplicates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – name of column containing array</p></li>
<li><p><strong>col2</strong> – name of column containing array</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">c1</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">],</span> <span class="n">c2</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">])])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_union</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">c1</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">c2</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(array_union(c1, c2)=[&#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;f&#39;])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.arrays_overlap">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">arrays_overlap</code><span class="sig-paren">(</span><em class="sig-param">a1</em>, <em class="sig-param">a2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#arrays_overlap"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.arrays_overlap" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns true if the arrays contain any common non-null element; if not,
returns null if both the arrays are non-empty and any of them contains a null element; returns
false otherwise.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">]),</span> <span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">])],</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">arrays_overlap</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;overlap&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(overlap=True), Row(overlap=False)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.arrays_zip">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">arrays_zip</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#arrays_zip"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.arrays_zip" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: Returns a merged array of structs in which the N-th struct contains all
N-th values of input arrays.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – columns of arrays to be merged.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">arrays_zip</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))],</span> <span class="p">[</span><span class="s1">&#39;vals1&#39;</span><span class="p">,</span> <span class="s1">&#39;vals2&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">arrays_zip</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">vals1</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">vals2</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;zipped&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.asc">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">asc</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.asc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the ascending order of the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.asc_nulls_first">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">asc_nulls_first</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.asc_nulls_first" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the ascending order of the given column name, and null values return before non-null values.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.asc_nulls_last">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">asc_nulls_last</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.asc_nulls_last" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the ascending order of the given column name, and null values appear after non-null values.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.ascii">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">ascii</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.ascii" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the numeric value of the first character of the string column.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.asin">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">asin</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.asin" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>inverse sine of <cite>col</cite>, as if computed by <cite>java.lang.Math.asin()</cite></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.atan">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">atan</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.atan" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>inverse tangent of <cite>col</cite>, as if computed by <cite>java.lang.Math.atan()</cite></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.atan2">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">atan2</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.atan2" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – coordinate on y-axis</p></li>
<li><p><strong>col2</strong> – coordinate on x-axis</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the <cite>theta</cite> component of the point
(<cite>r</cite>, <cite>theta</cite>)
in polar coordinates that corresponds to the point
(<cite>x</cite>, <cite>y</cite>) in Cartesian coordinates,
as if computed by <cite>java.lang.Math.atan2()</cite></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.avg">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">avg</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.avg" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the average of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.base64">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">base64</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.base64" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the BASE64 encoding of a binary column and returns it as a string column.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.functions.basestring">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">basestring</code><a class="headerlink" href="#pyspark.sql.functions.basestring" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">builtins.str</span></code></p>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.bin">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">bin</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#bin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.bin" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the string representation of the binary value of the given column.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">bin</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=&#39;10&#39;), Row(c=&#39;101&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.bitwiseNOT">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">bitwiseNOT</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.bitwiseNOT" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes bitwise not.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.broadcast">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">broadcast</code><span class="sig-paren">(</span><em class="sig-param">df</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#broadcast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks a DataFrame as small enough for use in broadcast joins.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.bround">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">bround</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">scale=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#bround"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.bround" title="Permalink to this definition">¶</a></dt>
<dd><p>Round the given value to <cite>scale</cite> decimal places using HALF_EVEN rounding mode if <cite>scale</cite> &gt;= 0
or at integral part when <cite>scale</cite> &lt; 0.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mf">2.5</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">bround</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=2.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cbrt">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">cbrt</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.cbrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the cube-root of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.ceil">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">ceil</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.ceil" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the ceiling of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.coalesce">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">coalesce</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#coalesce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.coalesce" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first column that is not null.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> <span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+----+</span>
<span class="go">|   a|   b|</span>
<span class="go">+----+----+</span>
<span class="go">|null|null|</span>
<span class="go">|   1|null|</span>
<span class="go">|null|   2|</span>
<span class="go">+----+----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">coalesce</span><span class="p">(</span><span class="n">cDf</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">],</span> <span class="n">cDf</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+--------------+</span>
<span class="go">|coalesce(a, b)|</span>
<span class="go">+--------------+</span>
<span class="go">|          null|</span>
<span class="go">|             1|</span>
<span class="go">|             2|</span>
<span class="go">+--------------+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">cDf</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">],</span> <span class="n">lit</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+----+----------------+</span>
<span class="go">|   a|   b|coalesce(a, 0.0)|</span>
<span class="go">+----+----+----------------+</span>
<span class="go">|null|null|             0.0|</span>
<span class="go">|   1|null|             1.0|</span>
<span class="go">|null|   2|             0.0|</span>
<span class="go">+----+----+----------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.col">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">col</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.col" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> based on the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.collect_list">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">collect_list</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.collect_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns a list of objects with duplicates.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The function is non-deterministic because the order of collected results depends
on order of rows which may be non-deterministic after a shuffle.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">2</span><span class="p">,),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,)],</span> <span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">collect_list</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(collect_list(age)=[2, 5, 5])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.collect_set">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">collect_set</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.collect_set" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns a set of objects with duplicate elements eliminated.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The function is non-deterministic because the order of collected results depends
on order of rows which may be non-deterministic after a shuffle.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">2</span><span class="p">,),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,)],</span> <span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">collect_set</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(collect_set(age)=[5, 2])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.column">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">column</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.column" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> based on the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.concat">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">concat</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#concat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.concat" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates multiple input columns together into a single column.
The function works with strings, binary and compatible array columns.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;abcd&#39;</span><span class="p">,</span><span class="s1">&#39;123&#39;</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">concat</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=&#39;abcd123&#39;)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">]),</span> <span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">])],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">concat</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;arr&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.concat_ws">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">concat_ws</code><span class="sig-paren">(</span><em class="sig-param">sep</em>, <em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#concat_ws"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.concat_ws" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates multiple input string columns together into a single string column,
using the given separator.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;abcd&#39;</span><span class="p">,</span><span class="s1">&#39;123&#39;</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">concat_ws</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=&#39;abcd-123&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.conv">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">conv</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">fromBase</em>, <em class="sig-param">toBase</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#conv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.conv" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a number in a string column from one base to another.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s2">&quot;010101&quot;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">conv</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;hex&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hex=&#39;15&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.corr">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">corr</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#corr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.corr" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> for the Pearson Correlation Coefficient for <code class="docutils literal notranslate"><span class="pre">col1</span></code>
and <code class="docutils literal notranslate"><span class="pre">col2</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">corr</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=1.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cos">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">cos</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.cos" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – angle in radians</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>cosine of the angle, as if computed by <cite>java.lang.Math.cos()</cite>.</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cosh">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">cosh</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.cosh" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – hyperbolic angle</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>hyperbolic cosine of the angle, as if computed by <cite>java.lang.Math.cosh()</cite></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.count">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">count</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the number of items in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.countDistinct">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">countDistinct</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#countDistinct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.countDistinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> for distinct count of <code class="docutils literal notranslate"><span class="pre">col</span></code> or <code class="docutils literal notranslate"><span class="pre">cols</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">countDistinct</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=2)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">countDistinct</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.covar_pop">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">covar_pop</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#covar_pop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.covar_pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> for the population covariance of <code class="docutils literal notranslate"><span class="pre">col1</span></code> and <code class="docutils literal notranslate"><span class="pre">col2</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">covar_pop</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=0.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.covar_samp">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">covar_samp</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#covar_samp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.covar_samp" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> for the sample covariance of <code class="docutils literal notranslate"><span class="pre">col1</span></code> and <code class="docutils literal notranslate"><span class="pre">col2</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">covar_samp</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=0.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.crc32">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">crc32</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#crc32"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.crc32" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the cyclic redundancy check value  (CRC32) of a binary column and
returns the value as a bigint.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;ABC&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">crc32</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;crc32&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(crc32=2743272264)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.create_map">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">create_map</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#create_map"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.create_map" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new map column.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of column names (string) or list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> expressions that are
grouped as key-value pairs, e.g. (key1, value1, key2, value2, …).</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">create_map</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;map&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(map={&#39;Alice&#39;: 2}), Row(map={&#39;Bob&#39;: 5})]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">create_map</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">])</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;map&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(map={&#39;Alice&#39;: 2}), Row(map={&#39;Bob&#39;: 5})]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cume_dist">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">cume_dist</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.cume_dist" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the cumulative distribution of values within a window partition,
i.e. the fraction of rows that are below the current row.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.current_date">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">current_date</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#current_date"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.current_date" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current date as a <code class="xref py py-class docutils literal notranslate"><span class="pre">DateType</span></code> column.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.current_timestamp">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">current_timestamp</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#current_timestamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.current_timestamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current timestamp as a <code class="xref py py-class docutils literal notranslate"><span class="pre">TimestampType</span></code> column.</p>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.date_add">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">date_add</code><span class="sig-paren">(</span><em class="sig-param">start</em>, <em class="sig-param">days</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#date_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.date_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the date that is <cite>days</cite> days after <cite>start</cite></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">date_add</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">dt</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;next_date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(next_date=datetime.date(2015, 4, 9))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.date_format">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">date_format</code><span class="sig-paren">(</span><em class="sig-param">date</em>, <em class="sig-param">format</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#date_format"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.date_format" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a date/timestamp/string to a value of string in the format specified by the date
format given by the second argument.</p>
<p>A pattern could be for instance <cite>dd.MM.yyyy</cite> and could return a string like ‘18.03.1993’. All
pattern letters of the Java class <cite>java.text.SimpleDateFormat</cite> can be used.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use when ever possible specialized functions like <cite>year</cite>. These benefit from a
specialized implementation.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">date_format</span><span class="p">(</span><span class="s1">&#39;dt&#39;</span><span class="p">,</span> <span class="s1">&#39;MM/dd/yyy&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(date=&#39;04/08/2015&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.date_sub">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">date_sub</code><span class="sig-paren">(</span><em class="sig-param">start</em>, <em class="sig-param">days</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#date_sub"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.date_sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the date that is <cite>days</cite> days before <cite>start</cite></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">date_sub</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">dt</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;prev_date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(prev_date=datetime.date(2015, 4, 7))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.date_trunc">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">date_trunc</code><span class="sig-paren">(</span><em class="sig-param">format</em>, <em class="sig-param">timestamp</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#date_trunc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.date_trunc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns timestamp truncated to the unit specified by the format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>format</strong> – ‘year’, ‘yyyy’, ‘yy’, ‘month’, ‘mon’, ‘mm’,
‘day’, ‘dd’, ‘hour’, ‘minute’, ‘second’, ‘week’, ‘quarter’</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;1997-02-28 05:02:11&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">date_trunc</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(year=datetime.datetime(1997, 1, 1, 0, 0))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">date_trunc</span><span class="p">(</span><span class="s1">&#39;mon&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;month&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(month=datetime.datetime(1997, 2, 1, 0, 0))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.datediff">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">datediff</code><span class="sig-paren">(</span><em class="sig-param">end</em>, <em class="sig-param">start</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#datediff"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.datediff" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of days from <cite>start</cite> to <cite>end</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,</span><span class="s1">&#39;2015-05-10&#39;</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;d1&#39;</span><span class="p">,</span> <span class="s1">&#39;d2&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">datediff</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">d1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;diff&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(diff=32)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.dayofmonth">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">dayofmonth</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#dayofmonth"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.dayofmonth" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the day of the month of a given date as integer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">dayofmonth</span><span class="p">(</span><span class="s1">&#39;dt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;day&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(day=8)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.dayofweek">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">dayofweek</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#dayofweek"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.dayofweek" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the day of the week of a given date as integer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">dayofweek</span><span class="p">(</span><span class="s1">&#39;dt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;day&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(day=4)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.dayofyear">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">dayofyear</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#dayofyear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.dayofyear" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the day of the year of a given date as integer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">dayofyear</span><span class="p">(</span><span class="s1">&#39;dt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;day&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(day=98)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.decode">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">decode</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">charset</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the first argument into a string from a binary using the provided character set
(one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.degrees">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">degrees</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.degrees" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an angle measured in radians to an approximately equivalent angle
measured in degrees.
:param col: angle in radians
:return: angle in degrees, as if computed by <cite>java.lang.Math.toDegrees()</cite></p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.dense_rank">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">dense_rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.dense_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the rank of rows within a window partition, without any gaps.</p>
<p>The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking
sequence when there are ties. That is, if you were ranking a competition using dense_rank
and had three people tie for second place, you would say that all three were in second
place and that the next person came in third. Rank would give me sequential numbers, making
the person that came in third place (after the ties) would register as coming in fifth.</p>
<p>This is equivalent to the DENSE_RANK function in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.desc">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">desc</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.desc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the descending order of the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.desc_nulls_first">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">desc_nulls_first</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.desc_nulls_first" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the descending order of the given column name, and null values appear before non-null values.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.desc_nulls_last">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">desc_nulls_last</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.desc_nulls_last" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the descending order of the given column name, and null values appear after non-null values</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.element_at">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">element_at</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">extraction</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#element_at"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.element_at" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: Returns element of array at given index in extraction if col is array.
Returns value for the given key in extraction if col is map.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – name of column containing array or map</p></li>
<li><p><strong>extraction</strong> – index to check for in array or key to check for in map</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The position is not zero based, but 1 based index.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">],),</span> <span class="p">([],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">element_at</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(element_at(data, 1)=&#39;a&#39;), Row(element_at(data, 1)=None)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([({</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">},),</span> <span class="p">({},)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">element_at</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.encode">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">encode</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">charset</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the first argument into a binary from a string using the provided character set
(one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.exp">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">exp</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the exponential of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.explode">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">explode</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#explode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.explode" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new row for each element in the given array or map.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">intlist</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">mapfield</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="s2">&quot;b&quot;</span><span class="p">})])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">explode</span><span class="p">(</span><span class="n">eDF</span><span class="o">.</span><span class="n">intlist</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;anInt&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(anInt=1), Row(anInt=2), Row(anInt=3)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">explode</span><span class="p">(</span><span class="n">eDF</span><span class="o">.</span><span class="n">mapfield</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|key|value|</span>
<span class="go">+---+-----+</span>
<span class="go">|  a|    b|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.explode_outer">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">explode_outer</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#explode_outer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.explode_outer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new row for each element in the given array or map.
Unlike explode, if the array/map is null or empty then null is produced.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="s2">&quot;bar&quot;</span><span class="p">],</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">[],</span> <span class="p">{}),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;an_array&quot;</span><span class="p">,</span> <span class="s2">&quot;a_map&quot;</span><span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;an_array&quot;</span><span class="p">,</span> <span class="n">explode_outer</span><span class="p">(</span><span class="s2">&quot;a_map&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+----------+----+-----+</span>
<span class="go">| id|  an_array| key|value|</span>
<span class="go">+---+----------+----+-----+</span>
<span class="go">|  1|[foo, bar]|   x|  1.0|</span>
<span class="go">|  2|        []|null| null|</span>
<span class="go">|  3|      null|null| null|</span>
<span class="go">+---+----------+----+-----+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;a_map&quot;</span><span class="p">,</span> <span class="n">explode_outer</span><span class="p">(</span><span class="s2">&quot;an_array&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+----------+----+</span>
<span class="go">| id|     a_map| col|</span>
<span class="go">+---+----------+----+</span>
<span class="go">|  1|[x -&gt; 1.0]| foo|</span>
<span class="go">|  1|[x -&gt; 1.0]| bar|</span>
<span class="go">|  2|        []|null|</span>
<span class="go">|  3|      null|null|</span>
<span class="go">+---+----------+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.expm1">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">expm1</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.expm1" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the exponential of the given value minus one.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.expr">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">expr</code><span class="sig-paren">(</span><em class="sig-param">str</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#expr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.expr" title="Permalink to this definition">¶</a></dt>
<dd><p>Parses the expression string into the column that it represents</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">expr</span><span class="p">(</span><span class="s2">&quot;length(name)&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(length(name)=5), Row(length(name)=3)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.factorial">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">factorial</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#factorial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.factorial" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the factorial of the given value.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">5</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">factorial</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;f&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f=120)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.first">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">first</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">ignorenulls=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#first"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.first" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the first value in a group.</p>
<p>The function by default returns the first values it sees. It will return the first non-null
value it sees when ignoreNulls is set to true. If all values are null, then null is returned.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The function is non-deterministic because its results depends on order of rows which
may be non-deterministic after a shuffle.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.flatten">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">flatten</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#flatten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: creates a single array from an array of arrays.
If a structure of nested arrays is deeper than two levels,
only one level of nesting is removed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">]],),</span> <span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">flatten</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=[1, 2, 3, 4, 5, 6]), Row(r=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.floor">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">floor</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.floor" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the floor of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.format_number">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">format_number</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#format_number"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.format_number" title="Permalink to this definition">¶</a></dt>
<dd><p>Formats the number X to a format like ‘#,–#,–#.–’, rounded to d decimal places
with HALF_EVEN round mode, and returns the result as a string.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – the column name of the numeric value to be formatted</p></li>
<li><p><strong>d</strong> – the N decimal places</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">5</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">format_number</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(v=&#39;5.0000&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.format_string">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">format_string</code><span class="sig-paren">(</span><em class="sig-param">format</em>, <em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#format_string"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.format_string" title="Permalink to this definition">¶</a></dt>
<dd><p>Formats the arguments in printf-style and returns the result as a string column.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>format</strong> – string that can contain embedded format tags and used as result column’s value</p></li>
<li><p><strong>cols</strong> – list of column names (string) or list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> expressions to
be used in formatting</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;hello&quot;</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">format_string</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%d</span><span class="s1"> </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(v=&#39;5 hello&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.from_json">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">from_json</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">schema</em>, <em class="sig-param">options={}</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#from_json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.from_json" title="Permalink to this definition">¶</a></dt>
<dd><p>Parses a column containing a JSON string into a <code class="xref py py-class docutils literal notranslate"><span class="pre">MapType</span></code> with <code class="xref py py-class docutils literal notranslate"><span class="pre">StringType</span></code>
as keys type, <code class="xref py py-class docutils literal notranslate"><span class="pre">StructType</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">ArrayType</span></code> with
the specified schema. Returns <cite>null</cite>, in the case of an unparseable string.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – string column in json format</p></li>
<li><p><strong>schema</strong> – a StructType or ArrayType of StructType to use when parsing the json column.</p></li>
<li><p><strong>options</strong> – options to control parsing. accepts the same options as the json datasource</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since Spark 2.3, the DDL-formatted string or a JSON format string is also
supported for <code class="docutils literal notranslate"><span class="pre">schema</span></code>.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;&#39;&#39;{&quot;a&quot;: 1}&#39;&#39;&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">())])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=Row(a=1))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;a INT&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=Row(a=1))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;MAP&lt;STRING,INT&gt;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json={&#39;a&#39;: 1})]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;&#39;&#39;[{&quot;a&quot;: 1}]&#39;&#39;&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">ArrayType</span><span class="p">(</span><span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">())]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=[Row(a=1)])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">schema_of_json</span><span class="p">(</span><span class="n">lit</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;{&quot;a&quot;: 0}&#39;&#39;&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=Row(a=1))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;&#39;&#39;[1, 2, 3]&#39;&#39;&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">ArrayType</span><span class="p">(</span><span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=[1, 2, 3])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.from_unixtime">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">from_unixtime</code><span class="sig-paren">(</span><em class="sig-param">timestamp</em>, <em class="sig-param">format='yyyy-MM-dd HH:mm:ss'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#from_unixtime"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.from_unixtime" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string
representing the timestamp of that moment in the current system time zone in the given
format.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.session.timeZone&quot;</span><span class="p">,</span> <span class="s2">&quot;America/Los_Angeles&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">time_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1428476400</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;unix_time&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">time_df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_unixtime</span><span class="p">(</span><span class="s1">&#39;unix_time&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;ts&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(ts=&#39;2015-04-08 00:00:00&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">unset</span><span class="p">(</span><span class="s2">&quot;spark.sql.session.timeZone&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.from_utc_timestamp">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">from_utc_timestamp</code><span class="sig-paren">(</span><em class="sig-param">timestamp</em>, <em class="sig-param">tz</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#from_utc_timestamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.from_utc_timestamp" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function
takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and
renders that timestamp as a timestamp in the given time zone.</p>
<p>However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not
timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to
the given timezone.</p>
<p>This function may return confusing result if the input is a string with timezone, e.g.
‘2018-03-13T06:18:23+00:00’. The reason is that, Spark firstly cast the string to timestamp
according to the timezone in the string, and finally display the result by converting the
timestamp to string according to the session local timezone.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>timestamp</strong> – the column that contains timestamps</p></li>
<li><p><strong>tz</strong> – a string that has the ID of timezone, e.g. “GMT”, “America/Los_Angeles”, etc</p></li>
</ul>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.4: </span><cite>tz</cite> can take a <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> containing timezone ID strings.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,</span> <span class="s1">&#39;JST&#39;</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;ts&#39;</span><span class="p">,</span> <span class="s1">&#39;tz&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_utc_timestamp</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">ts</span><span class="p">,</span> <span class="s2">&quot;PST&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;local_time&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_utc_timestamp</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">tz</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;local_time&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.get_json_object">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">get_json_object</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#get_json_object"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.get_json_object" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts json object from a json string based on json path specified, and returns json string
of the extracted json object. It will return null if the input json string is invalid.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – string column in json format</p></li>
<li><p><strong>path</strong> – path to the json object to extract</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s1">&#39;&#39;&#39;{&quot;f1&quot;: &quot;value1&quot;, &quot;f2&quot;: &quot;value2&quot;}&#39;&#39;&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s1">&#39;&#39;&#39;{&quot;f1&quot;: &quot;value12&quot;}&#39;&#39;&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;jstring&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">get_json_object</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">jstring</span><span class="p">,</span> <span class="s1">&#39;$.f1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;c0&quot;</span><span class="p">),</span> \
<span class="gp">... </span>                  <span class="n">get_json_object</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">jstring</span><span class="p">,</span> <span class="s1">&#39;$.f2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;c1&quot;</span><span class="p">)</span> <span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(key=&#39;1&#39;, c0=&#39;value1&#39;, c1=&#39;value2&#39;), Row(key=&#39;2&#39;, c0=&#39;value12&#39;, c1=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.greatest">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">greatest</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#greatest"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.greatest" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the greatest value of the list of column names, skipping null values.
This function takes at least 2 parameters. It will return null iff all parameters are null.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">greatest</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;greatest&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(greatest=4)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.grouping">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">grouping</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#grouping"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.grouping" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated
or not, returns 1 for aggregated or 0 for not aggregated in the result set.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">cube</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">grouping</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+--------------+--------+</span>
<span class="go">| name|grouping(name)|sum(age)|</span>
<span class="go">+-----+--------------+--------+</span>
<span class="go">| null|             1|       7|</span>
<span class="go">|Alice|             0|       2|</span>
<span class="go">|  Bob|             0|       5|</span>
<span class="go">+-----+--------------+--------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.grouping_id">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">grouping_id</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#grouping_id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.grouping_id" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the level of grouping, equals to</p>
<blockquote>
<div><p>(grouping(c1) &lt;&lt; (n-1)) + (grouping(c2) &lt;&lt; (n-2)) + … + grouping(cn)</p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The list of columns should match with grouping columns exactly, or empty (means all
the grouping columns).</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">cube</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">grouping_id</span><span class="p">(),</span> <span class="nb">sum</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+-------------+--------+</span>
<span class="go">| name|grouping_id()|sum(age)|</span>
<span class="go">+-----+-------------+--------+</span>
<span class="go">| null|            1|       7|</span>
<span class="go">|Alice|            0|       2|</span>
<span class="go">|  Bob|            0|       5|</span>
<span class="go">+-----+-------------+--------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.hash">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">hash</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#hash"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.hash" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the hash code of given columns, and returns the result as an int column.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;ABC&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">hash</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;hash&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hash=-757602832)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.hex">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">hex</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#hex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.hex" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes hex value of the given column, which could be <a class="reference internal" href="#pyspark.sql.types.StringType" title="pyspark.sql.types.StringType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StringType</span></code></a>,
<a class="reference internal" href="#pyspark.sql.types.BinaryType" title="pyspark.sql.types.BinaryType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.BinaryType</span></code></a>, <a class="reference internal" href="#pyspark.sql.types.IntegerType" title="pyspark.sql.types.IntegerType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.IntegerType</span></code></a> or
<a class="reference internal" href="#pyspark.sql.types.LongType" title="pyspark.sql.types.LongType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.LongType</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;ABC&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">hex</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">),</span> <span class="nb">hex</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hex(a)=&#39;414243&#39;, hex(b)=&#39;3&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.hour">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">hour</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#hour"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.hour" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the hours of a given date as integer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08 13:08:15&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;ts&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">hour</span><span class="p">(</span><span class="s1">&#39;ts&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;hour&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hour=13)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.hypot">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">hypot</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.hypot" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <code class="docutils literal notranslate"><span class="pre">sqrt(a^2</span> <span class="pre">+</span> <span class="pre">b^2)</span></code> without intermediate overflow or underflow.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.initcap">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">initcap</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#initcap"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.initcap" title="Permalink to this definition">¶</a></dt>
<dd><p>Translate the first letter of each word to upper case in the sentence.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;ab cd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">initcap</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(v=&#39;Ab Cd&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.input_file_name">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">input_file_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#input_file_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.input_file_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a string column for the file name of the current Spark task.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.instr">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">instr</code><span class="sig-paren">(</span><em class="sig-param">str</em>, <em class="sig-param">substr</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#instr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.instr" title="Permalink to this definition">¶</a></dt>
<dd><p>Locate the position of the first occurrence of substr column in the given string.
Returns null if either of the arguments are null.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The position is not zero based, but 1 based index. Returns 0 if substr
could not be found in str.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;abcd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">instr</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.isnan">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">isnan</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#isnan"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.isnan" title="Permalink to this definition">¶</a></dt>
<dd><p>An expression that returns true iff the column is NaN.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)),</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">),</span> <span class="mf">2.0</span><span class="p">)],</span> <span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">isnan</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;r1&quot;</span><span class="p">),</span> <span class="n">isnan</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;r2&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r1=False, r2=False), Row(r1=True, r2=True)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.isnull">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">isnull</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#isnull"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.isnull" title="Permalink to this definition">¶</a></dt>
<dd><p>An expression that returns true iff the column is null.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> <span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">isnull</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;r1&quot;</span><span class="p">),</span> <span class="n">isnull</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;r2&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r1=False, r2=False), Row(r1=True, r2=True)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.json_tuple">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">json_tuple</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">*fields</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#json_tuple"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.json_tuple" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new row for a json column according to the given field names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – string column in json format</p></li>
<li><p><strong>fields</strong> – list of fields to extract</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s1">&#39;&#39;&#39;{&quot;f1&quot;: &quot;value1&quot;, &quot;f2&quot;: &quot;value2&quot;}&#39;&#39;&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s1">&#39;&#39;&#39;{&quot;f1&quot;: &quot;value12&quot;}&#39;&#39;&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;jstring&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">json_tuple</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">jstring</span><span class="p">,</span> <span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="s1">&#39;f2&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(key=&#39;1&#39;, c0=&#39;value1&#39;, c1=&#39;value2&#39;), Row(key=&#39;2&#39;, c0=&#39;value12&#39;, c1=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.kurtosis">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">kurtosis</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.kurtosis" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the kurtosis of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lag">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">lag</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">count=1</em>, <em class="sig-param">default=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#lag"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.lag" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the value that is <cite>offset</cite> rows before the current row, and
<cite>defaultValue</cite> if there is less than <cite>offset</cite> rows before the current row. For example,
an <cite>offset</cite> of one will return the previous row at any given point in the window partition.</p>
<p>This is equivalent to the LAG function in SQL.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – name of column or expression</p></li>
<li><p><strong>count</strong> – number of row to extend</p></li>
<li><p><strong>default</strong> – default value</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.last">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">last</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">ignorenulls=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#last"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.last" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the last value in a group.</p>
<p>The function by default returns the last values it sees. It will return the last non-null
value it sees when ignoreNulls is set to true. If all values are null, then null is returned.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The function is non-deterministic because its results depends on order of rows
which may be non-deterministic after a shuffle.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.last_day">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">last_day</code><span class="sig-paren">(</span><em class="sig-param">date</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#last_day"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.last_day" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the last day of the month which the given date belongs to.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;1997-02-10&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">last_day</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(date=datetime.date(1997, 2, 28))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lead">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">lead</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">count=1</em>, <em class="sig-param">default=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#lead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.lead" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the value that is <cite>offset</cite> rows after the current row, and
<cite>defaultValue</cite> if there is less than <cite>offset</cite> rows after the current row. For example,
an <cite>offset</cite> of one will return the next row at any given point in the window partition.</p>
<p>This is equivalent to the LEAD function in SQL.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – name of column or expression</p></li>
<li><p><strong>count</strong> – number of row to extend</p></li>
<li><p><strong>default</strong> – default value</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.least">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">least</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#least"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.least" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the least value of the list of column names, skipping null values.
This function takes at least 2 parameters. It will return null iff all parameters are null.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">least</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;least&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(least=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.length">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">length</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#length"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.length" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the character length of string data or number of bytes of binary data.
The length of character data includes the trailing spaces. The length of binary data
includes binary zeros.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;ABC &#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;length&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(length=4)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.levenshtein">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">levenshtein</code><span class="sig-paren">(</span><em class="sig-param">left</em>, <em class="sig-param">right</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#levenshtein"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.levenshtein" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Levenshtein distance of the two given strings.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df0</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;kitten&#39;</span><span class="p">,</span> <span class="s1">&#39;sitting&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df0</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">levenshtein</span><span class="p">(</span><span class="s1">&#39;l&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=3)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lit">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">lit</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.lit" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> of literal value.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">lit</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;height&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;spark_user&#39;</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">[Row(height=5, spark_user=True)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.locate">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">locate</code><span class="sig-paren">(</span><em class="sig-param">substr</em>, <em class="sig-param">str</em>, <em class="sig-param">pos=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#locate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.locate" title="Permalink to this definition">¶</a></dt>
<dd><p>Locate the position of the first occurrence of substr in a string column, after position pos.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The position is not zero based, but 1 based index. Returns 0 if substr
could not be found in str.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>substr</strong> – a string</p></li>
<li><p><strong>str</strong> – a Column of <a class="reference internal" href="#pyspark.sql.types.StringType" title="pyspark.sql.types.StringType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StringType</span></code></a></p></li>
<li><p><strong>pos</strong> – start position (zero based)</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;abcd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">locate</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">log</code><span class="sig-paren">(</span><em class="sig-param">arg1</em>, <em class="sig-param">arg2=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#log"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first argument-based logarithm of the second argument.</p>
<p>If there is only one argument, then this takes the natural logarithm of the argument.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;ten&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">ten</span><span class="p">)[:</span><span class="mi">7</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[&#39;0.30102&#39;, &#39;0.69897&#39;]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;e&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">e</span><span class="p">)[:</span><span class="mi">7</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[&#39;0.69314&#39;, &#39;1.60943&#39;]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log10">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">log10</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.log10" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the logarithm of the given value in Base 10.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log1p">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">log1p</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.log1p" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the natural logarithm of the given value plus one.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log2">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">log2</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#log2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.log2" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the base-2 logarithm of the argument.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">4</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">log2</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;log2&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(log2=2.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lower">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">lower</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.lower" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a string column to lower case.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lpad">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">lpad</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">len</em>, <em class="sig-param">pad</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#lpad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.lpad" title="Permalink to this definition">¶</a></dt>
<dd><p>Left-pad the string column to width <cite>len</cite> with <cite>pad</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;abcd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">lpad</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;#&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=&#39;##abcd&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.ltrim">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">ltrim</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.ltrim" title="Permalink to this definition">¶</a></dt>
<dd><p>Trim the spaces from left end for the specified string value.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.map_concat">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">map_concat</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#map_concat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.map_concat" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the union of all the given maps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of column names (string) or list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> expressions</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">map_concat</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT map(1, &#39;a&#39;, 2, &#39;b&#39;) as map1, map(3, &#39;c&#39;, 1, &#39;d&#39;) as map2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">map_concat</span><span class="p">(</span><span class="s2">&quot;map1&quot;</span><span class="p">,</span> <span class="s2">&quot;map2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;map3&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">+--------------------------------+</span>
<span class="go">|map3                            |</span>
<span class="go">+--------------------------------+</span>
<span class="go">|[1 -&gt; a, 2 -&gt; b, 3 -&gt; c, 1 -&gt; d]|</span>
<span class="go">+--------------------------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.map_from_arrays">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">map_from_arrays</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#map_from_arrays"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.map_from_arrays" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new map from two arrays.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col1</strong> – name of column containing a set of keys. All elements should not be null</p></li>
<li><p><strong>col2</strong> – name of column containing a set of values</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">])],</span> <span class="p">[</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">map_from_arrays</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;map&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----------------+</span>
<span class="go">|             map|</span>
<span class="go">+----------------+</span>
<span class="go">|[2 -&gt; a, 5 -&gt; b]|</span>
<span class="go">+----------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.map_from_entries">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">map_from_entries</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#map_from_entries"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.map_from_entries" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: Returns a map created from the given array of entries.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">map_from_entries</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT array(struct(1, &#39;a&#39;), struct(2, &#39;b&#39;)) as data&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">map_from_entries</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;map&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----------------+</span>
<span class="go">|             map|</span>
<span class="go">+----------------+</span>
<span class="go">|[1 -&gt; a, 2 -&gt; b]|</span>
<span class="go">+----------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.map_keys">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">map_keys</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#map_keys"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.map_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: Returns an unordered array containing the keys of the map.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">map_keys</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT map(1, &#39;a&#39;, 2, &#39;b&#39;) as data&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">map_keys</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;keys&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+------+</span>
<span class="go">|  keys|</span>
<span class="go">+------+</span>
<span class="go">|[1, 2]|</span>
<span class="go">+------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.map_values">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">map_values</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#map_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.map_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: Returns an unordered array containing the values of the map.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">map_values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT map(1, &#39;a&#39;, 2, &#39;b&#39;) as data&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">map_values</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;values&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+------+</span>
<span class="go">|values|</span>
<span class="go">+------+</span>
<span class="go">|[a, b]|</span>
<span class="go">+------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.max">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">max</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the maximum value of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.md5">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">md5</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#md5"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.md5" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the MD5 digest and returns the value as a 32 character hex string.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;ABC&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">md5</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;hash&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hash=&#39;902fbdd2b1df0c4f70b4a5d23525e932&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.mean">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">mean</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the average of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.min">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">min</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.min" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the minimum value of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.minute">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">minute</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#minute"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.minute" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the minutes of a given date as integer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08 13:08:15&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;ts&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">minute</span><span class="p">(</span><span class="s1">&#39;ts&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;minute&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(minute=8)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.monotonically_increasing_id">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">monotonically_increasing_id</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#monotonically_increasing_id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.monotonically_increasing_id" title="Permalink to this definition">¶</a></dt>
<dd><p>A column that generates monotonically increasing 64-bit integers.</p>
<p>The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.
The current implementation puts the partition ID in the upper 31 bits, and the record number
within each partition in the lower 33 bits. The assumption is that the data frame has
less than 1 billion partitions, and each partition has less than 8 billion records.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The function is non-deterministic because its result depends on partition IDs.</p>
</div>
<p>As an example, consider a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code> with two partitions, each with 3 records.
This expression would return the following IDs:
0, 1, 2, 8589934592 (1L &lt;&lt; 33), 8589934593, 8589934594.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df0</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)])</span><span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="s1">&#39;col1&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df0</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">monotonically_increasing_id</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.month">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">month</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#month"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.month" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Extract the month of a given date as integer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">month</span><span class="p">(</span><span class="s1">&#39;dt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;month&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(month=4)]</span>
</pre></div>
</div>
</div></blockquote>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.months_between">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">months_between</code><span class="sig-paren">(</span><em class="sig-param">date1</em>, <em class="sig-param">date2</em>, <em class="sig-param">roundOff=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#months_between"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.months_between" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns number of months between dates date1 and date2.
If date1 is later than date2, then the result is positive.
If date1 and date2 are on the same day of month, or both are the last day of month,
returns an integer (time of day will be ignored).
The result is rounded off to 8 digits unless <cite>roundOff</cite> is set to <cite>False</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,</span> <span class="s1">&#39;1996-10-30&#39;</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;date1&#39;</span><span class="p">,</span> <span class="s1">&#39;date2&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">months_between</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">date1</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">date2</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;months&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(months=3.94959677)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">months_between</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">date1</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">date2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;months&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(months=3.9495967741935485)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.nanvl">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">nanvl</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#nanvl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.nanvl" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns col1 if it is not NaN, or col2 if col1 is NaN.</p>
<p>Both inputs should be floating point columns (<code class="xref py py-class docutils literal notranslate"><span class="pre">DoubleType</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">FloatType</span></code>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)),</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">),</span> <span class="mf">2.0</span><span class="p">)],</span> <span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">nanvl</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;r1&quot;</span><span class="p">),</span> <span class="n">nanvl</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;r2&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.next_day">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">next_day</code><span class="sig-paren">(</span><em class="sig-param">date</em>, <em class="sig-param">dayOfWeek</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#next_day"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.next_day" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first date which is later than the value of the date column.</p>
<dl class="simple">
<dt>Day of the week parameter is case insensitive, and accepts:</dt><dd><p>“Mon”, “Tue”, “Wed”, “Thu”, “Fri”, “Sat”, “Sun”.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-07-27&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">next_day</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="s1">&#39;Sun&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(date=datetime.date(2015, 8, 2))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.ntile">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">ntile</code><span class="sig-paren">(</span><em class="sig-param">n</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#ntile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.ntile" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the ntile group id (from 1 to <cite>n</cite> inclusive)
in an ordered window partition. For example, if <cite>n</cite> is 4, the first
quarter of the rows will get value 1, the second quarter will get 2,
the third quarter will get 3, and the last quarter will get 4.</p>
<p>This is equivalent to the NTILE function in SQL.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>n</strong> – an integer</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.pandas_udf">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">pandas_udf</code><span class="sig-paren">(</span><em class="sig-param">f=None</em>, <em class="sig-param">returnType=None</em>, <em class="sig-param">functionType=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#pandas_udf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.pandas_udf" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a vectorized user defined function (UDF).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>f</strong> – user-defined function. A python function if used as a standalone function</p></li>
<li><p><strong>returnType</strong> – the return type of the user-defined function. The value can be either a
<a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType</span></code></a> object or a DDL-formatted type string.</p></li>
<li><p><strong>functionType</strong> – an enum value in <a class="reference internal" href="#pyspark.sql.functions.PandasUDFType" title="pyspark.sql.functions.PandasUDFType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.functions.PandasUDFType</span></code></a>.
Default: SCALAR.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Experimental</p>
</div>
<p>The function type of the UDF can be one of the following:</p>
<ol class="arabic">
<li><p>SCALAR</p>
<p>A scalar UDF defines a transformation: One or more <cite>pandas.Series</cite> -&gt; A <cite>pandas.Series</cite>.
The length of the returned <cite>pandas.Series</cite> must be of the same as the input <cite>pandas.Series</cite>.</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">MapType</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">StructType</span></code> are currently not supported as output types.</p>
<p>Scalar UDFs are used with <a class="reference internal" href="#pyspark.sql.DataFrame.withColumn" title="pyspark.sql.DataFrame.withColumn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.DataFrame.withColumn()</span></code></a> and
<a class="reference internal" href="#pyspark.sql.DataFrame.select" title="pyspark.sql.DataFrame.select"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.DataFrame.select()</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="n">StringType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slen</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len</span><span class="p">(),</span> <span class="n">IntegerType</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="n">StringType</span><span class="p">())</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">to_upper</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s2">&quot;integer&quot;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">SCALAR</span><span class="p">)</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">add_one</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;John Doe&quot;</span><span class="p">,</span> <span class="mi">21</span><span class="p">)],</span>
<span class="gp">... </span>                           <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">slen</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;slen(name)&quot;</span><span class="p">),</span> <span class="n">to_upper</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">),</span> <span class="n">add_one</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">))</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">show</span><span class="p">()</span>  
<span class="go">+----------+--------------+------------+</span>
<span class="go">|slen(name)|to_upper(name)|add_one(age)|</span>
<span class="go">+----------+--------------+------------+</span>
<span class="go">|         8|      JOHN DOE|          22|</span>
<span class="go">+----------+--------------+------------+</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The length of <cite>pandas.Series</cite> within a scalar UDF is not that of the whole input
column, but is the length of an internal batch used for each call to the function.
Therefore, this can be used, for example, to ensure the length of each returned
<cite>pandas.Series</cite>, and can not be used as the column length.</p>
</div>
</li>
<li><p>GROUPED_MAP</p>
<p>A grouped map UDF defines transformation: A <cite>pandas.DataFrame</cite> -&gt; A <cite>pandas.DataFrame</cite>
The returnType should be a <code class="xref py py-class docutils literal notranslate"><span class="pre">StructType</span></code> describing the schema of the returned
<cite>pandas.DataFrame</cite>. The column labels of the returned <cite>pandas.DataFrame</cite> must either match
the field names in the defined returnType schema if specified as strings, or match the
field data types by position if not strings, e.g. integer indices.
The length of the returned <cite>pandas.DataFrame</cite> can be arbitrary.</p>
<p>Grouped map UDFs are used with <a class="reference internal" href="#pyspark.sql.GroupedData.apply" title="pyspark.sql.GroupedData.apply"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.GroupedData.apply()</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s2">&quot;id long, v double&quot;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_MAP</span><span class="p">)</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">pdf</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">v</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">v</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">pdf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">v</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">normalize</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  
<span class="go">+---+-------------------+</span>
<span class="go">| id|                  v|</span>
<span class="go">+---+-------------------+</span>
<span class="go">|  1|-0.7071067811865475|</span>
<span class="go">|  1| 0.7071067811865475|</span>
<span class="go">|  2|-0.8320502943378437|</span>
<span class="go">|  2|-0.2773500981126146|</span>
<span class="go">|  2| 1.1094003924504583|</span>
<span class="go">+---+-------------------+</span>
</pre></div>
</div>
<p>Alternatively, the user can define a function that takes two arguments.
In this case, the grouping key(s) will be passed as the first argument and the data will
be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy
data types, e.g., <cite>numpy.int32</cite> and <cite>numpy.float64</cite>. The data will still be passed in
as a <cite>pandas.DataFrame</cite> containing all columns from the original Spark DataFrame.
This is useful when the user does not want to hardcode grouping key(s) in the function.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s2">&quot;id long, v double&quot;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_MAP</span><span class="p">)</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">mean_udf</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">pdf</span><span class="p">):</span>
<span class="gp">... </span>    <span class="c1"># key is a tuple of one numpy.int64, which is the value</span>
<span class="gp">... </span>    <span class="c1"># of &#39;id&#39; for the current group</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">key</span> <span class="o">+</span> <span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">(),)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">mean_udf</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  
<span class="go">+---+---+</span>
<span class="go">| id|  v|</span>
<span class="go">+---+---+</span>
<span class="go">|  1|1.5|</span>
<span class="go">|  2|6.0|</span>
<span class="go">+---+---+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span>
<span class="gp">... </span>   <span class="s2">&quot;id long, `ceil(v / 2)` long, v double&quot;</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_MAP</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">sum_udf</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">pdf</span><span class="p">):</span>
<span class="gp">... </span>    <span class="c1"># key is a tuple of two numpy.int64s, which is the values</span>
<span class="gp">... </span>    <span class="c1"># of &#39;id&#39; and &#39;ceil(df.v / 2)&#39; for the current group</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">key</span> <span class="o">+</span> <span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">v</span><span class="o">.</span><span class="n">sum</span><span class="p">(),)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">ceil</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">v</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">sum_udf</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  
<span class="go">+---+-----------+----+</span>
<span class="go">| id|ceil(v / 2)|   v|</span>
<span class="go">+---+-----------+----+</span>
<span class="go">|  2|          5|10.0|</span>
<span class="go">|  1|          1| 3.0|</span>
<span class="go">|  2|          3| 5.0|</span>
<span class="go">|  2|          2| 3.0|</span>
<span class="go">+---+-----------+----+</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If returning a new <cite>pandas.DataFrame</cite> constructed with a dictionary, it is
recommended to explicitly index the columns by name to ensure the positions are correct,
or alternatively use an <cite>OrderedDict</cite>.
For example, <cite>pd.DataFrame({‘id’: ids, ‘a’: data}, columns=[‘id’, ‘a’])</cite> or
<cite>pd.DataFrame(OrderedDict([(‘id’, ids), (‘a’, data)]))</cite>.</p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#pyspark.sql.GroupedData.apply" title="pyspark.sql.GroupedData.apply"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.GroupedData.apply()</span></code></a></p>
</div>
</li>
<li><p>GROUPED_AGG</p>
<p>A grouped aggregate UDF defines a transformation: One or more <cite>pandas.Series</cite> -&gt; A scalar
The <cite>returnType</cite> should be a primitive data type, e.g., <code class="xref py py-class docutils literal notranslate"><span class="pre">DoubleType</span></code>.
The returned scalar can be either a python primitive type, e.g., <cite>int</cite> or <cite>float</cite>
or a numpy data type, e.g., <cite>numpy.int64</cite> or <cite>numpy.float64</cite>.</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">MapType</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">StructType</span></code> are currently not supported as output types.</p>
<p>Group aggregate UDFs are used with <a class="reference internal" href="#pyspark.sql.GroupedData.agg" title="pyspark.sql.GroupedData.agg"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.GroupedData.agg()</span></code></a> and
<a class="reference internal" href="#pyspark.sql.Window" title="pyspark.sql.Window"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.Window</span></code></a></p>
<p>This example shows using grouped aggregated UDFs with groupby:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_AGG</span><span class="p">)</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">mean_udf</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">mean_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;v&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  
<span class="go">+---+-----------+</span>
<span class="go">| id|mean_udf(v)|</span>
<span class="go">+---+-----------+</span>
<span class="go">|  1|        1.5|</span>
<span class="go">|  2|        6.0|</span>
<span class="go">+---+-----------+</span>
</pre></div>
</div>
<p>This example shows using grouped aggregated UDFs as window functions. Note that only
unbounded window frame is supported at the moment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="k">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_AGG</span><span class="p">)</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">mean_udf</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">Window</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">rowsBetween</span><span class="p">(</span><span class="n">Window</span><span class="o">.</span><span class="n">unboundedPreceding</span><span class="p">,</span> <span class="n">Window</span><span class="o">.</span><span class="n">unboundedFollowing</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;mean_v&#39;</span><span class="p">,</span> <span class="n">mean_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;v&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">w</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  
<span class="go">+---+----+------+</span>
<span class="go">| id|   v|mean_v|</span>
<span class="go">+---+----+------+</span>
<span class="go">|  1| 1.0|   1.5|</span>
<span class="go">|  1| 2.0|   1.5|</span>
<span class="go">|  2| 3.0|   6.0|</span>
<span class="go">|  2| 5.0|   6.0|</span>
<span class="go">|  2|10.0|   6.0|</span>
<span class="go">+---+----+------+</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#pyspark.sql.GroupedData.agg" title="pyspark.sql.GroupedData.agg"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyspark.sql.GroupedData.agg()</span></code></a> and <a class="reference internal" href="#pyspark.sql.Window" title="pyspark.sql.Window"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.Window</span></code></a></p>
</div>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user-defined functions are considered deterministic by default. Due to
optimization, duplicate invocations may be eliminated or the function may even be invoked
more times than it is present in the query. If your function is not deterministic, call
<cite>asNondeterministic</cite> on the user defined function. E.g.:</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s1">&#39;double&#39;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">SCALAR</span><span class="p">)</span>  
<span class="gp">... </span><span class="k">def</span> <span class="nf">random</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
<span class="gp">... </span>    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">... </span>    <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">asNondeterministic</span><span class="p">()</span>  
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user-defined functions do not support conditional expressions or short circuiting
in boolean expressions and it ends up with being executed all internally. If the functions
can fail on special rows, the workaround is to incorporate the condition into the functions.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user-defined functions do not take keyword arguments on the calling side.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.percent_rank">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">percent_rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.percent_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the relative rank (i.e. percentile) of rows within a window partition.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.posexplode">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">posexplode</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#posexplode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.posexplode" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new row for each element with position in the given array or map.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">intlist</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">mapfield</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="s2">&quot;b&quot;</span><span class="p">})])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">posexplode</span><span class="p">(</span><span class="n">eDF</span><span class="o">.</span><span class="n">intlist</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">posexplode</span><span class="p">(</span><span class="n">eDF</span><span class="o">.</span><span class="n">mapfield</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+---+-----+</span>
<span class="go">|pos|key|value|</span>
<span class="go">+---+---+-----+</span>
<span class="go">|  0|  a|    b|</span>
<span class="go">+---+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.posexplode_outer">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">posexplode_outer</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#posexplode_outer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.posexplode_outer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new row for each element with position in the given array or map.
Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="s2">&quot;bar&quot;</span><span class="p">],</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">[],</span> <span class="p">{}),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;an_array&quot;</span><span class="p">,</span> <span class="s2">&quot;a_map&quot;</span><span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;an_array&quot;</span><span class="p">,</span> <span class="n">posexplode_outer</span><span class="p">(</span><span class="s2">&quot;a_map&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+----------+----+----+-----+</span>
<span class="go">| id|  an_array| pos| key|value|</span>
<span class="go">+---+----------+----+----+-----+</span>
<span class="go">|  1|[foo, bar]|   0|   x|  1.0|</span>
<span class="go">|  2|        []|null|null| null|</span>
<span class="go">|  3|      null|null|null| null|</span>
<span class="go">+---+----------+----+----+-----+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;a_map&quot;</span><span class="p">,</span> <span class="n">posexplode_outer</span><span class="p">(</span><span class="s2">&quot;an_array&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+----------+----+----+</span>
<span class="go">| id|     a_map| pos| col|</span>
<span class="go">+---+----------+----+----+</span>
<span class="go">|  1|[x -&gt; 1.0]|   0| foo|</span>
<span class="go">|  1|[x -&gt; 1.0]|   1| bar|</span>
<span class="go">|  2|        []|null|null|</span>
<span class="go">|  3|      null|null|null|</span>
<span class="go">+---+----------+----+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.pow">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">pow</code><span class="sig-paren">(</span><em class="sig-param">col1</em>, <em class="sig-param">col2</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.pow" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of the first argument raised to the power of the second argument.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.quarter">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">quarter</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#quarter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.quarter" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the quarter of a given date as integer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">quarter</span><span class="p">(</span><span class="s1">&#39;dt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;quarter&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(quarter=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.radians">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">radians</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.radians" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an angle measured in degrees to an approximately equivalent angle
measured in radians.
:param col: angle in degrees
:return: angle in radians, as if computed by <cite>java.lang.Math.toRadians()</cite></p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rand">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">rand</code><span class="sig-paren">(</span><em class="sig-param">seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#rand"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.rand" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a random column with independent and identically distributed (i.i.d.) samples
from U[0.0, 1.0].</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The function is non-deterministic in general case.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;rand&#39;</span><span class="p">,</span> <span class="n">rand</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;, rand=1.1568609015300986),</span>
<span class="go"> Row(age=5, name=&#39;Bob&#39;, rand=1.403379671529166)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.randn">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">randn</code><span class="sig-paren">(</span><em class="sig-param">seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#randn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.randn" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a column with independent and identically distributed (i.i.d.) samples from
the standard normal distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The function is non-deterministic in general case.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;randn&#39;</span><span class="p">,</span> <span class="n">randn</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=&#39;Alice&#39;, randn=-0.7556247885860078),</span>
<span class="go">Row(age=5, name=&#39;Bob&#39;, randn=-0.0861619008451133)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rank">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the rank of rows within a window partition.</p>
<p>The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking
sequence when there are ties. That is, if you were ranking a competition using dense_rank
and had three people tie for second place, you would say that all three were in second
place and that the next person came in third. Rank would give me sequential numbers, making
the person that came in third place (after the ties) would register as coming in fifth.</p>
<p>This is equivalent to the RANK function in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.regexp_extract">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">regexp_extract</code><span class="sig-paren">(</span><em class="sig-param">str</em>, <em class="sig-param">pattern</em>, <em class="sig-param">idx</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#regexp_extract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.regexp_extract" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract a specific group matched by a Java regex, from the specified string column.
If the regex did not match, or the specified group did not match, an empty string is returned.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;100-200&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;str&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">regexp_extract</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;(\d+)-(\d+)&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=&#39;100&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;foo&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;str&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">regexp_extract</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;(\d+)&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=&#39;&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;aaaac&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;str&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">regexp_extract</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">,</span> <span class="s1">&#39;(a+)(b)?(c)&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=&#39;&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.regexp_replace">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">regexp_replace</code><span class="sig-paren">(</span><em class="sig-param">str</em>, <em class="sig-param">pattern</em>, <em class="sig-param">replacement</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#regexp_replace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.regexp_replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace all substrings of the specified string value that match regexp with rep.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;100-200&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;str&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">regexp_replace</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;(\d+)&#39;</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=&#39;-----&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.repeat">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">repeat</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">n</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#repeat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.repeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeats a string column n times, and returns it as a new string column.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;ab&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">repeat</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=&#39;ababab&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.reverse">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">reverse</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#reverse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.reverse" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns a reversed string or an array with reverse order of elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;Spark SQL&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">reverse</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=&#39;LQS krapS&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],)</span> <span class="p">,([</span><span class="mi">1</span><span class="p">],)</span> <span class="p">,([],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">reverse</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rint">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">rint</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.rint" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the double value that is closest in value to the argument and is equal to a mathematical integer.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.round">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">round</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">scale=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#round"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.round" title="Permalink to this definition">¶</a></dt>
<dd><p>Round the given value to <cite>scale</cite> decimal places using HALF_UP rounding mode if <cite>scale</cite> &gt;= 0
or at integral part when <cite>scale</cite> &lt; 0.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mf">2.5</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=3.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.row_number">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">row_number</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.row_number" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns a sequential number starting at 1 within a window partition.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rpad">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">rpad</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">len</em>, <em class="sig-param">pad</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#rpad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.rpad" title="Permalink to this definition">¶</a></dt>
<dd><p>Right-pad the string column to width <cite>len</cite> with <cite>pad</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;abcd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">rpad</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;#&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=&#39;abcd##&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rtrim">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">rtrim</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.rtrim" title="Permalink to this definition">¶</a></dt>
<dd><p>Trim the spaces from right end for the specified string value.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.schema_of_json">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">schema_of_json</code><span class="sig-paren">(</span><em class="sig-param">json</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#schema_of_json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.schema_of_json" title="Permalink to this definition">¶</a></dt>
<dd><p>Parses a JSON string and infers its schema in DDL format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>json</strong> – a JSON string or a string literal containing a JSON string.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">schema_of_json</span><span class="p">(</span><span class="s1">&#39;{&quot;a&quot;: 0}&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=&#39;struct&lt;a:bigint&gt;&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.second">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">second</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#second"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.second" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the seconds of a given date as integer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08 13:08:15&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;ts&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">second</span><span class="p">(</span><span class="s1">&#39;ts&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;second&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(second=15)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sequence">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">sequence</code><span class="sig-paren">(</span><em class="sig-param">start</em>, <em class="sig-param">stop</em>, <em class="sig-param">step=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a sequence of integers from <cite>start</cite> to <cite>stop</cite>, incrementing by <cite>step</cite>.
If <cite>step</cite> is not set, incrementing by 1 if <cite>start</cite> is less than or equal to <cite>stop</cite>,
otherwise -1.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> <span class="p">(</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span> <span class="s1">&#39;C2&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sequence</span><span class="p">(</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span> <span class="s1">&#39;C2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=[-2, -1, 0, 1, 2])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)],</span> <span class="p">(</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span> <span class="s1">&#39;C2&#39;</span><span class="p">,</span> <span class="s1">&#39;C3&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sequence</span><span class="p">(</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span> <span class="s1">&#39;C2&#39;</span><span class="p">,</span> <span class="s1">&#39;C3&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=[4, 2, 0, -2, -4])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sha1">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">sha1</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#sha1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.sha1" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the hex string result of SHA-1.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;ABC&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sha1</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;hash&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hash=&#39;3c01bdbb26f358bab27f267924aa2c9a03fcfdb8&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sha2">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">sha2</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">numBits</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#sha2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.sha2" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,
and SHA-512). The numBits indicates the desired bit length of the result, which must have a
value of 224, 256, 384, 512, or 0 (which is equivalent to 256).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">digests</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sha2</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">digests</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">Row(s=&#39;3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">digests</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">Row(s=&#39;cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961&#39;)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.shiftLeft">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">shiftLeft</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">numBits</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#shiftLeft"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.shiftLeft" title="Permalink to this definition">¶</a></dt>
<dd><p>Shift the given value numBits left.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">21</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">shiftLeft</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=42)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.shiftRight">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">shiftRight</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">numBits</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#shiftRight"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.shiftRight" title="Permalink to this definition">¶</a></dt>
<dd><p>(Signed) shift the given value numBits right.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">42</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">shiftRight</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=21)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.shiftRightUnsigned">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">shiftRightUnsigned</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">numBits</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#shiftRightUnsigned"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.shiftRightUnsigned" title="Permalink to this definition">¶</a></dt>
<dd><p>Unsigned shift the given value numBits right.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="o">-</span><span class="mi">42</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">shiftRightUnsigned</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=9223372036854775787)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.shuffle">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">shuffle</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#shuffle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.shuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: Generates a random permutation of the given array.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The function is non-deterministic.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],),</span> <span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">shuffle</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.signum">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">signum</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.signum" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the signum of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sin">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">sin</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.sin" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – angle in radians</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>sine of the angle, as if computed by <cite>java.lang.Math.sin()</cite></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sinh">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">sinh</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.sinh" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – hyperbolic angle</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>hyperbolic sine of the given value,
as if computed by <cite>java.lang.Math.sinh()</cite></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.size">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">size</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.size" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns the length of the array or map stored in the column.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],),([</span><span class="mi">1</span><span class="p">],),([],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.skewness">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">skewness</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.skewness" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the skewness of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.slice">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">slice</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">start</em>, <em class="sig-param">length</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#slice"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.slice" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns an array containing  all the elements in <cite>x</cite> from index <cite>start</cite>
(or starting from the end if <cite>start</cite> is negative) with the specified <cite>length</cite>.
&gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], [‘x’])
&gt;&gt;&gt; df.select(slice(df.x, 2, 2).alias(“sliced”)).collect()
[Row(sliced=[2, 3]), Row(sliced=[5])]</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sort_array">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">sort_array</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">asc=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#sort_array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.sort_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: sorts the input array in ascending or descending order according
to the natural ordering of the array elements. Null elements will be placed at the beginning
of the returned array in ascending order or at the end of the returned array in descending
order.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – name of column or expression</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">],),([</span><span class="mi">1</span><span class="p">],),([],)],</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sort_array</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sort_array</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">asc</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.soundex">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">soundex</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#soundex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.soundex" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the SoundEx encoding for a string</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s2">&quot;Peters&quot;</span><span class="p">,),(</span><span class="s2">&quot;Uhrbach&quot;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">soundex</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;soundex&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(soundex=&#39;P362&#39;), Row(soundex=&#39;U612&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.spark_partition_id">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">spark_partition_id</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#spark_partition_id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.spark_partition_id" title="Permalink to this definition">¶</a></dt>
<dd><p>A column for partition ID.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is indeterministic because it depends on data partitioning and task scheduling.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">spark_partition_id</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;pid&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(pid=0), Row(pid=0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.split">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">split</code><span class="sig-paren">(</span><em class="sig-param">str</em>, <em class="sig-param">pattern</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#split"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits str around pattern (pattern is a regular expression).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>pattern is a string represent the regular expression.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;ab12cd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">split</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="s1">&#39;[0-9]+&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=[&#39;ab&#39;, &#39;cd&#39;])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sqrt">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">sqrt</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the square root of the specified float value.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.stddev">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">stddev</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.stddev" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: alias for stddev_samp.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.stddev_pop">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">stddev_pop</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.stddev_pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns population standard deviation of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.stddev_samp">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">stddev_samp</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.stddev_samp" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the unbiased sample standard deviation of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.struct">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">struct</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#struct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.struct" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new struct column.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – list of column names (string) or list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> expressions</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">struct</span><span class="p">(</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;struct&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(struct=Row(age=2, name=&#39;Alice&#39;)), Row(struct=Row(age=5, name=&#39;Bob&#39;))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">struct</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">])</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;struct&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(struct=Row(age=2, name=&#39;Alice&#39;)), Row(struct=Row(age=5, name=&#39;Bob&#39;))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.substring">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">substring</code><span class="sig-paren">(</span><em class="sig-param">str</em>, <em class="sig-param">pos</em>, <em class="sig-param">len</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#substring"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.substring" title="Permalink to this definition">¶</a></dt>
<dd><p>Substring starts at <cite>pos</cite> and is of length <cite>len</cite> when str is String type or
returns the slice of byte array that starts at <cite>pos</cite> in byte and is of length <cite>len</cite>
when str is Binary type.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The position is not zero based, but 1 based index.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;abcd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">substring</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=&#39;ab&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.substring_index">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">substring_index</code><span class="sig-paren">(</span><em class="sig-param">str</em>, <em class="sig-param">delim</em>, <em class="sig-param">count</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#substring_index"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.substring_index" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the substring from string str before count occurrences of the delimiter delim.
If count is positive, everything the left of the final delimiter (counting from left) is
returned. If count is negative, every to the right of the final delimiter (counting from the
right) is returned. substring_index performs a case-sensitive match when searching for delim.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;a.b.c.d&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">substring_index</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=&#39;a.b&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">substring_index</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=&#39;b.c.d&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sum">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">sum</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the sum of all values in the expression.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sumDistinct">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">sumDistinct</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.sumDistinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the sum of distinct values in the expression.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.tan">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">tan</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.tan" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – angle in radians</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tangent of the given value, as if computed by <cite>java.lang.Math.tan()</cite></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.tanh">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">tanh</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.tanh" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>col</strong> – hyperbolic angle</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>hyperbolic tangent of the given value,
as if computed by <cite>java.lang.Math.tanh()</cite></p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.toDegrees">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">toDegrees</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.toDegrees" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deprecated in 2.1, use <a class="reference internal" href="#pyspark.sql.functions.degrees" title="pyspark.sql.functions.degrees"><code class="xref py py-func docutils literal notranslate"><span class="pre">degrees()</span></code></a> instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.toRadians">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">toRadians</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.toRadians" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Deprecated in 2.1, use <a class="reference internal" href="#pyspark.sql.functions.radians" title="pyspark.sql.functions.radians"><code class="xref py py-func docutils literal notranslate"><span class="pre">radians()</span></code></a> instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.to_date">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">to_date</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">format=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#to_date"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.to_date" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> of <a class="reference internal" href="#pyspark.sql.types.StringType" title="pyspark.sql.types.StringType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StringType</span></code></a> or
<a class="reference internal" href="#pyspark.sql.types.TimestampType" title="pyspark.sql.types.TimestampType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.TimestampType</span></code></a> into <a class="reference internal" href="#pyspark.sql.types.DateType" title="pyspark.sql.types.DateType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DateType</span></code></a>
using the optionally specified format. Specify formats according to
<a class="reference external" href="http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html">SimpleDateFormats</a>.
By default, it follows casting rules to <a class="reference internal" href="#pyspark.sql.types.DateType" title="pyspark.sql.types.DateType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DateType</span></code></a> if the format
is omitted (equivalent to <code class="docutils literal notranslate"><span class="pre">col.cast(&quot;date&quot;)</span></code>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_date</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(date=datetime.date(1997, 2, 28))]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_date</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;yyyy-MM-dd HH:mm:ss&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(date=datetime.date(1997, 2, 28))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.to_json">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">to_json</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">options={}</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#to_json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.to_json" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a column containing a <code class="xref py py-class docutils literal notranslate"><span class="pre">StructType</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ArrayType</span></code> or a <code class="xref py py-class docutils literal notranslate"><span class="pre">MapType</span></code>
into a JSON string. Throws an exception, in the case of an unsupported type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col</strong> – name of column containing a struct, an array or a map.</p></li>
<li><p><strong>options</strong> – options to control converting. accepts the same options as the JSON datasource</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">2</span><span class="p">))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=&#39;{&quot;age&quot;:2,&quot;name&quot;:&quot;Alice&quot;}&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Bob&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">3</span><span class="p">)])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=&#39;[{&quot;age&quot;:2,&quot;name&quot;:&quot;Alice&quot;},{&quot;age&quot;:3,&quot;name&quot;:&quot;Bob&quot;}]&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Alice&quot;</span><span class="p">})]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=&#39;{&quot;name&quot;:&quot;Alice&quot;}&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="p">[{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Alice&quot;</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Bob&quot;</span><span class="p">}])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=&#39;[{&quot;name&quot;:&quot;Alice&quot;},{&quot;name&quot;:&quot;Bob&quot;}]&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;Alice&quot;</span><span class="p">,</span> <span class="s2">&quot;Bob&quot;</span><span class="p">])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_json</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(json=&#39;[&quot;Alice&quot;,&quot;Bob&quot;]&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.to_timestamp">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">to_timestamp</code><span class="sig-paren">(</span><em class="sig-param">col</em>, <em class="sig-param">format=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#to_timestamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.to_timestamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> of <a class="reference internal" href="#pyspark.sql.types.StringType" title="pyspark.sql.types.StringType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StringType</span></code></a> or
<a class="reference internal" href="#pyspark.sql.types.TimestampType" title="pyspark.sql.types.TimestampType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.TimestampType</span></code></a> into <a class="reference internal" href="#pyspark.sql.types.DateType" title="pyspark.sql.types.DateType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DateType</span></code></a>
using the optionally specified format. Specify formats according to
<a class="reference external" href="http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html">SimpleDateFormats</a>.
By default, it follows casting rules to <a class="reference internal" href="#pyspark.sql.types.TimestampType" title="pyspark.sql.types.TimestampType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.TimestampType</span></code></a> if the format
is omitted (equivalent to <code class="docutils literal notranslate"><span class="pre">col.cast(&quot;timestamp&quot;)</span></code>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_timestamp</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;dt&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_timestamp</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;yyyy-MM-dd HH:mm:ss&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;dt&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.to_utc_timestamp">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">to_utc_timestamp</code><span class="sig-paren">(</span><em class="sig-param">timestamp</em>, <em class="sig-param">tz</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#to_utc_timestamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.to_utc_timestamp" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function
takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given
timezone, and renders that timestamp as a timestamp in UTC.</p>
<p>However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not
timezone-agnostic. So in Spark this function just shift the timestamp value from the given
timezone to UTC timezone.</p>
<p>This function may return confusing result if the input is a string with timezone, e.g.
‘2018-03-13T06:18:23+00:00’. The reason is that, Spark firstly cast the string to timestamp
according to the timezone in the string, and finally display the result by converting the
timestamp to string according to the session local timezone.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>timestamp</strong> – the column that contains timestamps</p></li>
<li><p><strong>tz</strong> – a string that has the ID of timezone, e.g. “GMT”, “America/Los_Angeles”, etc</p></li>
</ul>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.4: </span><cite>tz</cite> can take a <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> containing timezone ID strings.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,</span> <span class="s1">&#39;JST&#39;</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;ts&#39;</span><span class="p">,</span> <span class="s1">&#39;tz&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_utc_timestamp</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">ts</span><span class="p">,</span> <span class="s2">&quot;PST&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;utc_time&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_utc_timestamp</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">tz</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;utc_time&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.translate">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">translate</code><span class="sig-paren">(</span><em class="sig-param">srcCol</em>, <em class="sig-param">matching</em>, <em class="sig-param">replace</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#translate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.translate" title="Permalink to this definition">¶</a></dt>
<dd><p>A function translate any character in the <cite>srcCol</cite> by a character in <cite>matching</cite>.
The characters in <cite>replace</cite> is corresponding to the characters in <cite>matching</cite>.
The translate will happen when any character in the string matching with the character
in the <cite>matching</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;translate&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">translate</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s2">&quot;rnlt&quot;</span><span class="p">,</span> <span class="s2">&quot;123&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=&#39;1a2s3ae&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.trim">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">trim</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.trim" title="Permalink to this definition">¶</a></dt>
<dd><p>Trim the spaces from both ends for the specified string column.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.trunc">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">trunc</code><span class="sig-paren">(</span><em class="sig-param">date</em>, <em class="sig-param">format</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#trunc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.trunc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns date truncated to the unit specified by the format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>format</strong> – ‘year’, ‘yyyy’, ‘yy’ or ‘month’, ‘mon’, ‘mm’</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;1997-02-28&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">trunc</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="s1">&#39;year&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(year=datetime.date(1997, 1, 1))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">trunc</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="s1">&#39;mon&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;month&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(month=datetime.date(1997, 2, 1))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.udf">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">udf</code><span class="sig-paren">(</span><em class="sig-param">f=None</em>, <em class="sig-param">returnType=StringType</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#udf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.udf" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a user defined function (UDF).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user-defined functions are considered deterministic by default. Due to
optimization, duplicate invocations may be eliminated or the function may even be invoked
more times than it is present in the query. If your function is not deterministic, call
<cite>asNondeterministic</cite> on the user defined function. E.g.:</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">random</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_udf</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span><span class="o">.</span><span class="n">asNondeterministic</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user-defined functions do not support conditional expressions or short circuiting
in boolean expressions and it ends up with being executed all internally. If the functions
can fail on special rows, the workaround is to incorporate the condition into the functions.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user-defined functions do not take keyword arguments on the calling side.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>f</strong> – python function if used as a standalone function</p></li>
<li><p><strong>returnType</strong> – the return type of the user-defined function. The value can be either a
<a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.DataType</span></code></a> object or a DDL-formatted type string.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slen</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">udf</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">to_upper</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">:</span><span class="n">udf</span><span class="p">(</span><span class="n">returnType</span><span class="o">=</span><span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">add_one</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;John Doe&quot;</span><span class="p">,</span> <span class="mi">21</span><span class="p">)],</span> <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">slen</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;slen(name)&quot;</span><span class="p">),</span> <span class="n">to_upper</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">),</span> <span class="n">add_one</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----------+--------------+------------+</span>
<span class="go">|slen(name)|to_upper(name)|add_one(age)|</span>
<span class="go">+----------+--------------+------------+</span>
<span class="go">|         8|      JOHN DOE|          22|</span>
<span class="go">+----------+--------------+------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.unbase64">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">unbase64</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.unbase64" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes a BASE64 encoded string column and returns it as a binary column.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.unhex">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">unhex</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#unhex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.unhex" title="Permalink to this definition">¶</a></dt>
<dd><p>Inverse of hex. Interprets each pair of characters as a hexadecimal number
and converts to the byte representation of number.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;414243&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">unhex</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(unhex(a)=bytearray(b&#39;ABC&#39;))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.unix_timestamp">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">unix_timestamp</code><span class="sig-paren">(</span><em class="sig-param">timestamp=None</em>, <em class="sig-param">format='yyyy-MM-dd HH:mm:ss'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#unix_timestamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.unix_timestamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert time string with given pattern (‘yyyy-MM-dd HH:mm:ss’, by default)
to Unix time stamp (in seconds), using the default timezone and the default
locale, return null if fail.</p>
<p>if <cite>timestamp</cite> is None, then it returns current timestamp.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.session.timeZone&quot;</span><span class="p">,</span> <span class="s2">&quot;America/Los_Angeles&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">time_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">time_df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">unix_timestamp</span><span class="p">(</span><span class="s1">&#39;dt&#39;</span><span class="p">,</span> <span class="s1">&#39;yyyy-MM-dd&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;unix_time&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(unix_time=1428476400)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">unset</span><span class="p">(</span><span class="s2">&quot;spark.sql.session.timeZone&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.upper">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">upper</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.upper" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a string column to upper case.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.var_pop">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">var_pop</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.var_pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the population variance of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.var_samp">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">var_samp</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.var_samp" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the unbiased sample variance of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.variance">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">variance</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="headerlink" href="#pyspark.sql.functions.variance" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: alias for var_samp.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.weekofyear">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">weekofyear</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#weekofyear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.weekofyear" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the week number of a given date as integer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">weekofyear</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">dt</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;week&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(week=15)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.when">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">when</code><span class="sig-paren">(</span><em class="sig-param">condition</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#when"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.when" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions.
If <code class="xref py py-func docutils literal notranslate"><span class="pre">Column.otherwise()</span></code> is not invoked, None is returned for unmatched conditions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>condition</strong> – a boolean <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> expression.</p></li>
<li><p><strong>value</strong> – a literal value, or a <code class="xref py py-class docutils literal notranslate"><span class="pre">Column</span></code> expression.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=3), Row(age=4)]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=3), Row(age=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.window">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">window</code><span class="sig-paren">(</span><em class="sig-param">timeColumn</em>, <em class="sig-param">windowDuration</em>, <em class="sig-param">slideDuration=None</em>, <em class="sig-param">startTime=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#window"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.window" title="Permalink to this definition">¶</a></dt>
<dd><p>Bucketize rows into one or more time windows given a timestamp specifying column. Window
starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
[12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
the order of months are not supported.</p>
<p>The time column must be of <a class="reference internal" href="#pyspark.sql.types.TimestampType" title="pyspark.sql.types.TimestampType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.TimestampType</span></code></a>.</p>
<p>Durations are provided as strings, e.g. ‘1 second’, ‘1 day 12 hours’, ‘2 minutes’. Valid
interval strings are ‘week’, ‘day’, ‘hour’, ‘minute’, ‘second’, ‘millisecond’, ‘microsecond’.
If the <code class="docutils literal notranslate"><span class="pre">slideDuration</span></code> is not provided, the windows will be tumbling windows.</p>
<p>The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start
window intervals. For example, in order to have hourly tumbling windows that start 15 minutes
past the hour, e.g. 12:15-13:15, 13:15-14:15… provide <cite>startTime</cite> as <cite>15 minutes</cite>.</p>
<p>The output column will be a struct called ‘window’ by default with the nested columns ‘start’
and ‘end’, where ‘start’ and ‘end’ will be of <a class="reference internal" href="#pyspark.sql.types.TimestampType" title="pyspark.sql.types.TimestampType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.TimestampType</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s2">&quot;2016-03-11 09:00:07&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">,</span> <span class="s2">&quot;val&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">window</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">,</span> <span class="s2">&quot;5 seconds&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">window</span><span class="o">.</span><span class="n">start</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&quot;string&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;start&quot;</span><span class="p">),</span>
<span class="gp">... </span>         <span class="n">w</span><span class="o">.</span><span class="n">window</span><span class="o">.</span><span class="n">end</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&quot;string&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;end&quot;</span><span class="p">),</span> <span class="s2">&quot;sum&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(start=&#39;2016-03-11 09:00:05&#39;, end=&#39;2016-03-11 09:00:10&#39;, sum=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.year">
<code class="sig-prename descclassname">pyspark.sql.functions.</code><code class="sig-name descname">year</code><span class="sig-paren">(</span><em class="sig-param">col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/functions.html#year"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.year" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the year of a given date as integer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s1">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s1">&#39;dt&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">year</span><span class="p">(</span><span class="s1">&#39;dt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(year=2015)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.5.</span></p>
</div>
</dd></dl>

</div>
<div class="section" id="module-pyspark.sql.streaming">
<span id="pyspark-sql-streaming-module"></span><h2>pyspark.sql.streaming module<a class="headerlink" href="#module-pyspark.sql.streaming" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pyspark.sql.streaming.StreamingQuery">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.streaming.</code><code class="sig-name descname">StreamingQuery</code><span class="sig-paren">(</span><em class="sig-param">jsq</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery" title="Permalink to this definition">¶</a></dt>
<dd><p>A handle to a query that is executing continuously in the background as new data arrives.
All these methods are thread-safe.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.awaitTermination">
<code class="sig-name descname">awaitTermination</code><span class="sig-paren">(</span><em class="sig-param">timeout=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.awaitTermination"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.awaitTermination" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for the termination of <cite>this</cite> query, either by <code class="xref py py-func docutils literal notranslate"><span class="pre">query.stop()</span></code> or by an
exception. If the query has terminated with an exception, then the exception will be thrown.
If <cite>timeout</cite> is set, it returns whether the query has terminated or not within the
<cite>timeout</cite> seconds.</p>
<p>If the query has terminated, then all subsequent calls to this method will either return
immediately (if the query was terminated by <a class="reference internal" href="#pyspark.sql.streaming.StreamingQuery.stop" title="pyspark.sql.streaming.StreamingQuery.stop"><code class="xref py py-func docutils literal notranslate"><span class="pre">stop()</span></code></a>), or throw the exception
immediately (if the query has terminated with exception).</p>
<p>throws <code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQueryException</span></code>, if <cite>this</cite> query has terminated with an exception</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.exception">
<code class="sig-name descname">exception</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.exception"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.exception" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the StreamingQueryException if the query was terminated by an exception, or None.</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.explain">
<code class="sig-name descname">explain</code><span class="sig-paren">(</span><em class="sig-param">extended=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.explain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.explain" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the (logical and physical) plans to the console for debugging purpose.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>extended</strong> – boolean, default <code class="docutils literal notranslate"><span class="pre">False</span></code>. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, prints only the physical plan.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;memory&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="s1">&#39;query_explain&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">processAllAvailable</span><span class="p">()</span> <span class="c1"># Wait a bit to generate the runtime plans.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">explain</span><span class="p">()</span>
<span class="go">== Physical Plan ==</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">== Parsed Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Analyzed Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Optimized Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Physical Plan ==</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.id">
<em class="property">property </em><code class="sig-name descname">id</code><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.id" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the unique id of this query that persists across restarts from checkpoint data.
That is, this id is generated when a query is started for the first time, and
will be the same every time it is restarted from checkpoint data.
There can only be one query with the same id active in a Spark cluster.
Also see, <cite>runId</cite>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.isActive">
<em class="property">property </em><code class="sig-name descname">isActive</code><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.isActive" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether this streaming query is currently active or not.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.lastProgress">
<em class="property">property </em><code class="sig-name descname">lastProgress</code><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.lastProgress" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the most recent <code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQueryProgress</span></code> update of this streaming query or
None if there were no progress updates
:return: a map</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.name" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the user-specified name of the query, or null if not specified.
This name can be specified in the <cite>org.apache.spark.sql.streaming.DataStreamWriter</cite>
as <cite>dataframe.writeStream.queryName(“query”).start()</cite>.
This name, if set, must be unique across all active queries.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.processAllAvailable">
<code class="sig-name descname">processAllAvailable</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.processAllAvailable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.processAllAvailable" title="Permalink to this definition">¶</a></dt>
<dd><p>Blocks until all available data in the source has been processed and committed to the
sink. This method is intended for testing.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case of continually arriving data, this method may block forever.
Additionally, this method is only guaranteed to block until data that has been
synchronously appended data to a stream source prior to invocation.
(i.e. <cite>getOffset</cite> must immediately reflect the addition).</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.recentProgress">
<em class="property">property </em><code class="sig-name descname">recentProgress</code><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.recentProgress" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an array of the most recent [[StreamingQueryProgress]] updates for this query.
The number of progress updates retained for each stream is configured by Spark session
configuration <cite>spark.sql.streaming.numRecentProgressUpdates</cite>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.runId">
<em class="property">property </em><code class="sig-name descname">runId</code><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.runId" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the unique id of this query that does not persist across restarts. That is, every
query that is started (or restarted from checkpoint) will have a different runId.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.status">
<em class="property">property </em><code class="sig-name descname">status</code><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.status" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current status of the query.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.stop">
<code class="sig-name descname">stop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.stop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stop this streaming query.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.streaming.StreamingQueryManager">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.streaming.</code><code class="sig-name descname">StreamingQueryManager</code><span class="sig-paren">(</span><em class="sig-param">jsqm</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQueryManager"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQueryManager" title="Permalink to this definition">¶</a></dt>
<dd><p>A class to manage all the <a class="reference internal" href="#pyspark.sql.streaming.StreamingQuery" title="pyspark.sql.streaming.StreamingQuery"><code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQuery</span></code></a> StreamingQueries active.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQueryManager.active">
<em class="property">property </em><code class="sig-name descname">active</code><a class="headerlink" href="#pyspark.sql.streaming.StreamingQueryManager.active" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of active queries associated with this SQLContext</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;memory&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="s1">&#39;this_query&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqm</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">streams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># get the list of active streaming queries</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">q</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">sqm</span><span class="o">.</span><span class="n">active</span><span class="p">]</span>
<span class="go">[&#39;this_query&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination">
<code class="sig-name descname">awaitAnyTermination</code><span class="sig-paren">(</span><em class="sig-param">timeout=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQueryManager.awaitAnyTermination"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination" title="Permalink to this definition">¶</a></dt>
<dd><p>Wait until any of the queries on the associated SQLContext has terminated since the
creation of the context, or since <a class="reference internal" href="#pyspark.sql.streaming.StreamingQueryManager.resetTerminated" title="pyspark.sql.streaming.StreamingQueryManager.resetTerminated"><code class="xref py py-func docutils literal notranslate"><span class="pre">resetTerminated()</span></code></a> was called. If any query was
terminated with an exception, then the exception will be thrown.
If <cite>timeout</cite> is set, it returns whether the query has terminated or not within the
<cite>timeout</cite> seconds.</p>
<p>If a query has terminated, then subsequent calls to <a class="reference internal" href="#pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination" title="pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination"><code class="xref py py-func docutils literal notranslate"><span class="pre">awaitAnyTermination()</span></code></a> will
either return immediately (if the query was terminated by <code class="xref py py-func docutils literal notranslate"><span class="pre">query.stop()</span></code>),
or throw the exception immediately (if the query was terminated with exception). Use
<a class="reference internal" href="#pyspark.sql.streaming.StreamingQueryManager.resetTerminated" title="pyspark.sql.streaming.StreamingQueryManager.resetTerminated"><code class="xref py py-func docutils literal notranslate"><span class="pre">resetTerminated()</span></code></a> to clear past terminations and wait for new terminations.</p>
<p>In the case where multiple queries have terminated since <code class="xref py py-func docutils literal notranslate"><span class="pre">resetTermination()</span></code>
was called, if any query has terminated with exception, then <a class="reference internal" href="#pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination" title="pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination"><code class="xref py py-func docutils literal notranslate"><span class="pre">awaitAnyTermination()</span></code></a>
will throw any of the exception. For correctly documenting exceptions across multiple
queries, users need to stop all of them after any of them terminates with exception, and
then check the <cite>query.exception()</cite> for each query.</p>
<p>throws <code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQueryException</span></code>, if <cite>this</cite> query has terminated with an exception</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQueryManager.get">
<code class="sig-name descname">get</code><span class="sig-paren">(</span><em class="sig-param">id</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQueryManager.get"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQueryManager.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an active query from this SQLContext or throws exception if an active query
with this name doesn’t exist.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;memory&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="s1">&#39;this_query&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">name</span>
<span class="go">&#39;this_query&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">sq</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">isActive</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">sq</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">isActive</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQueryManager.resetTerminated">
<code class="sig-name descname">resetTerminated</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQueryManager.resetTerminated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQueryManager.resetTerminated" title="Permalink to this definition">¶</a></dt>
<dd><p>Forget about past terminated queries so that <a class="reference internal" href="#pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination" title="pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination"><code class="xref py py-func docutils literal notranslate"><span class="pre">awaitAnyTermination()</span></code></a> can be used
again to wait for new terminations.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">resetTerminated</span><span class="p">()</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.streaming.DataStreamReader">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.streaming.</code><code class="sig-name descname">DataStreamReader</code><span class="sig-paren">(</span><em class="sig-param">spark</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface used to load a streaming <code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code> from external storage systems
(e.g. file systems, key-value stores, etc). Use <code class="xref py py-func docutils literal notranslate"><span class="pre">spark.readStream()</span></code>
to access this.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.csv">
<code class="sig-name descname">csv</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">sep=None</em>, <em class="sig-param">encoding=None</em>, <em class="sig-param">quote=None</em>, <em class="sig-param">escape=None</em>, <em class="sig-param">comment=None</em>, <em class="sig-param">header=None</em>, <em class="sig-param">inferSchema=None</em>, <em class="sig-param">ignoreLeadingWhiteSpace=None</em>, <em class="sig-param">ignoreTrailingWhiteSpace=None</em>, <em class="sig-param">nullValue=None</em>, <em class="sig-param">nanValue=None</em>, <em class="sig-param">positiveInf=None</em>, <em class="sig-param">negativeInf=None</em>, <em class="sig-param">dateFormat=None</em>, <em class="sig-param">timestampFormat=None</em>, <em class="sig-param">maxColumns=None</em>, <em class="sig-param">maxCharsPerColumn=None</em>, <em class="sig-param">maxMalformedLogPerPartition=None</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">columnNameOfCorruptRecord=None</em>, <em class="sig-param">multiLine=None</em>, <em class="sig-param">charToEscapeQuoteEscaping=None</em>, <em class="sig-param">enforceSchema=None</em>, <em class="sig-param">emptyValue=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.csv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.csv" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a CSV file stream and returns the result as a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>.</p>
<p>This function will go through the input once to determine the input schema if
<code class="docutils literal notranslate"><span class="pre">inferSchema</span></code> is enabled. To avoid going through the entire data once, disable
<code class="docutils literal notranslate"><span class="pre">inferSchema</span></code> option or specify the schema explicitly using <code class="docutils literal notranslate"><span class="pre">schema</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – string, or list of strings, for input path(s).</p></li>
<li><p><strong>schema</strong> – an optional <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> for the input schema
or a DDL-formatted string (For example <code class="docutils literal notranslate"><span class="pre">col0</span> <span class="pre">INT,</span> <span class="pre">col1</span> <span class="pre">DOUBLE</span></code>).</p></li>
<li><p><strong>sep</strong> – sets a single character as a separator for each field and value.
If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">,</span></code>.</p></li>
<li><p><strong>encoding</strong> – decodes the CSV files by the given encoding type. If None is set,
it uses the default value, <code class="docutils literal notranslate"><span class="pre">UTF-8</span></code>.</p></li>
<li><p><strong>quote</strong> – sets a single character used for escaping quoted values where the
separator can be part of the value. If None is set, it uses the default
value, <code class="docutils literal notranslate"><span class="pre">&quot;</span></code>. If you would like to turn off quotations, you need to set an
empty string.</p></li>
<li><p><strong>escape</strong> – sets a single character used for escaping quotes inside an already
quoted value. If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">\</span></code>.</p></li>
<li><p><strong>comment</strong> – sets a single character used for skipping lines beginning with this
character. By default (None), it is disabled.</p></li>
<li><p><strong>header</strong> – uses the first line as names of columns. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>inferSchema</strong> – infers the input schema automatically from data. It requires one extra
pass over the data. If None is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>enforceSchema</strong> – If it is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the specified or inferred schema will be
forcibly applied to datasource files, and headers in CSV files will be
ignored. If the option is set to <code class="docutils literal notranslate"><span class="pre">false</span></code>, the schema will be
validated against all headers in CSV files or the first header in RDD
if the <code class="docutils literal notranslate"><span class="pre">header</span></code> option is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>. Field names in the schema
and column names in CSV headers are checked by their positions
taking into account <code class="docutils literal notranslate"><span class="pre">spark.sql.caseSensitive</span></code>. If None is set,
<code class="docutils literal notranslate"><span class="pre">true</span></code> is used by default. Though the default value is <code class="docutils literal notranslate"><span class="pre">true</span></code>,
it is recommended to disable the <code class="docutils literal notranslate"><span class="pre">enforceSchema</span></code> option
to avoid incorrect results.</p></li>
<li><p><strong>ignoreLeadingWhiteSpace</strong> – a flag indicating whether or not leading whitespaces from
values being read should be skipped. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>ignoreTrailingWhiteSpace</strong> – a flag indicating whether or not trailing whitespaces from
values being read should be skipped. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>nullValue</strong> – sets the string representation of a null value. If None is set, it uses
the default value, empty string. Since 2.0.1, this <code class="docutils literal notranslate"><span class="pre">nullValue</span></code> param
applies to all supported types including the string type.</p></li>
<li><p><strong>nanValue</strong> – sets the string representation of a non-number value. If None is set, it
uses the default value, <code class="docutils literal notranslate"><span class="pre">NaN</span></code>.</p></li>
<li><p><strong>positiveInf</strong> – sets the string representation of a positive infinity value. If None
is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">Inf</span></code>.</p></li>
<li><p><strong>negativeInf</strong> – sets the string representation of a negative infinity value. If None
is set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">Inf</span></code>.</p></li>
<li><p><strong>dateFormat</strong> – sets the string that indicates a date format. Custom date formats
follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>. This
applies to date type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd</span></code>.</p></li>
<li><p><strong>timestampFormat</strong> – sets the string that indicates a timestamp format. Custom date
formats follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>.
This applies to timestamp type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd'T'HH:mm:ss.SSSXXX</span></code>.</p></li>
<li><p><strong>maxColumns</strong> – defines a hard limit of how many columns a record can have. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">20480</span></code>.</p></li>
<li><p><strong>maxCharsPerColumn</strong> – defines the maximum number of characters allowed for any given
value being read. If None is set, it uses the default value,
<code class="docutils literal notranslate"><span class="pre">-1</span></code> meaning unlimited length.</p></li>
<li><p><strong>maxMalformedLogPerPartition</strong> – this parameter is no longer used since Spark 2.2.0.
If specified, it is ignored.</p></li>
<li><p><strong>mode</strong> – <dl class="simple">
<dt>allows a mode for dealing with corrupt records during parsing. If None is</dt><dd><p>set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code>.</p>
</dd>
</dl>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> : when it meets a corrupted record, puts the malformed string into a field configured by <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code>, and sets other fields to <code class="docutils literal notranslate"><span class="pre">null</span></code>. To keep corrupt records, an user can set a string type field named <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code> in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. A record with less/more tokens than schema is not a corrupted record to CSV. When it meets a record having fewer tokens than the length of the schema, sets <code class="docutils literal notranslate"><span class="pre">null</span></code> to extra fields. When the record has more tokens than the length of the schema, it drops extra tokens.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DROPMALFORMED</span></code> : ignores the whole corrupted records.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FAILFAST</span></code> : throws an exception when it meets corrupted records.</p></li>
</ul>
</p></li>
<li><p><strong>columnNameOfCorruptRecord</strong> – allows renaming the new field having malformed string
created by <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> mode. This overrides
<code class="docutils literal notranslate"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></code>. If None is set,
it uses the value specified in
<code class="docutils literal notranslate"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></code>.</p></li>
<li><p><strong>multiLine</strong> – parse one record, which may span multiple lines. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>charToEscapeQuoteEscaping</strong> – sets a single character used for escaping the escape for
the quote character. If None is set, the default value is
escape character when escape and quote characters are
different, <code class="docutils literal notranslate"><span class="pre">\0</span></code> otherwise..</p></li>
<li><p><strong>emptyValue</strong> – sets the string representation of an empty value. If None is set, it uses
the default value, empty string.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">csv_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="n">schema</span> <span class="o">=</span> <span class="n">sdf_schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csv_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csv_sdf</span><span class="o">.</span><span class="n">schema</span> <span class="o">==</span> <span class="n">sdf_schema</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.format">
<code class="sig-name descname">format</code><span class="sig-paren">(</span><em class="sig-param">source</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.format"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the input data source format.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>source</strong> – string, name of the data source, e.g. ‘json’, ‘parquet’.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.json">
<code class="sig-name descname">json</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">primitivesAsString=None</em>, <em class="sig-param">prefersDecimal=None</em>, <em class="sig-param">allowComments=None</em>, <em class="sig-param">allowUnquotedFieldNames=None</em>, <em class="sig-param">allowSingleQuotes=None</em>, <em class="sig-param">allowNumericLeadingZero=None</em>, <em class="sig-param">allowBackslashEscapingAnyCharacter=None</em>, <em class="sig-param">mode=None</em>, <em class="sig-param">columnNameOfCorruptRecord=None</em>, <em class="sig-param">dateFormat=None</em>, <em class="sig-param">timestampFormat=None</em>, <em class="sig-param">multiLine=None</em>, <em class="sig-param">allowUnquotedControlChars=None</em>, <em class="sig-param">lineSep=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.json" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a JSON file stream and returns the results as a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>.</p>
<p><a class="reference external" href="http://jsonlines.org/">JSON Lines</a> (newline-delimited JSON) is supported by default.
For JSON (one record per file), set the <code class="docutils literal notranslate"><span class="pre">multiLine</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">schema</span></code> parameter is not specified, this function goes
through the input once to determine the input schema.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – string represents path to the JSON dataset,
or RDD of Strings storing JSON objects.</p></li>
<li><p><strong>schema</strong> – an optional <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> for the input schema
or a DDL-formatted string (For example <code class="docutils literal notranslate"><span class="pre">col0</span> <span class="pre">INT,</span> <span class="pre">col1</span> <span class="pre">DOUBLE</span></code>).</p></li>
<li><p><strong>primitivesAsString</strong> – infers all primitive values as a string type. If None is set,
it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>prefersDecimal</strong> – infers all floating-point values as a decimal type. If the values
do not fit in decimal, then it infers them as doubles. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>allowComments</strong> – ignores Java/C++ style comment in JSON records. If None is set,
it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>allowUnquotedFieldNames</strong> – allows unquoted JSON field names. If None is set,
it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>allowSingleQuotes</strong> – allows single quotes in addition to double quotes. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
<li><p><strong>allowNumericLeadingZero</strong> – allows leading zeros in numbers (e.g. 00012). If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>allowBackslashEscapingAnyCharacter</strong> – allows accepting quoting of all character
using backslash quoting mechanism. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>mode</strong> – <dl class="simple">
<dt>allows a mode for dealing with corrupt records during parsing. If None is</dt><dd><p>set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code>.</p>
</dd>
</dl>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> : when it meets a corrupted record, puts the malformed string                   into a field configured by <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code>, and sets other                   fields to <code class="docutils literal notranslate"><span class="pre">null</span></code>. To keep corrupt records, an user can set a string type                   field named <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code> in an user-defined schema. If a                   schema does not have the field, it drops corrupt records during parsing.                   When inferring a schema, it implicitly adds a <code class="docutils literal notranslate"><span class="pre">columnNameOfCorruptRecord</span></code>                   field in an output schema.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DROPMALFORMED</span></code> : ignores the whole corrupted records.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FAILFAST</span></code> : throws an exception when it meets corrupted records.</p></li>
</ul>
</p></li>
<li><p><strong>columnNameOfCorruptRecord</strong> – allows renaming the new field having malformed string
created by <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> mode. This overrides
<code class="docutils literal notranslate"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></code>. If None is set,
it uses the value specified in
<code class="docutils literal notranslate"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></code>.</p></li>
<li><p><strong>dateFormat</strong> – sets the string that indicates a date format. Custom date formats
follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>. This
applies to date type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd</span></code>.</p></li>
<li><p><strong>timestampFormat</strong> – sets the string that indicates a timestamp format. Custom date
formats follow the formats at <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code>.
This applies to timestamp type. If None is set, it uses the
default value, <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd'T'HH:mm:ss.SSSXXX</span></code>.</p></li>
<li><p><strong>multiLine</strong> – parse one record, which may span multiple lines, per file. If None is
set, it uses the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p><strong>allowUnquotedControlChars</strong> – allows JSON Strings to contain unquoted control
characters (ASCII characters with value less than 32,
including tab and line feed characters) or not.</p></li>
<li><p><strong>lineSep</strong> – defines the line separator that should be used for parsing. If None is
set, it covers all <code class="docutils literal notranslate"><span class="pre">\r</span></code>, <code class="docutils literal notranslate"><span class="pre">\r\n</span></code> and <code class="docutils literal notranslate"><span class="pre">\n</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="n">schema</span> <span class="o">=</span> <span class="n">sdf_schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span><span class="o">.</span><span class="n">schema</span> <span class="o">==</span> <span class="n">sdf_schema</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.load">
<code class="sig-name descname">load</code><span class="sig-paren">(</span><em class="sig-param">path=None</em>, <em class="sig-param">format=None</em>, <em class="sig-param">schema=None</em>, <em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.load"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a data stream from a data source and returns it as a :class`DataFrame`.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – optional string for file-system backed data sources.</p></li>
<li><p><strong>format</strong> – optional string for format of the data source. Default to ‘parquet’.</p></li>
<li><p><strong>schema</strong> – optional <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> for the input schema
or a DDL-formatted string (For example <code class="docutils literal notranslate"><span class="pre">col0</span> <span class="pre">INT,</span> <span class="pre">col1</span> <span class="pre">DOUBLE</span></code>).</p></li>
<li><p><strong>options</strong> – all other string options</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sdf_schema</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span><span class="o">.</span><span class="n">schema</span> <span class="o">==</span> <span class="n">sdf_schema</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.option">
<code class="sig-name descname">option</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.option"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.option" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds an input option for the underlying data source.</p>
<dl class="simple">
<dt>You can set the following option(s) for reading files:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">timeZone</span></code>: sets the string that indicates a timezone to be used to parse timestamps</dt><dd><p>in the JSON/CSV datasources or partition values.
If it isn’t set, it uses the default value, session local timezone.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.options">
<code class="sig-name descname">options</code><span class="sig-paren">(</span><em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.options"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.options" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds input options for the underlying data source.</p>
<dl class="simple">
<dt>You can set the following option(s) for reading files:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">timeZone</span></code>: sets the string that indicates a timezone to be used to parse timestamps</dt><dd><p>in the JSON/CSV datasources or partition values.
If it isn’t set, it uses the default value, session local timezone.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">options</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.orc">
<code class="sig-name descname">orc</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.orc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.orc" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a ORC file stream, returning the result as a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">orc_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sdf_schema</span><span class="p">)</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">orc_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">orc_sdf</span><span class="o">.</span><span class="n">schema</span> <span class="o">==</span> <span class="n">sdf_schema</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.parquet">
<code class="sig-name descname">parquet</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.parquet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.parquet" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a Parquet file stream, returning the result as a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>.</p>
<dl class="simple">
<dt>You can set the following Parquet-specific option(s) for reading Parquet files:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mergeSchema</span></code>: sets whether we should merge schemas collected from all                 Parquet part-files. This will override <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.mergeSchema</span></code>.                 The default value is specified in <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.mergeSchema</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">parquet_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sdf_schema</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parquet_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parquet_sdf</span><span class="o">.</span><span class="n">schema</span> <span class="o">==</span> <span class="n">sdf_schema</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.schema">
<code class="sig-name descname">schema</code><span class="sig-paren">(</span><em class="sig-param">schema</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.schema"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.schema" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the input schema.</p>
<p>Some data sources (e.g. JSON) can infer the input schema automatically from data.
By specifying the schema here, the underlying data source can skip the schema
inference step, and thus speed up data loading.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>schema</strong> – a <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.types.StructType</span></code></a> object or a DDL-formatted string
(For example <code class="docutils literal notranslate"><span class="pre">col0</span> <span class="pre">INT,</span> <span class="pre">col1</span> <span class="pre">DOUBLE</span></code>).</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sdf_schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="s2">&quot;col0 INT, col1 DOUBLE&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.text">
<code class="sig-name descname">text</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">wholetext=False</em>, <em class="sig-param">lineSep=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.text"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a text file stream and returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code> whose schema starts with a
string column named “value”, and followed by partitioned columns if there
are any.</p>
<p>By default, each line in the text file is a new row in the resulting DataFrame.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>paths</strong> – string, or list of strings, for input path(s).</p></li>
<li><p><strong>wholetext</strong> – if true, read each file from input path(s) as a single row.</p></li>
<li><p><strong>lineSep</strong> – defines the line separator that should be used for parsing. If None is
set, it covers all <code class="docutils literal notranslate"><span class="pre">\r</span></code>, <code class="docutils literal notranslate"><span class="pre">\r\n</span></code> and <code class="docutils literal notranslate"><span class="pre">\n</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;value&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">text_sdf</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.streaming.DataStreamWriter">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.sql.streaming.</code><code class="sig-name descname">DataStreamWriter</code><span class="sig-paren">(</span><em class="sig-param">df</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface used to write a streaming <code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code> to external storage systems
(e.g. file systems, key-value stores, etc). Use <code class="xref py py-func docutils literal notranslate"><span class="pre">DataFrame.writeStream()</span></code>
to access this.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.foreach">
<code class="sig-name descname">foreach</code><span class="sig-paren">(</span><em class="sig-param">f</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.foreach"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.foreach" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the output of the streaming query to be processed using the provided writer <code class="docutils literal notranslate"><span class="pre">f</span></code>.
This is often used to write the output of a streaming query to arbitrary storage systems.
The processing logic can be specified in two ways.</p>
<ol class="arabic">
<li><dl class="simple">
<dt>A <strong>function</strong> that takes a row as input.</dt><dd><p>This is a simple way to express your processing logic. Note that this does
not allow you to deduplicate generated data when failures cause reprocessing of
some input data. That would require you to specify the processing logic in the next
way.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>An <strong>object</strong> with a <code class="docutils literal notranslate"><span class="pre">process</span></code> method and optional <code class="docutils literal notranslate"><span class="pre">open</span></code> and <code class="docutils literal notranslate"><span class="pre">close</span></code> methods.</dt><dd><p>The object can have the following methods.</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">open(partition_id,</span> <span class="pre">epoch_id)</span></code>: <em>Optional</em> method that initializes the processing</dt><dd><p>(for example, open a connection, start a transaction, etc). Additionally, you can
use the <cite>partition_id</cite> and <cite>epoch_id</cite> to deduplicate regenerated data
(discussed later).</p>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">process(row)</span></code>: <em>Non-optional</em> method that processes each <code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code>.</p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">close(error)</span></code>: <em>Optional</em> method that finalizes and cleans up (for example,</dt><dd><p>close connection, commit transaction, etc.) after all rows have been processed.</p>
</dd>
</dl>
</li>
</ul>
<p>The object will be used by Spark in the following way.</p>
<ul>
<li><dl class="simple">
<dt>A single copy of this object is responsible of all the data generated by a</dt><dd><p>single task in a query. In other words, one instance is responsible for
processing one partition of the data generated in a distributed manner.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>This object must be serializable because each task will get a fresh</dt><dd><p>serialized-deserialized copy of the provided object. Hence, it is strongly
recommended that any initialization for writing data (e.g. opening a
connection or starting a transaction) is done after the <cite>open(…)</cite>
method has been called, which signifies that the task is ready to generate data.</p>
</dd>
</dl>
</li>
<li><p>The lifecycle of the methods are as follows.</p>
<blockquote>
<div><p>For each partition with <code class="docutils literal notranslate"><span class="pre">partition_id</span></code>:</p>
<p>… For each batch/epoch of streaming data with <code class="docutils literal notranslate"><span class="pre">epoch_id</span></code>:</p>
<p>……. Method <code class="docutils literal notranslate"><span class="pre">open(partitionId,</span> <span class="pre">epochId)</span></code> is called.</p>
<dl class="simple">
<dt>……. If <code class="docutils literal notranslate"><span class="pre">open(...)</span></code> returns true, for each row in the partition and</dt><dd><p>batch/epoch, method <code class="docutils literal notranslate"><span class="pre">process(row)</span></code> is called.</p>
</dd>
<dt>……. Method <code class="docutils literal notranslate"><span class="pre">close(errorOrNull)</span></code> is called with error (if any) seen while</dt><dd><p>processing rows.</p>
</dd>
</dl>
</div></blockquote>
</li>
</ul>
<p>Important points to note:</p>
<ul class="simple">
<li><dl class="simple">
<dt>The <cite>partitionId</cite> and <cite>epochId</cite> can be used to deduplicate generated data when</dt><dd><p>failures cause reprocessing of some input data. This depends on the execution
mode of the query. If the streaming query is being executed in the micro-batch
mode, then every partition represented by a unique tuple (partition_id, epoch_id)
is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used
to deduplicate and/or transactionally commit data and achieve exactly-once
guarantees. However, if the streaming query is being executed in the continuous
mode, then this guarantee does not hold and therefore should not be used for
deduplication.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>The <code class="docutils literal notranslate"><span class="pre">close()</span></code> method (if exists) will be called if <cite>open()</cite> method exists and</dt><dd><p>returns successfully (irrespective of the return value), except if the Python
crashes in the middle.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print every row using a function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">print_row</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">print_row</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print every row using a object with process() method</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">RowPrinter</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">open</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition_id</span><span class="p">,</span> <span class="n">epoch_id</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Opened </span><span class="si">%d</span><span class="s2">, </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">partition_id</span><span class="p">,</span> <span class="n">epoch_id</span><span class="p">))</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="kc">True</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">print</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Closed with error: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">RowPrinter</span><span class="p">())</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.foreachBatch">
<code class="sig-name descname">foreachBatch</code><span class="sig-paren">(</span><em class="sig-param">func</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.foreachBatch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.foreachBatch" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the output of the streaming query to be processed using the provided
function. This is supported only the in the micro-batch execution modes (that is, when the
trigger is not continuous). In every micro-batch, the provided function will be called in
every micro-batch with (i) the output rows as a DataFrame and (ii) the batch identifier.
The batchId can be used deduplicate and transactionally write the output
(that is, the provided Dataset) to external systems. The output DataFrame is guaranteed
to exactly same for the same batchId (assuming all operations are deterministic in the
query).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">batch_df</span><span class="p">,</span> <span class="n">batch_id</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">batch_df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.format">
<code class="sig-name descname">format</code><span class="sig-paren">(</span><em class="sig-param">source</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.format"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the underlying output data source.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>source</strong> – string, name of the data source, which for now can be ‘parquet’.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;json&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.option">
<code class="sig-name descname">option</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.option"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.option" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds an output option for the underlying data source.</p>
<dl class="simple">
<dt>You can set the following option(s) for writing files:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">timeZone</span></code>: sets the string that indicates a timezone to be used to format</dt><dd><p>timestamps in the JSON/CSV datasources or partition values.
If it isn’t set, it uses the default value, session local timezone.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.options">
<code class="sig-name descname">options</code><span class="sig-paren">(</span><em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.options"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.options" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds output options for the underlying data source.</p>
<blockquote>
<div><dl class="simple">
<dt>You can set the following option(s) for writing files:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">timeZone</span></code>: sets the string that indicates a timezone to be used to format</dt><dd><p>timestamps in the JSON/CSV datasources or partition values.
If it isn’t set, it uses the default value, session local timezone.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.outputMode">
<code class="sig-name descname">outputMode</code><span class="sig-paren">(</span><em class="sig-param">outputMode</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.outputMode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.outputMode" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.</p>
<blockquote>
<div><p>Options include:</p>
<ul class="simple">
<li><dl class="simple">
<dt><cite>append</cite>:Only the new rows in the streaming DataFrame/Dataset will be written to</dt><dd><p>the sink</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><cite>complete</cite>:All the rows in the streaming DataFrame/Dataset will be written to the sink</dt><dd><p>every time these is some updates</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><cite>update</cite>:only the rows that were updated in the streaming DataFrame/Dataset will be</dt><dd><p>written to the sink every time there are some updates. If the query doesn’t contain
aggregations, it will be equivalent to <cite>append</cite> mode.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s1">&#39;append&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.partitionBy">
<code class="sig-name descname">partitionBy</code><span class="sig-paren">(</span><em class="sig-param">*cols</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.partitionBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Partitions the output by the given columns on the file system.</p>
<p>If specified, the output is laid out on the file system similar
to Hive’s partitioning scheme.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cols</strong> – name of columns</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.queryName">
<code class="sig-name descname">queryName</code><span class="sig-paren">(</span><em class="sig-param">queryName</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.queryName"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.queryName" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the name of the <a class="reference internal" href="#pyspark.sql.streaming.StreamingQuery" title="pyspark.sql.streaming.StreamingQuery"><code class="xref py py-class docutils literal notranslate"><span class="pre">StreamingQuery</span></code></a> that can be started with
<a class="reference internal" href="#pyspark.sql.streaming.DataStreamWriter.start" title="pyspark.sql.streaming.DataStreamWriter.start"><code class="xref py py-func docutils literal notranslate"><span class="pre">start()</span></code></a>. This name must be unique among all the currently active queries
in the associated SparkSession.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queryName</strong> – unique name for the query</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="s1">&#39;streaming_query&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.start">
<code class="sig-name descname">start</code><span class="sig-paren">(</span><em class="sig-param">path=None</em>, <em class="sig-param">format=None</em>, <em class="sig-param">outputMode=None</em>, <em class="sig-param">partitionBy=None</em>, <em class="sig-param">queryName=None</em>, <em class="sig-param">**options</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.start"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.start" title="Permalink to this definition">¶</a></dt>
<dd><p>Streams the contents of the <code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code> to a data source.</p>
<p>The data source is specified by the <code class="docutils literal notranslate"><span class="pre">format</span></code> and a set of <code class="docutils literal notranslate"><span class="pre">options</span></code>.
If <code class="docutils literal notranslate"><span class="pre">format</span></code> is not specified, the default data source configured by
<code class="docutils literal notranslate"><span class="pre">spark.sql.sources.default</span></code> will be used.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path in a Hadoop supported file system</p></li>
<li><p><strong>format</strong> – the format used to save</p></li>
<li><p><strong>outputMode</strong> – <dl class="simple">
<dt>specifies how data of a streaming DataFrame/Dataset is written to a</dt><dd><p>streaming sink.</p>
</dd>
</dl>
<ul>
<li><p><cite>append</cite>:Only the new rows in the streaming DataFrame/Dataset will be written to the
sink</p></li>
<li><dl class="simple">
<dt><cite>complete</cite>:All the rows in the streaming DataFrame/Dataset will be written to the sink</dt><dd><p>every time these is some updates</p>
</dd>
</dl>
</li>
<li><p><cite>update</cite>:only the rows that were updated in the streaming DataFrame/Dataset will be
written to the sink every time there are some updates. If the query doesn’t contain
aggregations, it will be equivalent to <cite>append</cite> mode.</p></li>
</ul>
</p></li>
<li><p><strong>partitionBy</strong> – names of partitioning columns</p></li>
<li><p><strong>queryName</strong> – unique name for the query</p></li>
<li><p><strong>options</strong> – All other string options. You may want to provide a <cite>checkpointLocation</cite>
for most streams, however it is not required for a <cite>memory</cite> stream.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;memory&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="s1">&#39;this_query&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">isActive</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">name</span>
<span class="go">&#39;this_query&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">isActive</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">processingTime</span><span class="o">=</span><span class="s1">&#39;5 seconds&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">queryName</span><span class="o">=</span><span class="s1">&#39;that_query&#39;</span><span class="p">,</span> <span class="n">outputMode</span><span class="o">=</span><span class="s2">&quot;append&quot;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;memory&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">name</span>
<span class="go">&#39;that_query&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">isActive</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.trigger">
<code class="sig-name descname">trigger</code><span class="sig-paren">(</span><em class="sig-param">processingTime=None</em>, <em class="sig-param">once=None</em>, <em class="sig-param">continuous=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.trigger"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.trigger" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the trigger for the stream query. If this is not set it will run the query as fast
as possible, which is equivalent to setting the trigger to <code class="docutils literal notranslate"><span class="pre">processingTime='0</span> <span class="pre">seconds'</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evolving.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>processingTime</strong> – a processing time interval as a string, e.g. ‘5 seconds’, ‘1 minute’.
Set a trigger that runs a query periodically based on the processing
time. Only one trigger can be set.</p></li>
<li><p><strong>once</strong> – if set to True, set a trigger that processes only one batch of data in a
streaming query then terminates the query. Only one trigger can be set.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># trigger the query for execution every 5 seconds</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">processingTime</span><span class="o">=</span><span class="s1">&#39;5 seconds&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># trigger the query for just once batch of data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">once</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># trigger the query for execution every 5 seconds</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">continuous</span><span class="o">=</span><span class="s1">&#39;5 seconds&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/spark-logo-hd.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">pyspark.sql module</a><ul>
<li><a class="reference internal" href="#module-pyspark.sql">Module Context</a></li>
<li><a class="reference internal" href="#module-pyspark.sql.types">pyspark.sql.types module</a></li>
<li><a class="reference internal" href="#module-pyspark.sql.functions">pyspark.sql.functions module</a></li>
<li><a class="reference internal" href="#module-pyspark.sql.streaming">pyspark.sql.streaming module</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="pyspark.html"
                        title="previous chapter">pyspark package</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="pyspark.streaming.html"
                        title="next chapter">pyspark.streaming module</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/pyspark.sql.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="pyspark.streaming.html" title="pyspark.streaming module"
             >next</a></li>
        <li class="right" >
          <a href="pyspark.html" title="pyspark package"
             >previous</a> |</li>
    
        <li class="nav-item nav-item-0"><a href="index.html">PySpark 2.4.4 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="pyspark.html" >pyspark package</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.2.0.
    </div>
  </body>
</html>