
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>pyspark.sql.streaming &#8212; PySpark 2.4.4 documentation</title>
    <link rel="stylesheet" href="../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pyspark.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../_static/pyspark.js"></script>
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
    
        <li class="nav-item nav-item-0"><a href="../../../index.html">PySpark 2.4.4 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for pyspark.sql.streaming</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span> <span class="o">&gt;=</span> <span class="s1">&#39;3&#39;</span><span class="p">:</span>
    <span class="n">basestring</span> <span class="o">=</span> <span class="nb">str</span>

<span class="kn">from</span> <span class="nn">py4j.java_gateway</span> <span class="k">import</span> <span class="n">java_import</span>

<span class="kn">from</span> <span class="nn">pyspark</span> <span class="k">import</span> <span class="n">since</span><span class="p">,</span> <span class="n">keyword_only</span>
<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="k">import</span> <span class="n">ignore_unicode_prefix</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.column</span> <span class="k">import</span> <span class="n">_to_seq</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.readwriter</span> <span class="k">import</span> <span class="n">OptionUtils</span><span class="p">,</span> <span class="n">to_str</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.utils</span> <span class="k">import</span> <span class="n">ForeachBatchFunction</span><span class="p">,</span> <span class="n">StreamingQueryException</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;StreamingQuery&quot;</span><span class="p">,</span> <span class="s2">&quot;StreamingQueryManager&quot;</span><span class="p">,</span> <span class="s2">&quot;DataStreamReader&quot;</span><span class="p">,</span> <span class="s2">&quot;DataStreamWriter&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="StreamingQuery"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.StreamingQuery">[docs]</a><span class="k">class</span> <span class="nc">StreamingQuery</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A handle to a query that is executing continuously in the background as new data arrives.</span>
<span class="sd">    All these methods are thread-safe.</span>

<span class="sd">    .. note:: Evolving</span>

<span class="sd">    .. versionadded:: 2.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jsq</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span> <span class="o">=</span> <span class="n">jsq</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the unique id of this query that persists across restarts from checkpoint data.</span>
<span class="sd">        That is, this id is generated when a query is started for the first time, and</span>
<span class="sd">        will be the same every time it is restarted from checkpoint data.</span>
<span class="sd">        There can only be one query with the same id active in a Spark cluster.</span>
<span class="sd">        Also see, `runId`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">id</span><span class="p">()</span><span class="o">.</span><span class="n">toString</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">runId</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the unique id of this query that does not persist across restarts. That is, every</span>
<span class="sd">        query that is started (or restarted from checkpoint) will have a different runId.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">runId</span><span class="p">()</span><span class="o">.</span><span class="n">toString</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the user-specified name of the query, or null if not specified.</span>
<span class="sd">        This name can be specified in the `org.apache.spark.sql.streaming.DataStreamWriter`</span>
<span class="sd">        as `dataframe.writeStream.queryName(&quot;query&quot;).start()`.</span>
<span class="sd">        This name, if set, must be unique across all active queries.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">name</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">isActive</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Whether this streaming query is currently active or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">isActive</span><span class="p">()</span>

<div class="viewcode-block" id="StreamingQuery.awaitTermination"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.StreamingQuery.awaitTermination">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">awaitTermination</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Waits for the termination of `this` query, either by :func:`query.stop()` or by an</span>
<span class="sd">        exception. If the query has terminated with an exception, then the exception will be thrown.</span>
<span class="sd">        If `timeout` is set, it returns whether the query has terminated or not within the</span>
<span class="sd">        `timeout` seconds.</span>

<span class="sd">        If the query has terminated, then all subsequent calls to this method will either return</span>
<span class="sd">        immediately (if the query was terminated by :func:`stop()`), or throw the exception</span>
<span class="sd">        immediately (if the query has terminated with exception).</span>

<span class="sd">        throws :class:`StreamingQueryException`, if `this` query has terminated with an exception</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">timeout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">or</span> <span class="n">timeout</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;timeout must be a positive integer or float. Got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">timeout</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">awaitTermination</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">timeout</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">awaitTermination</span><span class="p">()</span></div>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">status</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the current status of the query.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">status</span><span class="p">()</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">recentProgress</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns an array of the most recent [[StreamingQueryProgress]] updates for this query.</span>
<span class="sd">        The number of progress updates retained for each stream is configured by Spark session</span>
<span class="sd">        configuration `spark.sql.streaming.numRecentProgressUpdates`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">json</span><span class="p">())</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">recentProgress</span><span class="p">()]</span>

    <span class="nd">@property</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">lastProgress</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the most recent :class:`StreamingQueryProgress` update of this streaming query or</span>
<span class="sd">        None if there were no progress updates</span>
<span class="sd">        :return: a map</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">lastProgress</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">lastProgress</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">lastProgress</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">lastProgress</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

<div class="viewcode-block" id="StreamingQuery.processAllAvailable"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.StreamingQuery.processAllAvailable">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">processAllAvailable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Blocks until all available data in the source has been processed and committed to the</span>
<span class="sd">        sink. This method is intended for testing.</span>

<span class="sd">        .. note:: In the case of continually arriving data, this method may block forever.</span>
<span class="sd">            Additionally, this method is only guaranteed to block until data that has been</span>
<span class="sd">            synchronously appended data to a stream source prior to invocation.</span>
<span class="sd">            (i.e. `getOffset` must immediately reflect the addition).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">processAllAvailable</span><span class="p">()</span></div>

<div class="viewcode-block" id="StreamingQuery.stop"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.StreamingQuery.stop">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Stop this streaming query.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span></div>

<div class="viewcode-block" id="StreamingQuery.explain"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.StreamingQuery.explain">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">explain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">extended</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Prints the (logical and physical) plans to the console for debugging purpose.</span>

<span class="sd">        :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.</span>

<span class="sd">        &gt;&gt;&gt; sq = sdf.writeStream.format(&#39;memory&#39;).queryName(&#39;query_explain&#39;).start()</span>
<span class="sd">        &gt;&gt;&gt; sq.processAllAvailable() # Wait a bit to generate the runtime plans.</span>
<span class="sd">        &gt;&gt;&gt; sq.explain()</span>
<span class="sd">        == Physical Plan ==</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; sq.explain(True)</span>
<span class="sd">        == Parsed Logical Plan ==</span>
<span class="sd">        ...</span>
<span class="sd">        == Analyzed Logical Plan ==</span>
<span class="sd">        ...</span>
<span class="sd">        == Optimized Logical Plan ==</span>
<span class="sd">        ...</span>
<span class="sd">        == Physical Plan ==</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; sq.stop()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Cannot call `_jsq.explain(...)` because it will print in the JVM process.</span>
        <span class="c1"># We should print it in the Python process.</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">explainInternal</span><span class="p">(</span><span class="n">extended</span><span class="p">))</span></div>

<div class="viewcode-block" id="StreamingQuery.exception"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.StreamingQuery.exception">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">exception</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :return: the StreamingQueryException if the query was terminated by an exception, or None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">exception</span><span class="p">()</span><span class="o">.</span><span class="n">isDefined</span><span class="p">():</span>
            <span class="n">je</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsq</span><span class="o">.</span><span class="n">exception</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="n">je</span><span class="o">.</span><span class="n">toString</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;: &#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Drop the Java StreamingQueryException type info</span>
            <span class="n">stackTrace</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n\t</span><span class="s1"> at &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">toString</span><span class="p">(),</span> <span class="n">je</span><span class="o">.</span><span class="n">getStackTrace</span><span class="p">()))</span>
            <span class="k">return</span> <span class="n">StreamingQueryException</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">stackTrace</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span></div></div>


<div class="viewcode-block" id="StreamingQueryManager"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.StreamingQueryManager">[docs]</a><span class="k">class</span> <span class="nc">StreamingQueryManager</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A class to manage all the :class:`StreamingQuery` StreamingQueries active.</span>

<span class="sd">    .. note:: Evolving</span>

<span class="sd">    .. versionadded:: 2.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jsqm</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jsqm</span> <span class="o">=</span> <span class="n">jsqm</span>

    <span class="nd">@property</span>
    <span class="nd">@ignore_unicode_prefix</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">active</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns a list of active queries associated with this SQLContext</span>

<span class="sd">        &gt;&gt;&gt; sq = sdf.writeStream.format(&#39;memory&#39;).queryName(&#39;this_query&#39;).start()</span>
<span class="sd">        &gt;&gt;&gt; sqm = spark.streams</span>
<span class="sd">        &gt;&gt;&gt; # get the list of active streaming queries</span>
<span class="sd">        &gt;&gt;&gt; [q.name for q in sqm.active]</span>
<span class="sd">        [u&#39;this_query&#39;]</span>
<span class="sd">        &gt;&gt;&gt; sq.stop()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">StreamingQuery</span><span class="p">(</span><span class="n">jsq</span><span class="p">)</span> <span class="k">for</span> <span class="n">jsq</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsqm</span><span class="o">.</span><span class="n">active</span><span class="p">()]</span>

<div class="viewcode-block" id="StreamingQueryManager.get"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.StreamingQueryManager.get">[docs]</a>    <span class="nd">@ignore_unicode_prefix</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns an active query from this SQLContext or throws exception if an active query</span>
<span class="sd">        with this name doesn&#39;t exist.</span>

<span class="sd">        &gt;&gt;&gt; sq = sdf.writeStream.format(&#39;memory&#39;).queryName(&#39;this_query&#39;).start()</span>
<span class="sd">        &gt;&gt;&gt; sq.name</span>
<span class="sd">        u&#39;this_query&#39;</span>
<span class="sd">        &gt;&gt;&gt; sq = spark.streams.get(sq.id)</span>
<span class="sd">        &gt;&gt;&gt; sq.isActive</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; sq = sqlContext.streams.get(sq.id)</span>
<span class="sd">        &gt;&gt;&gt; sq.isActive</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; sq.stop()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">StreamingQuery</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jsqm</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">))</span></div>

<div class="viewcode-block" id="StreamingQueryManager.awaitAnyTermination"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">awaitAnyTermination</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Wait until any of the queries on the associated SQLContext has terminated since the</span>
<span class="sd">        creation of the context, or since :func:`resetTerminated()` was called. If any query was</span>
<span class="sd">        terminated with an exception, then the exception will be thrown.</span>
<span class="sd">        If `timeout` is set, it returns whether the query has terminated or not within the</span>
<span class="sd">        `timeout` seconds.</span>

<span class="sd">        If a query has terminated, then subsequent calls to :func:`awaitAnyTermination()` will</span>
<span class="sd">        either return immediately (if the query was terminated by :func:`query.stop()`),</span>
<span class="sd">        or throw the exception immediately (if the query was terminated with exception). Use</span>
<span class="sd">        :func:`resetTerminated()` to clear past terminations and wait for new terminations.</span>

<span class="sd">        In the case where multiple queries have terminated since :func:`resetTermination()`</span>
<span class="sd">        was called, if any query has terminated with exception, then :func:`awaitAnyTermination()`</span>
<span class="sd">        will throw any of the exception. For correctly documenting exceptions across multiple</span>
<span class="sd">        queries, users need to stop all of them after any of them terminates with exception, and</span>
<span class="sd">        then check the `query.exception()` for each query.</span>

<span class="sd">        throws :class:`StreamingQueryException`, if `this` query has terminated with an exception</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">timeout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">or</span> <span class="n">timeout</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;timeout must be a positive integer or float. Got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">timeout</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsqm</span><span class="o">.</span><span class="n">awaitAnyTermination</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">timeout</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jsqm</span><span class="o">.</span><span class="n">awaitAnyTermination</span><span class="p">()</span></div>

<div class="viewcode-block" id="StreamingQueryManager.resetTerminated"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.StreamingQueryManager.resetTerminated">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">resetTerminated</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Forget about past terminated queries so that :func:`awaitAnyTermination()` can be used</span>
<span class="sd">        again to wait for new terminations.</span>

<span class="sd">        &gt;&gt;&gt; spark.streams.resetTerminated()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jsqm</span><span class="o">.</span><span class="n">resetTerminated</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="DataStreamReader"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader">[docs]</a><span class="k">class</span> <span class="nc">DataStreamReader</span><span class="p">(</span><span class="n">OptionUtils</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Interface used to load a streaming :class:`DataFrame` from external storage systems</span>
<span class="sd">    (e.g. file systems, key-value stores, etc). Use :func:`spark.readStream`</span>
<span class="sd">    to access this.</span>

<span class="sd">    .. note:: Evolving.</span>

<span class="sd">    .. versionadded:: 2.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spark</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">_ssql_ctx</span><span class="o">.</span><span class="n">readStream</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_spark</span> <span class="o">=</span> <span class="n">spark</span>

    <span class="k">def</span> <span class="nf">_df</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jdf</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="k">import</span> <span class="n">DataFrame</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spark</span><span class="p">)</span>

<div class="viewcode-block" id="DataStreamReader.format"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.format">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">format</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Specifies the input data source format.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param source: string, name of the data source, e.g. &#39;json&#39;, &#39;parquet&#39;.</span>

<span class="sd">        &gt;&gt;&gt; s = spark.readStream.format(&quot;text&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamReader.schema"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.schema">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">schema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Specifies the input schema.</span>

<span class="sd">        Some data sources (e.g. JSON) can infer the input schema automatically from data.</span>
<span class="sd">        By specifying the schema here, the underlying data source can skip the schema</span>
<span class="sd">        inference step, and thus speed up data loading.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param schema: a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string</span>
<span class="sd">                       (For example ``col0 INT, col1 DOUBLE``).</span>

<span class="sd">        &gt;&gt;&gt; s = spark.readStream.schema(sdf_schema)</span>
<span class="sd">        &gt;&gt;&gt; s = spark.readStream.schema(&quot;col0 INT, col1 DOUBLE&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">SparkSession</span>
        <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">StructType</span><span class="p">):</span>
            <span class="n">jschema</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="o">.</span><span class="n">parseDataType</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">jschema</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">basestring</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;schema should be StructType or string&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamReader.option"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.option">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">option</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds an input option for the underlying data source.</span>

<span class="sd">        You can set the following option(s) for reading files:</span>
<span class="sd">            * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps</span>
<span class="sd">                in the JSON/CSV datasources or partition values.</span>
<span class="sd">                If it isn&#39;t set, it uses the default value, session local timezone.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        &gt;&gt;&gt; s = spark.readStream.option(&quot;x&quot;, 1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">to_str</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamReader.options"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.options">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds input options for the underlying data source.</span>

<span class="sd">        You can set the following option(s) for reading files:</span>
<span class="sd">            * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps</span>
<span class="sd">                in the JSON/CSV datasources or partition values.</span>
<span class="sd">                If it isn&#39;t set, it uses the default value, session local timezone.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        &gt;&gt;&gt; s = spark.readStream.options(x=&quot;1&quot;, y=2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">options</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">to_str</span><span class="p">(</span><span class="n">options</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamReader.load"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.load">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a data stream from a data source and returns it as a :class`DataFrame`.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param path: optional string for file-system backed data sources.</span>
<span class="sd">        :param format: optional string for format of the data source. Default to &#39;parquet&#39;.</span>
<span class="sd">        :param schema: optional :class:`pyspark.sql.types.StructType` for the input schema</span>
<span class="sd">                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).</span>
<span class="sd">        :param options: all other string options</span>

<span class="sd">        &gt;&gt;&gt; json_sdf = spark.readStream.format(&quot;json&quot;) \\</span>
<span class="sd">        ...     .schema(sdf_schema) \\</span>
<span class="sd">        ...     .load(tempfile.mkdtemp())</span>
<span class="sd">        &gt;&gt;&gt; json_sdf.isStreaming</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; json_sdf.schema == sdf_schema</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">format</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">format</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">str</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;If the path is provided for stream, it needs to be a &quot;</span> <span class="o">+</span>
                                 <span class="s2">&quot;non-empty string. List of paths are not supported.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">load</span><span class="p">())</span></div>

<div class="viewcode-block" id="DataStreamReader.json"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.json">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">json</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">primitivesAsString</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefersDecimal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">allowComments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">allowUnquotedFieldNames</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">allowSingleQuotes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">allowNumericLeadingZero</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">allowBackslashEscapingAnyCharacter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">columnNameOfCorruptRecord</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dateFormat</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">timestampFormat</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">multiLine</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">allowUnquotedControlChars</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lineSep</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads a JSON file stream and returns the results as a :class:`DataFrame`.</span>

<span class="sd">        `JSON Lines &lt;http://jsonlines.org/&gt;`_ (newline-delimited JSON) is supported by default.</span>
<span class="sd">        For JSON (one record per file), set the ``multiLine`` parameter to ``true``.</span>

<span class="sd">        If the ``schema`` parameter is not specified, this function goes</span>
<span class="sd">        through the input once to determine the input schema.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param path: string represents path to the JSON dataset,</span>
<span class="sd">                     or RDD of Strings storing JSON objects.</span>
<span class="sd">        :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema</span>
<span class="sd">                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).</span>
<span class="sd">        :param primitivesAsString: infers all primitive values as a string type. If None is set,</span>
<span class="sd">                                   it uses the default value, ``false``.</span>
<span class="sd">        :param prefersDecimal: infers all floating-point values as a decimal type. If the values</span>
<span class="sd">                               do not fit in decimal, then it infers them as doubles. If None is</span>
<span class="sd">                               set, it uses the default value, ``false``.</span>
<span class="sd">        :param allowComments: ignores Java/C++ style comment in JSON records. If None is set,</span>
<span class="sd">                              it uses the default value, ``false``.</span>
<span class="sd">        :param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,</span>
<span class="sd">                                        it uses the default value, ``false``.</span>
<span class="sd">        :param allowSingleQuotes: allows single quotes in addition to double quotes. If None is</span>
<span class="sd">                                        set, it uses the default value, ``true``.</span>
<span class="sd">        :param allowNumericLeadingZero: allows leading zeros in numbers (e.g. 00012). If None is</span>
<span class="sd">                                        set, it uses the default value, ``false``.</span>
<span class="sd">        :param allowBackslashEscapingAnyCharacter: allows accepting quoting of all character</span>
<span class="sd">                                                   using backslash quoting mechanism. If None is</span>
<span class="sd">                                                   set, it uses the default value, ``false``.</span>
<span class="sd">        :param mode: allows a mode for dealing with corrupt records during parsing. If None is</span>
<span class="sd">                     set, it uses the default value, ``PERMISSIVE``.</span>

<span class="sd">                * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \</span>
<span class="sd">                  into a field configured by ``columnNameOfCorruptRecord``, and sets other \</span>
<span class="sd">                  fields to ``null``. To keep corrupt records, an user can set a string type \</span>
<span class="sd">                  field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \</span>
<span class="sd">                  schema does not have the field, it drops corrupt records during parsing. \</span>
<span class="sd">                  When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord`` \</span>
<span class="sd">                  field in an output schema.</span>
<span class="sd">                *  ``DROPMALFORMED`` : ignores the whole corrupted records.</span>
<span class="sd">                *  ``FAILFAST`` : throws an exception when it meets corrupted records.</span>

<span class="sd">        :param columnNameOfCorruptRecord: allows renaming the new field having malformed string</span>
<span class="sd">                                          created by ``PERMISSIVE`` mode. This overrides</span>
<span class="sd">                                          ``spark.sql.columnNameOfCorruptRecord``. If None is set,</span>
<span class="sd">                                          it uses the value specified in</span>
<span class="sd">                                          ``spark.sql.columnNameOfCorruptRecord``.</span>
<span class="sd">        :param dateFormat: sets the string that indicates a date format. Custom date formats</span>
<span class="sd">                           follow the formats at ``java.text.SimpleDateFormat``. This</span>
<span class="sd">                           applies to date type. If None is set, it uses the</span>
<span class="sd">                           default value, ``yyyy-MM-dd``.</span>
<span class="sd">        :param timestampFormat: sets the string that indicates a timestamp format. Custom date</span>
<span class="sd">                                formats follow the formats at ``java.text.SimpleDateFormat``.</span>
<span class="sd">                                This applies to timestamp type. If None is set, it uses the</span>
<span class="sd">                                default value, ``yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSSXXX``.</span>
<span class="sd">        :param multiLine: parse one record, which may span multiple lines, per file. If None is</span>
<span class="sd">                          set, it uses the default value, ``false``.</span>
<span class="sd">        :param allowUnquotedControlChars: allows JSON Strings to contain unquoted control</span>
<span class="sd">                                          characters (ASCII characters with value less than 32,</span>
<span class="sd">                                          including tab and line feed characters) or not.</span>
<span class="sd">        :param lineSep: defines the line separator that should be used for parsing. If None is</span>
<span class="sd">                        set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.</span>

<span class="sd">        &gt;&gt;&gt; json_sdf = spark.readStream.json(tempfile.mkdtemp(), schema = sdf_schema)</span>
<span class="sd">        &gt;&gt;&gt; json_sdf.isStreaming</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; json_sdf.schema == sdf_schema</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_opts</span><span class="p">(</span>
            <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">primitivesAsString</span><span class="o">=</span><span class="n">primitivesAsString</span><span class="p">,</span> <span class="n">prefersDecimal</span><span class="o">=</span><span class="n">prefersDecimal</span><span class="p">,</span>
            <span class="n">allowComments</span><span class="o">=</span><span class="n">allowComments</span><span class="p">,</span> <span class="n">allowUnquotedFieldNames</span><span class="o">=</span><span class="n">allowUnquotedFieldNames</span><span class="p">,</span>
            <span class="n">allowSingleQuotes</span><span class="o">=</span><span class="n">allowSingleQuotes</span><span class="p">,</span> <span class="n">allowNumericLeadingZero</span><span class="o">=</span><span class="n">allowNumericLeadingZero</span><span class="p">,</span>
            <span class="n">allowBackslashEscapingAnyCharacter</span><span class="o">=</span><span class="n">allowBackslashEscapingAnyCharacter</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">columnNameOfCorruptRecord</span><span class="o">=</span><span class="n">columnNameOfCorruptRecord</span><span class="p">,</span> <span class="n">dateFormat</span><span class="o">=</span><span class="n">dateFormat</span><span class="p">,</span>
            <span class="n">timestampFormat</span><span class="o">=</span><span class="n">timestampFormat</span><span class="p">,</span> <span class="n">multiLine</span><span class="o">=</span><span class="n">multiLine</span><span class="p">,</span>
            <span class="n">allowUnquotedControlChars</span><span class="o">=</span><span class="n">allowUnquotedControlChars</span><span class="p">,</span> <span class="n">lineSep</span><span class="o">=</span><span class="n">lineSep</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">basestring</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;path can be only a single string&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="DataStreamReader.orc"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.orc">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.3</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">orc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a ORC file stream, returning the result as a :class:`DataFrame`.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        &gt;&gt;&gt; orc_sdf = spark.readStream.schema(sdf_schema).orc(tempfile.mkdtemp())</span>
<span class="sd">        &gt;&gt;&gt; orc_sdf.isStreaming</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; orc_sdf.schema == sdf_schema</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">basestring</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;path can be only a single string&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="DataStreamReader.parquet"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.parquet">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">parquet</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a Parquet file stream, returning the result as a :class:`DataFrame`.</span>

<span class="sd">        You can set the following Parquet-specific option(s) for reading Parquet files:</span>
<span class="sd">            * ``mergeSchema``: sets whether we should merge schemas collected from all \</span>
<span class="sd">                Parquet part-files. This will override ``spark.sql.parquet.mergeSchema``. \</span>
<span class="sd">                The default value is specified in ``spark.sql.parquet.mergeSchema``.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        &gt;&gt;&gt; parquet_sdf = spark.readStream.schema(sdf_schema).parquet(tempfile.mkdtemp())</span>
<span class="sd">        &gt;&gt;&gt; parquet_sdf.isStreaming</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; parquet_sdf.schema == sdf_schema</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">basestring</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;path can be only a single string&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="DataStreamReader.text"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.text">[docs]</a>    <span class="nd">@ignore_unicode_prefix</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">wholetext</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lineSep</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads a text file stream and returns a :class:`DataFrame` whose schema starts with a</span>
<span class="sd">        string column named &quot;value&quot;, and followed by partitioned columns if there</span>
<span class="sd">        are any.</span>

<span class="sd">        By default, each line in the text file is a new row in the resulting DataFrame.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param paths: string, or list of strings, for input path(s).</span>
<span class="sd">        :param wholetext: if true, read each file from input path(s) as a single row.</span>
<span class="sd">        :param lineSep: defines the line separator that should be used for parsing. If None is</span>
<span class="sd">                        set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.</span>

<span class="sd">        &gt;&gt;&gt; text_sdf = spark.readStream.text(tempfile.mkdtemp())</span>
<span class="sd">        &gt;&gt;&gt; text_sdf.isStreaming</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; &quot;value&quot; in str(text_sdf.schema)</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_opts</span><span class="p">(</span><span class="n">wholetext</span><span class="o">=</span><span class="n">wholetext</span><span class="p">,</span> <span class="n">lineSep</span><span class="o">=</span><span class="n">lineSep</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">basestring</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;path can be only a single string&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="DataStreamReader.csv"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamReader.csv">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">csv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">quote</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">escape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">comment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignoreLeadingWhiteSpace</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">ignoreTrailingWhiteSpace</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nullValue</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nanValue</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">positiveInf</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">negativeInf</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dateFormat</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">timestampFormat</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">maxColumns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">maxCharsPerColumn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">maxMalformedLogPerPartition</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">columnNameOfCorruptRecord</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">multiLine</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">charToEscapeQuoteEscaping</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">enforceSchema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">emptyValue</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Loads a CSV file stream and returns the result as a :class:`DataFrame`.</span>

<span class="sd">        This function will go through the input once to determine the input schema if</span>
<span class="sd">        ``inferSchema`` is enabled. To avoid going through the entire data once, disable</span>
<span class="sd">        ``inferSchema`` option or specify the schema explicitly using ``schema``.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param path: string, or list of strings, for input path(s).</span>
<span class="sd">        :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema</span>
<span class="sd">                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).</span>
<span class="sd">        :param sep: sets a single character as a separator for each field and value.</span>
<span class="sd">                    If None is set, it uses the default value, ``,``.</span>
<span class="sd">        :param encoding: decodes the CSV files by the given encoding type. If None is set,</span>
<span class="sd">                         it uses the default value, ``UTF-8``.</span>
<span class="sd">        :param quote: sets a single character used for escaping quoted values where the</span>
<span class="sd">                      separator can be part of the value. If None is set, it uses the default</span>
<span class="sd">                      value, ``&quot;``. If you would like to turn off quotations, you need to set an</span>
<span class="sd">                      empty string.</span>
<span class="sd">        :param escape: sets a single character used for escaping quotes inside an already</span>
<span class="sd">                       quoted value. If None is set, it uses the default value, ``\``.</span>
<span class="sd">        :param comment: sets a single character used for skipping lines beginning with this</span>
<span class="sd">                        character. By default (None), it is disabled.</span>
<span class="sd">        :param header: uses the first line as names of columns. If None is set, it uses the</span>
<span class="sd">                       default value, ``false``.</span>
<span class="sd">        :param inferSchema: infers the input schema automatically from data. It requires one extra</span>
<span class="sd">                       pass over the data. If None is set, it uses the default value, ``false``.</span>
<span class="sd">        :param enforceSchema: If it is set to ``true``, the specified or inferred schema will be</span>
<span class="sd">                              forcibly applied to datasource files, and headers in CSV files will be</span>
<span class="sd">                              ignored. If the option is set to ``false``, the schema will be</span>
<span class="sd">                              validated against all headers in CSV files or the first header in RDD</span>
<span class="sd">                              if the ``header`` option is set to ``true``. Field names in the schema</span>
<span class="sd">                              and column names in CSV headers are checked by their positions</span>
<span class="sd">                              taking into account ``spark.sql.caseSensitive``. If None is set,</span>
<span class="sd">                              ``true`` is used by default. Though the default value is ``true``,</span>
<span class="sd">                              it is recommended to disable the ``enforceSchema`` option</span>
<span class="sd">                              to avoid incorrect results.</span>
<span class="sd">        :param ignoreLeadingWhiteSpace: a flag indicating whether or not leading whitespaces from</span>
<span class="sd">                                        values being read should be skipped. If None is set, it</span>
<span class="sd">                                        uses the default value, ``false``.</span>
<span class="sd">        :param ignoreTrailingWhiteSpace: a flag indicating whether or not trailing whitespaces from</span>
<span class="sd">                                         values being read should be skipped. If None is set, it</span>
<span class="sd">                                         uses the default value, ``false``.</span>
<span class="sd">        :param nullValue: sets the string representation of a null value. If None is set, it uses</span>
<span class="sd">                          the default value, empty string. Since 2.0.1, this ``nullValue`` param</span>
<span class="sd">                          applies to all supported types including the string type.</span>
<span class="sd">        :param nanValue: sets the string representation of a non-number value. If None is set, it</span>
<span class="sd">                         uses the default value, ``NaN``.</span>
<span class="sd">        :param positiveInf: sets the string representation of a positive infinity value. If None</span>
<span class="sd">                            is set, it uses the default value, ``Inf``.</span>
<span class="sd">        :param negativeInf: sets the string representation of a negative infinity value. If None</span>
<span class="sd">                            is set, it uses the default value, ``Inf``.</span>
<span class="sd">        :param dateFormat: sets the string that indicates a date format. Custom date formats</span>
<span class="sd">                           follow the formats at ``java.text.SimpleDateFormat``. This</span>
<span class="sd">                           applies to date type. If None is set, it uses the</span>
<span class="sd">                           default value, ``yyyy-MM-dd``.</span>
<span class="sd">        :param timestampFormat: sets the string that indicates a timestamp format. Custom date</span>
<span class="sd">                                formats follow the formats at ``java.text.SimpleDateFormat``.</span>
<span class="sd">                                This applies to timestamp type. If None is set, it uses the</span>
<span class="sd">                                default value, ``yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSSXXX``.</span>
<span class="sd">        :param maxColumns: defines a hard limit of how many columns a record can have. If None is</span>
<span class="sd">                           set, it uses the default value, ``20480``.</span>
<span class="sd">        :param maxCharsPerColumn: defines the maximum number of characters allowed for any given</span>
<span class="sd">                                  value being read. If None is set, it uses the default value,</span>
<span class="sd">                                  ``-1`` meaning unlimited length.</span>
<span class="sd">        :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.</span>
<span class="sd">                                            If specified, it is ignored.</span>
<span class="sd">        :param mode: allows a mode for dealing with corrupt records during parsing. If None is</span>
<span class="sd">                     set, it uses the default value, ``PERMISSIVE``.</span>

<span class="sd">                * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \</span>
<span class="sd">                  into a field configured by ``columnNameOfCorruptRecord``, and sets other \</span>
<span class="sd">                  fields to ``null``. To keep corrupt records, an user can set a string type \</span>
<span class="sd">                  field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \</span>
<span class="sd">                  schema does not have the field, it drops corrupt records during parsing. \</span>
<span class="sd">                  A record with less/more tokens than schema is not a corrupted record to CSV. \</span>
<span class="sd">                  When it meets a record having fewer tokens than the length of the schema, \</span>
<span class="sd">                  sets ``null`` to extra fields. When the record has more tokens than the \</span>
<span class="sd">                  length of the schema, it drops extra tokens.</span>
<span class="sd">                * ``DROPMALFORMED`` : ignores the whole corrupted records.</span>
<span class="sd">                * ``FAILFAST`` : throws an exception when it meets corrupted records.</span>

<span class="sd">        :param columnNameOfCorruptRecord: allows renaming the new field having malformed string</span>
<span class="sd">                                          created by ``PERMISSIVE`` mode. This overrides</span>
<span class="sd">                                          ``spark.sql.columnNameOfCorruptRecord``. If None is set,</span>
<span class="sd">                                          it uses the value specified in</span>
<span class="sd">                                          ``spark.sql.columnNameOfCorruptRecord``.</span>
<span class="sd">        :param multiLine: parse one record, which may span multiple lines. If None is</span>
<span class="sd">                          set, it uses the default value, ``false``.</span>
<span class="sd">        :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for</span>
<span class="sd">                                          the quote character. If None is set, the default value is</span>
<span class="sd">                                          escape character when escape and quote characters are</span>
<span class="sd">                                          different, ``\0`` otherwise..</span>
<span class="sd">        :param emptyValue: sets the string representation of an empty value. If None is set, it uses</span>
<span class="sd">                           the default value, empty string.</span>

<span class="sd">        &gt;&gt;&gt; csv_sdf = spark.readStream.csv(tempfile.mkdtemp(), schema = sdf_schema)</span>
<span class="sd">        &gt;&gt;&gt; csv_sdf.isStreaming</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; csv_sdf.schema == sdf_schema</span>
<span class="sd">        True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_opts</span><span class="p">(</span>
            <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="n">sep</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span> <span class="n">quote</span><span class="o">=</span><span class="n">quote</span><span class="p">,</span> <span class="n">escape</span><span class="o">=</span><span class="n">escape</span><span class="p">,</span> <span class="n">comment</span><span class="o">=</span><span class="n">comment</span><span class="p">,</span>
            <span class="n">header</span><span class="o">=</span><span class="n">header</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="n">inferSchema</span><span class="p">,</span> <span class="n">ignoreLeadingWhiteSpace</span><span class="o">=</span><span class="n">ignoreLeadingWhiteSpace</span><span class="p">,</span>
            <span class="n">ignoreTrailingWhiteSpace</span><span class="o">=</span><span class="n">ignoreTrailingWhiteSpace</span><span class="p">,</span> <span class="n">nullValue</span><span class="o">=</span><span class="n">nullValue</span><span class="p">,</span>
            <span class="n">nanValue</span><span class="o">=</span><span class="n">nanValue</span><span class="p">,</span> <span class="n">positiveInf</span><span class="o">=</span><span class="n">positiveInf</span><span class="p">,</span> <span class="n">negativeInf</span><span class="o">=</span><span class="n">negativeInf</span><span class="p">,</span>
            <span class="n">dateFormat</span><span class="o">=</span><span class="n">dateFormat</span><span class="p">,</span> <span class="n">timestampFormat</span><span class="o">=</span><span class="n">timestampFormat</span><span class="p">,</span> <span class="n">maxColumns</span><span class="o">=</span><span class="n">maxColumns</span><span class="p">,</span>
            <span class="n">maxCharsPerColumn</span><span class="o">=</span><span class="n">maxCharsPerColumn</span><span class="p">,</span>
            <span class="n">maxMalformedLogPerPartition</span><span class="o">=</span><span class="n">maxMalformedLogPerPartition</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
            <span class="n">columnNameOfCorruptRecord</span><span class="o">=</span><span class="n">columnNameOfCorruptRecord</span><span class="p">,</span> <span class="n">multiLine</span><span class="o">=</span><span class="n">multiLine</span><span class="p">,</span>
            <span class="n">charToEscapeQuoteEscaping</span><span class="o">=</span><span class="n">charToEscapeQuoteEscaping</span><span class="p">,</span> <span class="n">enforceSchema</span><span class="o">=</span><span class="n">enforceSchema</span><span class="p">,</span>
            <span class="n">emptyValue</span><span class="o">=</span><span class="n">emptyValue</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">basestring</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jreader</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;path can be only a single string&quot;</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DataStreamWriter"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter">[docs]</a><span class="k">class</span> <span class="nc">DataStreamWriter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Interface used to write a streaming :class:`DataFrame` to external storage systems</span>
<span class="sd">    (e.g. file systems, key-value stores, etc). Use :func:`DataFrame.writeStream`</span>
<span class="sd">    to access this.</span>

<span class="sd">    .. note:: Evolving.</span>

<span class="sd">    .. versionadded:: 2.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_df</span> <span class="o">=</span> <span class="n">df</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_spark</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sql_ctx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">_jdf</span><span class="o">.</span><span class="n">writeStream</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_sq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jsq</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.streaming</span> <span class="k">import</span> <span class="n">StreamingQuery</span>
        <span class="k">return</span> <span class="n">StreamingQuery</span><span class="p">(</span><span class="n">jsq</span><span class="p">)</span>

<div class="viewcode-block" id="DataStreamWriter.outputMode"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter.outputMode">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">outputMode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputMode</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.</span>

<span class="sd">        Options include:</span>

<span class="sd">        * `append`:Only the new rows in the streaming DataFrame/Dataset will be written to</span>
<span class="sd">           the sink</span>
<span class="sd">        * `complete`:All the rows in the streaming DataFrame/Dataset will be written to the sink</span>
<span class="sd">           every time these is some updates</span>
<span class="sd">        * `update`:only the rows that were updated in the streaming DataFrame/Dataset will be</span>
<span class="sd">           written to the sink every time there are some updates. If the query doesn&#39;t contain</span>
<span class="sd">           aggregations, it will be equivalent to `append` mode.</span>

<span class="sd">       .. note:: Evolving.</span>

<span class="sd">        &gt;&gt;&gt; writer = sdf.writeStream.outputMode(&#39;append&#39;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">outputMode</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">outputMode</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">str</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputMode</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The output mode must be a non-empty string. Got: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">outputMode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="n">outputMode</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamWriter.format"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter.format">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">format</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Specifies the underlying output data source.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param source: string, name of the data source, which for now can be &#39;parquet&#39;.</span>

<span class="sd">        &gt;&gt;&gt; writer = sdf.writeStream.format(&#39;json&#39;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamWriter.option"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter.option">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">option</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds an output option for the underlying data source.</span>

<span class="sd">        You can set the following option(s) for writing files:</span>
<span class="sd">            * ``timeZone``: sets the string that indicates a timezone to be used to format</span>
<span class="sd">                timestamps in the JSON/CSV datasources or partition values.</span>
<span class="sd">                If it isn&#39;t set, it uses the default value, session local timezone.</span>

<span class="sd">        .. note:: Evolving.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">to_str</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamWriter.options"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter.options">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds output options for the underlying data source.</span>

<span class="sd">        You can set the following option(s) for writing files:</span>
<span class="sd">            * ``timeZone``: sets the string that indicates a timezone to be used to format</span>
<span class="sd">                timestamps in the JSON/CSV datasources or partition values.</span>
<span class="sd">                If it isn&#39;t set, it uses the default value, session local timezone.</span>

<span class="sd">       .. note:: Evolving.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">options</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">to_str</span><span class="p">(</span><span class="n">options</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamWriter.partitionBy"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter.partitionBy">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">partitionBy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">cols</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Partitions the output by the given columns on the file system.</span>

<span class="sd">        If specified, the output is laid out on the file system similar</span>
<span class="sd">        to Hive&#39;s partitioning scheme.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param cols: name of columns</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">cols</span> <span class="o">=</span> <span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_spark</span><span class="o">.</span><span class="n">_sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamWriter.queryName"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter.queryName">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">queryName</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queryName</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Specifies the name of the :class:`StreamingQuery` that can be started with</span>
<span class="sd">        :func:`start`. This name must be unique among all the currently active queries</span>
<span class="sd">        in the associated SparkSession.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param queryName: unique name for the query</span>

<span class="sd">        &gt;&gt;&gt; writer = sdf.writeStream.queryName(&#39;streaming_query&#39;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">queryName</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">queryName</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">str</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">queryName</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The queryName must be a non-empty string. Got: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">queryName</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="n">queryName</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamWriter.trigger"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter.trigger">[docs]</a>    <span class="nd">@keyword_only</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">trigger</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">processingTime</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">once</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set the trigger for the stream query. If this is not set it will run the query as fast</span>
<span class="sd">        as possible, which is equivalent to setting the trigger to ``processingTime=&#39;0 seconds&#39;``.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param processingTime: a processing time interval as a string, e.g. &#39;5 seconds&#39;, &#39;1 minute&#39;.</span>
<span class="sd">                               Set a trigger that runs a query periodically based on the processing</span>
<span class="sd">                               time. Only one trigger can be set.</span>
<span class="sd">        :param once: if set to True, set a trigger that processes only one batch of data in a</span>
<span class="sd">                     streaming query then terminates the query. Only one trigger can be set.</span>

<span class="sd">        &gt;&gt;&gt; # trigger the query for execution every 5 seconds</span>
<span class="sd">        &gt;&gt;&gt; writer = sdf.writeStream.trigger(processingTime=&#39;5 seconds&#39;)</span>
<span class="sd">        &gt;&gt;&gt; # trigger the query for just once batch of data</span>
<span class="sd">        &gt;&gt;&gt; writer = sdf.writeStream.trigger(once=True)</span>
<span class="sd">        &gt;&gt;&gt; # trigger the query for execution every 5 seconds</span>
<span class="sd">        &gt;&gt;&gt; writer = sdf.writeStream.trigger(continuous=&#39;5 seconds&#39;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">processingTime</span><span class="p">,</span> <span class="n">once</span><span class="p">,</span> <span class="n">continuous</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;No trigger provided&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">params</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Multiple triggers not allowed.&#39;</span><span class="p">)</span>

        <span class="n">jTrigger</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">processingTime</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">processingTime</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">str</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">processingTime</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Value for processingTime must be a non empty string. Got: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                                 <span class="n">processingTime</span><span class="p">)</span>
            <span class="n">interval</span> <span class="o">=</span> <span class="n">processingTime</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">jTrigger</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spark</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">streaming</span><span class="o">.</span><span class="n">Trigger</span><span class="o">.</span><span class="n">ProcessingTime</span><span class="p">(</span>
                <span class="n">interval</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">once</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">once</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">True</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Value for once must be True. Got: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">once</span><span class="p">)</span>
            <span class="n">jTrigger</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spark</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">streaming</span><span class="o">.</span><span class="n">Trigger</span><span class="o">.</span><span class="n">Once</span><span class="p">()</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">continuous</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">str</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Value for continuous must be a non empty string. Got: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                                 <span class="n">continuous</span><span class="p">)</span>
            <span class="n">interval</span> <span class="o">=</span> <span class="n">continuous</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">jTrigger</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spark</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">streaming</span><span class="o">.</span><span class="n">Trigger</span><span class="o">.</span><span class="n">Continuous</span><span class="p">(</span>
                <span class="n">interval</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">jTrigger</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamWriter.foreach"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter.foreach">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">foreach</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the output of the streaming query to be processed using the provided writer ``f``.</span>
<span class="sd">        This is often used to write the output of a streaming query to arbitrary storage systems.</span>
<span class="sd">        The processing logic can be specified in two ways.</span>

<span class="sd">        #. A **function** that takes a row as input.</span>
<span class="sd">            This is a simple way to express your processing logic. Note that this does</span>
<span class="sd">            not allow you to deduplicate generated data when failures cause reprocessing of</span>
<span class="sd">            some input data. That would require you to specify the processing logic in the next</span>
<span class="sd">            way.</span>

<span class="sd">        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.</span>
<span class="sd">            The object can have the following methods.</span>

<span class="sd">            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing</span>
<span class="sd">                (for example, open a connection, start a transaction, etc). Additionally, you can</span>
<span class="sd">                use the `partition_id` and `epoch_id` to deduplicate regenerated data</span>
<span class="sd">                (discussed later).</span>

<span class="sd">            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.</span>

<span class="sd">            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,</span>
<span class="sd">                close connection, commit transaction, etc.) after all rows have been processed.</span>

<span class="sd">            The object will be used by Spark in the following way.</span>

<span class="sd">            * A single copy of this object is responsible of all the data generated by a</span>
<span class="sd">                single task in a query. In other words, one instance is responsible for</span>
<span class="sd">                processing one partition of the data generated in a distributed manner.</span>

<span class="sd">            * This object must be serializable because each task will get a fresh</span>
<span class="sd">                serialized-deserialized copy of the provided object. Hence, it is strongly</span>
<span class="sd">                recommended that any initialization for writing data (e.g. opening a</span>
<span class="sd">                connection or starting a transaction) is done after the `open(...)`</span>
<span class="sd">                method has been called, which signifies that the task is ready to generate data.</span>

<span class="sd">            * The lifecycle of the methods are as follows.</span>

<span class="sd">                For each partition with ``partition_id``:</span>

<span class="sd">                ... For each batch/epoch of streaming data with ``epoch_id``:</span>

<span class="sd">                ....... Method ``open(partitionId, epochId)`` is called.</span>

<span class="sd">                ....... If ``open(...)`` returns true, for each row in the partition and</span>
<span class="sd">                        batch/epoch, method ``process(row)`` is called.</span>

<span class="sd">                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while</span>
<span class="sd">                        processing rows.</span>

<span class="sd">            Important points to note:</span>

<span class="sd">            * The `partitionId` and `epochId` can be used to deduplicate generated data when</span>
<span class="sd">                failures cause reprocessing of some input data. This depends on the execution</span>
<span class="sd">                mode of the query. If the streaming query is being executed in the micro-batch</span>
<span class="sd">                mode, then every partition represented by a unique tuple (partition_id, epoch_id)</span>
<span class="sd">                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used</span>
<span class="sd">                to deduplicate and/or transactionally commit data and achieve exactly-once</span>
<span class="sd">                guarantees. However, if the streaming query is being executed in the continuous</span>
<span class="sd">                mode, then this guarantee does not hold and therefore should not be used for</span>
<span class="sd">                deduplication.</span>

<span class="sd">            * The ``close()`` method (if exists) will be called if `open()` method exists and</span>
<span class="sd">                returns successfully (irrespective of the return value), except if the Python</span>
<span class="sd">                crashes in the middle.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        &gt;&gt;&gt; # Print every row using a function</span>
<span class="sd">        &gt;&gt;&gt; def print_row(row):</span>
<span class="sd">        ...     print(row)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; writer = sdf.writeStream.foreach(print_row)</span>
<span class="sd">        &gt;&gt;&gt; # Print every row using a object with process() method</span>
<span class="sd">        &gt;&gt;&gt; class RowPrinter:</span>
<span class="sd">        ...     def open(self, partition_id, epoch_id):</span>
<span class="sd">        ...         print(&quot;Opened %d, %d&quot; % (partition_id, epoch_id))</span>
<span class="sd">        ...         return True</span>
<span class="sd">        ...     def process(self, row):</span>
<span class="sd">        ...         print(row)</span>
<span class="sd">        ...     def close(self, error):</span>
<span class="sd">        ...         print(&quot;Closed with error: %s&quot; % str(error))</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; writer = sdf.writeStream.foreach(RowPrinter())</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="k">import</span> <span class="n">_wrap_function</span>
        <span class="kn">from</span> <span class="nn">pyspark.serializers</span> <span class="k">import</span> <span class="n">PickleSerializer</span><span class="p">,</span> <span class="n">AutoBatchedSerializer</span>
        <span class="kn">from</span> <span class="nn">pyspark.taskcontext</span> <span class="k">import</span> <span class="n">TaskContext</span>

        <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
            <span class="c1"># The provided object is a callable function that is supposed to be called on each row.</span>
            <span class="c1"># Construct a function that takes an iterator and calls the provided function on each</span>
            <span class="c1"># row.</span>
            <span class="k">def</span> <span class="nf">func_without_process</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">iterator</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>
                    <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">iter</span><span class="p">([])</span>

            <span class="n">func</span> <span class="o">=</span> <span class="n">func_without_process</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># The provided object is not a callable function. Then it is expected to have a</span>
            <span class="c1"># &#39;process(row)&#39; method, and optional &#39;open(partition_id, epoch_id)&#39; and</span>
            <span class="c1"># &#39;close(error)&#39; methods.</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s1">&#39;process&#39;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Provided object does not have a &#39;process&#39; method&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s1">&#39;process&#39;</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Attribute &#39;process&#39; in provided object is not callable&quot;</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">doesMethodExist</span><span class="p">(</span><span class="n">method_name</span><span class="p">):</span>
                <span class="n">exists</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">method_name</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">exists</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">method_name</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                        <span class="s2">&quot;Attribute &#39;</span><span class="si">%s</span><span class="s2">&#39; in provided object is not callable&quot;</span> <span class="o">%</span> <span class="n">method_name</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">exists</span>

            <span class="n">open_exists</span> <span class="o">=</span> <span class="n">doesMethodExist</span><span class="p">(</span><span class="s1">&#39;open&#39;</span><span class="p">)</span>
            <span class="n">close_exists</span> <span class="o">=</span> <span class="n">doesMethodExist</span><span class="p">(</span><span class="s1">&#39;close&#39;</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">func_with_open_process_close</span><span class="p">(</span><span class="n">partition_id</span><span class="p">,</span> <span class="n">iterator</span><span class="p">):</span>
                <span class="n">epoch_id</span> <span class="o">=</span> <span class="n">TaskContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span><span class="o">.</span><span class="n">getLocalProperty</span><span class="p">(</span><span class="s1">&#39;streaming.sql.batchId&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">epoch_id</span><span class="p">:</span>
                    <span class="n">epoch_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">epoch_id</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Could not get batch id from TaskContext&quot;</span><span class="p">)</span>

                <span class="c1"># Check if the data should be processed</span>
                <span class="n">should_process</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">if</span> <span class="n">open_exists</span><span class="p">:</span>
                    <span class="n">should_process</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">partition_id</span><span class="p">,</span> <span class="n">epoch_id</span><span class="p">)</span>

                <span class="n">error</span> <span class="o">=</span> <span class="kc">None</span>

                <span class="k">try</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">should_process</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>
                            <span class="n">f</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">ex</span><span class="p">:</span>
                    <span class="n">error</span> <span class="o">=</span> <span class="n">ex</span>
                <span class="k">finally</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">close_exists</span><span class="p">:</span>
                        <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">error</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">error</span>

                <span class="k">return</span> <span class="nb">iter</span><span class="p">([])</span>

            <span class="n">func</span> <span class="o">=</span> <span class="n">func_with_open_process_close</span>

        <span class="n">serializer</span> <span class="o">=</span> <span class="n">AutoBatchedSerializer</span><span class="p">(</span><span class="n">PickleSerializer</span><span class="p">())</span>
        <span class="n">wrapped_func</span> <span class="o">=</span> <span class="n">_wrap_function</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_spark</span><span class="o">.</span><span class="n">_sc</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">serializer</span><span class="p">,</span> <span class="n">serializer</span><span class="p">)</span>
        <span class="n">jForeachWriter</span> <span class="o">=</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">_spark</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">execution</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">PythonForeachWriter</span><span class="p">(</span>
                <span class="n">wrapped_func</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span><span class="o">.</span><span class="n">_jdf</span><span class="o">.</span><span class="n">schema</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">jForeachWriter</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamWriter.foreachBatch"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter.foreachBatch">[docs]</a>    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">foreachBatch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the output of the streaming query to be processed using the provided</span>
<span class="sd">        function. This is supported only the in the micro-batch execution modes (that is, when the</span>
<span class="sd">        trigger is not continuous). In every micro-batch, the provided function will be called in</span>
<span class="sd">        every micro-batch with (i) the output rows as a DataFrame and (ii) the batch identifier.</span>
<span class="sd">        The batchId can be used deduplicate and transactionally write the output</span>
<span class="sd">        (that is, the provided Dataset) to external systems. The output DataFrame is guaranteed</span>
<span class="sd">        to exactly same for the same batchId (assuming all operations are deterministic in the</span>
<span class="sd">        query).</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        &gt;&gt;&gt; def func(batch_df, batch_id):</span>
<span class="sd">        ...     batch_df.collect()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; writer = sdf.writeStream.foreach(func)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">pyspark.java_gateway</span> <span class="k">import</span> <span class="n">ensure_callback_server_started</span>
        <span class="n">gw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spark</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_gateway</span>
        <span class="n">java_import</span><span class="p">(</span><span class="n">gw</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s2">&quot;org.apache.spark.sql.execution.streaming.sources.*&quot;</span><span class="p">)</span>

        <span class="n">wrapped_func</span> <span class="o">=</span> <span class="n">ForeachBatchFunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_spark</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
        <span class="n">gw</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">PythonForeachBatchHelper</span><span class="o">.</span><span class="n">callForeachBatch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="p">,</span> <span class="n">wrapped_func</span><span class="p">)</span>
        <span class="n">ensure_callback_server_started</span><span class="p">(</span><span class="n">gw</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="DataStreamWriter.start"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter.start">[docs]</a>    <span class="nd">@ignore_unicode_prefix</span>
    <span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">outputMode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">partitionBy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">queryName</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="o">**</span><span class="n">options</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Streams the contents of the :class:`DataFrame` to a data source.</span>

<span class="sd">        The data source is specified by the ``format`` and a set of ``options``.</span>
<span class="sd">        If ``format`` is not specified, the default data source configured by</span>
<span class="sd">        ``spark.sql.sources.default`` will be used.</span>

<span class="sd">        .. note:: Evolving.</span>

<span class="sd">        :param path: the path in a Hadoop supported file system</span>
<span class="sd">        :param format: the format used to save</span>
<span class="sd">        :param outputMode: specifies how data of a streaming DataFrame/Dataset is written to a</span>
<span class="sd">                           streaming sink.</span>

<span class="sd">            * `append`:Only the new rows in the streaming DataFrame/Dataset will be written to the</span>
<span class="sd">              sink</span>
<span class="sd">            * `complete`:All the rows in the streaming DataFrame/Dataset will be written to the sink</span>
<span class="sd">               every time these is some updates</span>
<span class="sd">            * `update`:only the rows that were updated in the streaming DataFrame/Dataset will be</span>
<span class="sd">              written to the sink every time there are some updates. If the query doesn&#39;t contain</span>
<span class="sd">              aggregations, it will be equivalent to `append` mode.</span>
<span class="sd">        :param partitionBy: names of partitioning columns</span>
<span class="sd">        :param queryName: unique name for the query</span>
<span class="sd">        :param options: All other string options. You may want to provide a `checkpointLocation`</span>
<span class="sd">                        for most streams, however it is not required for a `memory` stream.</span>

<span class="sd">        &gt;&gt;&gt; sq = sdf.writeStream.format(&#39;memory&#39;).queryName(&#39;this_query&#39;).start()</span>
<span class="sd">        &gt;&gt;&gt; sq.isActive</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; sq.name</span>
<span class="sd">        u&#39;this_query&#39;</span>
<span class="sd">        &gt;&gt;&gt; sq.stop()</span>
<span class="sd">        &gt;&gt;&gt; sq.isActive</span>
<span class="sd">        False</span>
<span class="sd">        &gt;&gt;&gt; sq = sdf.writeStream.trigger(processingTime=&#39;5 seconds&#39;).start(</span>
<span class="sd">        ...     queryName=&#39;that_query&#39;, outputMode=&quot;append&quot;, format=&#39;memory&#39;)</span>
<span class="sd">        &gt;&gt;&gt; sq.name</span>
<span class="sd">        u&#39;that_query&#39;</span>
<span class="sd">        &gt;&gt;&gt; sq.isActive</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; sq.stop()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">outputMode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="n">outputMode</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">partitionBy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="n">partitionBy</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">format</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">format</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">queryName</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="n">queryName</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="o">.</span><span class="n">start</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jwrite</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">path</span><span class="p">))</span></div></div>


<span class="k">def</span> <span class="nf">_test</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">import</span> <span class="nn">tempfile</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">SQLContext</span>
    <span class="kn">import</span> <span class="nn">pyspark.sql.streaming</span>

    <span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;SPARK_HOME&quot;</span><span class="p">])</span>

    <span class="n">globs</span> <span class="o">=</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">streaming</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">py4j</span><span class="o">.</span><span class="n">protocol</span><span class="o">.</span><span class="n">Py4JError</span><span class="p">:</span>
        <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>

    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;tempfile&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tempfile</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;os&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;spark&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spark</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;sqlContext&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="p">)</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;sdf&#39;</span><span class="p">]</span> <span class="o">=</span> \
        <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/streaming&#39;</span><span class="p">)</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;sdf_schema&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">)])</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;df&#39;</span><span class="p">]</span> <span class="o">=</span> \
        <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;spark&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;python/test_support/sql/streaming&#39;</span><span class="p">)</span>

    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span>
        <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">streaming</span><span class="p">,</span> <span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
        <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">NORMALIZE_WHITESPACE</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">REPORT_NDIFF</span><span class="p">)</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;spark&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/spark-logo-hd.png" alt="Logo"/>
            </a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
    
        <li class="nav-item nav-item-0"><a href="../../../index.html">PySpark 2.4.4 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.2.0.
    </div>
  </body>
</html>