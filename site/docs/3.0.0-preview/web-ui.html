
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Web UI - Spark 3.0.0-preview Documentation</title>
        
          <meta name="description" content="Web UI guide for Spark 3.0.0-preview">
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html">
                      <img src="img/spark-logo-hd.png" style="height:50px;"/></a><span class="version">3.0.0-preview</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">Overview</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Programming Guides<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">Quick Start</a></li>
                                <li><a href="rdd-programming-guide.html">RDDs, Accumulators, Broadcasts Vars</a></li>
                                <li><a href="sql-programming-guide.html">SQL, DataFrames, and Datasets</a></li>
                                <li><a href="structured-streaming-programming-guide.html">Structured Streaming</a></li>
                                <li><a href="streaming-programming-guide.html">Spark Streaming (DStreams)</a></li>
                                <li><a href="ml-guide.html">MLlib (Machine Learning)</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX (Graph Processing)</a></li>
                                <li><a href="sparkr.html">SparkR (R on Spark)</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API Docs<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">Scala</a></li>
                                <li><a href="api/java/index.html">Java</a></li>
                                <li><a href="api/python/index.html">Python</a></li>
                                <li><a href="api/R/index.html">R</a></li>
                                <li><a href="api/sql/index.html">SQL, Built-in Functions</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Deploying<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">Overview</a></li>
                                <li><a href="submitting-applications.html">Submitting Applications</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark Standalone</a></li>
                                <li><a href="running-on-mesos.html">Mesos</a></li>
                                <li><a href="running-on-yarn.html">YARN</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">More<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">Configuration</a></li>
                                <li><a href="monitoring.html">Monitoring</a></li>
                                <li><a href="tuning.html">Tuning Guide</a></li>
                                <li><a href="job-scheduling.html">Job Scheduling</a></li>
                                <li><a href="security.html">Security</a></li>
                                <li><a href="hardware-provisioning.html">Hardware Provisioning</a></li>
                                <li><a href="migration-guide.html">Migration Guide</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">Building Spark</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">Contributing to Spark</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">Third Party Projects</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v3.0.0-preview</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                <div class="content" id="content">
                    
                        <h1 class="title">Web UI</h1>
                    

                    <p>Apache Spark provides a suite of web user interfaces (UIs) that you can use
to monitor the status and resource consumption of your Spark cluster.</p>

<p><strong>Table of Contents</strong></p>

<ul id="markdown-toc">
  <li><a href="#jobs-tab" id="markdown-toc-jobs-tab">Jobs Tab</a>    <ul>
      <li><a href="#jobs-detail" id="markdown-toc-jobs-detail">Jobs detail</a></li>
    </ul>
  </li>
  <li><a href="#stages-tab" id="markdown-toc-stages-tab">Stages Tab</a>    <ul>
      <li><a href="#stage-detail" id="markdown-toc-stage-detail">Stage detail</a></li>
    </ul>
  </li>
  <li><a href="#storage-tab" id="markdown-toc-storage-tab">Storage Tab</a></li>
  <li><a href="#environment-tab" id="markdown-toc-environment-tab">Environment Tab</a></li>
  <li><a href="#executors-tab" id="markdown-toc-executors-tab">Executors Tab</a></li>
  <li><a href="#sql-tab" id="markdown-toc-sql-tab">SQL Tab</a>    <ul>
      <li><a href="#sql-metrics" id="markdown-toc-sql-metrics">SQL metrics</a></li>
    </ul>
  </li>
  <li><a href="#streaming-tab" id="markdown-toc-streaming-tab">Streaming Tab</a></li>
  <li><a href="#jdbcodbc-server-tab" id="markdown-toc-jdbcodbc-server-tab">JDBC/ODBC Server Tab</a></li>
</ul>

<h2 id="jobs-tab">Jobs Tab</h2>
<p>The Jobs tab displays a summary page of all jobs in the Spark application and a details page
for each job. The summary page shows high-level information, such as the status, duration, and
progress of all jobs and the overall event timeline. When you click on a job on the summary
page, you see the details page for that job. The details page further shows the event timeline,
DAG visualization, and all stages of the job.</p>

<p>The information that is displayed in this section is</p>
<ul>
  <li>User: Current Spark user</li>
  <li>Total uptime: Time since Spark application started</li>
  <li>Scheduling mode: See <a href="job-scheduling.html#configuring-pool-properties">job scheduling</a></li>
  <li>Number of jobs per status: Active, Completed, Failed</li>
</ul>

<p style="text-align: center;">
  <img src="img/AllJobsPageDetail1.png" title="Basic info" alt="Basic info" width="20%" />
</p>

<ul>
  <li>Event timeline: Displays in chronological order the events related to the executors (added, removed) and the jobs</li>
</ul>

<p style="text-align: center;">
  <img src="img/AllJobsPageDetail2.png" title="Event timeline" alt="Event timeline" />
</p>

<ul>
  <li>Details of jobs grouped by status: Displays detailed information of the jobs including Job ID, description (with a link to detailed job page), submitted time, duration, stages summary and tasks progress bar</li>
</ul>

<p style="text-align: center;">
  <img src="img/AllJobsPageDetail3.png" title="Details of jobs grouped by status" alt="Details of jobs grouped by status" />
</p>

<p>When you click on a specific job, you can see the detailed information of this job.</p>

<h3 id="jobs-detail">Jobs detail</h3>

<p>This page displays the details of a specific job identified by its job ID.</p>
<ul>
  <li>Job Status: (running, succeeded, failed)</li>
  <li>Number of stages per status (active, pending, completed, skipped, failed)</li>
  <li>Associated SQL Query: Link to the sql tab for this job</li>
  <li>Event timeline: Displays in chronological order the events related to the executors (added, removed) and the stages of the job</li>
</ul>

<p style="text-align: center;">
  <img src="img/JobPageDetail1.png" title="Event timeline" alt="Event timeline" />
</p>

<ul>
  <li>DAG visualization: Visual representation of the directed acyclic graph of this job where vertices represent the RDDs or DataFrames and the edges represent an operation to be applied on RDD.</li>
</ul>

<p style="text-align: center;">
  <img src="img/JobPageDetail2.png" title="DAG" alt="DAG" width="40%" />
</p>

<ul>
  <li>List of stages (grouped by state active, pending, completed, skipped, and failed)
    <ul>
      <li>Stage ID</li>
      <li>Description of the stage</li>
      <li>Submitted timestamp</li>
      <li>Duration of the stage</li>
      <li>Tasks progress bar</li>
      <li>Input: Bytes read from storage in this stage</li>
      <li>Output: Bytes written in storage in this stage</li>
      <li>Shuffle read: Total shuffle bytes and records read, includes both data read locally and data read from remote executors</li>
      <li>Shuffle write: Bytes and records written to disk in order to be read by a shuffle in a future stage</li>
    </ul>
  </li>
</ul>

<p style="text-align: center;">
  <img src="img/JobPageDetail3.png" title="DAG" alt="DAG" />
</p>

<h2 id="stages-tab">Stages Tab</h2>

<p>The Stages tab displays a summary page that shows the current state of all stages of all jobs in
the Spark application.</p>

<p>At the beginning of the page is the summary with the count of all stages by status (active, pending, completed, sikipped, and failed)</p>

<p style="text-align: center;">
  <img src="img/AllStagesPageDetail1.png" title="Stages header" alt="Stages header" width="30%" />
</p>

<p>In <a href="job-scheduling.html#scheduling-within-an-application">Fair scheduling mode</a> there is a table that displays <a href="job-scheduling.html#configuring-pool-properties">pools properties</a></p>

<p style="text-align: center;">
  <img src="img/AllStagesPageDetail2.png" title="Pool properties" alt="Pool properties" />
</p>

<p>After that are the details of stages per status (active, pending, completed, skipped, failed). In active stages, it&#8217;s possible to kill the stage with the kill link. Only in failed stages, failure reason is shown. Task detail can be accessed by clicking on the description.</p>

<p style="text-align: center;">
  <img src="img/AllStagesPageDetail3.png" title="Stages detail" alt="Stages detail" />
</p>

<h3 id="stage-detail">Stage detail</h3>
<p>The stage detail page begins with information like total time across all tasks, <a href="tuning.html#data-locality">Locality level summary</a>, <a href="rdd-programming-guide.html#shuffle-operations">Shuffle Read Size / Records</a> and Associated Job IDs.</p>

<p style="text-align: center;">
  <img src="img/AllStagesPageDetail4.png" title="Stage header" alt="Stage header" width="30%" />
</p>

<p>There is also a visual representation of the directed acyclic graph (DAG) of this stage, where vertices represent the RDDs or DataFrames and the edges represent an operation to be applied.</p>

<p style="text-align: center;">
  <img src="img/AllStagesPageDetail5.png" title="Stage DAG" alt="Stage DAG" width="50%" />
</p>

<p>Summary metrics for all task are represented in a table and in a timeline.</p>
<ul>
  <li><strong><a href="configuration.html#compression-and-serialization">Tasks deserialization time</a></strong></li>
  <li><strong>Duration of tasks</strong>.</li>
  <li><strong>GC time</strong> is the total JVM garbage collection time.</li>
  <li><strong>Result serialization time</strong> is the time spent serializing the task result on a executor before sending it back to the driver.</li>
  <li><strong>Getting result time</strong> is the time that the driver spends fetching task results from workers.</li>
  <li><strong>Scheduler delay</strong> is the time the task waits to be scheduled for execution.</li>
  <li><strong>Peak execution memory</strong> is the maximum memory used by the internal data structures created during shuffles, aggregations and joins.</li>
  <li><strong>Shuffle Read Size / Records</strong>. Total shuffle bytes read, includes both data read locally and data read from remote executors.</li>
  <li><strong>Shuffle Read Blocked Time</strong> is the time that tasks spent blocked waiting for shuffle data to be read from remote machines.</li>
  <li><strong>Shuffle Remote Reads</strong> is the total shuffle bytes read from remote executors.</li>
  <li><strong>Shuffle spill (memory)</strong> is the size of the deserialized form of the shuffled data in memory.</li>
  <li><strong>Shuffle spill (disk)</strong> is the size of the serialized form of the data on disk.</li>
</ul>

<p style="text-align: center;">
  <img src="img/AllStagesPageDetail6.png" title="Stages metrics" alt="Stages metrics" />
</p>

<p>Aggregated metrics by executor show the same information aggregated by executor.</p>

<p style="text-align: center;">
  <img src="img/AllStagesPageDetail7.png" title="Stages metrics per executor" alt="Stages metrics per executors" />
</p>

<p><strong><a href="rdd-programming-guide.html#accumulators">Accumulators</a></strong> are a type of shared variables. It provides a mutable variable that can be updated inside of a variety of transformations. It is possible to create accumulators with and without name, but only named accumulators are displayed.</p>

<p style="text-align: center;">
  <img src="img/AllStagesPageDetail8.png" title="Stage accumulator" alt="Stage accumulator" />
</p>

<p>Tasks details basically includes the same information as in the summary section but detailed by task. It also includes links to review the logs and the task attempt number if it fails for any reason. If there are named accumulators, here it is possible to see the accumulator value at the end of each task.</p>

<p style="text-align: center;">
  <img src="img/AllStagesPageDetail9.png" title="Tasks" alt="Tasks" />
</p>

<h2 id="storage-tab">Storage Tab</h2>
<p>The Storage tab displays the persisted RDDs and DataFrames, if any, in the application. The summary
page shows the storage levels, sizes and partitions of all RDDs, and the details page shows the
sizes and using executors for all partitions in an RDD or DataFrame.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">import</span> <span class="nn">org.apache.spark.storage.StorageLevel._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.storage.StorageLevel._</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">rdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="mi">100</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">5</span><span class="o">).</span><span class="n">setName</span><span class="o">(</span><span class="s">&quot;rdd&quot;</span><span class="o">)</span>
<span class="n">rdd</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="n">rdd</span> <span class="nc">MapPartitionsRDD</span><span class="o">[</span><span class="err">1</span><span class="o">]</span> <span class="n">at</span> <span class="n">range</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">27</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">persist</span><span class="o">(</span><span class="nc">MEMORY_ONLY_SER</span><span class="o">)</span>
<span class="n">res0</span><span class="k">:</span> <span class="kt">rdd.</span><span class="k">type</span> <span class="o">=</span> <span class="n">rdd</span> <span class="nc">MapPartitionsRDD</span><span class="o">[</span><span class="err">1</span><span class="o">]</span> <span class="n">at</span> <span class="n">range</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">27</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">count</span>
<span class="n">res1</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">100</span>                                                                

<span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span> <span class="s">&quot;andy&quot;</span><span class="o">),</span> <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="s">&quot;bob&quot;</span><span class="o">),</span> <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="s">&quot;andy&quot;</span><span class="o">)).</span><span class="n">toDF</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">,</span> <span class="s">&quot;name&quot;</span><span class="o">)</span>
<span class="n">df</span><span class="k">:</span> <span class="kt">org.apache.spark.sql.DataFrame</span> <span class="o">=</span> <span class="o">[</span><span class="kt">count:</span> <span class="kt">int</span>, <span class="kt">name:</span> <span class="kt">string</span><span class="o">]</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">df</span><span class="o">.</span><span class="n">persist</span><span class="o">(</span><span class="nc">DISK_ONLY</span><span class="o">)</span>
<span class="n">res2</span><span class="k">:</span> <span class="kt">df.</span><span class="k">type</span> <span class="o">=</span> <span class="o">[</span><span class="kt">count:</span> <span class="kt">int</span>, <span class="kt">name:</span> <span class="kt">string</span><span class="o">]</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">df</span><span class="o">.</span><span class="n">count</span>
<span class="n">res3</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">3</span></code></pre></figure>

<p style="text-align: center;">
  <img src="img/webui-storage-tab.png" title="Storage tab" alt="Storage tab" width="100%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>

<p>After running the above example, we can find two RDDs listed in the Storage tab. Basic information like
storage level, number of partitions and memory overhead are provided. Note that the newly persisted RDDs
or DataFrames are not shown in the tab before they are materialized. To monitor a specific RDD or DataFrame,
make sure an action operation has been triggered.</p>

<p style="text-align: center;">
  <img src="img/webui-storage-detail.png" title="Storage detail" alt="Storage detail" width="100%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>

<p>You can click the RDD name &#8216;rdd&#8217; for obtaining the details of data persistence, such as the data
distribution on the cluster.</p>

<h2 id="environment-tab">Environment Tab</h2>
<p>The Environment tab displays the values for the different environment and configuration variables,
including JVM, Spark, and system properties.</p>

<p style="text-align: center;">
  <img src="img/webui-env-tab.png" title="Env tab" alt="Env tab" width="100%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>

<p>This environment page has five parts. It is a useful place to check whether your properties have
been set correctly.
The first part &#8216;Runtime Information&#8217; simply contains the <a href="configuration.html#runtime-environment">runtime properties</a>
like versions of Java and Scala.
The second part &#8216;Spark Properties&#8217; lists the <a href="configuration.html#application-properties">application properties</a> like
<a href="configuration.html#application-properties">&#8216;spark.app.name&#8217;</a> and &#8216;spark.driver.memory&#8217;.</p>

<p style="text-align: center;">
  <img src="img/webui-env-hadoop.png" title="Hadoop Properties" alt="Hadoop Properties" width="100%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>
<p>Clicking the &#8216;Hadoop Properties&#8217; link displays properties relative to Hadoop and YARN. Note that properties like
<a href="configuration.html#execution-behavior">&#8216;spark.hadoop.*&#8217;</a> are shown not in this part but in &#8216;Spark Properties&#8217;.</p>

<p style="text-align: center;">
  <img src="img/webui-env-sys.png" title="System Properties" alt="System Properties" width="100%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>
<p>&#8216;System Properties&#8217; shows more details about the JVM.</p>

<p style="text-align: center;">
  <img src="img/webui-env-class.png" title="Classpath Entries" alt="Classpath Entries" width="100%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>

<p>The last part &#8216;Classpath Entries&#8217; lists the classes loaded from different sources, which is very useful
to resolve class conflicts.</p>

<h2 id="executors-tab">Executors Tab</h2>
<p>The Executors tab displays summary information about the executors that were created for the
application, including memory and disk usage and task and shuffle information. The Storage Memory
column shows the amount of memory used and reserved for caching data.</p>

<p style="text-align: center;">
  <img src="img/webui-exe-tab.png" title="Executors Tab" alt="Executors Tab" width="80%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>

<p>The Executors tab provides not only resource information (amount of memory, disk, and cores used by each executor)
but also performance information (<a href="tuning.html#garbage-collection-tuning">GC time</a> and shuffle information).</p>

<p style="text-align: center;">
  <img src="img/webui-exe-err.png" title="Stderr Log" alt="Stderr Log" width="80%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>

<p>Clicking the &#8216;stderr&#8217; link of executor 0 displays detailed <a href="spark-standalone.html#monitoring-and-logging">standard error log</a>
in its console.</p>

<p style="text-align: center;">
  <img src="img/webui-exe-thread.png" title="Thread Dump" alt="Thread Dump" width="80%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>

<p>Clicking the &#8216;Thread Dump&#8217; link of executor 0 displays the thread dump of JVM on executor 0, which is pretty useful
for performance analysis.</p>

<h2 id="sql-tab">SQL Tab</h2>
<p>If the application executes Spark SQL queries, the SQL tab displays information, such as the duration,
jobs, and physical and logical plans for the queries. Here we include a basic example to illustrate
this tab:</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span> <span class="s">&quot;andy&quot;</span><span class="o">),</span> <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="s">&quot;bob&quot;</span><span class="o">),</span> <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="s">&quot;andy&quot;</span><span class="o">)).</span><span class="n">toDF</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">,</span> <span class="s">&quot;name&quot;</span><span class="o">)</span>
<span class="n">df</span><span class="k">:</span> <span class="kt">org.apache.spark.sql.DataFrame</span> <span class="o">=</span> <span class="o">[</span><span class="kt">count:</span> <span class="kt">int</span>, <span class="kt">name:</span> <span class="kt">string</span><span class="o">]</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">df</span><span class="o">.</span><span class="n">count</span>
<span class="n">res0</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">3</span>                                                                  

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">df</span><span class="o">.</span><span class="n">createGlobalTempView</span><span class="o">(</span><span class="s">&quot;df&quot;</span><span class="o">)</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;select name,sum(count) from global_temp.df group by name&quot;</span><span class="o">).</span><span class="n">show</span>
<span class="o">+----+----------+</span>
<span class="o">|</span><span class="n">name</span><span class="o">|</span><span class="n">sum</span><span class="o">(</span><span class="n">count</span><span class="o">)|</span>
<span class="o">+----+----------+</span>
<span class="o">|</span><span class="n">andy</span><span class="o">|</span>         <span class="mi">3</span><span class="o">|</span>
<span class="o">|</span> <span class="n">bob</span><span class="o">|</span>         <span class="mi">2</span><span class="o">|</span>
<span class="o">+----+----------+</span></code></pre></figure>

<p style="text-align: center;">
  <img src="img/webui-sql-tab.png" title="SQL tab" alt="SQL tab" width="80%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>

<p>Now the above three dataframe/SQL operators are shown in the list. If we click the
&#8216;show at &lt;console&gt;: 24&#8217; link of the last query, we will see the DAG of the job.</p>

<p style="text-align: center;">
  <img src="img/webui-sql-dag.png" title="SQL DAG" alt="SQL DAG" width="50%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>

<p>We can see that details information of each stage. The first block &#8216;WholeStageCodegen&#8217;<br />
compile multiple operator (&#8216;LocalTableScan&#8217; and &#8216;HashAggregate&#8217;) together into a single Java
function to improve performance, and metrics like number of rows and spill size are listed in
the block. The second block &#8216;Exchange&#8217; shows the metrics on the shuffle exchange, including
number of written shuffle records, total data size, etc.</p>

<p style="text-align: center;">
  <img src="img/webui-sql-plan.png" title="logical plans and the physical plan" alt="logical plans and the physical plan" width="80%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>
<p>Clicking the &#8216;Details&#8217; link on the bottom displays the logical plans and the physical plan, which
illustrate how Spark parses, analyzes, optimizes and performs the query.</p>

<h3 id="sql-metrics">SQL metrics</h3>

<p>The metrics of SQL operators are shown in the block of physical operators. The SQL metrics can be useful
when we want to dive into the execution details of each operator. For example, &#8220;number of output rows&#8221;
can answer how many rows are output after a Filter operator, &#8220;shuffle bytes written total&#8221; in an Exchange
operator shows the number of bytes written by a shuffle.</p>

<p>Here is the list of SQL metrics:</p>

<table class="table">
<tr><th>SQL metrics</th><th>Meaning</th><th>Operators</th></tr>
<tr><td> <code>number of output rows</code> </td><td> the number of output rows of the operator </td><td> Aggregate operators, Join operators, Sample, Range, Scan operators, Filter, etc.</td></tr>
<tr><td> <code>data size</code> </td><td> the size of broadcast/shuffled/collected data of the operator </td><td> BroadcastExchange, ShuffleExchange, Subquery </td></tr>
<tr><td> <code>time to collect</code> </td><td> the time spent on collecting data </td><td> BroadcastExchange, Subquery </td></tr>
<tr><td> <code>scan time</code> </td><td> the time spent on scanning data </td><td> ColumnarBatchScan, FileSourceScan </td></tr>
<tr><td> <code>metadata time</code> </td><td> the time spent on getting metadata like number of partitions, number of files </td><td> FileSourceScan </td></tr>
<tr><td> <code>shuffle bytes written</code> </td><td> the number of bytes written </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>
<tr><td> <code>shuffle records written</code> </td><td> the number of records written </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>
<tr><td> <code>shuffle write time</code> </td><td> the time spent on shuffle writing </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>
<tr><td> <code>remote blocks read</code> </td><td> the number of blocks read remotely </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange</td></tr>
<tr><td> <code>remote bytes read</code> </td><td> the number of bytes read remotely </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>
<tr><td> <code>remote bytes read to disk</code> </td><td> the number of bytes read from remote to local disk </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>
<tr><td> <code>local blocks read</code> </td><td> the number of blocks read locally </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>
<tr><td> <code>local bytes read</code> </td><td> the number of bytes read locally </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>
<tr><td> <code>fetch wait time</code> </td><td> the time spent on fetching data (local and remote)</td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>
<tr><td> <code>records read</code> </td><td> the number of read records </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>
<tr><td> <code>sort time</code> </td><td> the time spent on sorting </td><td> Sort </td></tr>
<tr><td> <code>peak memory</code> </td><td> the peak memory usage in the operator </td><td> Sort, HashAggregate </td></tr>
<tr><td> <code>spill size</code> </td><td> number of bytes spilled to disk from memory in the operator </td><td> Sort, HashAggregate </td></tr>
<tr><td> <code>time in aggregation build</code> </td><td> the time spent on aggregation </td><td> HashAggregate, ObjectHashAggregate </td></tr>
<tr><td> <code>avg hash probe bucket list iters</code> </td><td> the average bucket list iterations per lookup during aggregation </td><td> HashAggregate </td></tr>
<tr><td> <code>data size of build side</code> </td><td> the size of built hash map </td><td> ShuffledHashJoin </td></tr>
<tr><td> <code>time to build hash map</code> </td><td> the time spent on building hash map </td><td> ShuffledHashJoin </td></tr>

</table>

<h2 id="streaming-tab">Streaming Tab</h2>
<p>The web UI includes a Streaming tab if the application uses Spark streaming. This tab displays
scheduling delay and processing time for each micro-batch in the data stream, which can be useful
for troubleshooting the streaming application.</p>

<h2 id="jdbcodbc-server-tab">JDBC/ODBC Server Tab</h2>
<p>We can see this tab when Spark is running as a <a href="sql-distributed-sql-engine.html">distributed SQL engine</a>. It shows information about sessions and submitted SQL operations.</p>

<p>The first section of the page displays general information about the JDBC/ODBC server: start time and uptime.</p>

<p style="text-align: center;">
  <img src="img/JDBCServer1.png" width="40%" title="JDBC/ODBC Header" alt="JDBC/ODBC Header" />
</p>

<p>The second section contains information about active and finished sessions.</p>
<ul>
  <li><strong>User</strong> and <strong>IP</strong> of the connection.</li>
  <li><strong>Session id</strong> link to access to session info.</li>
  <li><strong>Start time</strong>, <strong>finish time</strong> and <strong>duration</strong> of the session.</li>
  <li><strong>Total execute</strong> is the number of operations submitted in this session.</li>
</ul>

<p style="text-align: center;">
  <img src="img/JDBCServer2.png" title="JDBC/ODBC sessions" alt="JDBC/ODBC sessions" />
</p>

<p>The third section has the SQL statistics of the submitted operations.</p>
<ul>
  <li><strong>User</strong> that submit the operation.</li>
  <li><strong>Job id</strong> link to <a href="web-ui.html#jobs-tab">jobs tab</a>.</li>
  <li><strong>Group id</strong> of the query that group all jobs together. An application can cancel all running jobs using this group id.</li>
  <li><strong>Start time</strong> of the operation.</li>
  <li><strong>Finish time</strong> of the execution, before fetching the results.</li>
  <li><strong>Close time</strong> of the operation after fetching the results.</li>
  <li><strong>Execution time</strong> is the difference between finish time and start time.</li>
  <li><strong>Duration time</strong> is the difference between close time and start time.</li>
  <li><strong>Statement</strong> is the operation being executed.</li>
  <li><strong>State</strong> of the process.
    <ul>
      <li><em>Started</em>, first state, when the process begins.</li>
      <li><em>Compiled</em>, execution plan generated.</li>
      <li><em>Failed</em>, final state when the execution failed or finished with error.</li>
      <li><em>Canceled</em>, final state when the execution is canceled.</li>
      <li><em>Finished</em> processing and waiting to fetch results.</li>
      <li><em>Closed</em>, final state when client closed the statement.</li>
    </ul>
  </li>
  <li><strong>Detail</strong> of the execution plan with parsed logical plan, analyzed logical plan, optimized logical plan and physical plan or errors in the the SQL statement.</li>
</ul>

<p style="text-align: center;">
  <img src="img/JDBCServer3.png" title="JDBC/ODBC SQL Statistics" alt="JDBC/ODBC SQL Statistics" />
</p>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-3.4.1.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    </body>
</html>
