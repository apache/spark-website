
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>pyspark.sql.functions &#8212; PySpark 2.4.1 documentation</title>
    <link rel="stylesheet" href="../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/pyspark.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../_static/pyspark.js"></script>
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
    
        <li class="nav-item nav-item-0"><a href="../../../index.html">PySpark 2.4.1 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for pyspark.sql.functions</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">A collections of builtin functions</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span> <span class="o">&lt;</span> <span class="s2">&quot;3&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">itertools</span> <span class="k">import</span> <span class="n">imap</span> <span class="k">as</span> <span class="nb">map</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span> <span class="o">&gt;=</span> <span class="s1">&#39;3&#39;</span><span class="p">:</span>
    <span class="n">basestring</span> <span class="o">=</span> <span class="nb">str</span>

<span class="kn">from</span> <span class="nn">pyspark</span> <span class="k">import</span> <span class="n">since</span><span class="p">,</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="k">import</span> <span class="n">ignore_unicode_prefix</span><span class="p">,</span> <span class="n">PythonEvalType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.column</span> <span class="k">import</span> <span class="n">Column</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">,</span> <span class="n">_to_seq</span><span class="p">,</span> <span class="n">_create_column_from_literal</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="k">import</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="k">import</span> <span class="n">StringType</span><span class="p">,</span> <span class="n">DataType</span>
<span class="c1"># Keep UserDefinedFunction import for backwards compatible import; moved in SPARK-22409</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.udf</span> <span class="k">import</span> <span class="n">UserDefinedFunction</span><span class="p">,</span> <span class="n">_create_udf</span>


<span class="k">def</span> <span class="nf">_create_function</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Create a function for aggregator by name&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
        <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="p">,</span> <span class="n">name</span><span class="p">)(</span><span class="n">col</span><span class="o">.</span><span class="n">_jc</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">Column</span><span class="p">)</span> <span class="k">else</span> <span class="n">col</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span>
    <span class="n">_</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">_</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
    <span class="k">return</span> <span class="n">_</span>


<span class="k">def</span> <span class="nf">_wrap_deprecated_function</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Wrap the deprecated function to print out deprecation warnings&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">col</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)(</span><span class="n">_</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_create_binary_mathfunction</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Create a binary mathfunction by name&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span><span class="p">):</span>
        <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
        <span class="c1"># users might write ints for simplicity. This would throw an error on the JVM side.</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="p">,</span> <span class="n">name</span><span class="p">)(</span><span class="n">col1</span><span class="o">.</span><span class="n">_jc</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span> <span class="n">Column</span><span class="p">)</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="n">col1</span><span class="p">),</span>
                                              <span class="n">col2</span><span class="o">.</span><span class="n">_jc</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">col2</span><span class="p">,</span> <span class="n">Column</span><span class="p">)</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="n">col2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span>
    <span class="n">_</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">_</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
    <span class="k">return</span> <span class="n">_</span>


<span class="k">def</span> <span class="nf">_create_window_function</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Create a window function by name &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_</span><span class="p">():</span>
        <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="p">,</span> <span class="n">name</span><span class="p">)()</span>
        <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span>
    <span class="n">_</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">_</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="s1">&#39;Window function: &#39;</span> <span class="o">+</span> <span class="n">doc</span>
    <span class="k">return</span> <span class="n">_</span>

<span class="n">_lit_doc</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Creates a :class:`Column` of literal value.</span>

<span class="s2">    &gt;&gt;&gt; df.select(lit(5).alias(&#39;height&#39;)).withColumn(&#39;spark_user&#39;, lit(True)).take(1)</span>
<span class="s2">    [Row(height=5, spark_user=True)]</span>
<span class="s2">    &quot;&quot;&quot;</span>
<span class="n">_functions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;lit&#39;</span><span class="p">:</span> <span class="n">_lit_doc</span><span class="p">,</span>
    <span class="s1">&#39;col&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns a :class:`Column` based on the given column name.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;column&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns a :class:`Column` based on the given column name.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;asc&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns a sort expression based on the ascending order of the given column name.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;desc&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns a sort expression based on the descending order of the given column name.&#39;</span><span class="p">,</span>

    <span class="s1">&#39;upper&#39;</span><span class="p">:</span> <span class="s1">&#39;Converts a string expression to upper case.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lower&#39;</span><span class="p">:</span> <span class="s1">&#39;Converts a string expression to upper case.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sqrt&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the square root of the specified float value.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;abs&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the absolute value.&#39;</span><span class="p">,</span>

    <span class="s1">&#39;max&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the maximum value of the expression in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;min&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the minimum value of the expression in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;count&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the number of items in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sum&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the sum of all values in the expression.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;avg&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the average of the values in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the average of the values in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sumDistinct&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the sum of distinct values in the expression.&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_functions_1_4</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># unary math functions</span>
    <span class="s1">&#39;acos&#39;</span><span class="p">:</span> <span class="s1">&#39;:return: inverse cosine of `col`, as if computed by `java.lang.Math.acos()`&#39;</span><span class="p">,</span>
    <span class="s1">&#39;asin&#39;</span><span class="p">:</span> <span class="s1">&#39;:return: inverse sine of `col`, as if computed by `java.lang.Math.asin()`&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atan&#39;</span><span class="p">:</span> <span class="s1">&#39;:return: inverse tangent of `col`, as if computed by `java.lang.Math.atan()`&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cbrt&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the cube-root of the given value.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ceil&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the ceiling of the given value.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cos&#39;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;:param col: angle in radians</span>
<span class="s2">           :return: cosine of the angle, as if computed by `java.lang.Math.cos()`.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;cosh&#39;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;:param col: hyperbolic angle</span>
<span class="s2">           :return: hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;exp&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the exponential of the given value.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;expm1&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the exponential of the given value minus one.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floor&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the floor of the given value.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the natural logarithm of the given value.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log10&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the logarithm of the given value in Base 10.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log1p&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the natural logarithm of the given value plus one.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;rint&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns the double value that is closest in value to the argument and&#39;</span> <span class="o">+</span>
            <span class="s1">&#39; is equal to a mathematical integer.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;signum&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the signum of the given value.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sin&#39;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;:param col: angle in radians</span>
<span class="s2">           :return: sine of the angle, as if computed by `java.lang.Math.sin()`&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;sinh&#39;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;:param col: hyperbolic angle</span>
<span class="s2">           :return: hyperbolic sine of the given value,</span>
<span class="s2">                    as if computed by `java.lang.Math.sinh()`&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;tan&#39;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;:param col: angle in radians</span>
<span class="s2">           :return: tangent of the given value, as if computed by `java.lang.Math.tan()`&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;tanh&#39;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;:param col: hyperbolic angle</span>
<span class="s2">            :return: hyperbolic tangent of the given value,</span>
<span class="s2">                     as if computed by `java.lang.Math.tanh()`&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;toDegrees&#39;</span><span class="p">:</span> <span class="s1">&#39;.. note:: Deprecated in 2.1, use :func:`degrees` instead.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;toRadians&#39;</span><span class="p">:</span> <span class="s1">&#39;.. note:: Deprecated in 2.1, use :func:`radians` instead.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bitwiseNOT&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes bitwise not.&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_functions_2_4</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;asc_nulls_first&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns a sort expression based on the ascending order of the given&#39;</span> <span class="o">+</span>
                       <span class="s1">&#39; column name, and null values return before non-null values.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;asc_nulls_last&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns a sort expression based on the ascending order of the given&#39;</span> <span class="o">+</span>
                      <span class="s1">&#39; column name, and null values appear after non-null values.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;desc_nulls_first&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns a sort expression based on the descending order of the given&#39;</span> <span class="o">+</span>
                        <span class="s1">&#39; column name, and null values appear before non-null values.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;desc_nulls_last&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns a sort expression based on the descending order of the given&#39;</span> <span class="o">+</span>
                       <span class="s1">&#39; column name, and null values appear after non-null values&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_collect_list_doc</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Aggregate function: returns a list of objects with duplicates.</span>

<span class="s2">    .. note:: The function is non-deterministic because the order of collected results depends</span>
<span class="s2">        on order of rows which may be non-deterministic after a shuffle.</span>

<span class="s2">    &gt;&gt;&gt; df2 = spark.createDataFrame([(2,), (5,), (5,)], (&#39;age&#39;,))</span>
<span class="s2">    &gt;&gt;&gt; df2.agg(collect_list(&#39;age&#39;)).collect()</span>
<span class="s2">    [Row(collect_list(age)=[2, 5, 5])]</span>
<span class="s2">    &quot;&quot;&quot;</span>
<span class="n">_collect_set_doc</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Aggregate function: returns a set of objects with duplicate elements eliminated.</span>

<span class="s2">    .. note:: The function is non-deterministic because the order of collected results depends</span>
<span class="s2">        on order of rows which may be non-deterministic after a shuffle.</span>

<span class="s2">    &gt;&gt;&gt; df2 = spark.createDataFrame([(2,), (5,), (5,)], (&#39;age&#39;,))</span>
<span class="s2">    &gt;&gt;&gt; df2.agg(collect_set(&#39;age&#39;)).collect()</span>
<span class="s2">    [Row(collect_set(age)=[5, 2])]</span>
<span class="s2">    &quot;&quot;&quot;</span>
<span class="n">_functions_1_6</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># unary math functions</span>
    <span class="s1">&#39;stddev&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the unbiased sample standard deviation of&#39;</span> <span class="o">+</span>
              <span class="s1">&#39; the expression in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stddev_samp&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the unbiased sample standard deviation of&#39;</span> <span class="o">+</span>
                   <span class="s1">&#39; the expression in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stddev_pop&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns population standard deviation of&#39;</span> <span class="o">+</span>
                  <span class="s1">&#39; the expression in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;variance&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the population variance of the values in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;var_samp&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the unbiased variance of the values in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;var_pop&#39;</span><span class="p">:</span>  <span class="s1">&#39;Aggregate function: returns the population variance of the values in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;skewness&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the skewness of the values in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;kurtosis&#39;</span><span class="p">:</span> <span class="s1">&#39;Aggregate function: returns the kurtosis of the values in a group.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;collect_list&#39;</span><span class="p">:</span> <span class="n">_collect_list_doc</span><span class="p">,</span>
    <span class="s1">&#39;collect_set&#39;</span><span class="p">:</span> <span class="n">_collect_set_doc</span>
<span class="p">}</span>

<span class="n">_functions_2_1</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># unary math functions</span>
    <span class="s1">&#39;degrees&#39;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">               Converts an angle measured in radians to an approximately equivalent angle</span>
<span class="s2">               measured in degrees.</span>
<span class="s2">               :param col: angle in radians</span>
<span class="s2">               :return: angle in degrees, as if computed by `java.lang.Math.toDegrees()`</span>
<span class="s2">               &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;radians&#39;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">               Converts an angle measured in degrees to an approximately equivalent angle</span>
<span class="s2">               measured in radians.</span>
<span class="s2">               :param col: angle in degrees</span>
<span class="s2">               :return: angle in radians, as if computed by `java.lang.Math.toRadians()`</span>
<span class="s2">               &quot;&quot;&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># math functions that take two arguments as input</span>
<span class="n">_binary_mathfunctions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;atan2&#39;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">             :param col1: coordinate on y-axis</span>
<span class="s2">             :param col2: coordinate on x-axis</span>
<span class="s2">             :return: the `theta` component of the point</span>
<span class="s2">                (`r`, `theta`)</span>
<span class="s2">                in polar coordinates that corresponds to the point</span>
<span class="s2">                (`x`, `y`) in Cartesian coordinates,</span>
<span class="s2">                as if computed by `java.lang.Math.atan2()`</span>
<span class="s2">             &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;hypot&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pow&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns the value of the first argument raised to the power of the second argument.&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_window_functions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;row_number&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;returns a sequential number starting at 1 within a window partition.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;dense_rank&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;returns the rank of rows within a window partition, without any gaps.</span>

<span class="sd">        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking</span>
<span class="sd">        sequence when there are ties. That is, if you were ranking a competition using dense_rank</span>
<span class="sd">        and had three people tie for second place, you would say that all three were in second</span>
<span class="sd">        place and that the next person came in third. Rank would give me sequential numbers, making</span>
<span class="sd">        the person that came in third place (after the ties) would register as coming in fifth.</span>

<span class="sd">        This is equivalent to the DENSE_RANK function in SQL.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;rank&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;returns the rank of rows within a window partition.</span>

<span class="sd">        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking</span>
<span class="sd">        sequence when there are ties. That is, if you were ranking a competition using dense_rank</span>
<span class="sd">        and had three people tie for second place, you would say that all three were in second</span>
<span class="sd">        place and that the next person came in third. Rank would give me sequential numbers, making</span>
<span class="sd">        the person that came in third place (after the ties) would register as coming in fifth.</span>

<span class="sd">        This is equivalent to the RANK function in SQL.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;cume_dist&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;returns the cumulative distribution of values within a window partition,</span>
<span class="sd">        i.e. the fraction of rows that are below the current row.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="s1">&#39;percent_rank&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;returns the relative rank (i.e. percentile) of rows within a window partition.&quot;&quot;&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Wraps deprecated functions (keys) with the messages (values).</span>
<span class="n">_functions_deprecated</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;toDegrees&#39;</span><span class="p">:</span> <span class="s1">&#39;Deprecated in 2.1, use degrees instead.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;toRadians&#39;</span><span class="p">:</span> <span class="s1">&#39;Deprecated in 2.1, use radians instead.&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span> <span class="ow">in</span> <span class="n">_functions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">since</span><span class="p">(</span><span class="mf">1.3</span><span class="p">)(</span><span class="n">_create_function</span><span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span><span class="p">))</span>
<span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span> <span class="ow">in</span> <span class="n">_functions_1_4</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)(</span><span class="n">_create_function</span><span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span><span class="p">))</span>
<span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span> <span class="ow">in</span> <span class="n">_binary_mathfunctions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)(</span><span class="n">_create_binary_mathfunction</span><span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span><span class="p">))</span>
<span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span> <span class="ow">in</span> <span class="n">_window_functions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)(</span><span class="n">_create_window_function</span><span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span><span class="p">))</span>
<span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span> <span class="ow">in</span> <span class="n">_functions_1_6</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)(</span><span class="n">_create_function</span><span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span><span class="p">))</span>
<span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span> <span class="ow">in</span> <span class="n">_functions_2_1</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)(</span><span class="n">_create_function</span><span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span><span class="p">))</span>
<span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_message</span> <span class="ow">in</span> <span class="n">_functions_deprecated</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">_wrap_deprecated_function</span><span class="p">(</span><span class="nb">globals</span><span class="p">()[</span><span class="n">_name</span><span class="p">],</span> <span class="n">_message</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span> <span class="ow">in</span> <span class="n">_functions_2_4</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)(</span><span class="n">_create_function</span><span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span><span class="p">))</span>
<span class="k">del</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span>


<div class="viewcode-block" id="approxCountDistinct"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.approxCountDistinct">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">approxCountDistinct</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">rsd</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. note:: Deprecated in 2.1, use :func:`approx_count_distinct` instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Deprecated in 2.1, use approx_count_distinct instead.&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">approx_count_distinct</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">rsd</span><span class="p">)</span></div>


<div class="viewcode-block" id="approx_count_distinct"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.approx_count_distinct">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">approx_count_distinct</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">rsd</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Aggregate function: returns a new :class:`Column` for approximate distinct count of</span>
<span class="sd">    column `col`.</span>

<span class="sd">    :param rsd: maximum estimation error allowed (default = 0.05). For rsd &lt; 0.01, it is more</span>
<span class="sd">        efficient to use :func:`countDistinct`</span>

<span class="sd">    &gt;&gt;&gt; df.agg(approx_count_distinct(df.age).alias(&#39;distinct_ages&#39;)).collect()</span>
<span class="sd">    [Row(distinct_ages=2)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="n">rsd</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">approx_count_distinct</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">approx_count_distinct</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">rsd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.broadcast">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Marks a DataFrame as small enough for use in broadcast joins.&quot;&quot;&quot;</span>

    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">_jdf</span><span class="p">),</span> <span class="n">df</span><span class="o">.</span><span class="n">sql_ctx</span><span class="p">)</span></div>


<div class="viewcode-block" id="coalesce"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.coalesce">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">coalesce</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the first column that is not null.</span>

<span class="sd">    &gt;&gt;&gt; cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (&quot;a&quot;, &quot;b&quot;))</span>
<span class="sd">    &gt;&gt;&gt; cDf.show()</span>
<span class="sd">    +----+----+</span>
<span class="sd">    |   a|   b|</span>
<span class="sd">    +----+----+</span>
<span class="sd">    |null|null|</span>
<span class="sd">    |   1|null|</span>
<span class="sd">    |null|   2|</span>
<span class="sd">    +----+----+</span>

<span class="sd">    &gt;&gt;&gt; cDf.select(coalesce(cDf[&quot;a&quot;], cDf[&quot;b&quot;])).show()</span>
<span class="sd">    +--------------+</span>
<span class="sd">    |coalesce(a, b)|</span>
<span class="sd">    +--------------+</span>
<span class="sd">    |          null|</span>
<span class="sd">    |             1|</span>
<span class="sd">    |             2|</span>
<span class="sd">    +--------------+</span>

<span class="sd">    &gt;&gt;&gt; cDf.select(&#39;*&#39;, coalesce(cDf[&quot;a&quot;], lit(0.0))).show()</span>
<span class="sd">    +----+----+----------------+</span>
<span class="sd">    |   a|   b|coalesce(a, 0.0)|</span>
<span class="sd">    +----+----+----------------+</span>
<span class="sd">    |null|null|             0.0|</span>
<span class="sd">    |   1|null|             1.0|</span>
<span class="sd">    |null|   2|             0.0|</span>
<span class="sd">    +----+----+----------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="corr"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.corr">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">corr</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a new :class:`Column` for the Pearson Correlation Coefficient for ``col1``</span>
<span class="sd">    and ``col2``.</span>

<span class="sd">    &gt;&gt;&gt; a = range(20)</span>
<span class="sd">    &gt;&gt;&gt; b = [2 * x for x in range(20)]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(zip(a, b), [&quot;a&quot;, &quot;b&quot;])</span>
<span class="sd">    &gt;&gt;&gt; df.agg(corr(&quot;a&quot;, &quot;b&quot;).alias(&#39;c&#39;)).collect()</span>
<span class="sd">    [Row(c=1.0)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col1</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">col2</span><span class="p">)))</span></div>


<div class="viewcode-block" id="covar_pop"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.covar_pop">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">covar_pop</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a new :class:`Column` for the population covariance of ``col1`` and ``col2``.</span>

<span class="sd">    &gt;&gt;&gt; a = [1] * 10</span>
<span class="sd">    &gt;&gt;&gt; b = [1] * 10</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(zip(a, b), [&quot;a&quot;, &quot;b&quot;])</span>
<span class="sd">    &gt;&gt;&gt; df.agg(covar_pop(&quot;a&quot;, &quot;b&quot;).alias(&#39;c&#39;)).collect()</span>
<span class="sd">    [Row(c=0.0)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">covar_pop</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col1</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">col2</span><span class="p">)))</span></div>


<div class="viewcode-block" id="covar_samp"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.covar_samp">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">covar_samp</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a new :class:`Column` for the sample covariance of ``col1`` and ``col2``.</span>

<span class="sd">    &gt;&gt;&gt; a = [1] * 10</span>
<span class="sd">    &gt;&gt;&gt; b = [1] * 10</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(zip(a, b), [&quot;a&quot;, &quot;b&quot;])</span>
<span class="sd">    &gt;&gt;&gt; df.agg(covar_samp(&quot;a&quot;, &quot;b&quot;).alias(&#39;c&#39;)).collect()</span>
<span class="sd">    [Row(c=0.0)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">covar_samp</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col1</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">col2</span><span class="p">)))</span></div>


<div class="viewcode-block" id="countDistinct"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.countDistinct">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">countDistinct</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.</span>

<span class="sd">    &gt;&gt;&gt; df.agg(countDistinct(df.age, df.name).alias(&#39;c&#39;)).collect()</span>
<span class="sd">    [Row(c=2)]</span>

<span class="sd">    &gt;&gt;&gt; df.agg(countDistinct(&quot;age&quot;, &quot;name&quot;).alias(&#39;c&#39;)).collect()</span>
<span class="sd">    [Row(c=2)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">countDistinct</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="first"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.first">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">first</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">ignorenulls</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Aggregate function: returns the first value in a group.</span>

<span class="sd">    The function by default returns the first values it sees. It will return the first non-null</span>
<span class="sd">    value it sees when ignoreNulls is set to true. If all values are null, then null is returned.</span>

<span class="sd">    .. note:: The function is non-deterministic because its results depends on order of rows which</span>
<span class="sd">        may be non-deterministic after a shuffle.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">first</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">ignorenulls</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="grouping"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.grouping">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">grouping</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated</span>
<span class="sd">    or not, returns 1 for aggregated or 0 for not aggregated in the result set.</span>

<span class="sd">    &gt;&gt;&gt; df.cube(&quot;name&quot;).agg(grouping(&quot;name&quot;), sum(&quot;age&quot;)).orderBy(&quot;name&quot;).show()</span>
<span class="sd">    +-----+--------------+--------+</span>
<span class="sd">    | name|grouping(name)|sum(age)|</span>
<span class="sd">    +-----+--------------+--------+</span>
<span class="sd">    | null|             1|       7|</span>
<span class="sd">    |Alice|             0|       2|</span>
<span class="sd">    |  Bob|             0|       5|</span>
<span class="sd">    +-----+--------------+--------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">grouping</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="grouping_id"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.grouping_id">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">grouping_id</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Aggregate function: returns the level of grouping, equals to</span>

<span class="sd">       (grouping(c1) &lt;&lt; (n-1)) + (grouping(c2) &lt;&lt; (n-2)) + ... + grouping(cn)</span>

<span class="sd">    .. note:: The list of columns should match with grouping columns exactly, or empty (means all</span>
<span class="sd">        the grouping columns).</span>

<span class="sd">    &gt;&gt;&gt; df.cube(&quot;name&quot;).agg(grouping_id(), sum(&quot;age&quot;)).orderBy(&quot;name&quot;).show()</span>
<span class="sd">    +-----+-------------+--------+</span>
<span class="sd">    | name|grouping_id()|sum(age)|</span>
<span class="sd">    +-----+-------------+--------+</span>
<span class="sd">    | null|            1|       7|</span>
<span class="sd">    |Alice|            0|       2|</span>
<span class="sd">    |  Bob|            0|       5|</span>
<span class="sd">    +-----+-------------+--------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">grouping_id</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="input_file_name"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.input_file_name">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">input_file_name</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Creates a string column for the file name of the current Spark task.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">input_file_name</span><span class="p">())</span></div>


<div class="viewcode-block" id="isnan"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.isnan">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">isnan</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An expression that returns true iff the column is NaN.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(1.0, float(&#39;nan&#39;)), (float(&#39;nan&#39;), 2.0)], (&quot;a&quot;, &quot;b&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(isnan(&quot;a&quot;).alias(&quot;r1&quot;), isnan(df.a).alias(&quot;r2&quot;)).collect()</span>
<span class="sd">    [Row(r1=False, r2=False), Row(r1=True, r2=True)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="isnull"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.isnull">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">isnull</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An expression that returns true iff the column is null.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(1, None), (None, 2)], (&quot;a&quot;, &quot;b&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(isnull(&quot;a&quot;).alias(&quot;r1&quot;), isnull(df.a).alias(&quot;r2&quot;)).collect()</span>
<span class="sd">    [Row(r1=False, r2=False), Row(r1=True, r2=True)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">isnull</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="last"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.last">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">last</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">ignorenulls</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Aggregate function: returns the last value in a group.</span>

<span class="sd">    The function by default returns the last values it sees. It will return the last non-null</span>
<span class="sd">    value it sees when ignoreNulls is set to true. If all values are null, then null is returned.</span>

<span class="sd">    .. note:: The function is non-deterministic because its results depends on order of rows</span>
<span class="sd">        which may be non-deterministic after a shuffle.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">last</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">ignorenulls</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="monotonically_increasing_id"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.monotonically_increasing_id">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">monotonically_increasing_id</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;A column that generates monotonically increasing 64-bit integers.</span>

<span class="sd">    The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.</span>
<span class="sd">    The current implementation puts the partition ID in the upper 31 bits, and the record number</span>
<span class="sd">    within each partition in the lower 33 bits. The assumption is that the data frame has</span>
<span class="sd">    less than 1 billion partitions, and each partition has less than 8 billion records.</span>

<span class="sd">    .. note:: The function is non-deterministic because its result depends on partition IDs.</span>

<span class="sd">    As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.</span>
<span class="sd">    This expression would return the following IDs:</span>
<span class="sd">    0, 1, 2, 8589934592 (1L &lt;&lt; 33), 8589934593, 8589934594.</span>

<span class="sd">    &gt;&gt;&gt; df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF([&#39;col1&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df0.select(monotonically_increasing_id().alias(&#39;id&#39;)).collect()</span>
<span class="sd">    [Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">monotonically_increasing_id</span><span class="p">())</span></div>


<div class="viewcode-block" id="nanvl"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.nanvl">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">nanvl</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns col1 if it is not NaN, or col2 if col1 is NaN.</span>

<span class="sd">    Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(1.0, float(&#39;nan&#39;)), (float(&#39;nan&#39;), 2.0)], (&quot;a&quot;, &quot;b&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(nanvl(&quot;a&quot;, &quot;b&quot;).alias(&quot;r1&quot;), nanvl(df.a, df.b).alias(&quot;r2&quot;)).collect()</span>
<span class="sd">    [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">nanvl</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col1</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">col2</span><span class="p">)))</span></div>


<div class="viewcode-block" id="rand"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.rand">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">rand</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generates a random column with independent and identically distributed (i.i.d.) samples</span>
<span class="sd">    from U[0.0, 1.0].</span>

<span class="sd">    .. note:: The function is non-deterministic in general case.</span>

<span class="sd">    &gt;&gt;&gt; df.withColumn(&#39;rand&#39;, rand(seed=42) * 3).collect()</span>
<span class="sd">    [Row(age=2, name=u&#39;Alice&#39;, rand=1.1568609015300986),</span>
<span class="sd">     Row(age=5, name=u&#39;Bob&#39;, rand=1.403379671529166)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="randn"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.randn">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">randn</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generates a column with independent and identically distributed (i.i.d.) samples from</span>
<span class="sd">    the standard normal distribution.</span>

<span class="sd">    .. note:: The function is non-deterministic in general case.</span>

<span class="sd">    &gt;&gt;&gt; df.withColumn(&#39;randn&#39;, randn(seed=42)).collect()</span>
<span class="sd">    [Row(age=2, name=u&#39;Alice&#39;, randn=-0.7556247885860078),</span>
<span class="sd">    Row(age=5, name=u&#39;Bob&#39;, randn=-0.0861619008451133)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="round"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.round">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` &gt;= 0</span>
<span class="sd">    or at integral part when `scale` &lt; 0.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(2.5,)], [&#39;a&#39;]).select(round(&#39;a&#39;, 0).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=3.0)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">scale</span><span class="p">))</span></div>


<div class="viewcode-block" id="bround"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.bround">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">bround</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` &gt;= 0</span>
<span class="sd">    or at integral part when `scale` &lt; 0.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(2.5,)], [&#39;a&#39;]).select(bround(&#39;a&#39;, 0).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=2.0)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">bround</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">scale</span><span class="p">))</span></div>


<div class="viewcode-block" id="shiftLeft"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.shiftLeft">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">shiftLeft</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">numBits</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Shift the given value numBits left.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(21,)], [&#39;a&#39;]).select(shiftLeft(&#39;a&#39;, 1).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=42)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">shiftLeft</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">numBits</span><span class="p">))</span></div>


<div class="viewcode-block" id="shiftRight"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.shiftRight">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">shiftRight</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">numBits</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;(Signed) shift the given value numBits right.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(42,)], [&#39;a&#39;]).select(shiftRight(&#39;a&#39;, 1).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=21)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">shiftRight</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">numBits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="shiftRightUnsigned"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.shiftRightUnsigned">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">shiftRightUnsigned</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">numBits</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Unsigned shift the given value numBits right.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(-42,)], [&#39;a&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(shiftRightUnsigned(&#39;a&#39;, 1).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=9223372036854775787)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">shiftRightUnsigned</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">numBits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="spark_partition_id"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.spark_partition_id">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">spark_partition_id</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;A column for partition ID.</span>

<span class="sd">    .. note:: This is indeterministic because it depends on data partitioning and task scheduling.</span>

<span class="sd">    &gt;&gt;&gt; df.repartition(1).select(spark_partition_id().alias(&quot;pid&quot;)).collect()</span>
<span class="sd">    [Row(pid=0), Row(pid=0)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">spark_partition_id</span><span class="p">())</span></div>


<div class="viewcode-block" id="expr"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.expr">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">expr</span><span class="p">(</span><span class="nb">str</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Parses the expression string into the column that it represents</span>

<span class="sd">    &gt;&gt;&gt; df.select(expr(&quot;length(name)&quot;)).collect()</span>
<span class="sd">    [Row(length(name)=5), Row(length(name)=3)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">expr</span><span class="p">(</span><span class="nb">str</span><span class="p">))</span></div>


<div class="viewcode-block" id="struct"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.struct">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">struct</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new struct column.</span>

<span class="sd">    :param cols: list of column names (string) or list of :class:`Column` expressions</span>

<span class="sd">    &gt;&gt;&gt; df.select(struct(&#39;age&#39;, &#39;name&#39;).alias(&quot;struct&quot;)).collect()</span>
<span class="sd">    [Row(struct=Row(age=2, name=u&#39;Alice&#39;)), Row(struct=Row(age=5, name=u&#39;Bob&#39;))]</span>
<span class="sd">    &gt;&gt;&gt; df.select(struct([df.age, df.name]).alias(&quot;struct&quot;)).collect()</span>
<span class="sd">    [Row(struct=Row(age=2, name=u&#39;Alice&#39;)), Row(struct=Row(age=5, name=u&#39;Bob&#39;))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">set</span><span class="p">)):</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">struct</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="greatest"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.greatest">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">greatest</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the greatest value of the list of column names, skipping null values.</span>
<span class="sd">    This function takes at least 2 parameters. It will return null iff all parameters are null.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(1, 4, 3)], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(greatest(df.a, df.b, df.c).alias(&quot;greatest&quot;)).collect()</span>
<span class="sd">    [Row(greatest=4)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;greatest should take at least two columns&quot;</span><span class="p">)</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">greatest</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">)))</span></div>


<div class="viewcode-block" id="least"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.least">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">least</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the least value of the list of column names, skipping null values.</span>
<span class="sd">    This function takes at least 2 parameters. It will return null iff all parameters are null.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(1, 4, 3)], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(least(df.a, df.b, df.c).alias(&quot;least&quot;)).collect()</span>
<span class="sd">    [Row(least=1)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;least should take at least two columns&quot;</span><span class="p">)</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">least</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">)))</span></div>


<div class="viewcode-block" id="when"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.when">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">when</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluates a list of conditions and returns one of multiple possible result expressions.</span>
<span class="sd">    If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.</span>

<span class="sd">    :param condition: a boolean :class:`Column` expression.</span>
<span class="sd">    :param value: a literal value, or a :class:`Column` expression.</span>

<span class="sd">    &gt;&gt;&gt; df.select(when(df[&#39;age&#39;] == 2, 3).otherwise(4).alias(&quot;age&quot;)).collect()</span>
<span class="sd">    [Row(age=3), Row(age=4)]</span>

<span class="sd">    &gt;&gt;&gt; df.select(when(df.age == 2, df.age + 1).alias(&quot;age&quot;)).collect()</span>
<span class="sd">    [Row(age=3), Row(age=None)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">Column</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;condition should be a Column&quot;</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">_jc</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Column</span><span class="p">)</span> <span class="k">else</span> <span class="n">value</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">condition</span><span class="o">.</span><span class="n">_jc</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="log"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.log">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the first argument-based logarithm of the second argument.</span>

<span class="sd">    If there is only one argument, then this takes the natural logarithm of the argument.</span>

<span class="sd">    &gt;&gt;&gt; df.select(log(10.0, df.age).alias(&#39;ten&#39;)).rdd.map(lambda l: str(l.ten)[:7]).collect()</span>
<span class="sd">    [&#39;0.30102&#39;, &#39;0.69897&#39;]</span>

<span class="sd">    &gt;&gt;&gt; df.select(log(df.age).alias(&#39;e&#39;)).rdd.map(lambda l: str(l.e)[:7]).collect()</span>
<span class="sd">    [&#39;0.69314&#39;, &#39;1.60943&#39;]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="n">arg2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">arg1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">arg2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="log2"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.log2">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">log2</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the base-2 logarithm of the argument.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(4,)], [&#39;a&#39;]).select(log2(&#39;a&#39;).alias(&#39;log2&#39;)).collect()</span>
<span class="sd">    [Row(log2=2.0)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="conv"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.conv">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">fromBase</span><span class="p">,</span> <span class="n">toBase</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert a number in a string column from one base to another.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&quot;010101&quot;,)], [&#39;n&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(conv(df.n, 2, 16).alias(&#39;hex&#39;)).collect()</span>
<span class="sd">    [Row(hex=u&#39;15&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">fromBase</span><span class="p">,</span> <span class="n">toBase</span><span class="p">))</span></div>


<div class="viewcode-block" id="factorial"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.factorial">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">factorial</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the factorial of the given value.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(5,)], [&#39;n&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(factorial(df.n).alias(&#39;f&#39;)).collect()</span>
<span class="sd">    [Row(f=120)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">factorial</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<span class="c1"># ---------------  Window functions ------------------------</span>

<div class="viewcode-block" id="lag"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.lag">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">lag</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Window function: returns the value that is `offset` rows before the current row, and</span>
<span class="sd">    `defaultValue` if there is less than `offset` rows before the current row. For example,</span>
<span class="sd">    an `offset` of one will return the previous row at any given point in the window partition.</span>

<span class="sd">    This is equivalent to the LAG function in SQL.</span>

<span class="sd">    :param col: name of column or expression</span>
<span class="sd">    :param count: number of row to extend</span>
<span class="sd">    :param default: default value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">lag</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">count</span><span class="p">,</span> <span class="n">default</span><span class="p">))</span></div>


<div class="viewcode-block" id="lead"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.lead">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">lead</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Window function: returns the value that is `offset` rows after the current row, and</span>
<span class="sd">    `defaultValue` if there is less than `offset` rows after the current row. For example,</span>
<span class="sd">    an `offset` of one will return the next row at any given point in the window partition.</span>

<span class="sd">    This is equivalent to the LEAD function in SQL.</span>

<span class="sd">    :param col: name of column or expression</span>
<span class="sd">    :param count: number of row to extend</span>
<span class="sd">    :param default: default value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">lead</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">count</span><span class="p">,</span> <span class="n">default</span><span class="p">))</span></div>


<div class="viewcode-block" id="ntile"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.ntile">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ntile</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Window function: returns the ntile group id (from 1 to `n` inclusive)</span>
<span class="sd">    in an ordered window partition. For example, if `n` is 4, the first</span>
<span class="sd">    quarter of the rows will get value 1, the second quarter will get 2,</span>
<span class="sd">    the third quarter will get 3, and the last quarter will get 4.</span>

<span class="sd">    This is equivalent to the NTILE function in SQL.</span>

<span class="sd">    :param n: an integer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">ntile</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)))</span></div>


<span class="c1"># ---------------------- Date/Timestamp functions ------------------------------</span>

<div class="viewcode-block" id="current_date"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.current_date">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">current_date</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the current date as a :class:`DateType` column.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">current_date</span><span class="p">())</span></div>


<div class="viewcode-block" id="current_timestamp"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.current_timestamp">[docs]</a><span class="k">def</span> <span class="nf">current_timestamp</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the current timestamp as a :class:`TimestampType` column.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">current_timestamp</span><span class="p">())</span></div>


<div class="viewcode-block" id="date_format"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.date_format">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">date_format</span><span class="p">(</span><span class="n">date</span><span class="p">,</span> <span class="nb">format</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a date/timestamp/string to a value of string in the format specified by the date</span>
<span class="sd">    format given by the second argument.</span>

<span class="sd">    A pattern could be for instance `dd.MM.yyyy` and could return a string like &#39;18.03.1993&#39;. All</span>
<span class="sd">    pattern letters of the Java class `java.text.SimpleDateFormat` can be used.</span>

<span class="sd">    .. note:: Use when ever possible specialized functions like `year`. These benefit from a</span>
<span class="sd">        specialized implementation.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(date_format(&#39;dt&#39;, &#39;MM/dd/yyy&#39;).alias(&#39;date&#39;)).collect()</span>
<span class="sd">    [Row(date=u&#39;04/08/2015&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">date_format</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">date</span><span class="p">),</span> <span class="nb">format</span><span class="p">))</span></div>


<div class="viewcode-block" id="year"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.year">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">year</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the year of a given date as integer.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(year(&#39;dt&#39;).alias(&#39;year&#39;)).collect()</span>
<span class="sd">    [Row(year=2015)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">year</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="quarter"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.quarter">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">quarter</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the quarter of a given date as integer.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(quarter(&#39;dt&#39;).alias(&#39;quarter&#39;)).collect()</span>
<span class="sd">    [Row(quarter=2)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">quarter</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="month"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.month">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">month</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the month of a given date as integer.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(month(&#39;dt&#39;).alias(&#39;month&#39;)).collect()</span>
<span class="sd">    [Row(month=4)]</span>
<span class="sd">   &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">month</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="dayofweek"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.dayofweek">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">dayofweek</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the day of the week of a given date as integer.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(dayofweek(&#39;dt&#39;).alias(&#39;day&#39;)).collect()</span>
<span class="sd">    [Row(day=4)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">dayofweek</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="dayofmonth"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.dayofmonth">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">dayofmonth</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the day of the month of a given date as integer.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(dayofmonth(&#39;dt&#39;).alias(&#39;day&#39;)).collect()</span>
<span class="sd">    [Row(day=8)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">dayofmonth</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="dayofyear"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.dayofyear">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">dayofyear</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the day of the year of a given date as integer.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(dayofyear(&#39;dt&#39;).alias(&#39;day&#39;)).collect()</span>
<span class="sd">    [Row(day=98)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">dayofyear</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="hour"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.hour">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">hour</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the hours of a given date as integer.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08 13:08:15&#39;,)], [&#39;ts&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(hour(&#39;ts&#39;).alias(&#39;hour&#39;)).collect()</span>
<span class="sd">    [Row(hour=13)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">hour</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="minute"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.minute">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">minute</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the minutes of a given date as integer.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08 13:08:15&#39;,)], [&#39;ts&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(minute(&#39;ts&#39;).alias(&#39;minute&#39;)).collect()</span>
<span class="sd">    [Row(minute=8)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">minute</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="second"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.second">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">second</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the seconds of a given date as integer.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08 13:08:15&#39;,)], [&#39;ts&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(second(&#39;ts&#39;).alias(&#39;second&#39;)).collect()</span>
<span class="sd">    [Row(second=15)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">second</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="weekofyear"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.weekofyear">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">weekofyear</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the week number of a given date as integer.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(weekofyear(df.dt).alias(&#39;week&#39;)).collect()</span>
<span class="sd">    [Row(week=15)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">weekofyear</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="date_add"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.date_add">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">date_add</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">days</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the date that is `days` days after `start`</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(date_add(df.dt, 1).alias(&#39;next_date&#39;)).collect()</span>
<span class="sd">    [Row(next_date=datetime.date(2015, 4, 9))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">date_add</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">start</span><span class="p">),</span> <span class="n">days</span><span class="p">))</span></div>


<div class="viewcode-block" id="date_sub"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.date_sub">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">date_sub</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">days</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the date that is `days` days before `start`</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(date_sub(df.dt, 1).alias(&#39;prev_date&#39;)).collect()</span>
<span class="sd">    [Row(prev_date=datetime.date(2015, 4, 7))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">date_sub</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">start</span><span class="p">),</span> <span class="n">days</span><span class="p">))</span></div>


<div class="viewcode-block" id="datediff"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.datediff">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">datediff</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="n">start</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the number of days from `start` to `end`.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,&#39;2015-05-10&#39;)], [&#39;d1&#39;, &#39;d2&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(datediff(df.d2, df.d1).alias(&#39;diff&#39;)).collect()</span>
<span class="sd">    [Row(diff=32)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">datediff</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">end</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">start</span><span class="p">)))</span></div>


<div class="viewcode-block" id="add_months"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.add_months">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">add_months</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">months</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the date that is `months` months after `start`</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(add_months(df.dt, 1).alias(&#39;next_month&#39;)).collect()</span>
<span class="sd">    [Row(next_month=datetime.date(2015, 5, 8))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">add_months</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">start</span><span class="p">),</span> <span class="n">months</span><span class="p">))</span></div>


<div class="viewcode-block" id="months_between"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.months_between">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">months_between</span><span class="p">(</span><span class="n">date1</span><span class="p">,</span> <span class="n">date2</span><span class="p">,</span> <span class="n">roundOff</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns number of months between dates date1 and date2.</span>
<span class="sd">    If date1 is later than date2, then the result is positive.</span>
<span class="sd">    If date1 and date2 are on the same day of month, or both are the last day of month,</span>
<span class="sd">    returns an integer (time of day will be ignored).</span>
<span class="sd">    The result is rounded off to 8 digits unless `roundOff` is set to `False`.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;, &#39;1996-10-30&#39;)], [&#39;date1&#39;, &#39;date2&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(months_between(df.date1, df.date2).alias(&#39;months&#39;)).collect()</span>
<span class="sd">    [Row(months=3.94959677)]</span>
<span class="sd">    &gt;&gt;&gt; df.select(months_between(df.date1, df.date2, False).alias(&#39;months&#39;)).collect()</span>
<span class="sd">    [Row(months=3.9495967741935485)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">months_between</span><span class="p">(</span>
        <span class="n">_to_java_column</span><span class="p">(</span><span class="n">date1</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">date2</span><span class="p">),</span> <span class="n">roundOff</span><span class="p">))</span></div>


<div class="viewcode-block" id="to_date"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.to_date">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.2</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">to_date</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or</span>
<span class="sd">    :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`</span>
<span class="sd">    using the optionally specified format. Specify formats according to</span>
<span class="sd">    `SimpleDateFormats &lt;http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html&gt;`_.</span>
<span class="sd">    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format</span>
<span class="sd">    is omitted (equivalent to ``col.cast(&quot;date&quot;)``).</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_date(df.t).alias(&#39;date&#39;)).collect()</span>
<span class="sd">    [Row(date=datetime.date(1997, 2, 28))]</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_date(df.t, &#39;yyyy-MM-dd HH:mm:ss&#39;).alias(&#39;date&#39;)).collect()</span>
<span class="sd">    [Row(date=datetime.date(1997, 2, 28))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="nb">format</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">to_date</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">to_date</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="nb">format</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="to_timestamp"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.to_timestamp">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.2</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">to_timestamp</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or</span>
<span class="sd">    :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`</span>
<span class="sd">    using the optionally specified format. Specify formats according to</span>
<span class="sd">    `SimpleDateFormats &lt;http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html&gt;`_.</span>
<span class="sd">    By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format</span>
<span class="sd">    is omitted (equivalent to ``col.cast(&quot;timestamp&quot;)``).</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_timestamp(df.t).alias(&#39;dt&#39;)).collect()</span>
<span class="sd">    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_timestamp(df.t, &#39;yyyy-MM-dd HH:mm:ss&#39;).alias(&#39;dt&#39;)).collect()</span>
<span class="sd">    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="nb">format</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">to_timestamp</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">to_timestamp</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="nb">format</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="trunc"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.trunc">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">trunc</span><span class="p">(</span><span class="n">date</span><span class="p">,</span> <span class="nb">format</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns date truncated to the unit specified by the format.</span>

<span class="sd">    :param format: &#39;year&#39;, &#39;yyyy&#39;, &#39;yy&#39; or &#39;month&#39;, &#39;mon&#39;, &#39;mm&#39;</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28&#39;,)], [&#39;d&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(trunc(df.d, &#39;year&#39;).alias(&#39;year&#39;)).collect()</span>
<span class="sd">    [Row(year=datetime.date(1997, 1, 1))]</span>
<span class="sd">    &gt;&gt;&gt; df.select(trunc(df.d, &#39;mon&#39;).alias(&#39;month&#39;)).collect()</span>
<span class="sd">    [Row(month=datetime.date(1997, 2, 1))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">trunc</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">date</span><span class="p">),</span> <span class="nb">format</span><span class="p">))</span></div>


<div class="viewcode-block" id="date_trunc"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.date_trunc">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">date_trunc</span><span class="p">(</span><span class="nb">format</span><span class="p">,</span> <span class="n">timestamp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns timestamp truncated to the unit specified by the format.</span>

<span class="sd">    :param format: &#39;year&#39;, &#39;yyyy&#39;, &#39;yy&#39;, &#39;month&#39;, &#39;mon&#39;, &#39;mm&#39;,</span>
<span class="sd">        &#39;day&#39;, &#39;dd&#39;, &#39;hour&#39;, &#39;minute&#39;, &#39;second&#39;, &#39;week&#39;, &#39;quarter&#39;</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 05:02:11&#39;,)], [&#39;t&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(date_trunc(&#39;year&#39;, df.t).alias(&#39;year&#39;)).collect()</span>
<span class="sd">    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]</span>
<span class="sd">    &gt;&gt;&gt; df.select(date_trunc(&#39;mon&#39;, df.t).alias(&#39;month&#39;)).collect()</span>
<span class="sd">    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">date_trunc</span><span class="p">(</span><span class="nb">format</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">timestamp</span><span class="p">)))</span></div>


<div class="viewcode-block" id="next_day"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.next_day">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">next_day</span><span class="p">(</span><span class="n">date</span><span class="p">,</span> <span class="n">dayOfWeek</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the first date which is later than the value of the date column.</span>

<span class="sd">    Day of the week parameter is case insensitive, and accepts:</span>
<span class="sd">        &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-07-27&#39;,)], [&#39;d&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(next_day(df.d, &#39;Sun&#39;).alias(&#39;date&#39;)).collect()</span>
<span class="sd">    [Row(date=datetime.date(2015, 8, 2))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">next_day</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">date</span><span class="p">),</span> <span class="n">dayOfWeek</span><span class="p">))</span></div>


<div class="viewcode-block" id="last_day"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.last_day">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">last_day</span><span class="p">(</span><span class="n">date</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the last day of the month which the given date belongs to.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-10&#39;,)], [&#39;d&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(last_day(df.d).alias(&#39;date&#39;)).collect()</span>
<span class="sd">    [Row(date=datetime.date(1997, 2, 28))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">last_day</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">date</span><span class="p">)))</span></div>


<div class="viewcode-block" id="from_unixtime"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.from_unixtime">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">from_unixtime</span><span class="p">(</span><span class="n">timestamp</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;yyyy-MM-dd HH:mm:ss&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string</span>
<span class="sd">    representing the timestamp of that moment in the current system time zone in the given</span>
<span class="sd">    format.</span>

<span class="sd">    &gt;&gt;&gt; spark.conf.set(&quot;spark.sql.session.timeZone&quot;, &quot;America/Los_Angeles&quot;)</span>
<span class="sd">    &gt;&gt;&gt; time_df = spark.createDataFrame([(1428476400,)], [&#39;unix_time&#39;])</span>
<span class="sd">    &gt;&gt;&gt; time_df.select(from_unixtime(&#39;unix_time&#39;).alias(&#39;ts&#39;)).collect()</span>
<span class="sd">    [Row(ts=u&#39;2015-04-08 00:00:00&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; spark.conf.unset(&quot;spark.sql.session.timeZone&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">from_unixtime</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">timestamp</span><span class="p">),</span> <span class="nb">format</span><span class="p">))</span></div>


<div class="viewcode-block" id="unix_timestamp"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.unix_timestamp">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">unix_timestamp</span><span class="p">(</span><span class="n">timestamp</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;yyyy-MM-dd HH:mm:ss&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert time string with given pattern (&#39;yyyy-MM-dd HH:mm:ss&#39;, by default)</span>
<span class="sd">    to Unix time stamp (in seconds), using the default timezone and the default</span>
<span class="sd">    locale, return null if fail.</span>

<span class="sd">    if `timestamp` is None, then it returns current timestamp.</span>

<span class="sd">    &gt;&gt;&gt; spark.conf.set(&quot;spark.sql.session.timeZone&quot;, &quot;America/Los_Angeles&quot;)</span>
<span class="sd">    &gt;&gt;&gt; time_df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])</span>
<span class="sd">    &gt;&gt;&gt; time_df.select(unix_timestamp(&#39;dt&#39;, &#39;yyyy-MM-dd&#39;).alias(&#39;unix_time&#39;)).collect()</span>
<span class="sd">    [Row(unix_time=1428476400)]</span>
<span class="sd">    &gt;&gt;&gt; spark.conf.unset(&quot;spark.sql.session.timeZone&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="n">timestamp</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">unix_timestamp</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">unix_timestamp</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">timestamp</span><span class="p">),</span> <span class="nb">format</span><span class="p">))</span></div>


<div class="viewcode-block" id="from_utc_timestamp"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.from_utc_timestamp">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">from_utc_timestamp</span><span class="p">(</span><span class="n">timestamp</span><span class="p">,</span> <span class="n">tz</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function</span>
<span class="sd">    takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and</span>
<span class="sd">    renders that timestamp as a timestamp in the given time zone.</span>

<span class="sd">    However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not</span>
<span class="sd">    timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to</span>
<span class="sd">    the given timezone.</span>

<span class="sd">    This function may return confusing result if the input is a string with timezone, e.g.</span>
<span class="sd">    &#39;2018-03-13T06:18:23+00:00&#39;. The reason is that, Spark firstly cast the string to timestamp</span>
<span class="sd">    according to the timezone in the string, and finally display the result by converting the</span>
<span class="sd">    timestamp to string according to the session local timezone.</span>

<span class="sd">    :param timestamp: the column that contains timestamps</span>
<span class="sd">    :param tz: a string that has the ID of timezone, e.g. &quot;GMT&quot;, &quot;America/Los_Angeles&quot;, etc</span>

<span class="sd">    .. versionchanged:: 2.4</span>
<span class="sd">       `tz` can take a :class:`Column` containing timezone ID strings.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;, &#39;JST&#39;)], [&#39;ts&#39;, &#39;tz&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(from_utc_timestamp(df.ts, &quot;PST&quot;).alias(&#39;local_time&#39;)).collect()</span>
<span class="sd">    [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]</span>
<span class="sd">    &gt;&gt;&gt; df.select(from_utc_timestamp(df.ts, df.tz).alias(&#39;local_time&#39;)).collect()</span>
<span class="sd">    [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tz</span><span class="p">,</span> <span class="n">Column</span><span class="p">):</span>
        <span class="n">tz</span> <span class="o">=</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">tz</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">from_utc_timestamp</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">timestamp</span><span class="p">),</span> <span class="n">tz</span><span class="p">))</span></div>


<div class="viewcode-block" id="to_utc_timestamp"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.to_utc_timestamp">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">to_utc_timestamp</span><span class="p">(</span><span class="n">timestamp</span><span class="p">,</span> <span class="n">tz</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function</span>
<span class="sd">    takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given</span>
<span class="sd">    timezone, and renders that timestamp as a timestamp in UTC.</span>

<span class="sd">    However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not</span>
<span class="sd">    timezone-agnostic. So in Spark this function just shift the timestamp value from the given</span>
<span class="sd">    timezone to UTC timezone.</span>

<span class="sd">    This function may return confusing result if the input is a string with timezone, e.g.</span>
<span class="sd">    &#39;2018-03-13T06:18:23+00:00&#39;. The reason is that, Spark firstly cast the string to timestamp</span>
<span class="sd">    according to the timezone in the string, and finally display the result by converting the</span>
<span class="sd">    timestamp to string according to the session local timezone.</span>

<span class="sd">    :param timestamp: the column that contains timestamps</span>
<span class="sd">    :param tz: a string that has the ID of timezone, e.g. &quot;GMT&quot;, &quot;America/Los_Angeles&quot;, etc</span>

<span class="sd">    .. versionchanged:: 2.4</span>
<span class="sd">       `tz` can take a :class:`Column` containing timezone ID strings.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;, &#39;JST&#39;)], [&#39;ts&#39;, &#39;tz&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_utc_timestamp(df.ts, &quot;PST&quot;).alias(&#39;utc_time&#39;)).collect()</span>
<span class="sd">    [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_utc_timestamp(df.ts, df.tz).alias(&#39;utc_time&#39;)).collect()</span>
<span class="sd">    [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tz</span><span class="p">,</span> <span class="n">Column</span><span class="p">):</span>
        <span class="n">tz</span> <span class="o">=</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">tz</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">to_utc_timestamp</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">timestamp</span><span class="p">),</span> <span class="n">tz</span><span class="p">))</span></div>


<div class="viewcode-block" id="window"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.window">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">window</span><span class="p">(</span><span class="n">timeColumn</span><span class="p">,</span> <span class="n">windowDuration</span><span class="p">,</span> <span class="n">slideDuration</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">startTime</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Bucketize rows into one or more time windows given a timestamp specifying column. Window</span>
<span class="sd">    starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window</span>
<span class="sd">    [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in</span>
<span class="sd">    the order of months are not supported.</span>

<span class="sd">    The time column must be of :class:`pyspark.sql.types.TimestampType`.</span>

<span class="sd">    Durations are provided as strings, e.g. &#39;1 second&#39;, &#39;1 day 12 hours&#39;, &#39;2 minutes&#39;. Valid</span>
<span class="sd">    interval strings are &#39;week&#39;, &#39;day&#39;, &#39;hour&#39;, &#39;minute&#39;, &#39;second&#39;, &#39;millisecond&#39;, &#39;microsecond&#39;.</span>
<span class="sd">    If the ``slideDuration`` is not provided, the windows will be tumbling windows.</span>

<span class="sd">    The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start</span>
<span class="sd">    window intervals. For example, in order to have hourly tumbling windows that start 15 minutes</span>
<span class="sd">    past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.</span>

<span class="sd">    The output column will be a struct called &#39;window&#39; by default with the nested columns &#39;start&#39;</span>
<span class="sd">    and &#39;end&#39;, where &#39;start&#39; and &#39;end&#39; will be of :class:`pyspark.sql.types.TimestampType`.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&quot;2016-03-11 09:00:07&quot;, 1)]).toDF(&quot;date&quot;, &quot;val&quot;)</span>
<span class="sd">    &gt;&gt;&gt; w = df.groupBy(window(&quot;date&quot;, &quot;5 seconds&quot;)).agg(sum(&quot;val&quot;).alias(&quot;sum&quot;))</span>
<span class="sd">    &gt;&gt;&gt; w.select(w.window.start.cast(&quot;string&quot;).alias(&quot;start&quot;),</span>
<span class="sd">    ...          w.window.end.cast(&quot;string&quot;).alias(&quot;end&quot;), &quot;sum&quot;).collect()</span>
<span class="sd">    [Row(start=u&#39;2016-03-11 09:00:05&#39;, end=u&#39;2016-03-11 09:00:10&#39;, sum=1)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">check_string_field</span><span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">fieldName</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">field</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">field</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">str</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> should be provided as a string&quot;</span> <span class="o">%</span> <span class="n">fieldName</span><span class="p">)</span>

    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">time_col</span> <span class="o">=</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">timeColumn</span><span class="p">)</span>
    <span class="n">check_string_field</span><span class="p">(</span><span class="n">windowDuration</span><span class="p">,</span> <span class="s2">&quot;windowDuration&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">slideDuration</span> <span class="ow">and</span> <span class="n">startTime</span><span class="p">:</span>
        <span class="n">check_string_field</span><span class="p">(</span><span class="n">slideDuration</span><span class="p">,</span> <span class="s2">&quot;slideDuration&quot;</span><span class="p">)</span>
        <span class="n">check_string_field</span><span class="p">(</span><span class="n">startTime</span><span class="p">,</span> <span class="s2">&quot;startTime&quot;</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="n">time_col</span><span class="p">,</span> <span class="n">windowDuration</span><span class="p">,</span> <span class="n">slideDuration</span><span class="p">,</span> <span class="n">startTime</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">slideDuration</span><span class="p">:</span>
        <span class="n">check_string_field</span><span class="p">(</span><span class="n">slideDuration</span><span class="p">,</span> <span class="s2">&quot;slideDuration&quot;</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="n">time_col</span><span class="p">,</span> <span class="n">windowDuration</span><span class="p">,</span> <span class="n">slideDuration</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">startTime</span><span class="p">:</span>
        <span class="n">check_string_field</span><span class="p">(</span><span class="n">startTime</span><span class="p">,</span> <span class="s2">&quot;startTime&quot;</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="n">time_col</span><span class="p">,</span> <span class="n">windowDuration</span><span class="p">,</span> <span class="n">windowDuration</span><span class="p">,</span> <span class="n">startTime</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="n">time_col</span><span class="p">,</span> <span class="n">windowDuration</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">res</span><span class="p">)</span></div>


<span class="c1"># ---------------------------- misc functions ----------------------------------</span>

<div class="viewcode-block" id="crc32"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.crc32">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">crc32</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the cyclic redundancy check value  (CRC32) of a binary column and</span>
<span class="sd">    returns the value as a bigint.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(&#39;ABC&#39;,)], [&#39;a&#39;]).select(crc32(&#39;a&#39;).alias(&#39;crc32&#39;)).collect()</span>
<span class="sd">    [Row(crc32=2743272264)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">crc32</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="md5"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.md5">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">md5</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculates the MD5 digest and returns the value as a 32 character hex string.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(&#39;ABC&#39;,)], [&#39;a&#39;]).select(md5(&#39;a&#39;).alias(&#39;hash&#39;)).collect()</span>
<span class="sd">    [Row(hash=u&#39;902fbdd2b1df0c4f70b4a5d23525e932&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="sha1"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.sha1">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sha1</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the hex string result of SHA-1.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(&#39;ABC&#39;,)], [&#39;a&#39;]).select(sha1(&#39;a&#39;).alias(&#39;hash&#39;)).collect()</span>
<span class="sd">    [Row(hash=u&#39;3c01bdbb26f358bab27f267924aa2c9a03fcfdb8&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">sha1</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="sha2"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.sha2">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sha2</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">numBits</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,</span>
<span class="sd">    and SHA-512). The numBits indicates the desired bit length of the result, which must have a</span>
<span class="sd">    value of 224, 256, 384, 512, or 0 (which is equivalent to 256).</span>

<span class="sd">    &gt;&gt;&gt; digests = df.select(sha2(df.name, 256).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    &gt;&gt;&gt; digests[0]</span>
<span class="sd">    Row(s=u&#39;3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043&#39;)</span>
<span class="sd">    &gt;&gt;&gt; digests[1]</span>
<span class="sd">    Row(s=u&#39;cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">sha2</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">numBits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="hash"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.hash">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">hash</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculates the hash code of given columns, and returns the result as an int column.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(&#39;ABC&#39;,)], [&#39;a&#39;]).select(hash(&#39;a&#39;).alias(&#39;hash&#39;)).collect()</span>
<span class="sd">    [Row(hash=-757602832)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">hash</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<span class="c1"># ---------------------- String/Binary functions ------------------------------</span>

<span class="n">_string_functions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;ascii&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the numeric value of the first character of the string column.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;base64&#39;</span><span class="p">:</span> <span class="s1">&#39;Computes the BASE64 encoding of a binary column and returns it as a string column.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unbase64&#39;</span><span class="p">:</span> <span class="s1">&#39;Decodes a BASE64 encoded string column and returns it as a binary column.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;initcap&#39;</span><span class="p">:</span> <span class="s1">&#39;Returns a new string column by converting the first letter of each word to &#39;</span> <span class="o">+</span>
               <span class="s1">&#39;uppercase. Words are delimited by whitespace.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lower&#39;</span><span class="p">:</span> <span class="s1">&#39;Converts a string column to lower case.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;upper&#39;</span><span class="p">:</span> <span class="s1">&#39;Converts a string column to upper case.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ltrim&#39;</span><span class="p">:</span> <span class="s1">&#39;Trim the spaces from left end for the specified string value.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;rtrim&#39;</span><span class="p">:</span> <span class="s1">&#39;Trim the spaces from right end for the specified string value.&#39;</span><span class="p">,</span>
    <span class="s1">&#39;trim&#39;</span><span class="p">:</span> <span class="s1">&#39;Trim the spaces from both ends for the specified string column.&#39;</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span> <span class="ow">in</span> <span class="n">_string_functions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)(</span><span class="n">_create_function</span><span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span><span class="p">))</span>
<span class="k">del</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_doc</span>


<div class="viewcode-block" id="concat_ws"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.concat_ws">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">concat_ws</span><span class="p">(</span><span class="n">sep</span><span class="p">,</span> <span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Concatenates multiple input string columns together into a single string column,</span>
<span class="sd">    using the given separator.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,&#39;123&#39;)], [&#39;s&#39;, &#39;d&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(concat_ws(&#39;-&#39;, df.s, df.d).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=u&#39;abcd-123&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">concat_ws</span><span class="p">(</span><span class="n">sep</span><span class="p">,</span> <span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">)))</span></div>


<div class="viewcode-block" id="decode"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.decode">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">charset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the first argument into a string from a binary using the provided character set</span>
<span class="sd">    (one of &#39;US-ASCII&#39;, &#39;ISO-8859-1&#39;, &#39;UTF-8&#39;, &#39;UTF-16BE&#39;, &#39;UTF-16LE&#39;, &#39;UTF-16&#39;).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">charset</span><span class="p">))</span></div>


<div class="viewcode-block" id="encode"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.encode">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">charset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the first argument into a binary from a string using the provided character set</span>
<span class="sd">    (one of &#39;US-ASCII&#39;, &#39;ISO-8859-1&#39;, &#39;UTF-8&#39;, &#39;UTF-16BE&#39;, &#39;UTF-16LE&#39;, &#39;UTF-16&#39;).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">charset</span><span class="p">))</span></div>


<div class="viewcode-block" id="format_number"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.format_number">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">format_number</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Formats the number X to a format like &#39;#,--#,--#.--&#39;, rounded to d decimal places</span>
<span class="sd">    with HALF_EVEN round mode, and returns the result as a string.</span>

<span class="sd">    :param col: the column name of the numeric value to be formatted</span>
<span class="sd">    :param d: the N decimal places</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(5,)], [&#39;a&#39;]).select(format_number(&#39;a&#39;, 4).alias(&#39;v&#39;)).collect()</span>
<span class="sd">    [Row(v=u&#39;5.0000&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">format_number</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">d</span><span class="p">))</span></div>


<div class="viewcode-block" id="format_string"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.format_string">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">format_string</span><span class="p">(</span><span class="nb">format</span><span class="p">,</span> <span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Formats the arguments in printf-style and returns the result as a string column.</span>

<span class="sd">    :param col: the column name of the numeric value to be formatted</span>
<span class="sd">    :param d: the N decimal places</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(5, &quot;hello&quot;)], [&#39;a&#39;, &#39;b&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(format_string(&#39;%d %s&#39;, df.a, df.b).alias(&#39;v&#39;)).collect()</span>
<span class="sd">    [Row(v=u&#39;5 hello&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">format_string</span><span class="p">(</span><span class="nb">format</span><span class="p">,</span> <span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">)))</span></div>


<div class="viewcode-block" id="instr"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.instr">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">instr</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">substr</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Locate the position of the first occurrence of substr column in the given string.</span>
<span class="sd">    Returns null if either of the arguments are null.</span>

<span class="sd">    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr</span>
<span class="sd">        could not be found in str.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,)], [&#39;s&#39;,])</span>
<span class="sd">    &gt;&gt;&gt; df.select(instr(df.s, &#39;b&#39;).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=2)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">instr</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="n">substr</span><span class="p">))</span></div>


<div class="viewcode-block" id="substring"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.substring">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">substring</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="nb">len</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Substring starts at `pos` and is of length `len` when str is String type or</span>
<span class="sd">    returns the slice of byte array that starts at `pos` in byte and is of length `len`</span>
<span class="sd">    when str is Binary type.</span>

<span class="sd">    .. note:: The position is not zero based, but 1 based index.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,)], [&#39;s&#39;,])</span>
<span class="sd">    &gt;&gt;&gt; df.select(substring(df.s, 1, 2).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=u&#39;ab&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">substring</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="n">pos</span><span class="p">,</span> <span class="nb">len</span><span class="p">))</span></div>


<div class="viewcode-block" id="substring_index"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.substring_index">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">substring_index</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">count</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the substring from string str before count occurrences of the delimiter delim.</span>
<span class="sd">    If count is positive, everything the left of the final delimiter (counting from left) is</span>
<span class="sd">    returned. If count is negative, every to the right of the final delimiter (counting from the</span>
<span class="sd">    right) is returned. substring_index performs a case-sensitive match when searching for delim.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;a.b.c.d&#39;,)], [&#39;s&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(substring_index(df.s, &#39;.&#39;, 2).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=u&#39;a.b&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; df.select(substring_index(df.s, &#39;.&#39;, -3).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=u&#39;b.c.d&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">substring_index</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="n">delim</span><span class="p">,</span> <span class="n">count</span><span class="p">))</span></div>


<div class="viewcode-block" id="levenshtein"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.levenshtein">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">levenshtein</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the Levenshtein distance of the two given strings.</span>

<span class="sd">    &gt;&gt;&gt; df0 = spark.createDataFrame([(&#39;kitten&#39;, &#39;sitting&#39;,)], [&#39;l&#39;, &#39;r&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df0.select(levenshtein(&#39;l&#39;, &#39;r&#39;).alias(&#39;d&#39;)).collect()</span>
<span class="sd">    [Row(d=3)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">levenshtein</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">left</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">right</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="locate"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.locate">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">locate</span><span class="p">(</span><span class="n">substr</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Locate the position of the first occurrence of substr in a string column, after position pos.</span>

<span class="sd">    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr</span>
<span class="sd">        could not be found in str.</span>

<span class="sd">    :param substr: a string</span>
<span class="sd">    :param str: a Column of :class:`pyspark.sql.types.StringType`</span>
<span class="sd">    :param pos: start position (zero based)</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,)], [&#39;s&#39;,])</span>
<span class="sd">    &gt;&gt;&gt; df.select(locate(&#39;b&#39;, df.s, 1).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=2)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">locate</span><span class="p">(</span><span class="n">substr</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="n">pos</span><span class="p">))</span></div>


<div class="viewcode-block" id="lpad"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.lpad">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">lpad</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="nb">len</span><span class="p">,</span> <span class="n">pad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Left-pad the string column to width `len` with `pad`.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,)], [&#39;s&#39;,])</span>
<span class="sd">    &gt;&gt;&gt; df.select(lpad(df.s, 6, &#39;#&#39;).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=u&#39;##abcd&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">lpad</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="nb">len</span><span class="p">,</span> <span class="n">pad</span><span class="p">))</span></div>


<div class="viewcode-block" id="rpad"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.rpad">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">rpad</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="nb">len</span><span class="p">,</span> <span class="n">pad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Right-pad the string column to width `len` with `pad`.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,)], [&#39;s&#39;,])</span>
<span class="sd">    &gt;&gt;&gt; df.select(rpad(df.s, 6, &#39;#&#39;).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=u&#39;abcd##&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">rpad</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="nb">len</span><span class="p">,</span> <span class="n">pad</span><span class="p">))</span></div>


<div class="viewcode-block" id="repeat"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.repeat">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">repeat</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Repeats a string column n times, and returns it as a new string column.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;ab&#39;,)], [&#39;s&#39;,])</span>
<span class="sd">    &gt;&gt;&gt; df.select(repeat(df.s, 3).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=u&#39;ababab&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">n</span><span class="p">))</span></div>


<div class="viewcode-block" id="split"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.split">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">pattern</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits str around pattern (pattern is a regular expression).</span>

<span class="sd">    .. note:: pattern is a string represent the regular expression.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;ab12cd&#39;,)], [&#39;s&#39;,])</span>
<span class="sd">    &gt;&gt;&gt; df.select(split(df.s, &#39;[0-9]+&#39;).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=[u&#39;ab&#39;, u&#39;cd&#39;])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="n">pattern</span><span class="p">))</span></div>


<div class="viewcode-block" id="regexp_extract"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.regexp_extract">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">regexp_extract</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">pattern</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Extract a specific group matched by a Java regex, from the specified string column.</span>
<span class="sd">    If the regex did not match, or the specified group did not match, an empty string is returned.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;100-200&#39;,)], [&#39;str&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(regexp_extract(&#39;str&#39;, r&#39;(\d+)-(\d+)&#39;, 1).alias(&#39;d&#39;)).collect()</span>
<span class="sd">    [Row(d=u&#39;100&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;foo&#39;,)], [&#39;str&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(regexp_extract(&#39;str&#39;, r&#39;(\d+)&#39;, 1).alias(&#39;d&#39;)).collect()</span>
<span class="sd">    [Row(d=u&#39;&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;aaaac&#39;,)], [&#39;str&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(regexp_extract(&#39;str&#39;, &#39;(a+)(b)?(c)&#39;, 2).alias(&#39;d&#39;)).collect()</span>
<span class="sd">    [Row(d=u&#39;&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">regexp_extract</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="n">pattern</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="regexp_replace"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.regexp_replace">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">regexp_replace</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">pattern</span><span class="p">,</span> <span class="n">replacement</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Replace all substrings of the specified string value that match regexp with rep.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;100-200&#39;,)], [&#39;str&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(regexp_replace(&#39;str&#39;, r&#39;(\d+)&#39;, &#39;--&#39;).alias(&#39;d&#39;)).collect()</span>
<span class="sd">    [Row(d=u&#39;-----&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">regexp_replace</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="n">pattern</span><span class="p">,</span> <span class="n">replacement</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="initcap"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.initcap">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">initcap</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Translate the first letter of each word to upper case in the sentence.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(&#39;ab cd&#39;,)], [&#39;a&#39;]).select(initcap(&quot;a&quot;).alias(&#39;v&#39;)).collect()</span>
<span class="sd">    [Row(v=u&#39;Ab Cd&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">initcap</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="soundex"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.soundex">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">soundex</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the SoundEx encoding for a string</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&quot;Peters&quot;,),(&quot;Uhrbach&quot;,)], [&#39;name&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(soundex(df.name).alias(&quot;soundex&quot;)).collect()</span>
<span class="sd">    [Row(soundex=u&#39;P362&#39;), Row(soundex=u&#39;U612&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">soundex</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="bin"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.bin">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">bin</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the string representation of the binary value of the given column.</span>

<span class="sd">    &gt;&gt;&gt; df.select(bin(df.age).alias(&#39;c&#39;)).collect()</span>
<span class="sd">    [Row(c=u&#39;10&#39;), Row(c=u&#39;101&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">bin</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="hex"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.hex">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">hex</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,</span>
<span class="sd">    :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or</span>
<span class="sd">    :class:`pyspark.sql.types.LongType`.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(&#39;ABC&#39;, 3)], [&#39;a&#39;, &#39;b&#39;]).select(hex(&#39;a&#39;), hex(&#39;b&#39;)).collect()</span>
<span class="sd">    [Row(hex(a)=u&#39;414243&#39;, hex(b)=u&#39;3&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">hex</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="unhex"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.unhex">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">unhex</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Inverse of hex. Interprets each pair of characters as a hexadecimal number</span>
<span class="sd">    and converts to the byte representation of number.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(&#39;414243&#39;,)], [&#39;a&#39;]).select(unhex(&#39;a&#39;)).collect()</span>
<span class="sd">    [Row(unhex(a)=bytearray(b&#39;ABC&#39;))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">unhex</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="length"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.length">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">length</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the character length of string data or number of bytes of binary data.</span>
<span class="sd">    The length of character data includes the trailing spaces. The length of binary data</span>
<span class="sd">    includes binary zeros.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(&#39;ABC &#39;,)], [&#39;a&#39;]).select(length(&#39;a&#39;).alias(&#39;length&#39;)).collect()</span>
<span class="sd">    [Row(length=4)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">length</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="translate"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.translate">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">srcCol</span><span class="p">,</span> <span class="n">matching</span><span class="p">,</span> <span class="n">replace</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A function translate any character in the `srcCol` by a character in `matching`.</span>
<span class="sd">    The characters in `replace` is corresponding to the characters in `matching`.</span>
<span class="sd">    The translate will happen when any character in the string matching with the character</span>
<span class="sd">    in the `matching`.</span>

<span class="sd">    &gt;&gt;&gt; spark.createDataFrame([(&#39;translate&#39;,)], [&#39;a&#39;]).select(translate(&#39;a&#39;, &quot;rnlt&quot;, &quot;123&quot;) \\</span>
<span class="sd">    ...     .alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=u&#39;1a2s3ae&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">srcCol</span><span class="p">),</span> <span class="n">matching</span><span class="p">,</span> <span class="n">replace</span><span class="p">))</span></div>


<span class="c1"># ---------------------- Collection functions ------------------------------</span>

<div class="viewcode-block" id="create_map"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.create_map">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">create_map</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new map column.</span>

<span class="sd">    :param cols: list of column names (string) or list of :class:`Column` expressions that are</span>
<span class="sd">        grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).</span>

<span class="sd">    &gt;&gt;&gt; df.select(create_map(&#39;name&#39;, &#39;age&#39;).alias(&quot;map&quot;)).collect()</span>
<span class="sd">    [Row(map={u&#39;Alice&#39;: 2}), Row(map={u&#39;Bob&#39;: 5})]</span>
<span class="sd">    &gt;&gt;&gt; df.select(create_map([df.name, df.age]).alias(&quot;map&quot;)).collect()</span>
<span class="sd">    [Row(map={u&#39;Alice&#39;: 2}), Row(map={u&#39;Bob&#39;: 5})]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">set</span><span class="p">)):</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="map_from_arrays"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.map_from_arrays">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">map_from_arrays</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new map from two arrays.</span>

<span class="sd">    :param col1: name of column containing a set of keys. All elements should not be null</span>
<span class="sd">    :param col2: name of column containing a set of values</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([2, 5], [&#39;a&#39;, &#39;b&#39;])], [&#39;k&#39;, &#39;v&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(map_from_arrays(df.k, df.v).alias(&quot;map&quot;)).show()</span>
<span class="sd">    +----------------+</span>
<span class="sd">    |             map|</span>
<span class="sd">    +----------------+</span>
<span class="sd">    |[2 -&gt; a, 5 -&gt; b]|</span>
<span class="sd">    +----------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">map_from_arrays</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col1</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">col2</span><span class="p">)))</span></div>


<div class="viewcode-block" id="array"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new array column.</span>

<span class="sd">    :param cols: list of column names (string) or list of :class:`Column` expressions that have</span>
<span class="sd">        the same data type.</span>

<span class="sd">    &gt;&gt;&gt; df.select(array(&#39;age&#39;, &#39;age&#39;).alias(&quot;arr&quot;)).collect()</span>
<span class="sd">    [Row(arr=[2, 2]), Row(arr=[5, 5])]</span>
<span class="sd">    &gt;&gt;&gt; df.select(array([df.age, df.age]).alias(&quot;arr&quot;)).collect()</span>
<span class="sd">    [Row(arr=[2, 2]), Row(arr=[5, 5])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">set</span><span class="p">)):</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="array_contains"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_contains">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_contains</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: returns null if the array is null, true if the array contains the</span>
<span class="sd">    given value, and false otherwise.</span>

<span class="sd">    :param col: name of column containing array</span>
<span class="sd">    :param value: value to check for in array</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],), ([],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_contains(df.data, &quot;a&quot;)).collect()</span>
<span class="sd">    [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_contains</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">value</span><span class="p">))</span></div>


<div class="viewcode-block" id="arrays_overlap"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.arrays_overlap">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">arrays_overlap</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: returns true if the arrays contain any common non-null element; if not,</span>
<span class="sd">    returns null if both the arrays are non-empty and any of them contains a null element; returns</span>
<span class="sd">    false otherwise.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([&quot;a&quot;, &quot;b&quot;], [&quot;b&quot;, &quot;c&quot;]), ([&quot;a&quot;], [&quot;b&quot;, &quot;c&quot;])], [&#39;x&#39;, &#39;y&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(arrays_overlap(df.x, df.y).alias(&quot;overlap&quot;)).collect()</span>
<span class="sd">    [Row(overlap=True), Row(overlap=False)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">arrays_overlap</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">a1</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">a2</span><span class="p">)))</span></div>


<div class="viewcode-block" id="slice"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.slice">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: returns an array containing  all the elements in `x` from index `start`</span>
<span class="sd">    (or starting from the end if `start` is negative) with the specified `length`.</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], [&#39;x&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(slice(df.x, 2, 2).alias(&quot;sliced&quot;)).collect()</span>
<span class="sd">    [Row(sliced=[2, 3]), Row(sliced=[5])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">start</span><span class="p">,</span> <span class="n">length</span><span class="p">))</span></div>


<div class="viewcode-block" id="array_join"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_join">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_join</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">delimiter</span><span class="p">,</span> <span class="n">null_replacement</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Concatenates the elements of `column` using the `delimiter`. Null values are replaced with</span>
<span class="sd">    `null_replacement` if set, otherwise they are ignored.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],), ([&quot;a&quot;, None],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_join(df.data, &quot;,&quot;).alias(&quot;joined&quot;)).collect()</span>
<span class="sd">    [Row(joined=u&#39;a,b,c&#39;), Row(joined=u&#39;a&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_join(df.data, &quot;,&quot;, &quot;NULL&quot;).alias(&quot;joined&quot;)).collect()</span>
<span class="sd">    [Row(joined=u&#39;a,b,c&#39;), Row(joined=u&#39;a,NULL&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="n">null_replacement</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_join</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">delimiter</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_join</span><span class="p">(</span>
            <span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">delimiter</span><span class="p">,</span> <span class="n">null_replacement</span><span class="p">))</span></div>


<div class="viewcode-block" id="concat"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.concat">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Concatenates multiple input columns together into a single column.</span>
<span class="sd">    The function works with strings, binary and compatible array columns.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,&#39;123&#39;)], [&#39;s&#39;, &#39;d&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(concat(df.s, df.d).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=u&#39;abcd123&#39;)]</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(concat(df.a, df.b, df.c).alias(&quot;arr&quot;)).collect()</span>
<span class="sd">    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">)))</span></div>


<div class="viewcode-block" id="array_position"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_position">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_position</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: Locates the position of the first occurrence of the given value</span>
<span class="sd">    in the given array. Returns null if either of the arguments are null.</span>

<span class="sd">    .. note:: The position is not zero based, but 1 based index. Returns 0 if the given</span>
<span class="sd">        value could not be found in the array.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([&quot;c&quot;, &quot;b&quot;, &quot;a&quot;],), ([],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_position(df.data, &quot;a&quot;)).collect()</span>
<span class="sd">    [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_position</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">value</span><span class="p">))</span></div>


<div class="viewcode-block" id="element_at"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.element_at">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">element_at</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">extraction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: Returns element of array at given index in extraction if col is array.</span>
<span class="sd">    Returns value for the given key in extraction if col is map.</span>

<span class="sd">    :param col: name of column containing array or map</span>
<span class="sd">    :param extraction: index to check for in array or key to check for in map</span>

<span class="sd">    .. note:: The position is not zero based, but 1 based index.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],), ([],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(element_at(df.data, 1)).collect()</span>
<span class="sd">    [Row(element_at(data, 1)=u&#39;a&#39;), Row(element_at(data, 1)=None)]</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([({&quot;a&quot;: 1.0, &quot;b&quot;: 2.0},), ({},)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(element_at(df.data, &quot;a&quot;)).collect()</span>
<span class="sd">    [Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">element_at</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">extraction</span><span class="p">))</span></div>


<div class="viewcode-block" id="array_remove"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_remove">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_remove</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">element</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: Remove all elements that equal to element from the given array.</span>

<span class="sd">    :param col: name of column containing array</span>
<span class="sd">    :param element: element to be removed from the array</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_remove(df.data, 1)).collect()</span>
<span class="sd">    [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_remove</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">element</span><span class="p">))</span></div>


<div class="viewcode-block" id="array_distinct"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_distinct">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_distinct</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: removes duplicate values from the array.</span>
<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_distinct(df.data)).collect()</span>
<span class="sd">    [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_distinct</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="array_intersect"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_intersect">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_intersect</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: returns an array of the elements in the intersection of col1 and col2,</span>
<span class="sd">    without duplicates.</span>

<span class="sd">    :param col1: name of column containing array</span>
<span class="sd">    :param col2: name of column containing array</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql import Row</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([Row(c1=[&quot;b&quot;, &quot;a&quot;, &quot;c&quot;], c2=[&quot;c&quot;, &quot;d&quot;, &quot;a&quot;, &quot;f&quot;])])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_intersect(df.c1, df.c2)).collect()</span>
<span class="sd">    [Row(array_intersect(c1, c2)=[u&#39;a&#39;, u&#39;c&#39;])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_intersect</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col1</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">col2</span><span class="p">)))</span></div>


<div class="viewcode-block" id="array_union"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_union">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_union</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: returns an array of the elements in the union of col1 and col2,</span>
<span class="sd">    without duplicates.</span>

<span class="sd">    :param col1: name of column containing array</span>
<span class="sd">    :param col2: name of column containing array</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql import Row</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([Row(c1=[&quot;b&quot;, &quot;a&quot;, &quot;c&quot;], c2=[&quot;c&quot;, &quot;d&quot;, &quot;a&quot;, &quot;f&quot;])])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_union(df.c1, df.c2)).collect()</span>
<span class="sd">    [Row(array_union(c1, c2)=[u&#39;b&#39;, u&#39;a&#39;, u&#39;c&#39;, u&#39;d&#39;, u&#39;f&#39;])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_union</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col1</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">col2</span><span class="p">)))</span></div>


<div class="viewcode-block" id="array_except"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_except">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_except</span><span class="p">(</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: returns an array of the elements in col1 but not in col2,</span>
<span class="sd">    without duplicates.</span>

<span class="sd">    :param col1: name of column containing array</span>
<span class="sd">    :param col2: name of column containing array</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql import Row</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([Row(c1=[&quot;b&quot;, &quot;a&quot;, &quot;c&quot;], c2=[&quot;c&quot;, &quot;d&quot;, &quot;a&quot;, &quot;f&quot;])])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_except(df.c1, df.c2)).collect()</span>
<span class="sd">    [Row(array_except(c1, c2)=[u&#39;b&#39;])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_except</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col1</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">col2</span><span class="p">)))</span></div>


<div class="viewcode-block" id="explode"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.explode">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">explode</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a new row for each element in the given array or map.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql import Row</span>
<span class="sd">    &gt;&gt;&gt; eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={&quot;a&quot;: &quot;b&quot;})])</span>
<span class="sd">    &gt;&gt;&gt; eDF.select(explode(eDF.intlist).alias(&quot;anInt&quot;)).collect()</span>
<span class="sd">    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]</span>

<span class="sd">    &gt;&gt;&gt; eDF.select(explode(eDF.mapfield).alias(&quot;key&quot;, &quot;value&quot;)).show()</span>
<span class="sd">    +---+-----+</span>
<span class="sd">    |key|value|</span>
<span class="sd">    +---+-----+</span>
<span class="sd">    |  a|    b|</span>
<span class="sd">    +---+-----+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">explode</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="posexplode"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.posexplode">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">posexplode</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a new row for each element with position in the given array or map.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql import Row</span>
<span class="sd">    &gt;&gt;&gt; eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={&quot;a&quot;: &quot;b&quot;})])</span>
<span class="sd">    &gt;&gt;&gt; eDF.select(posexplode(eDF.intlist)).collect()</span>
<span class="sd">    [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]</span>

<span class="sd">    &gt;&gt;&gt; eDF.select(posexplode(eDF.mapfield)).show()</span>
<span class="sd">    +---+---+-----+</span>
<span class="sd">    |pos|key|value|</span>
<span class="sd">    +---+---+-----+</span>
<span class="sd">    |  0|  a|    b|</span>
<span class="sd">    +---+---+-----+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">posexplode</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="explode_outer"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.explode_outer">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">explode_outer</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a new row for each element in the given array or map.</span>
<span class="sd">    Unlike explode, if the array/map is null or empty then null is produced.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">    ...     [(1, [&quot;foo&quot;, &quot;bar&quot;], {&quot;x&quot;: 1.0}), (2, [], {}), (3, None, None)],</span>
<span class="sd">    ...     (&quot;id&quot;, &quot;an_array&quot;, &quot;a_map&quot;)</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; df.select(&quot;id&quot;, &quot;an_array&quot;, explode_outer(&quot;a_map&quot;)).show()</span>
<span class="sd">    +---+----------+----+-----+</span>
<span class="sd">    | id|  an_array| key|value|</span>
<span class="sd">    +---+----------+----+-----+</span>
<span class="sd">    |  1|[foo, bar]|   x|  1.0|</span>
<span class="sd">    |  2|        []|null| null|</span>
<span class="sd">    |  3|      null|null| null|</span>
<span class="sd">    +---+----------+----+-----+</span>

<span class="sd">    &gt;&gt;&gt; df.select(&quot;id&quot;, &quot;a_map&quot;, explode_outer(&quot;an_array&quot;)).show()</span>
<span class="sd">    +---+----------+----+</span>
<span class="sd">    | id|     a_map| col|</span>
<span class="sd">    +---+----------+----+</span>
<span class="sd">    |  1|[x -&gt; 1.0]| foo|</span>
<span class="sd">    |  1|[x -&gt; 1.0]| bar|</span>
<span class="sd">    |  2|        []|null|</span>
<span class="sd">    |  3|      null|null|</span>
<span class="sd">    +---+----------+----+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">explode_outer</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="posexplode_outer"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.posexplode_outer">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">posexplode_outer</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a new row for each element with position in the given array or map.</span>
<span class="sd">    Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">    ...     [(1, [&quot;foo&quot;, &quot;bar&quot;], {&quot;x&quot;: 1.0}), (2, [], {}), (3, None, None)],</span>
<span class="sd">    ...     (&quot;id&quot;, &quot;an_array&quot;, &quot;a_map&quot;)</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; df.select(&quot;id&quot;, &quot;an_array&quot;, posexplode_outer(&quot;a_map&quot;)).show()</span>
<span class="sd">    +---+----------+----+----+-----+</span>
<span class="sd">    | id|  an_array| pos| key|value|</span>
<span class="sd">    +---+----------+----+----+-----+</span>
<span class="sd">    |  1|[foo, bar]|   0|   x|  1.0|</span>
<span class="sd">    |  2|        []|null|null| null|</span>
<span class="sd">    |  3|      null|null|null| null|</span>
<span class="sd">    +---+----------+----+----+-----+</span>
<span class="sd">    &gt;&gt;&gt; df.select(&quot;id&quot;, &quot;a_map&quot;, posexplode_outer(&quot;an_array&quot;)).show()</span>
<span class="sd">    +---+----------+----+----+</span>
<span class="sd">    | id|     a_map| pos| col|</span>
<span class="sd">    +---+----------+----+----+</span>
<span class="sd">    |  1|[x -&gt; 1.0]|   0| foo|</span>
<span class="sd">    |  1|[x -&gt; 1.0]|   1| bar|</span>
<span class="sd">    |  2|        []|null|null|</span>
<span class="sd">    |  3|      null|null|null|</span>
<span class="sd">    +---+----------+----+----+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">posexplode_outer</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_json_object"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.get_json_object">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_json_object</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts json object from a json string based on json path specified, and returns json string</span>
<span class="sd">    of the extracted json object. It will return null if the input json string is invalid.</span>

<span class="sd">    :param col: string column in json format</span>
<span class="sd">    :param path: path to the json object to extract</span>

<span class="sd">    &gt;&gt;&gt; data = [(&quot;1&quot;, &#39;&#39;&#39;{&quot;f1&quot;: &quot;value1&quot;, &quot;f2&quot;: &quot;value2&quot;}&#39;&#39;&#39;), (&quot;2&quot;, &#39;&#39;&#39;{&quot;f1&quot;: &quot;value12&quot;}&#39;&#39;&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(data, (&quot;key&quot;, &quot;jstring&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(df.key, get_json_object(df.jstring, &#39;$.f1&#39;).alias(&quot;c0&quot;), \\</span>
<span class="sd">    ...                   get_json_object(df.jstring, &#39;$.f2&#39;).alias(&quot;c1&quot;) ).collect()</span>
<span class="sd">    [Row(key=u&#39;1&#39;, c0=u&#39;value1&#39;, c1=u&#39;value2&#39;), Row(key=u&#39;2&#39;, c0=u&#39;value12&#39;, c1=None)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">get_json_object</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="json_tuple"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.json_tuple">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">1.6</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">json_tuple</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="o">*</span><span class="n">fields</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a new row for a json column according to the given field names.</span>

<span class="sd">    :param col: string column in json format</span>
<span class="sd">    :param fields: list of fields to extract</span>

<span class="sd">    &gt;&gt;&gt; data = [(&quot;1&quot;, &#39;&#39;&#39;{&quot;f1&quot;: &quot;value1&quot;, &quot;f2&quot;: &quot;value2&quot;}&#39;&#39;&#39;), (&quot;2&quot;, &#39;&#39;&#39;{&quot;f1&quot;: &quot;value12&quot;}&#39;&#39;&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(data, (&quot;key&quot;, &quot;jstring&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(df.key, json_tuple(df.jstring, &#39;f1&#39;, &#39;f2&#39;)).collect()</span>
<span class="sd">    [Row(key=u&#39;1&#39;, c0=u&#39;value1&#39;, c1=u&#39;value2&#39;), Row(key=u&#39;2&#39;, c0=u&#39;value12&#39;, c1=None)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">json_tuple</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">fields</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="from_json"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.from_json">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">from_json</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{}):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`</span>
<span class="sd">    as keys type, :class:`StructType` or :class:`ArrayType` with</span>
<span class="sd">    the specified schema. Returns `null`, in the case of an unparseable string.</span>

<span class="sd">    :param col: string column in json format</span>
<span class="sd">    :param schema: a StructType or ArrayType of StructType to use when parsing the json column.</span>
<span class="sd">    :param options: options to control parsing. accepts the same options as the json datasource</span>

<span class="sd">    .. note:: Since Spark 2.3, the DDL-formatted string or a JSON format string is also</span>
<span class="sd">              supported for ``schema``.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql.types import *</span>
<span class="sd">    &gt;&gt;&gt; data = [(1, &#39;&#39;&#39;{&quot;a&quot;: 1}&#39;&#39;&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; schema = StructType([StructField(&quot;a&quot;, IntegerType())])</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(data, (&quot;key&quot;, &quot;value&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(from_json(df.value, schema).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=Row(a=1))]</span>
<span class="sd">    &gt;&gt;&gt; df.select(from_json(df.value, &quot;a INT&quot;).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=Row(a=1))]</span>
<span class="sd">    &gt;&gt;&gt; df.select(from_json(df.value, &quot;MAP&lt;STRING,INT&gt;&quot;).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json={u&#39;a&#39;: 1})]</span>
<span class="sd">    &gt;&gt;&gt; data = [(1, &#39;&#39;&#39;[{&quot;a&quot;: 1}]&#39;&#39;&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; schema = ArrayType(StructType([StructField(&quot;a&quot;, IntegerType())]))</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(data, (&quot;key&quot;, &quot;value&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(from_json(df.value, schema).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=[Row(a=1)])]</span>
<span class="sd">    &gt;&gt;&gt; schema = schema_of_json(lit(&#39;&#39;&#39;{&quot;a&quot;: 0}&#39;&#39;&#39;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(from_json(df.value, schema).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=Row(a=1))]</span>
<span class="sd">    &gt;&gt;&gt; data = [(1, &#39;&#39;&#39;[1, 2, 3]&#39;&#39;&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; schema = ArrayType(IntegerType())</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(data, (&quot;key&quot;, &quot;value&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(from_json(df.value, schema).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=[1, 2, 3])]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">DataType</span><span class="p">):</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="n">schema</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">Column</span><span class="p">):</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">from_json</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">schema</span><span class="p">,</span> <span class="n">options</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="to_json"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.to_json">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">2.1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">to_json</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{}):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`</span>
<span class="sd">    into a JSON string. Throws an exception, in the case of an unsupported type.</span>

<span class="sd">    :param col: name of column containing a struct, an array or a map.</span>
<span class="sd">    :param options: options to control converting. accepts the same options as the JSON datasource</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql import Row</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.sql.types import *</span>
<span class="sd">    &gt;&gt;&gt; data = [(1, Row(name=&#39;Alice&#39;, age=2))]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(data, (&quot;key&quot;, &quot;value&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_json(df.value).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=u&#39;{&quot;age&quot;:2,&quot;name&quot;:&quot;Alice&quot;}&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; data = [(1, [Row(name=&#39;Alice&#39;, age=2), Row(name=&#39;Bob&#39;, age=3)])]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(data, (&quot;key&quot;, &quot;value&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_json(df.value).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=u&#39;[{&quot;age&quot;:2,&quot;name&quot;:&quot;Alice&quot;},{&quot;age&quot;:3,&quot;name&quot;:&quot;Bob&quot;}]&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; data = [(1, {&quot;name&quot;: &quot;Alice&quot;})]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(data, (&quot;key&quot;, &quot;value&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_json(df.value).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=u&#39;{&quot;name&quot;:&quot;Alice&quot;}&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; data = [(1, [{&quot;name&quot;: &quot;Alice&quot;}, {&quot;name&quot;: &quot;Bob&quot;}])]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(data, (&quot;key&quot;, &quot;value&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_json(df.value).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=u&#39;[{&quot;name&quot;:&quot;Alice&quot;},{&quot;name&quot;:&quot;Bob&quot;}]&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; data = [(1, [&quot;Alice&quot;, &quot;Bob&quot;])]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame(data, (&quot;key&quot;, &quot;value&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(to_json(df.value).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=u&#39;[&quot;Alice&quot;,&quot;Bob&quot;]&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">to_json</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">options</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="schema_of_json"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.schema_of_json">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">schema_of_json</span><span class="p">(</span><span class="n">json</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parses a JSON string and infers its schema in DDL format.</span>

<span class="sd">    :param json: a JSON string or a string literal containing a JSON string.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.range(1)</span>
<span class="sd">    &gt;&gt;&gt; df.select(schema_of_json(&#39;{&quot;a&quot;: 0}&#39;).alias(&quot;json&quot;)).collect()</span>
<span class="sd">    [Row(json=u&#39;struct&lt;a:bigint&gt;&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">basestring</span><span class="p">):</span>
        <span class="n">col</span> <span class="o">=</span> <span class="n">_create_column_from_literal</span><span class="p">(</span><span class="n">json</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">Column</span><span class="p">):</span>
        <span class="n">col</span> <span class="o">=</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">json</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;schema argument should be a column or string&quot;</span><span class="p">)</span>

    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">schema_of_json</span><span class="p">(</span><span class="n">col</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="size"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.size">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: returns the length of the array or map stored in the column.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(size(df.data)).collect()</span>
<span class="sd">    [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="array_min"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_min">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_min</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: returns the minimum value of the array.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_min(df.data).alias(&#39;min&#39;)).collect()</span>
<span class="sd">    [Row(min=1), Row(min=-1)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_min</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="array_max"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_max">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_max</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: returns the maximum value of the array.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_max(df.data).alias(&#39;max&#39;)).collect()</span>
<span class="sd">    [Row(max=3), Row(max=10)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_max</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="sort_array"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.sort_array">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sort_array</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">asc</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: sorts the input array in ascending or descending order according</span>
<span class="sd">    to the natural ordering of the array elements. Null elements will be placed at the beginning</span>
<span class="sd">    of the returned array in ascending order or at the end of the returned array in descending</span>
<span class="sd">    order.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(sort_array(df.data).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]</span>
<span class="sd">    &gt;&gt;&gt; df.select(sort_array(df.data, asc=False).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">sort_array</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">asc</span><span class="p">))</span></div>


<div class="viewcode-block" id="array_sort"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_sort">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_sort</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: sorts the input array in ascending order. The elements of the input array</span>
<span class="sd">    must be orderable. Null elements will be placed at the end of the returned array.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_sort(df.data).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_sort</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="shuffle"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.shuffle">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: Generates a random permutation of the given array.</span>

<span class="sd">    .. note:: The function is non-deterministic.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(shuffle(df.data).alias(&#39;s&#39;)).collect()  # doctest: +SKIP</span>
<span class="sd">    [Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="reverse"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.reverse">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="nd">@ignore_unicode_prefix</span>
<span class="k">def</span> <span class="nf">reverse</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: returns a reversed string or an array with reverse order of elements.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;Spark SQL&#39;,)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(reverse(df.data).alias(&#39;s&#39;)).collect()</span>
<span class="sd">    [Row(s=u&#39;LQS krapS&#39;)]</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(reverse(df.data).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]</span>
<span class="sd">     &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="flatten"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.flatten">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: creates a single array from an array of arrays.</span>
<span class="sd">    If a structure of nested arrays is deeper than two levels,</span>
<span class="sd">    only one level of nesting is removed.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(flatten(df.data).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=[1, 2, 3, 4, 5, 6]), Row(r=None)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="map_keys"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.map_keys">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">map_keys</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: Returns an unordered array containing the keys of the map.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql.functions import map_keys</span>
<span class="sd">    &gt;&gt;&gt; df = spark.sql(&quot;SELECT map(1, &#39;a&#39;, 2, &#39;b&#39;) as data&quot;)</span>
<span class="sd">    &gt;&gt;&gt; df.select(map_keys(&quot;data&quot;).alias(&quot;keys&quot;)).show()</span>
<span class="sd">    +------+</span>
<span class="sd">    |  keys|</span>
<span class="sd">    +------+</span>
<span class="sd">    |[1, 2]|</span>
<span class="sd">    +------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">map_keys</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="map_values"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.map_values">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">map_values</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: Returns an unordered array containing the values of the map.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql.functions import map_values</span>
<span class="sd">    &gt;&gt;&gt; df = spark.sql(&quot;SELECT map(1, &#39;a&#39;, 2, &#39;b&#39;) as data&quot;)</span>
<span class="sd">    &gt;&gt;&gt; df.select(map_values(&quot;data&quot;).alias(&quot;values&quot;)).show()</span>
<span class="sd">    +------+</span>
<span class="sd">    |values|</span>
<span class="sd">    +------+</span>
<span class="sd">    |[a, b]|</span>
<span class="sd">    +------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">map_values</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="map_from_entries"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.map_from_entries">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">map_from_entries</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: Returns a map created from the given array of entries.</span>

<span class="sd">    :param col: name of column or expression</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql.functions import map_from_entries</span>
<span class="sd">    &gt;&gt;&gt; df = spark.sql(&quot;SELECT array(struct(1, &#39;a&#39;), struct(2, &#39;b&#39;)) as data&quot;)</span>
<span class="sd">    &gt;&gt;&gt; df.select(map_from_entries(&quot;data&quot;).alias(&quot;map&quot;)).show()</span>
<span class="sd">    +----------------+</span>
<span class="sd">    |             map|</span>
<span class="sd">    +----------------+</span>
<span class="sd">    |[1 -&gt; a, 2 -&gt; b]|</span>
<span class="sd">    +----------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">map_from_entries</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span></div>


<div class="viewcode-block" id="array_repeat"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.array_repeat">[docs]</a><span class="nd">@ignore_unicode_prefix</span>
<span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">array_repeat</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">count</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: creates an array containing a column repeated count times.</span>

<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;ab&#39;,)], [&#39;data&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(array_repeat(df.data, 3).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=[u&#39;ab&#39;, u&#39;ab&#39;, u&#39;ab&#39;])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">array_repeat</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">col</span><span class="p">),</span> <span class="n">count</span><span class="p">))</span></div>


<div class="viewcode-block" id="arrays_zip"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.arrays_zip">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">arrays_zip</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Collection function: Returns a merged array of structs in which the N-th struct contains all</span>
<span class="sd">    N-th values of input arrays.</span>

<span class="sd">    :param cols: columns of arrays to be merged.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql.functions import arrays_zip</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], [&#39;vals1&#39;, &#39;vals2&#39;])</span>
<span class="sd">    &gt;&gt;&gt; df.select(arrays_zip(df.vals1, df.vals2).alias(&#39;zipped&#39;)).collect()</span>
<span class="sd">    [Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">arrays_zip</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">)))</span></div>


<div class="viewcode-block" id="map_concat"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.map_concat">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">map_concat</span><span class="p">(</span><span class="o">*</span><span class="n">cols</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the union of all the given maps.</span>

<span class="sd">    :param cols: list of column names (string) or list of :class:`Column` expressions</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql.functions import map_concat</span>
<span class="sd">    &gt;&gt;&gt; df = spark.sql(&quot;SELECT map(1, &#39;a&#39;, 2, &#39;b&#39;) as map1, map(3, &#39;c&#39;, 1, &#39;d&#39;) as map2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; df.select(map_concat(&quot;map1&quot;, &quot;map2&quot;).alias(&quot;map3&quot;)).show(truncate=False)</span>
<span class="sd">    +--------------------------------+</span>
<span class="sd">    |map3                            |</span>
<span class="sd">    +--------------------------------+</span>
<span class="sd">    |[1 -&gt; a, 2 -&gt; b, 3 -&gt; c, 1 -&gt; d]|</span>
<span class="sd">    +--------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">set</span><span class="p">)):</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">jc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">map_concat</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">_to_java_column</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">jc</span><span class="p">)</span></div>


<div class="viewcode-block" id="sequence"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.sequence">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.4</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sequence</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a sequence of integers from `start` to `stop`, incrementing by `step`.</span>
<span class="sd">    If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,</span>
<span class="sd">    otherwise -1.</span>

<span class="sd">    &gt;&gt;&gt; df1 = spark.createDataFrame([(-2, 2)], (&#39;C1&#39;, &#39;C2&#39;))</span>
<span class="sd">    &gt;&gt;&gt; df1.select(sequence(&#39;C1&#39;, &#39;C2&#39;).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=[-2, -1, 0, 1, 2])]</span>
<span class="sd">    &gt;&gt;&gt; df2 = spark.createDataFrame([(4, -4, -2)], (&#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;))</span>
<span class="sd">    &gt;&gt;&gt; df2.select(sequence(&#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;).alias(&#39;r&#39;)).collect()</span>
<span class="sd">    [Row(r=[4, 2, 0, -2, -4])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">_active_spark_context</span>
    <span class="k">if</span> <span class="n">step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">sequence</span><span class="p">(</span><span class="n">_to_java_column</span><span class="p">(</span><span class="n">start</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">stop</span><span class="p">)))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">sequence</span><span class="p">(</span>
            <span class="n">_to_java_column</span><span class="p">(</span><span class="n">start</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">stop</span><span class="p">),</span> <span class="n">_to_java_column</span><span class="p">(</span><span class="n">step</span><span class="p">)))</span></div>


<span class="c1"># ---------------------------- User Defined Function ----------------------------------</span>

<div class="viewcode-block" id="PandasUDFType"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.PandasUDFType">[docs]</a><span class="k">class</span> <span class="nc">PandasUDFType</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pandas UDF Types. See :meth:`pyspark.sql.functions.pandas_udf`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">SCALAR</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span>

    <span class="n">GROUPED_MAP</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span>

    <span class="n">GROUPED_AGG</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_AGG_PANDAS_UDF</span></div>


<div class="viewcode-block" id="udf"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.udf">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">1.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">udf</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">StringType</span><span class="p">()):</span>
    <span class="sd">&quot;&quot;&quot;Creates a user defined function (UDF).</span>

<span class="sd">    .. note:: The user-defined functions are considered deterministic by default. Due to</span>
<span class="sd">        optimization, duplicate invocations may be eliminated or the function may even be invoked</span>
<span class="sd">        more times than it is present in the query. If your function is not deterministic, call</span>
<span class="sd">        `asNondeterministic` on the user defined function. E.g.:</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql.types import IntegerType</span>
<span class="sd">    &gt;&gt;&gt; import random</span>
<span class="sd">    &gt;&gt;&gt; random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()</span>

<span class="sd">    .. note:: The user-defined functions do not support conditional expressions or short circuiting</span>
<span class="sd">        in boolean expressions and it ends up with being executed all internally. If the functions</span>
<span class="sd">        can fail on special rows, the workaround is to incorporate the condition into the functions.</span>

<span class="sd">    .. note:: The user-defined functions do not take keyword arguments on the calling side.</span>

<span class="sd">    :param f: python function if used as a standalone function</span>
<span class="sd">    :param returnType: the return type of the user-defined function. The value can be either a</span>
<span class="sd">        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql.types import IntegerType</span>
<span class="sd">    &gt;&gt;&gt; slen = udf(lambda s: len(s), IntegerType())</span>
<span class="sd">    &gt;&gt;&gt; @udf</span>
<span class="sd">    ... def to_upper(s):</span>
<span class="sd">    ...     if s is not None:</span>
<span class="sd">    ...         return s.upper()</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; @udf(returnType=IntegerType())</span>
<span class="sd">    ... def add_one(x):</span>
<span class="sd">    ...     if x is not None:</span>
<span class="sd">    ...         return x + 1</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; df = spark.createDataFrame([(1, &quot;John Doe&quot;, 21)], (&quot;id&quot;, &quot;name&quot;, &quot;age&quot;))</span>
<span class="sd">    &gt;&gt;&gt; df.select(slen(&quot;name&quot;).alias(&quot;slen(name)&quot;), to_upper(&quot;name&quot;), add_one(&quot;age&quot;)).show()</span>
<span class="sd">    +----------+--------------+------------+</span>
<span class="sd">    |slen(name)|to_upper(name)|add_one(age)|</span>
<span class="sd">    +----------+--------------+------------+</span>
<span class="sd">    |         8|      JOHN DOE|          22|</span>
<span class="sd">    +----------+--------------+------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># decorator @udf, @udf(), @udf(dataType())</span>
    <span class="k">if</span> <span class="n">f</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">DataType</span><span class="p">)):</span>
        <span class="c1"># If DataType has been passed as a positional argument</span>
        <span class="c1"># for decorator use it as a returnType</span>
        <span class="n">return_type</span> <span class="o">=</span> <span class="n">f</span> <span class="ow">or</span> <span class="n">returnType</span>
        <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_create_udf</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">return_type</span><span class="p">,</span>
                                 <span class="n">evalType</span><span class="o">=</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_BATCHED_UDF</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_create_udf</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">returnType</span><span class="p">,</span>
                           <span class="n">evalType</span><span class="o">=</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_BATCHED_UDF</span><span class="p">)</span></div>


<div class="viewcode-block" id="pandas_udf"><a class="viewcode-back" href="../../../pyspark.sql.html#pyspark.sql.functions.pandas_udf">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">pandas_udf</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">functionType</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a vectorized user defined function (UDF).</span>

<span class="sd">    :param f: user-defined function. A python function if used as a standalone function</span>
<span class="sd">    :param returnType: the return type of the user-defined function. The value can be either a</span>
<span class="sd">        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>
<span class="sd">    :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.</span>
<span class="sd">                         Default: SCALAR.</span>

<span class="sd">    .. note:: Experimental</span>

<span class="sd">    The function type of the UDF can be one of the following:</span>

<span class="sd">    1. SCALAR</span>

<span class="sd">       A scalar UDF defines a transformation: One or more `pandas.Series` -&gt; A `pandas.Series`.</span>
<span class="sd">       The length of the returned `pandas.Series` must be of the same as the input `pandas.Series`.</span>

<span class="sd">       :class:`MapType`, :class:`StructType` are currently not supported as output types.</span>

<span class="sd">       Scalar UDFs are used with :meth:`pyspark.sql.DataFrame.withColumn` and</span>
<span class="sd">       :meth:`pyspark.sql.DataFrame.select`.</span>

<span class="sd">       &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf, PandasUDFType</span>
<span class="sd">       &gt;&gt;&gt; from pyspark.sql.types import IntegerType, StringType</span>
<span class="sd">       &gt;&gt;&gt; slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP</span>
<span class="sd">       &gt;&gt;&gt; @pandas_udf(StringType())  # doctest: +SKIP</span>
<span class="sd">       ... def to_upper(s):</span>
<span class="sd">       ...     return s.str.upper()</span>
<span class="sd">       ...</span>
<span class="sd">       &gt;&gt;&gt; @pandas_udf(&quot;integer&quot;, PandasUDFType.SCALAR)  # doctest: +SKIP</span>
<span class="sd">       ... def add_one(x):</span>
<span class="sd">       ...     return x + 1</span>
<span class="sd">       ...</span>
<span class="sd">       &gt;&gt;&gt; df = spark.createDataFrame([(1, &quot;John Doe&quot;, 21)],</span>
<span class="sd">       ...                            (&quot;id&quot;, &quot;name&quot;, &quot;age&quot;))  # doctest: +SKIP</span>
<span class="sd">       &gt;&gt;&gt; df.select(slen(&quot;name&quot;).alias(&quot;slen(name)&quot;), to_upper(&quot;name&quot;), add_one(&quot;age&quot;)) \\</span>
<span class="sd">       ...     .show()  # doctest: +SKIP</span>
<span class="sd">       +----------+--------------+------------+</span>
<span class="sd">       |slen(name)|to_upper(name)|add_one(age)|</span>
<span class="sd">       +----------+--------------+------------+</span>
<span class="sd">       |         8|      JOHN DOE|          22|</span>
<span class="sd">       +----------+--------------+------------+</span>

<span class="sd">       .. note:: The length of `pandas.Series` within a scalar UDF is not that of the whole input</span>
<span class="sd">           column, but is the length of an internal batch used for each call to the function.</span>
<span class="sd">           Therefore, this can be used, for example, to ensure the length of each returned</span>
<span class="sd">           `pandas.Series`, and can not be used as the column length.</span>

<span class="sd">    2. GROUPED_MAP</span>

<span class="sd">       A grouped map UDF defines transformation: A `pandas.DataFrame` -&gt; A `pandas.DataFrame`</span>
<span class="sd">       The returnType should be a :class:`StructType` describing the schema of the returned</span>
<span class="sd">       `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match</span>
<span class="sd">       the field names in the defined returnType schema if specified as strings, or match the</span>
<span class="sd">       field data types by position if not strings, e.g. integer indices.</span>
<span class="sd">       The length of the returned `pandas.DataFrame` can be arbitrary.</span>

<span class="sd">       Grouped map UDFs are used with :meth:`pyspark.sql.GroupedData.apply`.</span>

<span class="sd">       &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf, PandasUDFType</span>
<span class="sd">       &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">       ...     (&quot;id&quot;, &quot;v&quot;))  # doctest: +SKIP</span>
<span class="sd">       &gt;&gt;&gt; @pandas_udf(&quot;id long, v double&quot;, PandasUDFType.GROUPED_MAP)  # doctest: +SKIP</span>
<span class="sd">       ... def normalize(pdf):</span>
<span class="sd">       ...     v = pdf.v</span>
<span class="sd">       ...     return pdf.assign(v=(v - v.mean()) / v.std())</span>
<span class="sd">       &gt;&gt;&gt; df.groupby(&quot;id&quot;).apply(normalize).show()  # doctest: +SKIP</span>
<span class="sd">       +---+-------------------+</span>
<span class="sd">       | id|                  v|</span>
<span class="sd">       +---+-------------------+</span>
<span class="sd">       |  1|-0.7071067811865475|</span>
<span class="sd">       |  1| 0.7071067811865475|</span>
<span class="sd">       |  2|-0.8320502943378437|</span>
<span class="sd">       |  2|-0.2773500981126146|</span>
<span class="sd">       |  2| 1.1094003924504583|</span>
<span class="sd">       +---+-------------------+</span>

<span class="sd">       Alternatively, the user can define a function that takes two arguments.</span>
<span class="sd">       In this case, the grouping key(s) will be passed as the first argument and the data will</span>
<span class="sd">       be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy</span>
<span class="sd">       data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in</span>
<span class="sd">       as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.</span>
<span class="sd">       This is useful when the user does not want to hardcode grouping key(s) in the function.</span>

<span class="sd">       &gt;&gt;&gt; import pandas as pd  # doctest: +SKIP</span>
<span class="sd">       &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf, PandasUDFType</span>
<span class="sd">       &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">       ...     (&quot;id&quot;, &quot;v&quot;))  # doctest: +SKIP</span>
<span class="sd">       &gt;&gt;&gt; @pandas_udf(&quot;id long, v double&quot;, PandasUDFType.GROUPED_MAP)  # doctest: +SKIP</span>
<span class="sd">       ... def mean_udf(key, pdf):</span>
<span class="sd">       ...     # key is a tuple of one numpy.int64, which is the value</span>
<span class="sd">       ...     # of &#39;id&#39; for the current group</span>
<span class="sd">       ...     return pd.DataFrame([key + (pdf.v.mean(),)])</span>
<span class="sd">       &gt;&gt;&gt; df.groupby(&#39;id&#39;).apply(mean_udf).show()  # doctest: +SKIP</span>
<span class="sd">       +---+---+</span>
<span class="sd">       | id|  v|</span>
<span class="sd">       +---+---+</span>
<span class="sd">       |  1|1.5|</span>
<span class="sd">       |  2|6.0|</span>
<span class="sd">       +---+---+</span>
<span class="sd">       &gt;&gt;&gt; @pandas_udf(</span>
<span class="sd">       ...    &quot;id long, `ceil(v / 2)` long, v double&quot;,</span>
<span class="sd">       ...    PandasUDFType.GROUPED_MAP)  # doctest: +SKIP</span>
<span class="sd">       &gt;&gt;&gt; def sum_udf(key, pdf):</span>
<span class="sd">       ...     # key is a tuple of two numpy.int64s, which is the values</span>
<span class="sd">       ...     # of &#39;id&#39; and &#39;ceil(df.v / 2)&#39; for the current group</span>
<span class="sd">       ...     return pd.DataFrame([key + (pdf.v.sum(),)])</span>
<span class="sd">       &gt;&gt;&gt; df.groupby(df.id, ceil(df.v / 2)).apply(sum_udf).show()  # doctest: +SKIP</span>
<span class="sd">       +---+-----------+----+</span>
<span class="sd">       | id|ceil(v / 2)|   v|</span>
<span class="sd">       +---+-----------+----+</span>
<span class="sd">       |  2|          5|10.0|</span>
<span class="sd">       |  1|          1| 3.0|</span>
<span class="sd">       |  2|          3| 5.0|</span>
<span class="sd">       |  2|          2| 3.0|</span>
<span class="sd">       +---+-----------+----+</span>

<span class="sd">       .. note:: If returning a new `pandas.DataFrame` constructed with a dictionary, it is</span>
<span class="sd">           recommended to explicitly index the columns by name to ensure the positions are correct,</span>
<span class="sd">           or alternatively use an `OrderedDict`.</span>
<span class="sd">           For example, `pd.DataFrame({&#39;id&#39;: ids, &#39;a&#39;: data}, columns=[&#39;id&#39;, &#39;a&#39;])` or</span>
<span class="sd">           `pd.DataFrame(OrderedDict([(&#39;id&#39;, ids), (&#39;a&#39;, data)]))`.</span>

<span class="sd">       .. seealso:: :meth:`pyspark.sql.GroupedData.apply`</span>

<span class="sd">    3. GROUPED_AGG</span>

<span class="sd">       A grouped aggregate UDF defines a transformation: One or more `pandas.Series` -&gt; A scalar</span>
<span class="sd">       The `returnType` should be a primitive data type, e.g., :class:`DoubleType`.</span>
<span class="sd">       The returned scalar can be either a python primitive type, e.g., `int` or `float`</span>
<span class="sd">       or a numpy data type, e.g., `numpy.int64` or `numpy.float64`.</span>

<span class="sd">       :class:`MapType` and :class:`StructType` are currently not supported as output types.</span>

<span class="sd">       Group aggregate UDFs are used with :meth:`pyspark.sql.GroupedData.agg` and</span>
<span class="sd">       :class:`pyspark.sql.Window`</span>

<span class="sd">       This example shows using grouped aggregated UDFs with groupby:</span>

<span class="sd">       &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf, PandasUDFType</span>
<span class="sd">       &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">       ...     (&quot;id&quot;, &quot;v&quot;))</span>
<span class="sd">       &gt;&gt;&gt; @pandas_udf(&quot;double&quot;, PandasUDFType.GROUPED_AGG)  # doctest: +SKIP</span>
<span class="sd">       ... def mean_udf(v):</span>
<span class="sd">       ...     return v.mean()</span>
<span class="sd">       &gt;&gt;&gt; df.groupby(&quot;id&quot;).agg(mean_udf(df[&#39;v&#39;])).show()  # doctest: +SKIP</span>
<span class="sd">       +---+-----------+</span>
<span class="sd">       | id|mean_udf(v)|</span>
<span class="sd">       +---+-----------+</span>
<span class="sd">       |  1|        1.5|</span>
<span class="sd">       |  2|        6.0|</span>
<span class="sd">       +---+-----------+</span>

<span class="sd">       This example shows using grouped aggregated UDFs as window functions. Note that only</span>
<span class="sd">       unbounded window frame is supported at the moment:</span>

<span class="sd">       &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf, PandasUDFType</span>
<span class="sd">       &gt;&gt;&gt; from pyspark.sql import Window</span>
<span class="sd">       &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">       ...     (&quot;id&quot;, &quot;v&quot;))</span>
<span class="sd">       &gt;&gt;&gt; @pandas_udf(&quot;double&quot;, PandasUDFType.GROUPED_AGG)  # doctest: +SKIP</span>
<span class="sd">       ... def mean_udf(v):</span>
<span class="sd">       ...     return v.mean()</span>
<span class="sd">       &gt;&gt;&gt; w = Window \\</span>
<span class="sd">       ...     .partitionBy(&#39;id&#39;) \\</span>
<span class="sd">       ...     .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)</span>
<span class="sd">       &gt;&gt;&gt; df.withColumn(&#39;mean_v&#39;, mean_udf(df[&#39;v&#39;]).over(w)).show()  # doctest: +SKIP</span>
<span class="sd">       +---+----+------+</span>
<span class="sd">       | id|   v|mean_v|</span>
<span class="sd">       +---+----+------+</span>
<span class="sd">       |  1| 1.0|   1.5|</span>
<span class="sd">       |  1| 2.0|   1.5|</span>
<span class="sd">       |  2| 3.0|   6.0|</span>
<span class="sd">       |  2| 5.0|   6.0|</span>
<span class="sd">       |  2|10.0|   6.0|</span>
<span class="sd">       +---+----+------+</span>

<span class="sd">       .. seealso:: :meth:`pyspark.sql.GroupedData.agg` and :class:`pyspark.sql.Window`</span>

<span class="sd">    .. note:: The user-defined functions are considered deterministic by default. Due to</span>
<span class="sd">        optimization, duplicate invocations may be eliminated or the function may even be invoked</span>
<span class="sd">        more times than it is present in the query. If your function is not deterministic, call</span>
<span class="sd">        `asNondeterministic` on the user defined function. E.g.:</span>

<span class="sd">    &gt;&gt;&gt; @pandas_udf(&#39;double&#39;, PandasUDFType.SCALAR)  # doctest: +SKIP</span>
<span class="sd">    ... def random(v):</span>
<span class="sd">    ...     import numpy as np</span>
<span class="sd">    ...     import pandas as pd</span>
<span class="sd">    ...     return pd.Series(np.random.randn(len(v))</span>
<span class="sd">    &gt;&gt;&gt; random = random.asNondeterministic()  # doctest: +SKIP</span>

<span class="sd">    .. note:: The user-defined functions do not support conditional expressions or short circuiting</span>
<span class="sd">        in boolean expressions and it ends up with being executed all internally. If the functions</span>
<span class="sd">        can fail on special rows, the workaround is to incorporate the condition into the functions.</span>

<span class="sd">    .. note:: The user-defined functions do not take keyword arguments on the calling side.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># decorator @pandas_udf(returnType, functionType)</span>
    <span class="n">is_decorator</span> <span class="o">=</span> <span class="n">f</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">DataType</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">is_decorator</span><span class="p">:</span>
        <span class="c1"># If DataType has been passed as a positional argument</span>
        <span class="c1"># for decorator use it as a returnType</span>
        <span class="n">return_type</span> <span class="o">=</span> <span class="n">f</span> <span class="ow">or</span> <span class="n">returnType</span>

        <span class="k">if</span> <span class="n">functionType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># @pandas_udf(dataType, functionType=functionType)</span>
            <span class="c1"># @pandas_udf(returnType=dataType, functionType=functionType)</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">functionType</span>
        <span class="k">elif</span> <span class="n">returnType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">returnType</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="c1"># @pandas_udf(dataType, functionType)</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">returnType</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># @pandas_udf(dataType) or @pandas_udf(returnType=dataType)</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">return_type</span> <span class="o">=</span> <span class="n">returnType</span>

        <span class="k">if</span> <span class="n">functionType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">functionType</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span>

    <span class="k">if</span> <span class="n">return_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid returnType: returnType can not be None&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">eval_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span><span class="p">,</span>
                         <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span><span class="p">,</span>
                         <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_AGG_PANDAS_UDF</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid functionType: &quot;</span>
                         <span class="s2">&quot;functionType must be one the values from PandasUDFType&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_decorator</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_create_udf</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">return_type</span><span class="p">,</span> <span class="n">evalType</span><span class="o">=</span><span class="n">eval_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_create_udf</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">return_type</span><span class="p">,</span> <span class="n">evalType</span><span class="o">=</span><span class="n">eval_type</span><span class="p">)</span></div>


<span class="n">blacklist</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;map&#39;</span><span class="p">,</span> <span class="s1">&#39;since&#39;</span><span class="p">,</span> <span class="s1">&#39;ignore_unicode_prefix&#39;</span><span class="p">]</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
           <span class="k">if</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">k</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">islower</span><span class="p">()</span> <span class="ow">and</span> <span class="n">callable</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="ow">and</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">blacklist</span><span class="p">]</span>
<span class="n">__all__</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;PandasUDFType&quot;</span><span class="p">]</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_test</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span><span class="p">,</span> <span class="n">SparkSession</span>
    <span class="kn">import</span> <span class="nn">pyspark.sql.functions</span>
    <span class="n">globs</span> <span class="o">=</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span>\
        <span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[4]&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;sql.functions tests&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;sc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sc</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;spark&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spark</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;df&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Bob&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">5</span><span class="p">)])</span>
    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span>
        <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">functions</span><span class="p">,</span> <span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
        <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">NORMALIZE_WHITESPACE</span><span class="p">)</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/spark-logo-hd.png" alt="Logo"/>
            </a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
    
        <li class="nav-item nav-item-0"><a href="../../../index.html">PySpark 2.4.1 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.4.
    </div>
  </body>
</html>