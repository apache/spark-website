<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (1.8.0_191) on Wed Mar 27 05:01:01 UTC 2019 -->
<title>DataFrameWriter (Spark 2.4.1 JavaDoc)</title>
<meta name="date" content="2019-03-27">
<link rel="stylesheet" type="text/css" href="../../../../stylesheet.css" title="Style">
<script type="text/javascript" src="../../../../script.js"></script>
</head>
<body>
<script type="text/javascript"><!--
    try {
        if (location.href.indexOf('is-external=true') == -1) {
            parent.document.title="DataFrameWriter (Spark 2.4.1 JavaDoc)";
        }
    }
    catch(err) {
    }
//-->
var methods = {"i0":10,"i1":10,"i2":10,"i3":10,"i4":10,"i5":10,"i6":10,"i7":10,"i8":10,"i9":10,"i10":10,"i11":10,"i12":10,"i13":10,"i14":10,"i15":10,"i16":10,"i17":10,"i18":10,"i19":10,"i20":10,"i21":10,"i22":10,"i23":10,"i24":10};
var tabs = {65535:["t0","All Methods"],2:["t2","Instance Methods"],8:["t4","Concrete Methods"]};
var altColor = "altColor";
var rowColor = "rowColor";
var tableTab = "tableTab";
var activeTableTab = "activeTableTab";
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar.top">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.top" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.top.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/sql/DataFrameStatFunctions.html" title="class in org.apache.spark.sql"><span class="typeNameLink">Prev&nbsp;Class</span></a></li>
<li><a href="../../../../org/apache/spark/sql/Dataset.html" title="class in org.apache.spark.sql"><span class="typeNameLink">Next&nbsp;Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/sql/DataFrameWriter.html" target="_top">Frames</a></li>
<li><a href="DataFrameWriter.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method.summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method.detail">Method</a></li>
</ul>
</div>
<a name="skip.navbar.top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.apache.spark.sql</div>
<h2 title="Class DataFrameWriter" class="title">Class DataFrameWriter&lt;T&gt;</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li>Object</li>
<li>
<ul class="inheritance">
<li>org.apache.spark.sql.DataFrameWriter&lt;T&gt;</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<hr>
<br>
<pre>public final class <span class="typeNameLabel">DataFrameWriter&lt;T&gt;</span>
extends Object</pre>
<div class="block">Interface used to write a <a href="../../../../org/apache/spark/sql/Dataset.html" title="class in org.apache.spark.sql"><code>Dataset</code></a> to external storage systems (e.g. file systems,
 key-value stores, etc). Use <code>Dataset.write</code> to access this.
 <p></div>
<dl>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method.summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="memberSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span id="t0" class="activeTableTab"><span>All Methods</span><span class="tabEnd">&nbsp;</span></span><span id="t2" class="tableTab"><span><a href="javascript:show(2);">Instance Methods</a></span><span class="tabEnd">&nbsp;</span></span><span id="t4" class="tableTab"><span><a href="javascript:show(8);">Concrete Methods</a></span><span class="tabEnd">&nbsp;</span></span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr id="i0" class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#bucketBy-int-java.lang.String-scala.collection.Seq-">bucketBy</a></span>(int&nbsp;numBuckets,
        String&nbsp;colName,
        scala.collection.Seq&lt;String&gt;&nbsp;colNames)</code>
<div class="block">Buckets the output by the given columns.</div>
</td>
</tr>
<tr id="i1" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#bucketBy-int-java.lang.String-java.lang.String...-">bucketBy</a></span>(int&nbsp;numBuckets,
        String&nbsp;colName,
        String...&nbsp;colNames)</code>
<div class="block">Buckets the output by the given columns.</div>
</td>
</tr>
<tr id="i2" class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#csv-java.lang.String-">csv</a></span>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> in CSV format at the specified path.</div>
</td>
</tr>
<tr id="i3" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#format-java.lang.String-">format</a></span>(String&nbsp;source)</code>
<div class="block">Specifies the underlying output data source.</div>
</td>
</tr>
<tr id="i4" class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#insertInto-java.lang.String-">insertInto</a></span>(String&nbsp;tableName)</code>
<div class="block">Inserts the content of the <code>DataFrame</code> to the specified table.</div>
</td>
</tr>
<tr id="i5" class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#jdbc-java.lang.String-java.lang.String-java.util.Properties-">jdbc</a></span>(String&nbsp;url,
    String&nbsp;table,
    java.util.Properties&nbsp;connectionProperties)</code>
<div class="block">Saves the content of the <code>DataFrame</code> to an external database table via JDBC.</div>
</td>
</tr>
<tr id="i6" class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#json-java.lang.String-">json</a></span>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> in JSON format (<a href="http://jsonlines.org/">
 JSON Lines text format or newline-delimited JSON</a>) at the specified path.</div>
</td>
</tr>
<tr id="i7" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#mode-org.apache.spark.sql.SaveMode-">mode</a></span>(<a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;saveMode)</code>
<div class="block">Specifies the behavior when data or table already exists.</div>
</td>
</tr>
<tr id="i8" class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#mode-java.lang.String-">mode</a></span>(String&nbsp;saveMode)</code>
<div class="block">Specifies the behavior when data or table already exists.</div>
</td>
</tr>
<tr id="i9" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#option-java.lang.String-boolean-">option</a></span>(String&nbsp;key,
      boolean&nbsp;value)</code>
<div class="block">Adds an output option for the underlying data source.</div>
</td>
</tr>
<tr id="i10" class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#option-java.lang.String-double-">option</a></span>(String&nbsp;key,
      double&nbsp;value)</code>
<div class="block">Adds an output option for the underlying data source.</div>
</td>
</tr>
<tr id="i11" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#option-java.lang.String-long-">option</a></span>(String&nbsp;key,
      long&nbsp;value)</code>
<div class="block">Adds an output option for the underlying data source.</div>
</td>
</tr>
<tr id="i12" class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#option-java.lang.String-java.lang.String-">option</a></span>(String&nbsp;key,
      String&nbsp;value)</code>
<div class="block">Adds an output option for the underlying data source.</div>
</td>
</tr>
<tr id="i13" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#options-scala.collection.Map-">options</a></span>(scala.collection.Map&lt;String,String&gt;&nbsp;options)</code>
<div class="block">(Scala-specific) Adds output options for the underlying data source.</div>
</td>
</tr>
<tr id="i14" class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#options-java.util.Map-">options</a></span>(java.util.Map&lt;String,String&gt;&nbsp;options)</code>
<div class="block">Adds output options for the underlying data source.</div>
</td>
</tr>
<tr id="i15" class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#orc-java.lang.String-">orc</a></span>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> in ORC format at the specified path.</div>
</td>
</tr>
<tr id="i16" class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#parquet-java.lang.String-">parquet</a></span>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> in Parquet format at the specified path.</div>
</td>
</tr>
<tr id="i17" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#partitionBy-scala.collection.Seq-">partitionBy</a></span>(scala.collection.Seq&lt;String&gt;&nbsp;colNames)</code>
<div class="block">Partitions the output by the given columns on the file system.</div>
</td>
</tr>
<tr id="i18" class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#partitionBy-java.lang.String...-">partitionBy</a></span>(String...&nbsp;colNames)</code>
<div class="block">Partitions the output by the given columns on the file system.</div>
</td>
</tr>
<tr id="i19" class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#save--">save</a></span>()</code>
<div class="block">Saves the content of the <code>DataFrame</code> as the specified table.</div>
</td>
</tr>
<tr id="i20" class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#save-java.lang.String-">save</a></span>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> at the specified path.</div>
</td>
</tr>
<tr id="i21" class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#saveAsTable-java.lang.String-">saveAsTable</a></span>(String&nbsp;tableName)</code>
<div class="block">Saves the content of the <code>DataFrame</code> as the specified table.</div>
</td>
</tr>
<tr id="i22" class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#sortBy-java.lang.String-scala.collection.Seq-">sortBy</a></span>(String&nbsp;colName,
      scala.collection.Seq&lt;String&gt;&nbsp;colNames)</code>
<div class="block">Sorts the output in each bucket by the given columns.</div>
</td>
</tr>
<tr id="i23" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#sortBy-java.lang.String-java.lang.String...-">sortBy</a></span>(String&nbsp;colName,
      String...&nbsp;colNames)</code>
<div class="block">Sorts the output in each bucket by the given columns.</div>
</td>
</tr>
<tr id="i24" class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#text-java.lang.String-">text</a></span>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> in a text file at the specified path.</div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods.inherited.from.class.Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;Object</h3>
<code>equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method.detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="bucketBy-int-java.lang.String-java.lang.String...-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>bucketBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;bucketBy(int&nbsp;numBuckets,
                                   String&nbsp;colName,
                                   String...&nbsp;colNames)</pre>
<div class="block">Buckets the output by the given columns. If specified, the output is laid out on the file
 system similar to Hive's bucketing scheme.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) starting with Spark
 2.1.0.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>numBuckets</code> - (undocumented)</dd>
<dd><code>colName</code> - (undocumented)</dd>
<dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>2.0</dd>
</dl>
</li>
</ul>
<a name="bucketBy-int-java.lang.String-scala.collection.Seq-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>bucketBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;bucketBy(int&nbsp;numBuckets,
                                   String&nbsp;colName,
                                   scala.collection.Seq&lt;String&gt;&nbsp;colNames)</pre>
<div class="block">Buckets the output by the given columns. If specified, the output is laid out on the file
 system similar to Hive's bucketing scheme.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) starting with Spark
 2.1.0.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>numBuckets</code> - (undocumented)</dd>
<dd><code>colName</code> - (undocumented)</dd>
<dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>2.0</dd>
</dl>
</li>
</ul>
<a name="csv-java.lang.String-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>csv</h4>
<pre>public&nbsp;void&nbsp;csv(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> in CSV format at the specified path.
 This is equivalent to:
 <pre><code>
   format("csv").save(path)
 </code></pre>
 <p>
 You can set the following CSV-specific option(s) for writing CSV files:
 <ul>
 <li><code>sep</code> (default <code>,</code>): sets a single character as a separator for each
 field and value.</li>
 <li><code>quote</code> (default <code>"</code>): sets a single character used for escaping quoted values where
 the separator can be part of the value. If an empty string is set, it uses <code>u0000</code>
 (null character).</li>
 <li><code>escape</code> (default <code>\</code>): sets a single character used for escaping quotes inside
 an already quoted value.</li>
 <li><code>charToEscapeQuoteEscaping</code> (default <code>escape</code> or <code>\0</code>): sets a single character used for
 escaping the escape for the quote character. The default value is escape character when escape
 and quote characters are different, <code>\0</code> otherwise.</li>
 <li><code>escapeQuotes</code> (default <code>true</code>): a flag indicating whether values containing
 quotes should always be enclosed in quotes. Default is to escape all values containing
 a quote character.</li>
 <li><code>quoteAll</code> (default <code>false</code>): a flag indicating whether all values should always be
 enclosed in quotes. Default is to only escape values containing a quote character.</li>
 <li><code>header</code> (default <code>false</code>): writes the names of columns as the first line.</li>
 <li><code>nullValue</code> (default empty string): sets the string representation of a null value.</li>
 <li><code>emptyValue</code> (default <code>""</code>): sets the string representation of an empty value.</li>
 <li><code>encoding</code> (by default it is not set): specifies encoding (charset) of saved csv
 files. If it is not set, the UTF-8 charset will be used.</li>
 <li><code>compression</code> (default <code>null</code>): compression codec to use when saving to file. This can be
 one of the known case-insensitive shorten names (<code>none</code>, <code>bzip2</code>, <code>gzip</code>, <code>lz4</code>,
 <code>snappy</code> and <code>deflate</code>). </li>
 <li><code>dateFormat</code> (default <code>yyyy-MM-dd</code>): sets the string that indicates a date format.
 Custom date formats follow the formats at <code>java.text.SimpleDateFormat</code>. This applies to
 date type.</li>
 <li><code>timestampFormat</code> (default <code>yyyy-MM-dd'T'HH:mm:ss.SSSXXX</code>): sets the string that
 indicates a timestamp format. Custom date formats follow the formats at
 <code>java.text.SimpleDateFormat</code>. This applies to timestamp type.</li>
 <li><code>ignoreLeadingWhiteSpace</code> (default <code>true</code>): a flag indicating whether or not leading
 whitespaces from values being written should be skipped.</li>
 <li><code>ignoreTrailingWhiteSpace</code> (default <code>true</code>): a flag indicating defines whether or not
 trailing whitespaces from values being written should be skipped.</li>
 </ul>
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>path</code> - (undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>2.0.0</dd>
</dl>
</li>
</ul>
<a name="format-java.lang.String-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>format</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;format(String&nbsp;source)</pre>
<div class="block">Specifies the underlying output data source. Built-in options include "parquet", "json", etc.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>source</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="insertInto-java.lang.String-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>insertInto</h4>
<pre>public&nbsp;void&nbsp;insertInto(String&nbsp;tableName)</pre>
<div class="block">Inserts the content of the <code>DataFrame</code> to the specified table. It requires that
 the schema of the <code>DataFrame</code> is the same as the schema of the table.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>tableName</code> - (undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
<dt><span class="simpleTagLabel">Note:</span></dt>
<dd>Unlike <code>saveAsTable</code>, <code>insertInto</code> ignores the column names and just uses position-based
 resolution. For example:
 <p>
 <pre><code>
    scala&gt; Seq((1, 2)).toDF("i", "j").write.mode("overwrite").saveAsTable("t1")
    scala&gt; Seq((3, 4)).toDF("j", "i").write.insertInto("t1")
    scala&gt; Seq((5, 6)).toDF("a", "b").write.insertInto("t1")
    scala&gt; sql("select * from t1").show
    +---+---+
    |  i|  j|
    +---+---+
    |  5|  6|
    |  3|  4|
    |  1|  2|
    +---+---+
 </code></pre>
 <p>
 Because it inserts data to an existing table, format or options will be ignored.
 <p></dd>
</dl>
</li>
</ul>
<a name="jdbc-java.lang.String-java.lang.String-java.util.Properties-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>jdbc</h4>
<pre>public&nbsp;void&nbsp;jdbc(String&nbsp;url,
                 String&nbsp;table,
                 java.util.Properties&nbsp;connectionProperties)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> to an external database table via JDBC. In the case the
 table already exists in the external database, behavior of this function depends on the
 save mode, specified by the <code>mode</code> function (default to throwing an exception).
 <p>
 Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
 your external database systems.
 <p>
 You can set the following JDBC-specific option(s) for storing JDBC:
 <ul>
 <li><code>truncate</code> (default <code>false</code>): use <code>TRUNCATE TABLE</code> instead of <code>DROP TABLE</code>.</li>
 </ul>
 <p>
 In case of failures, users should turn off <code>truncate</code> option to use <code>DROP TABLE</code> again. Also,
 due to the different behavior of <code>TRUNCATE TABLE</code> among DBMS, it's not always safe to use this.
 MySQLDialect, DB2Dialect, MsSqlServerDialect, DerbyDialect, and OracleDialect supports this
 while PostgresDialect and default JDBCDirect doesn't. For unknown and unsupported JDBCDirect,
 the user option <code>truncate</code> is ignored.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>url</code> - JDBC database url of the form <code>jdbc:subprotocol:subname</code></dd>
<dd><code>table</code> - Name of the table in the external database.</dd>
<dd><code>connectionProperties</code> - JDBC database connection arguments, a list of arbitrary string
                             tag/value. Normally at least a "user" and "password" property
                             should be included. "batchsize" can be used to control the
                             number of rows per insert. "isolationLevel" can be one of
                             "NONE", "READ_COMMITTED", "READ_UNCOMMITTED", "REPEATABLE_READ",
                             or "SERIALIZABLE", corresponding to standard transaction
                             isolation levels defined by JDBC's Connection object, with default
                             of "READ_UNCOMMITTED".</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="json-java.lang.String-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>json</h4>
<pre>public&nbsp;void&nbsp;json(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> in JSON format (<a href="http://jsonlines.org/">
 JSON Lines text format or newline-delimited JSON</a>) at the specified path.
 This is equivalent to:
 <pre><code>
   format("json").save(path)
 </code></pre>
 <p>
 You can set the following JSON-specific option(s) for writing JSON files:
 <ul>
 <li><code>compression</code> (default <code>null</code>): compression codec to use when saving to file. This can be
 one of the known case-insensitive shorten names (<code>none</code>, <code>bzip2</code>, <code>gzip</code>, <code>lz4</code>,
 <code>snappy</code> and <code>deflate</code>). </li>
 <li><code>dateFormat</code> (default <code>yyyy-MM-dd</code>): sets the string that indicates a date format.
 Custom date formats follow the formats at <code>java.text.SimpleDateFormat</code>. This applies to
 date type.</li>
 <li><code>timestampFormat</code> (default <code>yyyy-MM-dd'T'HH:mm:ss.SSSXXX</code>): sets the string that
 indicates a timestamp format. Custom date formats follow the formats at
 <code>java.text.SimpleDateFormat</code>. This applies to timestamp type.</li>
 <li><code>encoding</code> (by default it is not set): specifies encoding (charset) of saved json
 files. If it is not set, the UTF-8 charset will be used. </li>
 <li><code>lineSep</code> (default <code>\n</code>): defines the line separator that should be used for writing.</li>
 </ul>
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>path</code> - (undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="mode-org.apache.spark.sql.SaveMode-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>mode</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;mode(<a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;saveMode)</pre>
<div class="block">Specifies the behavior when data or table already exists. Options include:
 <ul>
 <li><code>SaveMode.Overwrite</code>: overwrite the existing data.</li>
 <li><code>SaveMode.Append</code>: append the data.</li>
 <li><code>SaveMode.Ignore</code>: ignore the operation (i.e. no-op).</li>
 <li><code>SaveMode.ErrorIfExists</code>: default option, throw an exception at runtime.</li>
 </ul>
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>saveMode</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="mode-java.lang.String-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>mode</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;mode(String&nbsp;saveMode)</pre>
<div class="block">Specifies the behavior when data or table already exists. Options include:
 <ul>
 <li><code>overwrite</code>: overwrite the existing data.</li>
 <li><code>append</code>: append the data.</li>
 <li><code>ignore</code>: ignore the operation (i.e. no-op).</li>
 <li><code>error</code> or <code>errorifexists</code>: default option, throw an exception at runtime.</li>
 </ul>
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>saveMode</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="option-java.lang.String-java.lang.String-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>option</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;option(String&nbsp;key,
                                 String&nbsp;value)</pre>
<div class="block">Adds an output option for the underlying data source.
 <p>
 You can set the following option(s):
 <ul>
 <li><code>timeZone</code> (default session local timezone): sets the string that indicates a timezone
 to be used to format timestamps in the JSON/CSV datasources or partition values.</li>
 </ul>
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>key</code> - (undocumented)</dd>
<dd><code>value</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="option-java.lang.String-boolean-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>option</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;option(String&nbsp;key,
                                 boolean&nbsp;value)</pre>
<div class="block">Adds an output option for the underlying data source.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>key</code> - (undocumented)</dd>
<dd><code>value</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>2.0.0</dd>
</dl>
</li>
</ul>
<a name="option-java.lang.String-long-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>option</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;option(String&nbsp;key,
                                 long&nbsp;value)</pre>
<div class="block">Adds an output option for the underlying data source.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>key</code> - (undocumented)</dd>
<dd><code>value</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>2.0.0</dd>
</dl>
</li>
</ul>
<a name="option-java.lang.String-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>option</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;option(String&nbsp;key,
                                 double&nbsp;value)</pre>
<div class="block">Adds an output option for the underlying data source.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>key</code> - (undocumented)</dd>
<dd><code>value</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>2.0.0</dd>
</dl>
</li>
</ul>
<a name="options-scala.collection.Map-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>options</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;options(scala.collection.Map&lt;String,String&gt;&nbsp;options)</pre>
<div class="block">(Scala-specific) Adds output options for the underlying data source.
 <p>
 You can set the following option(s):
 <ul>
 <li><code>timeZone</code> (default session local timezone): sets the string that indicates a timezone
 to be used to format timestamps in the JSON/CSV datasources or partition values.</li>
 </ul>
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>options</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="options-java.util.Map-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>options</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;options(java.util.Map&lt;String,String&gt;&nbsp;options)</pre>
<div class="block">Adds output options for the underlying data source.
 <p>
 You can set the following option(s):
 <ul>
 <li><code>timeZone</code> (default session local timezone): sets the string that indicates a timezone
 to be used to format timestamps in the JSON/CSV datasources or partition values.</li>
 </ul>
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>options</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="orc-java.lang.String-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>orc</h4>
<pre>public&nbsp;void&nbsp;orc(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> in ORC format at the specified path.
 This is equivalent to:
 <pre><code>
   format("orc").save(path)
 </code></pre>
 <p>
 You can set the following ORC-specific option(s) for writing ORC files:
 <ul>
 <li><code>compression</code> (default is the value specified in <code>spark.sql.orc.compression.codec</code>):
 compression codec to use when saving to file. This can be one of the known case-insensitive
 shorten names(<code>none</code>, <code>snappy</code>, <code>zlib</code>, and <code>lzo</code>). This will override
 <code>orc.compress</code> and <code>spark.sql.orc.compression.codec</code>. If <code>orc.compress</code> is given,
 it overrides <code>spark.sql.orc.compression.codec</code>.</li>
 </ul>
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>path</code> - (undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.5.0</dd>
<dt><span class="simpleTagLabel">Note:</span></dt>
<dd>Currently, this method can only be used after enabling Hive support</dd>
</dl>
</li>
</ul>
<a name="parquet-java.lang.String-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>parquet</h4>
<pre>public&nbsp;void&nbsp;parquet(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> in Parquet format at the specified path.
 This is equivalent to:
 <pre><code>
   format("parquet").save(path)
 </code></pre>
 <p>
 You can set the following Parquet-specific option(s) for writing Parquet files:
 <ul>
 <li><code>compression</code> (default is the value specified in <code>spark.sql.parquet.compression.codec</code>):
 compression codec to use when saving to file. This can be one of the known case-insensitive
 shorten names(<code>none</code>, <code>uncompressed</code>, <code>snappy</code>, <code>gzip</code>, <code>lzo</code>, <code>brotli</code>, <code>lz4</code>, and <code>zstd</code>).
 This will override <code>spark.sql.parquet.compression.codec</code>.</li>
 </ul>
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>path</code> - (undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="partitionBy-java.lang.String...-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>partitionBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;partitionBy(String...&nbsp;colNames)</pre>
<div class="block">Partitions the output by the given columns on the file system. If specified, the output is
 laid out on the file system similar to Hive's partitioning scheme. As an example, when we
 partition a dataset by year and then month, the directory layout would look like:
 <ul>
 <li>year=2016/month=01/</li>
 <li>year=2016/month=02/</li>
 </ul>
 <p>
 Partitioning is one of the most widely used techniques to optimize physical data layout.
 It provides a coarse-grained index for skipping unnecessary data reads when queries have
 predicates on the partitioned columns. In order for partitioning to work well, the number
 of distinct values in each column should typically be less than tens of thousands.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) starting with Spark
 2.1.0.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="partitionBy-scala.collection.Seq-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>partitionBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;partitionBy(scala.collection.Seq&lt;String&gt;&nbsp;colNames)</pre>
<div class="block">Partitions the output by the given columns on the file system. If specified, the output is
 laid out on the file system similar to Hive's partitioning scheme. As an example, when we
 partition a dataset by year and then month, the directory layout would look like:
 <ul>
 <li>year=2016/month=01/</li>
 <li>year=2016/month=02/</li>
 </ul>
 <p>
 Partitioning is one of the most widely used techniques to optimize physical data layout.
 It provides a coarse-grained index for skipping unnecessary data reads when queries have
 predicates on the partitioned columns. In order for partitioning to work well, the number
 of distinct values in each column should typically be less than tens of thousands.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) starting with Spark
 2.1.0.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="save-java.lang.String-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>save</h4>
<pre>public&nbsp;void&nbsp;save(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> at the specified path.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>path</code> - (undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="save--">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>save</h4>
<pre>public&nbsp;void&nbsp;save()</pre>
<div class="block">Saves the content of the <code>DataFrame</code> as the specified table.
 <p></div>
<dl>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="saveAsTable-java.lang.String-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsTable</h4>
<pre>public&nbsp;void&nbsp;saveAsTable(String&nbsp;tableName)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> as the specified table.
 <p>
 In the case the table already exists, behavior of this function depends on the
 save mode, specified by the <code>mode</code> function (default to throwing an exception).
 When <code>mode</code> is <code>Overwrite</code>, the schema of the <code>DataFrame</code> does not need to be
 the same as that of the existing table.
 <p>
 When <code>mode</code> is <code>Append</code>, if there is an existing table, we will use the format and options of
 the existing table. The column order in the schema of the <code>DataFrame</code> doesn't need to be same
 as that of the existing table. Unlike <code>insertInto</code>, <code>saveAsTable</code> will use the column names to
 find the correct column positions. For example:
 <p>
 <pre><code>
    scala&gt; Seq((1, 2)).toDF("i", "j").write.mode("overwrite").saveAsTable("t1")
    scala&gt; Seq((3, 4)).toDF("j", "i").write.mode("append").saveAsTable("t1")
    scala&gt; sql("select * from t1").show
    +---+---+
    |  i|  j|
    +---+---+
    |  1|  2|
    |  4|  3|
    +---+---+
 </code></pre>
 <p>
 In this method, save mode is used to determine the behavior if the data source table exists in
 Spark catalog. We will always overwrite the underlying data of data source (e.g. a table in
 JDBC data source) if the table doesn't exist in Spark catalog, and will always append to the
 underlying data of data source if the table already exists.
 <p>
 When the DataFrame is created from a non-partitioned <code>HadoopFsRelation</code> with a single input
 path, and the data source provider can be mapped to an existing Hive builtin SerDe (i.e. ORC
 and Parquet), the table is persisted in a Hive compatible format, which means other systems
 like Hive will be able to read this table. Otherwise, the table is persisted in a Spark SQL
 specific format.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>tableName</code> - (undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.4.0</dd>
</dl>
</li>
</ul>
<a name="sortBy-java.lang.String-java.lang.String...-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sortBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;sortBy(String&nbsp;colName,
                                 String...&nbsp;colNames)</pre>
<div class="block">Sorts the output in each bucket by the given columns.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) starting with Spark
 2.1.0.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>colName</code> - (undocumented)</dd>
<dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>2.0</dd>
</dl>
</li>
</ul>
<a name="sortBy-java.lang.String-scala.collection.Seq-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sortBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;sortBy(String&nbsp;colName,
                                 scala.collection.Seq&lt;String&gt;&nbsp;colNames)</pre>
<div class="block">Sorts the output in each bucket by the given columns.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) starting with Spark
 2.1.0.
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>colName</code> - (undocumented)</dd>
<dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>(undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>2.0</dd>
</dl>
</li>
</ul>
<a name="text-java.lang.String-">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>text</h4>
<pre>public&nbsp;void&nbsp;text(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> in a text file at the specified path.
 The DataFrame must have only one column that is of string type.
 Each row becomes a new line in the output file. For example:
 <pre><code>
   // Scala:
   df.write.text("/path/to/output")

   // Java:
   df.write().text("/path/to/output")
 </code></pre>
 <p>
 You can set the following option(s) for writing text files:
 <ul>
 <li><code>compression</code> (default <code>null</code>): compression codec to use when saving to file. This can be
 one of the known case-insensitive shorten names (<code>none</code>, <code>bzip2</code>, <code>gzip</code>, <code>lz4</code>,
 <code>snappy</code> and <code>deflate</code>). </li>
 <li><code>lineSep</code> (default <code>\n</code>): defines the line separator that should be used for writing.</li>
 </ul>
 <p></div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>path</code> - (undocumented)</dd>
<dt><span class="simpleTagLabel">Since:</span></dt>
<dd>1.6.0</dd>
</dl>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar.bottom">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.bottom" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.bottom.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/sql/DataFrameStatFunctions.html" title="class in org.apache.spark.sql"><span class="typeNameLink">Prev&nbsp;Class</span></a></li>
<li><a href="../../../../org/apache/spark/sql/Dataset.html" title="class in org.apache.spark.sql"><span class="typeNameLink">Next&nbsp;Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/sql/DataFrameWriter.html" target="_top">Frames</a></li>
<li><a href="DataFrameWriter.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method.summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method.detail">Method</a></li>
</ul>
</div>
<a name="skip.navbar.bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
<script defer="defer" type="text/javascript" src="../../../../lib/jquery.js"></script><script defer="defer" type="text/javascript" src="../../../../lib/api-javadocs.js"></script></body>
</html>
