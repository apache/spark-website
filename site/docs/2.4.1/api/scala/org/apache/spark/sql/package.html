<!DOCTYPE html >
<html>
        <head>
          <title>sql - Spark 2.4.1 ScalaDoc - org.apache.spark.sql</title>
          <meta name="description" content="sql - Spark 2.4.1 ScalaDoc - org.apache.spark.sql" />
          <meta name="keywords" content="sql Spark 2.4.1 ScalaDoc org.apache.spark.sql" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      <link href="../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript" src="../../../../lib/jquery.js" id="jquery-js"></script>
      <script type="text/javascript" src="../../../../lib/jquery-ui.js"></script>
      <script type="text/javascript" src="../../../../lib/template.js"></script>
      <script type="text/javascript" src="../../../../lib/tools.tooltip.js"></script>
      
      <script type="text/javascript">
         if(top === self) {
            var url = '../../../../index.html';
            var hash = 'org.apache.spark.sql.package';
            var anchor = window.location.hash;
            var anchor_opt = '';
            if (anchor.length >= 1)
              anchor_opt = '@' + anchor.substring(1);
            window.location.href = url + '#' + hash + anchor_opt;
         }
   	  </script>
    
        </head>
        <body class="value">
      <div id="definition">
        <img alt="Package" src="../../../../lib/package_big.png" />
        <p id="owner"><a href="../../../package.html" class="extype" name="org">org</a>.<a href="../../package.html" class="extype" name="org.apache">apache</a>.<a href="../package.html" class="extype" name="org.apache.spark">spark</a></p>
        <h1>sql</h1><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">sql</span>
      </span>
      </h4>
      
          <div id="comment" class="fullcommenttop"><div class="comment cmt"><p>Allows the execution of relational queries, including those expressed in SQL using Spark.
</p></div><dl class="attributes block"> <dt>Source</dt><dd><a href="https://github.com/apache/spark/tree/v2.4.1/sql/core/src/main/scala/org/apache/spark/sql/package.scala" target="_blank">package.scala</a></dd></dl><div class="toggleContainer block">
          <span class="toggle">Linear Supertypes</span>
          <div class="superTypes hiddenContent"><span class="extype" name="scala.AnyRef">AnyRef</span>, <span class="extype" name="scala.Any">Any</span></div>
        </div></div>
        

      <div id="mbrsel">
        <div id="textfilter"><span class="pre"></span><span class="input"><input id="mbrsel-input" type="text" accesskey="/" /></span><span class="post"></span></div>
        <div id="order">
              <span class="filtertype">Ordering</span>
              <ol>
                
                <li class="alpha in"><span>Alphabetic</span></li>
                <li class="inherit out"><span>By Inheritance</span></li>
              </ol>
            </div>
        <div id="ancestors">
                <span class="filtertype">Inherited<br />
                </span>
                <ol id="linearization">
                  <li class="in" name="org.apache.spark.sql"><span>sql</span></li><li class="in" name="scala.AnyRef"><span>AnyRef</span></li><li class="in" name="scala.Any"><span>Any</span></li>
                </ol>
              </div><div id="ancestors">
            <span class="filtertype"></span>
            <ol>
              <li class="hideall out"><span>Hide All</span></li>
              <li class="showall in"><span>Show All</span></li>
            </ol>
          </div>
        <div id="visbl">
            <span class="filtertype">Visibility</span>
            <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
          </div>
      </div>

      <div id="template">
        <div id="allMembers">
        

        <div id="types" class="types members">
              <h3>Type Members</h3>
              <ol><li name="org.apache.spark.sql.AnalysisException" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="AnalysisExceptionextendsExceptionwithSerializable"></a>
      <a id="AnalysisException:AnalysisException"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="AnalysisException.html"><span class="name">AnalysisException</span></a><span class="result"> extends <span class="extype" name="scala.Exception">Exception</span> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@AnalysisExceptionextendsExceptionwithSerializable" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Thrown when a query fails to analyze, usually because the query itself is invalid.</p><div class="fullcomment"><div class="comment cmt"><p>Thrown when a query fails to analyze, usually because the query itself is invalid.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.3.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.Column" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ColumnextendsLogging"></a>
      <a id="Column:Column"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="Column.html"><span class="name">Column</span></a><span class="result"> extends <a href="../internal/Logging.html" class="extype" name="org.apache.spark.internal.Logging">Logging</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@ColumnextendsLogging" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A column that will be computed based on the data in a <code>DataFrame</code>.</p><div class="fullcomment"><div class="comment cmt"><p>A column that will be computed based on the data in a <code>DataFrame</code>.</p><p>A new column can be constructed based on the input columns present in a DataFrame:</p><pre>df(<span class="lit">"columnName"</span>)            <span class="cmt">// On a specific `df` DataFrame.</span>
col(<span class="lit">"columnName"</span>)           <span class="cmt">// A generic column not yet associated with a DataFrame.</span>
col(<span class="lit">"columnName.field"</span>)     <span class="cmt">// Extracting a struct field</span>
col(<span class="lit">"`a.column.with.dots`"</span>) <span class="cmt">// Escape `.` in column names.</span>
$<span class="lit">"columnName"</span>               <span class="cmt">// Scala short hand for a named column.</span></pre><p><a href="Column.html" class="extype" name="org.apache.spark.sql.Column">Column</a> objects can be composed to form complex expressions:</p><pre>$<span class="lit">"a"</span> + <span class="num">1</span>
$<span class="lit">"a"</span> === $<span class="lit">"b"</span></pre></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.3.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>The internal Catalyst expression can be accessed via <a href="Column.html#expr:org.apache.spark.sql.catalyst.expressions.Expression" class="extmbr" name="org.apache.spark.sql.Column#expr">expr</a>, but this method is for
debugging purposes only and can change in any future Spark releases.</p></span></dd></dl></div>
    </li><li name="org.apache.spark.sql.ColumnName" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ColumnNameextendsColumn"></a>
      <a id="ColumnName:ColumnName"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="ColumnName.html"><span class="name">ColumnName</span></a><span class="result"> extends <a href="Column.html" class="extype" name="org.apache.spark.sql.Column">Column</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@ColumnNameextendsColumn" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A convenient class used for constructing schema.</p><div class="fullcomment"><div class="comment cmt"><p>A convenient class used for constructing schema.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.3.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.DataFrame" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]"></a>
      <a id="DataFrame:DataFrame"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">type</span>
      </span>
      <span class="symbol">
        <span class="name">DataFrame</span><span class="result"> = <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a>[<a href="Row.html" class="extype" name="org.apache.spark.sql.Row">Row</a>]</span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.apache.spark.sql.DataFrameNaFunctions" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DataFrameNaFunctionsextendsAnyRef"></a>
      <a id="DataFrameNaFunctions:DataFrameNaFunctions"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="DataFrameNaFunctions.html"><span class="name">DataFrameNaFunctions</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@DataFrameNaFunctionsextendsAnyRef" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Functionality for working with missing data in <code>DataFrame</code>s.</p><div class="fullcomment"><div class="comment cmt"><p>Functionality for working with missing data in <code>DataFrame</code>s.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.3.1</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.DataFrameReader" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DataFrameReaderextendsLogging"></a>
      <a id="DataFrameReader:DataFrameReader"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="DataFrameReader.html"><span class="name">DataFrameReader</span></a><span class="result"> extends <a href="../internal/Logging.html" class="extype" name="org.apache.spark.internal.Logging">Logging</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@DataFrameReaderextendsLogging" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Interface used to load a <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a> from external storage systems (e.g.</p><div class="fullcomment"><div class="comment cmt"><p>Interface used to load a <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a> from external storage systems (e.g. file systems,
key-value stores, etc). Use <code>SparkSession.read</code> to access this.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.4.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.DataFrameStatFunctions" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DataFrameStatFunctionsextendsAnyRef"></a>
      <a id="DataFrameStatFunctions:DataFrameStatFunctions"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="DataFrameStatFunctions.html"><span class="name">DataFrameStatFunctions</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@DataFrameStatFunctionsextendsAnyRef" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Statistic functions for <code>DataFrame</code>s.</p><div class="fullcomment"><div class="comment cmt"><p>Statistic functions for <code>DataFrame</code>s.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.4.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.DataFrameWriter" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DataFrameWriter[T]extendsAnyRef"></a>
      <a id="DataFrameWriter[T]:DataFrameWriter[T]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="DataFrameWriter.html"><span class="name">DataFrameWriter</span></a><span class="tparams">[<span name="T">T</span>]</span><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@DataFrameWriter[T]extendsAnyRef" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Interface used to write a <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a> to external storage systems (e.g.</p><div class="fullcomment"><div class="comment cmt"><p>Interface used to write a <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a> to external storage systems (e.g. file systems,
key-value stores, etc). Use <code>Dataset.write</code> to access this.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.4.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.Dataset" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Dataset[T]extendsSerializable"></a>
      <a id="Dataset[T]:Dataset[T]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="Dataset.html"><span class="name">Dataset</span></a><span class="tparams">[<span name="T">T</span>]</span><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@Dataset[T]extendsSerializable" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A Dataset is a strongly typed collection of domain-specific objects that can be transformed
in parallel using functional or relational operations.</p><div class="fullcomment"><div class="comment cmt"><p>A Dataset is a strongly typed collection of domain-specific objects that can be transformed
in parallel using functional or relational operations. Each Dataset also has an untyped view
called a <code>DataFrame</code>, which is a Dataset of <a href="Row.html" class="extype" name="org.apache.spark.sql.Row">Row</a>.</p><p>Operations available on Datasets are divided into transformations and actions. Transformations
are the ones that produce new Datasets, and actions are the ones that trigger computation and
return results. Example transformations include map, filter, select, and aggregate (<code>groupBy</code>).
Example actions count, show, or writing data out to file systems.</p><p>Datasets are &quot;lazy&quot;, i.e. computations are only triggered when an action is invoked. Internally,
a Dataset represents a logical plan that describes the computation required to produce the data.
When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
physical plan for efficient execution in a parallel and distributed manner. To explore the
logical plan as well as optimized physical plan, use the <code>explain</code> function.</p><p>To efficiently support domain-specific objects, an <a href="Encoder.html" class="extype" name="org.apache.spark.sql.Encoder">Encoder</a> is required. The encoder maps
the domain specific type <code>T</code> to Spark's internal type system. For example, given a class <code>Person</code>
with two fields, <code>name</code> (string) and <code>age</code> (int), an encoder is used to tell Spark to generate
code at runtime to serialize the <code>Person</code> object into a binary structure. This binary structure
often has much lower memory footprint as well as are optimized for efficiency in data processing
(e.g. in a columnar format). To understand the internal binary representation for data, use the
<code>schema</code> function.</p><p>There are typically two ways to create a Dataset. The most common way is by pointing Spark
to some files on storage systems, using the <code>read</code> function available on a <code>SparkSession</code>.</p><pre><span class="kw">val</span> people = spark.read.parquet(<span class="lit">"..."</span>).as[Person]  <span class="cmt">// Scala</span>
Dataset&lt;Person&gt; people = spark.read().parquet(<span class="lit">"..."</span>).as(Encoders.bean(Person.<span class="kw">class</span>)); <span class="cmt">// Java</span></pre><p>Datasets can also be created through transformations available on existing Datasets. For example,
the following creates a new Dataset by applying a filter on the existing one:</p><pre><span class="kw">val</span> names = people.map(_.name)  <span class="cmt">// in Scala; names is a Dataset[String]</span>
Dataset&lt;<span class="std">String</span>&gt; names = people.map((Person p) -&gt; p.name, Encoders.STRING));</pre><p>Dataset operations can also be untyped, through various domain-specific-language (DSL)
functions defined in: Dataset (this class), <a href="Column.html" class="extype" name="org.apache.spark.sql.Column">Column</a>, and <a href="functions$.html" class="extype" name="org.apache.spark.sql.functions">functions</a>. These operations
are very similar to the operations available in the data frame abstraction in R or Python.</p><p>To select a column from the Dataset, use <code>apply</code> method in Scala and <code>col</code> in Java.</p><pre><span class="kw">val</span> ageCol = people(<span class="lit">"age"</span>)  <span class="cmt">// in Scala</span>
Column ageCol = people.col(<span class="lit">"age"</span>); <span class="cmt">// in Java</span></pre><p>Note that the <a href="Column.html" class="extype" name="org.apache.spark.sql.Column">Column</a> type can also be manipulated through its various functions.</p><pre><span class="cmt">// The following creates a new column that increases everybody's age by 10.</span>
people(<span class="lit">"age"</span>) + <span class="num">10</span>  <span class="cmt">// in Scala</span>
people.col(<span class="lit">"age"</span>).plus(<span class="num">10</span>);  <span class="cmt">// in Java</span></pre><p>A more concrete example in Scala:</p><pre><span class="cmt">// To create Dataset[Row] using SparkSession</span>
<span class="kw">val</span> people = spark.read.parquet(<span class="lit">"..."</span>)
<span class="kw">val</span> department = spark.read.parquet(<span class="lit">"..."</span>)

people.filter(<span class="lit">"age > 30"</span>)
  .join(department, people(<span class="lit">"deptId"</span>) === department(<span class="lit">"id"</span>))
  .groupBy(department(<span class="lit">"name"</span>), people(<span class="lit">"gender"</span>))
  .agg(avg(people(<span class="lit">"salary"</span>)), max(people(<span class="lit">"age"</span>)))</pre><p>and in Java:</p><pre><span class="cmt">// To create Dataset<Row> using SparkSession</span>
Dataset&lt;Row&gt; people = spark.read().parquet(<span class="lit">"..."</span>);
Dataset&lt;Row&gt; department = spark.read().parquet(<span class="lit">"..."</span>);

people.filter(people.col(<span class="lit">"age"</span>).gt(<span class="num">30</span>))
  .join(department, people.col(<span class="lit">"deptId"</span>).equalTo(department.col(<span class="lit">"id"</span>)))
  .groupBy(department.col(<span class="lit">"name"</span>), people.col(<span class="lit">"gender"</span>))
  .agg(avg(people.col(<span class="lit">"salary"</span>)), max(people.col(<span class="lit">"age"</span>)));</pre></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.DatasetHolder" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DatasetHolder[T]extendsProductwithSerializable"></a>
      <a id="DatasetHolder[T]:DatasetHolder[T]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">case class</span>
      </span>
      <span class="symbol">
        <a href="DatasetHolder.html"><span class="name">DatasetHolder</span></a><span class="tparams">[<span name="T">T</span>]</span><span class="result"> extends <span class="extype" name="scala.Product">Product</span> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@DatasetHolder[T]extendsProductwithSerializable" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A container for a <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a>, used for implicit conversions in Scala.</p><div class="fullcomment"><div class="comment cmt"><p>A container for a <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a>, used for implicit conversions in Scala.</p><p>To use this, import implicit conversions in SQL:</p><pre><span class="kw">val</span> spark: SparkSession = ...
<span class="kw">import</span> spark.implicits._</pre></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.Encoder" visbl="pub" data-isabs="true" fullComment="yes" group="Ungrouped">
      <a id="Encoder[T]extendsSerializable"></a>
      <a id="Encoder[T]:Encoder[T]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a href="Encoder.html"><span class="name">Encoder</span></a><span class="tparams">[<span name="T">T</span>]</span><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@Encoder[T]extendsSerializable" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">:: Experimental ::
Used to convert a JVM object of type <code>T</code> to and from the internal Spark SQL representation.</p><div class="fullcomment"><div class="comment cmt"><p>:: Experimental ::
Used to convert a JVM object of type <code>T</code> to and from the internal Spark SQL representation.</p><h4> Scala </h4><p>Encoders are generally created automatically through implicits from a <code>SparkSession</code>, or can be
explicitly created by calling static methods on <a href="Encoders$.html" class="extype" name="org.apache.spark.sql.Encoders">Encoders</a>.</p><pre><span class="kw">import</span> spark.implicits._

<span class="kw">val</span> ds = <span class="std">Seq</span>(<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>).toDS() <span class="cmt">// implicitly provided (spark.implicits.newIntEncoder)</span></pre><h4> Java </h4><p>Encoders are specified by calling static methods on <a href="Encoders$.html" class="extype" name="org.apache.spark.sql.Encoders">Encoders</a>.</p><pre><span class="std">List</span>&lt;<span class="std">String</span>&gt; data = Arrays.asList(<span class="lit">"abc"</span>, <span class="lit">"abc"</span>, <span class="lit">"xyz"</span>);
Dataset&lt;<span class="std">String</span>&gt; ds = context.createDataset(data, Encoders.STRING());</pre><p>Encoders can be composed into tuples:</p><pre>Encoder&lt;Tuple2&lt;Integer, <span class="std">String</span>&gt;&gt; encoder2 = Encoders.tuple(Encoders.INT(), Encoders.STRING());
<span class="std">List</span>&lt;Tuple2&lt;Integer, <span class="std">String</span>&gt;&gt; data2 = Arrays.asList(<span class="kw">new</span> scala.Tuple2(<span class="num">1</span>, <span class="lit">"a"</span>);
Dataset&lt;Tuple2&lt;Integer, <span class="std">String</span>&gt;&gt; ds2 = context.createDataset(data2, encoder2);</pre><p>Or constructed from Java Beans:</p><pre>Encoders.bean(MyClass.<span class="kw">class</span>);</pre><h4> Implementation </h4><ul><li>Encoders are not required to be thread-safe and thus they do not need to use locks to guard
   against concurrent access if they reuse internal buffers to improve performance.
</li></ul></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Experimental</span><span class="args">()</span>
              
                <span class="name">@Evolving</span><span class="args">()</span>
              
                <span class="name">@implicitNotFound</span><span class="args">(<span>
      
      <span class="defval" name="&quot;Unable to find encoder for type ${T}. An implicit Encoder[${T}] is needed to &quot; +<br/>  &quot;store ${T} instances in a Dataset. Primitive types (Int, String, etc) and Product types (case &quot; +<br/>  &quot;classes) are supported by importing spark.implicits._  Support for serializing other types &quot; +<br/>  &quot;will be added in future releases.&quot;">...</span>
    </span>)</span>
              
        </dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.ExperimentalMethods" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ExperimentalMethodsextendsAnyRef"></a>
      <a id="ExperimentalMethods:ExperimentalMethods"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="ExperimentalMethods.html"><span class="name">ExperimentalMethods</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@ExperimentalMethodsextendsAnyRef" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">:: Experimental ::
Holder for experimental methods for the bravest.</p><div class="fullcomment"><div class="comment cmt"><p>:: Experimental ::
Holder for experimental methods for the bravest. We make NO guarantee about the stability
regarding binary compatibility and source compatibility of methods here.</p><pre>spark.experimental.extraStrategies += ...</pre></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Experimental</span><span class="args">()</span>
              
                <span class="name">@Unstable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.3.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.ForeachWriter" visbl="pub" data-isabs="true" fullComment="yes" group="Ungrouped">
      <a id="ForeachWriter[T]extendsSerializable"></a>
      <a id="ForeachWriter[T]:ForeachWriter[T]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">abstract </span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="ForeachWriter.html"><span class="name">ForeachWriter</span></a><span class="tparams">[<span name="T">T</span>]</span><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@ForeachWriter[T]extendsSerializable" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">The abstract class for writing custom logic to process data generated by a query.</p><div class="fullcomment"><div class="comment cmt"><p>The abstract class for writing custom logic to process data generated by a query.
This is often used to write the output of a streaming query to arbitrary storage systems.
Any implementation of this base class will be used by Spark in the following way.</p><ul><li>A single instance of this class is responsible of all the data generated by a single task
    in a query. In other words, one instance is responsible for processing one partition of the
    data generated in a distributed manner.</li><li>Any implementation of this class must be serializable because each task will get a fresh
    serialized-deserialized copy of the provided object. Hence, it is strongly recommended that
    any initialization for writing data (e.g. opening a connection or starting a transaction)
    is done after the <code>open(...)</code> method has been called, which signifies that the task is
    ready to generate data.</li><li>The lifecycle of the methods are as follows.</li></ul><p><pre>
  For each partition with `partitionId`:
      For each batch/epoch of streaming data (if its streaming query) with `epochId`:
          Method `open(partitionId, epochId)` is called.
          If `open` returns true:
               For each row in the partition and batch/epoch, method `process(row)` is called.
          Method `close(errorOrNull)` is called with error (if any) seen while processing rows.
</pre></p><p>Important points to note:</p><ul><li>The <code>partitionId</code> and <code>epochId</code> can be used to deduplicate generated data when failures
    cause reprocessing of some input data. This depends on the execution mode of the query. If
    the streaming query is being executed in the micro-batch mode, then every partition
    represented by a unique tuple (partitionId, epochId) is guaranteed to have the same data.
    Hence, (partitionId, epochId) can be used to deduplicate and/or transactionally commit data
    and achieve exactly-once guarantees. However, if the streaming query is being executed in the
    continuous mode, then this guarantee does not hold and therefore should not be used for
    deduplication.</li><li>The <code>close()</code> method will be called if <code>open()</code> method returns successfully (irrespective
    of the return value), except if the JVM crashes in the middle.</li></ul><p>Scala example:</p><pre>datasetOfString.writeStream.foreach(<span class="kw">new</span> ForeachWriter[<span class="std">String</span>] {

  <span class="kw">def</span> open(partitionId: <span class="std">Long</span>, version: <span class="std">Long</span>): <span class="std">Boolean</span> = {
    <span class="cmt">// open connection</span>
  }

  <span class="kw">def</span> process(record: <span class="std">String</span>) = {
    <span class="cmt">// write string to connection</span>
  }

  <span class="kw">def</span> close(errorOrNull: Throwable): <span class="std">Unit</span> = {
    <span class="cmt">// close the connection</span>
  }
})</pre><p>Java example:</p><pre>datasetOfString.writeStream().foreach(<span class="kw">new</span> ForeachWriter&lt;<span class="std">String</span>&gt;() {

  @Override
  public boolean open(long partitionId, long version) {
    <span class="cmt">// open connection</span>
  }

  @Override
  public void process(<span class="std">String</span> value) {
    <span class="cmt">// write string to connection</span>
  }

  @Override
  public void close(Throwable errorOrNull) {
    <span class="cmt">// close the connection</span>
  }
});</pre></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Evolving</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.KeyValueGroupedDataset" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="KeyValueGroupedDataset[K,V]extendsSerializable"></a>
      <a id="KeyValueGroupedDataset[K,V]:KeyValueGroupedDataset[K,V]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="KeyValueGroupedDataset.html"><span class="name">KeyValueGroupedDataset</span></a><span class="tparams">[<span name="K">K</span>, <span name="V">V</span>]</span><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@KeyValueGroupedDataset[K,V]extendsSerializable" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">:: Experimental ::
A <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a> has been logically grouped by a user specified grouping key.</p><div class="fullcomment"><div class="comment cmt"><p>:: Experimental ::
A <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a> has been logically grouped by a user specified grouping key.  Users should not
construct a <a href="KeyValueGroupedDataset.html" class="extype" name="org.apache.spark.sql.KeyValueGroupedDataset">KeyValueGroupedDataset</a> directly, but should instead call <code>groupByKey</code> on
an existing <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a>.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Experimental</span><span class="args">()</span>
              
                <span class="name">@Evolving</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.LowPrioritySQLImplicits" visbl="pub" data-isabs="true" fullComment="yes" group="Ungrouped">
      <a id="LowPrioritySQLImplicitsextendsAnyRef"></a>
      <a id="LowPrioritySQLImplicits:LowPrioritySQLImplicits"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a href="LowPrioritySQLImplicits.html"><span class="name">LowPrioritySQLImplicits</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@LowPrioritySQLImplicitsextendsAnyRef" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Lower priority implicit methods for converting Scala objects into <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a>s.</p><div class="fullcomment"><div class="comment cmt"><p>Lower priority implicit methods for converting Scala objects into <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a>s.
Conflicting implicits are placed here to disambiguate resolution.</p><p>Reasons for including specific implicits:
newProductEncoder - to disambiguate for <code>List</code>s which are both <code>Seq</code> and <code>Product</code>
</p></div></div>
    </li><li name="org.apache.spark.sql.RelationalGroupedDataset" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="RelationalGroupedDatasetextendsAnyRef"></a>
      <a id="RelationalGroupedDataset:RelationalGroupedDataset"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="RelationalGroupedDataset.html"><span class="name">RelationalGroupedDataset</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@RelationalGroupedDatasetextendsAnyRef" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A set of methods for aggregations on a <code>DataFrame</code>, created by <a href="Dataset.html#groupBy(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" class="extmbr" name="org.apache.spark.sql.Dataset#groupBy">groupBy</a>,
<a href="Dataset.html#cube(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" class="extmbr" name="org.apache.spark.sql.Dataset#cube">cube</a> or <a href="Dataset.html#rollup(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" class="extmbr" name="org.apache.spark.sql.Dataset#rollup">rollup</a> (and also <code>pivot</code>).</p><div class="fullcomment"><div class="comment cmt"><p>A set of methods for aggregations on a <code>DataFrame</code>, created by <a href="Dataset.html#groupBy(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" class="extmbr" name="org.apache.spark.sql.Dataset#groupBy">groupBy</a>,
<a href="Dataset.html#cube(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" class="extmbr" name="org.apache.spark.sql.Dataset#cube">cube</a> or <a href="Dataset.html#rollup(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset" class="extmbr" name="org.apache.spark.sql.Dataset#rollup">rollup</a> (and also <code>pivot</code>).</p><p>The main method is the <code>agg</code> function, which has multiple variants. This class also contains
some first-order statistics such as <code>mean</code>, <code>sum</code> for convenience.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>This class was named <code>GroupedData</code> in Spark 1.x.</p></span></dd></dl></div>
    </li><li name="org.apache.spark.sql.Row" visbl="pub" data-isabs="true" fullComment="yes" group="Ungrouped">
      <a id="RowextendsSerializable"></a>
      <a id="Row:Row"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a href="Row.html"><span class="name">Row</span></a><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@RowextendsSerializable" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Represents one row of output from a relational operator.</p><div class="fullcomment"><div class="comment cmt"><p>Represents one row of output from a relational operator.  Allows both generic access by ordinal,
which will incur boxing overhead for primitives, as well as native primitive access.</p><p>It is invalid to use the native primitive interface to retrieve a value that is null, instead a
user must check <code>isNullAt</code> before attempting to retrieve a value that might be null.</p><p>To create a new Row, use <code>RowFactory.create()</code> in Java or <code>Row.apply()</code> in Scala.</p><p>A <a href="Row.html" class="extype" name="org.apache.spark.sql.Row">Row</a> object can be constructed by providing field values. Example:</p><pre><span class="kw">import</span> org.apache.spark.sql._

<span class="cmt">// Create a Row from values.</span>
Row(value1, value2, value3, ...)
<span class="cmt">// Create a Row from a Seq of values.</span>
Row.fromSeq(<span class="std">Seq</span>(value1, value2, ...))</pre><p>A value of a row can be accessed through both generic access by ordinal,
which will incur boxing overhead for primitives, as well as native primitive access.
An example of generic access by ordinal:</p><pre><span class="kw">import</span> org.apache.spark.sql._

<span class="kw">val</span> row = Row(<span class="num">1</span>, <span class="kw">true</span>, <span class="lit">"a string"</span>, <span class="kw">null</span>)
<span class="cmt">// row: Row = [1,true,a string,null]</span>
<span class="kw">val</span> firstValue = row(<span class="num">0</span>)
<span class="cmt">// firstValue: Any = 1</span>
<span class="kw">val</span> fourthValue = row(<span class="num">3</span>)
<span class="cmt">// fourthValue: Any = null</span></pre><p>For native primitive access, it is invalid to use the native primitive interface to retrieve
a value that is null, instead a user must check <code>isNullAt</code> before attempting to retrieve a
value that might be null.
An example of native primitive access:</p><pre><span class="cmt">// using the row from the previous example.</span>
<span class="kw">val</span> firstValue = row.getInt(<span class="num">0</span>)
<span class="cmt">// firstValue: Int = 1</span>
<span class="kw">val</span> isNull = row.isNullAt(<span class="num">3</span>)
<span class="cmt">// isNull: Boolean = true</span></pre><p>In Scala, fields in a <a href="Row.html" class="extype" name="org.apache.spark.sql.Row">Row</a> object can be extracted in a pattern match. Example:</p><pre><span class="kw">import</span> org.apache.spark.sql._

<span class="kw">val</span> pairs = sql(<span class="lit">"SELECT key, value FROM src"</span>).rdd.map {
  <span class="kw">case</span> Row(key: <span class="std">Int</span>, value: <span class="std">String</span>) <span class="kw">=&gt;</span>
    key -&gt; value
}</pre></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.3.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.RowFactory" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="RowFactoryextendsObject"></a>
      <a id="RowFactory:RowFactory"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="RowFactory.html"><span class="name">RowFactory</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@RowFactoryextendsObject" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.apache.spark.sql.RuntimeConfig" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="RuntimeConfigextendsAnyRef"></a>
      <a id="RuntimeConfig:RuntimeConfig"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="RuntimeConfig.html"><span class="name">RuntimeConfig</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@RuntimeConfigextendsAnyRef" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Runtime configuration interface for Spark.</p><div class="fullcomment"><div class="comment cmt"><p>Runtime configuration interface for Spark. To access this, use <code>SparkSession.conf</code>.</p><p>Options set here are automatically propagated to the Hadoop configuration during I/O.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.SQLContext" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SQLContextextendsLoggingwithSerializable"></a>
      <a id="SQLContext:SQLContext"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="SQLContext.html"><span class="name">SQLContext</span></a><span class="result"> extends <a href="../internal/Logging.html" class="extype" name="org.apache.spark.internal.Logging">Logging</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@SQLContextextendsLoggingwithSerializable" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">The entry point for working with structured data (rows and columns) in Spark 1.x.</p><div class="fullcomment"><div class="comment cmt"><p>The entry point for working with structured data (rows and columns) in Spark 1.x.</p><p>As of Spark 2.0, this is replaced by <a href="SparkSession.html" class="extype" name="org.apache.spark.sql.SparkSession">SparkSession</a>. However, we are keeping the class
here for backward compatibility.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.0.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.SQLImplicits" visbl="pub" data-isabs="true" fullComment="yes" group="Ungrouped">
      <a id="SQLImplicitsextendsLowPrioritySQLImplicits"></a>
      <a id="SQLImplicits:SQLImplicits"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">abstract </span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="SQLImplicits.html"><span class="name">SQLImplicits</span></a><span class="result"> extends <a href="LowPrioritySQLImplicits.html" class="extype" name="org.apache.spark.sql.LowPrioritySQLImplicits">LowPrioritySQLImplicits</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@SQLImplicitsextendsLowPrioritySQLImplicits" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A collection of implicit methods for converting common Scala objects into <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a>s.</p><div class="fullcomment"><div class="comment cmt"><p>A collection of implicit methods for converting common Scala objects into <a href="Dataset.html" class="extype" name="org.apache.spark.sql.Dataset">Dataset</a>s.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Evolving</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.SaveMode" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="SaveModeextendsEnum[org.apache.spark.sql.SaveMode]"></a>
      <a id="SaveMode:SaveMode"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="SaveMode.html"><span class="name">SaveMode</span></a><span class="result"> extends <span class="extype" name="java.lang.Enum">Enum</span>[<a href="SaveMode.html" class="extype" name="org.apache.spark.sql.SaveMode">SaveMode</a>]</span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@SaveModeextendsEnum[org.apache.spark.sql.SaveMode]" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.apache.spark.sql.SparkSession" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SparkSessionextendsSerializablewithCloseablewithLogging"></a>
      <a id="SparkSession:SparkSession"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="SparkSession.html"><span class="name">SparkSession</span></a><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span> with <span class="extype" name="java.io.Closeable">Closeable</span> with <a href="../internal/Logging.html" class="extype" name="org.apache.spark.internal.Logging">Logging</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@SparkSessionextendsSerializablewithCloseablewithLogging" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">The entry point to programming Spark with the Dataset and DataFrame API.</p><div class="fullcomment"><div class="comment cmt"><p>The entry point to programming Spark with the Dataset and DataFrame API.</p><p>In environments that this has been created upfront (e.g. REPL, notebooks), use the builder
to get an existing session:</p><pre>SparkSession.builder().getOrCreate()</pre><p>The builder can also be used to create a new session:</p><pre>SparkSession.builder
  .master(<span class="lit">"local"</span>)
  .appName(<span class="lit">"Word Count"</span>)
  .config(<span class="lit">"spark.some.config.option"</span>, <span class="lit">"some-value"</span>)
  .getOrCreate()</pre></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd></dl></div>
    </li><li name="org.apache.spark.sql.SparkSessionExtensions" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SparkSessionExtensionsextendsAnyRef"></a>
      <a id="SparkSessionExtensions:SparkSessionExtensions"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="SparkSessionExtensions.html"><span class="name">SparkSessionExtensions</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@SparkSessionExtensionsextendsAnyRef" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">:: Experimental ::
Holder for injection points to the <a href="SparkSession.html" class="extype" name="org.apache.spark.sql.SparkSession">SparkSession</a>.</p><div class="fullcomment"><div class="comment cmt"><p>:: Experimental ::
Holder for injection points to the <a href="SparkSession.html" class="extype" name="org.apache.spark.sql.SparkSession">SparkSession</a>. We make NO guarantee about the stability
regarding binary compatibility and source compatibility of methods here.</p><p>This current provides the following extension points:</p><ul><li>Analyzer Rules.</li><li>Check Analysis Rules.</li><li>Optimizer Rules.</li><li>Planning Strategies.</li><li>Customized Parser.</li><li>(External) Catalog listeners.</li></ul><p>The extensions can be used by calling withExtension on the <a href="SparkSession$$Builder.html" class="extype" name="org.apache.spark.sql.SparkSession.Builder">SparkSession.Builder</a>, for
example:</p><pre>SparkSession.builder()
  .master(<span class="lit">"..."</span>)
  .conf(<span class="lit">"..."</span>, <span class="kw">true</span>)
  .withExtensions { extensions <span class="kw">=&gt;</span>
    extensions.injectResolutionRule { session <span class="kw">=&gt;</span>
      ...
    }
    extensions.injectParser { (session, parser) <span class="kw">=&gt;</span>
      ...
    }
  }
  .getOrCreate()</pre><p>Note that none of the injected builders should assume that the <a href="SparkSession.html" class="extype" name="org.apache.spark.sql.SparkSession">SparkSession</a> is fully
initialized and should not touch the session's internals (e.g. the SessionState).
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@DeveloperApi</span><span class="args">()</span>
              
                <span class="name">@Experimental</span><span class="args">()</span>
              
                <span class="name">@Unstable</span><span class="args">()</span>
              
        </dd></dl></div>
    </li><li name="org.apache.spark.sql.Strategy" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Strategy=org.apache.spark.sql.execution.SparkStrategy"></a>
      <a id="Strategy:Strategy"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">type</span>
      </span>
      <span class="symbol">
        <span class="name">Strategy</span><span class="result"> = <span class="extype" name="org.apache.spark.sql.execution.SparkStrategy">SparkStrategy</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@Strategy=org.apache.spark.sql.execution.SparkStrategy" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Converts a logical plan into zero or more SparkPlans.</p><div class="fullcomment"><div class="comment cmt"><p>Converts a logical plan into zero or more SparkPlans.  This API is exposed for experimenting
with the query planner and is not designed to be stable across spark releases.  Developers
writing libraries should instead consider using the stable APIs provided in
<a href="sources/package.html" class="extype" name="org.apache.spark.sql.sources">org.apache.spark.sql.sources</a>
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@DeveloperApi</span><span class="args">()</span>
              
                <span class="name">@Unstable</span><span class="args">()</span>
              
        </dd></dl></div>
    </li><li name="org.apache.spark.sql.TypedColumn" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="TypedColumn[-T,U]extendsColumn"></a>
      <a id="TypedColumn[-T,U]:TypedColumn[T,U]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="TypedColumn.html"><span class="name">TypedColumn</span></a><span class="tparams">[<span name="T">-T</span>, <span name="U">U</span>]</span><span class="result"> extends <a href="Column.html" class="extype" name="org.apache.spark.sql.Column">Column</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@TypedColumn[-T,U]extendsColumn" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A <a href="Column.html" class="extype" name="org.apache.spark.sql.Column">Column</a> where an <a href="Encoder.html" class="extype" name="org.apache.spark.sql.Encoder">Encoder</a> has been given for the expected input and return type.</p><div class="fullcomment"><div class="comment cmt"><p>A <a href="Column.html" class="extype" name="org.apache.spark.sql.Column">Column</a> where an <a href="Encoder.html" class="extype" name="org.apache.spark.sql.Encoder">Encoder</a> has been given for the expected input and return type.
To create a <a href="TypedColumn.html" class="extype" name="org.apache.spark.sql.TypedColumn">TypedColumn</a>, use the <code>as</code> function on a <a href="Column.html" class="extype" name="org.apache.spark.sql.Column">Column</a>.
</p></div><dl class="paramcmts block"><dt class="tparam">T</dt><dd class="cmt"><p>The input type expected for this expression.  Can be <code>Any</code> if the expression is type
          checked by the analyzer instead of the compiler (i.e. <code>expr(&quot;sum(...)&quot;)</code>).</p></dd><dt class="tparam">U</dt><dd class="cmt"><p>The output type of this column.</p></dd></dl><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.UDFRegistration" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="UDFRegistrationextendsLogging"></a>
      <a id="UDFRegistration:UDFRegistration"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="UDFRegistration.html"><span class="name">UDFRegistration</span></a><span class="result"> extends <a href="../internal/Logging.html" class="extype" name="org.apache.spark.internal.Logging">Logging</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@UDFRegistrationextendsLogging" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Functions for registering user-defined functions.</p><div class="fullcomment"><div class="comment cmt"><p>Functions for registering user-defined functions. Use <code>SparkSession.udf</code> to access this:</p><pre>spark.udf</pre></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.3.0</p></dd></dl></div>
    </li></ol>
            </div>

        

        <div id="values" class="values members">
              <h3>Value Members</h3>
              <ol><li name="org.apache.spark.sql.Encoders" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Encoders"></a>
      <a id="Encoders:Encoders"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="Encoders$.html"><span class="name">Encoders</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@Encoders" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">:: Experimental ::
Methods for creating an <a href="Encoder.html" class="extype" name="org.apache.spark.sql.Encoder">Encoder</a>.</p><div class="fullcomment"><div class="comment cmt"><p>:: Experimental ::
Methods for creating an <a href="Encoder.html" class="extype" name="org.apache.spark.sql.Encoder">Encoder</a>.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Experimental</span><span class="args">()</span>
              
                <span class="name">@Evolving</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.Row" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Row"></a>
      <a id="Row:Row"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="Row$.html"><span class="name">Row</span></a><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@Row" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt"></p><div class="fullcomment"><div class="comment cmt"></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.3.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.SQLContext" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SQLContext"></a>
      <a id="SQLContext:SQLContext"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="SQLContext$.html"><span class="name">SQLContext</span></a><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@SQLContext" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">This SQLContext object contains utility functions to create a singleton SQLContext instance,
or to get the created SQLContext instance.</p><div class="fullcomment"><div class="comment cmt"><p>This SQLContext object contains utility functions to create a singleton SQLContext instance,
or to get the created SQLContext instance.</p><p>It also provides utility functions to support preference for threads in multiple sessions
scenario, setActive could set a SQLContext for current thread, which will be returned by
getOrCreate instead of the global one.
</p></div></div>
    </li><li name="org.apache.spark.sql.SparkSession" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SparkSession"></a>
      <a id="SparkSession:SparkSession"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="SparkSession$.html"><span class="name">SparkSession</span></a><span class="result"> extends <a href="../internal/Logging.html" class="extype" name="org.apache.spark.internal.Logging">Logging</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@SparkSession" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <div class="fullcomment"><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd></dl></div>
    </li><li name="org.apache.spark.sql.api" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="api"></a>
      <a id="api:api"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="api/package.html"><span class="name">api</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@api" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Contains API classes that are specific to a single language (i.e.</p><div class="fullcomment"><div class="comment cmt"><p>Contains API classes that are specific to a single language (i.e. Java).
</p></div></div>
    </li><li name="org.apache.spark.sql.catalog" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="catalog"></a>
      <a id="catalog:catalog"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="catalog/package.html"><span class="name">catalog</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@catalog" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.apache.spark.sql.expressions" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="expressions"></a>
      <a id="expressions:expressions"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="expressions/package.html"><span class="name">expressions</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@expressions" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.apache.spark.sql.functions" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="functions"></a>
      <a id="functions:functions"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="functions$.html"><span class="name">functions</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@functions" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Commonly used functions available for DataFrame operations.</p><div class="fullcomment"><div class="comment cmt"><p>Commonly used functions available for DataFrame operations. Using functions defined here provides
a little bit more compile-time safety to make sure the function exists.</p><p>Spark also includes more built-in functions that are less common and are not defined here.
You can still access them (and all the functions defined here) using the <code>functions.expr()</code> API
and calling them through a SQL expression string. You can find the entire list of functions
at SQL API documentation.</p><p>As an example, <code>isnan</code> is a function that is defined here. You can use <code>isnan(col(&quot;myCol&quot;))</code>
to invoke the <code>isnan</code> function. This way the programming language's compiler ensures <code>isnan</code>
exists and is of the proper form. You can also use <code>expr(&quot;isnan(myCol)&quot;)</code> function to invoke the
same function. In this case, Spark itself will ensure <code>isnan</code> exists when it analyzes the query.</p><p><code>regr_count</code> is an example of a function that is built-in but not defined here, because it is
less commonly used. To invoke it, use <code>expr(&quot;regr_count(yCol, xCol)&quot;)</code>.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Stable</span><span class="args">()</span>
              
        </dd><dt>Since</dt><dd><p>1.3.0</p></dd></dl></div>
    </li><li name="org.apache.spark.sql.hive" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="hive"></a>
      <a id="hive:hive"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="hive/package.html"><span class="name">hive</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@hive" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Support for running Spark SQL queries using functionality from Apache Hive (does not require an
existing Hive installation).</p><div class="fullcomment"><div class="comment cmt"><p>Support for running Spark SQL queries using functionality from Apache Hive (does not require an
existing Hive installation).  Supported Hive features include:</p><ul><li>Using HiveQL to express queries.</li><li>Reading metadata from the Hive Metastore using HiveSerDes.</li><li>Hive UDFs, UDAs, UDTs</li></ul><p>Users that would like access to this functionality should create a
<a href="hive/HiveContext.html" class="extype" name="org.apache.spark.sql.hive.HiveContext">HiveContext</a> instead of a <a href="SQLContext.html" class="extype" name="org.apache.spark.sql.SQLContext">SQLContext</a>.
</p></div></div>
    </li><li name="org.apache.spark.sql.jdbc" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="jdbc"></a>
      <a id="jdbc:jdbc"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="jdbc/package.html"><span class="name">jdbc</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@jdbc" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.apache.spark.sql.sources" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="sources"></a>
      <a id="sources:sources"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="sources/package.html"><span class="name">sources</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@sources" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">A set of APIs for adding data sources to Spark SQL.</p>
    </li><li name="org.apache.spark.sql.streaming" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="streaming"></a>
      <a id="streaming:streaming"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="streaming/package.html"><span class="name">streaming</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@streaming" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.apache.spark.sql.types" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="types"></a>
      <a id="types:types"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="types/package.html"><span class="name">types</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@types" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Contains a type system for attributes produced by relations, including complex types like
structs, arrays and maps.</p>
    </li><li name="org.apache.spark.sql.util" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="util"></a>
      <a id="util:util"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="util/package.html"><span class="name">util</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@util" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.apache.spark.sql.vectorized" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="vectorized"></a>
      <a id="vectorized:vectorized"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="vectorized/package.html"><span class="name">vectorized</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../index.html#org.apache.spark.sql.package@vectorized" title="Permalink" target="_top">
        <img src="../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li></ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        <div class="parent" name="scala.AnyRef">
              <h3>Inherited from <span class="extype" name="scala.AnyRef">AnyRef</span></h3>
            </div><div class="parent" name="scala.Any">
              <h3>Inherited from <span class="extype" name="scala.Any">Any</span></h3>
            </div>
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>


    </body>
      </html>
