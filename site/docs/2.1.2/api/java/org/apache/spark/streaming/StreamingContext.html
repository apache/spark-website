<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (1.7.0_151) on Mon Oct 02 14:47:23 PDT 2017 -->
<title>StreamingContext (Spark 2.1.2 JavaDoc)</title>
<meta name="date" content="2017-10-02">
<link rel="stylesheet" type="text/css" href="../../../../stylesheet.css" title="Style">
</head>
<body>
<script type="text/javascript"><!--
    try {
        if (location.href.indexOf('is-external=true') == -1) {
            parent.document.title="StreamingContext (Spark 2.1.2 JavaDoc)";
        }
    }
    catch(err) {
    }
//-->
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar_top">
<!--   -->
</a><a href="#skip-navbar_top" title="Skip navigation links"></a><a name="navbar_top_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/streaming/StateSpec.html" title="class in org.apache.spark.streaming"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../org/apache/spark/streaming/StreamingContextPythonHelper.html" title="class in org.apache.spark.streaming"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/streaming/StreamingContext.html" target="_top">Frames</a></li>
<li><a href="StreamingContext.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.apache.spark.streaming</div>
<h2 title="Class StreamingContext" class="title">Class StreamingContext</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li>Object</li>
<li>
<ul class="inheritance">
<li>org.apache.spark.streaming.StreamingContext</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<hr>
<br>
<pre>public class <span class="strong">StreamingContext</span>
extends Object</pre>
<div class="block">Main entry point for Spark Streaming functionality. It provides methods used to create
 <a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream"><code>DStream</code></a>s from various input sources. It can be either
 created by providing a Spark master URL and an appName, or from a org.apache.spark.SparkConf
 configuration (see core Spark documentation), or from an existing org.apache.spark.SparkContext.
 The associated SparkContext can be accessed using <code>context.sparkContext</code>. After
 creating and transforming DStreams, the streaming computation can be started and stopped
 using <code>context.start()</code> and <code>context.stop()</code>, respectively.
 <code>context.awaitTermination()</code> allows the current thread to wait for the termination
 of the context by <code>stop()</code> or by an exception.</div>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_summary">
<!--   -->
</a>
<h3>Constructor Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Constructor Summary table, listing constructors, and an explanation">
<caption><span>Constructors</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Constructor and Description</th>
</tr>
<tr class="altColor">
<td class="colOne"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#StreamingContext(org.apache.spark.SparkConf,%20org.apache.spark.streaming.Duration)">StreamingContext</a></strong>(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
                <a href="../../../../org/apache/spark/streaming/Duration.html" title="class in org.apache.spark.streaming">Duration</a>&nbsp;batchDuration)</code>
<div class="block">Create a StreamingContext by providing the configuration necessary for a new SparkContext.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#StreamingContext(org.apache.spark.SparkContext,%20org.apache.spark.streaming.Duration)">StreamingContext</a></strong>(<a href="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</a>&nbsp;sparkContext,
                <a href="../../../../org/apache/spark/streaming/Duration.html" title="class in org.apache.spark.streaming">Duration</a>&nbsp;batchDuration)</code>
<div class="block">Create a StreamingContext using an existing SparkContext.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#StreamingContext(java.lang.String)">StreamingContext</a></strong>(String&nbsp;path)</code>
<div class="block">Recreate a StreamingContext from a checkpoint file.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#StreamingContext(java.lang.String,%20org.apache.hadoop.conf.Configuration)">StreamingContext</a></strong>(String&nbsp;path,
                org.apache.hadoop.conf.Configuration&nbsp;hadoopConf)</code>
<div class="block">Recreate a StreamingContext from a checkpoint file.</div>
</td>
</tr>
<tr class="altColor">
<td class="colOne"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#StreamingContext(java.lang.String,%20org.apache.spark.SparkContext)">StreamingContext</a></strong>(String&nbsp;path,
                <a href="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</a>&nbsp;sparkContext)</code>
<div class="block">Recreate a StreamingContext from a checkpoint file using an existing SparkContext.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#StreamingContext(java.lang.String,%20java.lang.String,%20org.apache.spark.streaming.Duration,%20java.lang.String,%20scala.collection.Seq,%20scala.collection.Map)">StreamingContext</a></strong>(String&nbsp;master,
                String&nbsp;appName,
                <a href="../../../../org/apache/spark/streaming/Duration.html" title="class in org.apache.spark.streaming">Duration</a>&nbsp;batchDuration,
                String&nbsp;sparkHome,
                scala.collection.Seq&lt;String&gt;&nbsp;jars,
                scala.collection.Map&lt;String,String&gt;&nbsp;environment)</code>
<div class="block">Create a StreamingContext by providing the details necessary for creating a new SparkContext.</div>
</td>
</tr>
</table>
</li>
</ul>
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method_summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span>Methods</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#addStreamingListener(org.apache.spark.streaming.scheduler.StreamingListener)">addStreamingListener</a></strong>(<a href="../../../../org/apache/spark/streaming/scheduler/StreamingListener.html" title="interface in org.apache.spark.streaming.scheduler">StreamingListener</a>&nbsp;streamingListener)</code>
<div class="block">Add a <a href="../../../../org/apache/spark/streaming/scheduler/StreamingListener.html" title="interface in org.apache.spark.streaming.scheduler"><code>StreamingListener</code></a> object for
 receiving system events related to streaming.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#awaitTermination()">awaitTermination</a></strong>()</code>
<div class="block">Wait for the execution to stop.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#awaitTerminationOrTimeout(long)">awaitTerminationOrTimeout</a></strong>(long&nbsp;timeout)</code>
<div class="block">Wait for the execution to stop.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;byte[]&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#binaryRecordsStream(java.lang.String,%20int)">binaryRecordsStream</a></strong>(String&nbsp;directory,
                   int&nbsp;recordLength)</code>
<div class="block">Create an input stream that monitors a Hadoop-compatible filesystem
 for new files and reads them as flat binary files, assuming a fixed length per record,
 generating one byte array per record.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#checkpoint(java.lang.String)">checkpoint</a></strong>(String&nbsp;directory)</code>
<div class="block">Set the context to periodically checkpoint the DStream operations for driver
 fault-tolerance.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>&lt;K,V,F extends org.apache.hadoop.mapreduce.InputFormat&lt;K,V&gt;&gt;&nbsp;<br><a href="../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</a>&lt;scala.Tuple2&lt;K,V&gt;&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#fileStream(java.lang.String,%20scala.reflect.ClassTag,%20scala.reflect.ClassTag,%20scala.reflect.ClassTag)">fileStream</a></strong>(String&nbsp;directory,
          scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$4,
          scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$5,
          scala.reflect.ClassTag&lt;F&gt;&nbsp;evidence$6)</code>
<div class="block">Create an input stream that monitors a Hadoop-compatible filesystem
 for new files and reads them using the given key-value types and input format.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>&lt;K,V,F extends org.apache.hadoop.mapreduce.InputFormat&lt;K,V&gt;&gt;&nbsp;<br><a href="../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</a>&lt;scala.Tuple2&lt;K,V&gt;&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#fileStream(java.lang.String,%20scala.Function1,%20boolean,%20scala.reflect.ClassTag,%20scala.reflect.ClassTag,%20scala.reflect.ClassTag)">fileStream</a></strong>(String&nbsp;directory,
          scala.Function1&lt;org.apache.hadoop.fs.Path,Object&gt;&nbsp;filter,
          boolean&nbsp;newFilesOnly,
          scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$7,
          scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$8,
          scala.reflect.ClassTag&lt;F&gt;&nbsp;evidence$9)</code>
<div class="block">Create an input stream that monitors a Hadoop-compatible filesystem
 for new files and reads them using the given key-value types and input format.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>&lt;K,V,F extends org.apache.hadoop.mapreduce.InputFormat&lt;K,V&gt;&gt;&nbsp;<br><a href="../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</a>&lt;scala.Tuple2&lt;K,V&gt;&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#fileStream(java.lang.String,%20scala.Function1,%20boolean,%20org.apache.hadoop.conf.Configuration,%20scala.reflect.ClassTag,%20scala.reflect.ClassTag,%20scala.reflect.ClassTag)">fileStream</a></strong>(String&nbsp;directory,
          scala.Function1&lt;org.apache.hadoop.fs.Path,Object&gt;&nbsp;filter,
          boolean&nbsp;newFilesOnly,
          org.apache.hadoop.conf.Configuration&nbsp;conf,
          scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$10,
          scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$11,
          scala.reflect.ClassTag&lt;F&gt;&nbsp;evidence$12)</code>
<div class="block">Create an input stream that monitors a Hadoop-compatible filesystem
 for new files and reads them using the given key-value types and input format.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static scala.Option&lt;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#getActive()">getActive</a></strong>()</code>
<div class="block">:: Experimental ::</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static <a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#getActiveOrCreate(scala.Function0)">getActiveOrCreate</a></strong>(scala.Function0&lt;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&gt;&nbsp;creatingFunc)</code>
<div class="block">:: Experimental ::</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static <a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#getActiveOrCreate(java.lang.String,%20scala.Function0,%20org.apache.hadoop.conf.Configuration,%20boolean)">getActiveOrCreate</a></strong>(String&nbsp;checkpointPath,
                 scala.Function0&lt;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&gt;&nbsp;creatingFunc,
                 org.apache.hadoop.conf.Configuration&nbsp;hadoopConf,
                 boolean&nbsp;createOnError)</code>
<div class="block">:: Experimental ::</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static <a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#getOrCreate(java.lang.String,%20scala.Function0,%20org.apache.hadoop.conf.Configuration,%20boolean)">getOrCreate</a></strong>(String&nbsp;checkpointPath,
           scala.Function0&lt;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&gt;&nbsp;creatingFunc,
           org.apache.hadoop.conf.Configuration&nbsp;hadoopConf,
           boolean&nbsp;createOnError)</code>
<div class="block">Either recreate a StreamingContext from checkpoint data or create a new StreamingContext.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/streaming/StreamingContextState.html" title="enum in org.apache.spark.streaming">StreamingContextState</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#getState()">getState</a></strong>()</code>
<div class="block">:: DeveloperApi ::</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static scala.Option&lt;String&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#jarOfClass(java.lang.Class)">jarOfClass</a></strong>(Class&lt;?&gt;&nbsp;cls)</code>
<div class="block">Find the JAR from which a given class was loaded, to make it easy for users to pass
 their JARs to StreamingContext.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</a>&lt;T&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#queueStream(scala.collection.mutable.Queue,%20boolean,%20scala.reflect.ClassTag)">queueStream</a></strong>(scala.collection.mutable.Queue&lt;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;T&gt;&gt;&nbsp;queue,
           boolean&nbsp;oneAtATime,
           scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$13)</code>
<div class="block">Create an input stream from a queue of RDDs.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</a>&lt;T&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#queueStream(scala.collection.mutable.Queue,%20boolean,%20org.apache.spark.rdd.RDD,%20scala.reflect.ClassTag)">queueStream</a></strong>(scala.collection.mutable.Queue&lt;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;T&gt;&gt;&nbsp;queue,
           boolean&nbsp;oneAtATime,
           <a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;T&gt;&nbsp;defaultRDD,
           scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$14)</code>
<div class="block">Create an input stream from a queue of RDDs.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</a>&lt;T&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#rawSocketStream(java.lang.String,%20int,%20org.apache.spark.storage.StorageLevel,%20scala.reflect.ClassTag)">rawSocketStream</a></strong>(String&nbsp;hostname,
               int&nbsp;port,
               <a href="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</a>&nbsp;storageLevel,
               scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$3)</code>
<div class="block">Create an input stream from network source hostname:port, where data is received
 as serialized blocks (serialized using the Spark's serializer) that can be directly
 pushed into the block manager without deserializing them.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</a>&lt;T&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#receiverStream(org.apache.spark.streaming.receiver.Receiver,%20scala.reflect.ClassTag)">receiverStream</a></strong>(<a href="../../../../org/apache/spark/streaming/receiver/Receiver.html" title="class in org.apache.spark.streaming.receiver">Receiver</a>&lt;T&gt;&nbsp;receiver,
              scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$1)</code>
<div class="block">Create an input stream with any arbitrary user implemented receiver.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#remember(org.apache.spark.streaming.Duration)">remember</a></strong>(<a href="../../../../org/apache/spark/streaming/Duration.html" title="class in org.apache.spark.streaming">Duration</a>&nbsp;duration)</code>
<div class="block">Set each DStream in this context to remember RDDs it generated in the last given duration.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</a>&lt;T&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#socketStream(java.lang.String,%20int,%20scala.Function1,%20org.apache.spark.storage.StorageLevel,%20scala.reflect.ClassTag)">socketStream</a></strong>(String&nbsp;hostname,
            int&nbsp;port,
            scala.Function1&lt;java.io.InputStream,scala.collection.Iterator&lt;T&gt;&gt;&nbsp;converter,
            <a href="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</a>&nbsp;storageLevel,
            scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$2)</code>
<div class="block">Creates an input stream from TCP source hostname:port.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</a>&lt;String&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#socketTextStream(java.lang.String,%20int,%20org.apache.spark.storage.StorageLevel)">socketTextStream</a></strong>(String&nbsp;hostname,
                int&nbsp;port,
                <a href="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</a>&nbsp;storageLevel)</code>
<div class="block">Creates an input stream from TCP source hostname:port.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#sparkContext()">sparkContext</a></strong>()</code>
<div class="block">Return the associated Spark context</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#start()">start</a></strong>()</code>
<div class="block">Start the execution of the streams.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#stop(boolean)">stop</a></strong>(boolean&nbsp;stopSparkContext)</code>
<div class="block">Stop the execution of the streams immediately (does not wait for all received data
 to be processed).</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#stop(boolean,%20boolean)">stop</a></strong>(boolean&nbsp;stopSparkContext,
    boolean&nbsp;stopGracefully)</code>
<div class="block">Stop the execution of the streams, with option of ensuring all received data
 has been processed.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;String&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#textFileStream(java.lang.String)">textFileStream</a></strong>(String&nbsp;directory)</code>
<div class="block">Create an input stream that monitors a Hadoop-compatible filesystem
 for new files and reads them as text files (using key as LongWritable, value
 as Text and input format as TextInputFormat).</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;T&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#transform(scala.collection.Seq,%20scala.Function2,%20scala.reflect.ClassTag)">transform</a></strong>(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;?&gt;&gt;&nbsp;dstreams,
         scala.Function2&lt;scala.collection.Seq&lt;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;?&gt;&gt;,<a href="../../../../org/apache/spark/streaming/Time.html" title="class in org.apache.spark.streaming">Time</a>,<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;T&gt;&gt;&nbsp;transformFunc,
         scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$16)</code>
<div class="block">Create a new DStream in which each RDD is generated by applying a function on RDDs of
 the DStreams.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;T&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/streaming/StreamingContext.html#union(scala.collection.Seq,%20scala.reflect.ClassTag)">union</a></strong>(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;T&gt;&gt;&nbsp;streams,
     scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$15)</code>
<div class="block">Create a unified DStream from multiple DStreams of the same type and same slide duration.</div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;Object</h3>
<code>equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_detail">
<!--   -->
</a>
<h3>Constructor Detail</h3>
<a name="StreamingContext(org.apache.spark.SparkContext, org.apache.spark.streaming.Duration)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>StreamingContext</h4>
<pre>public&nbsp;StreamingContext(<a href="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</a>&nbsp;sparkContext,
                <a href="../../../../org/apache/spark/streaming/Duration.html" title="class in org.apache.spark.streaming">Duration</a>&nbsp;batchDuration)</pre>
<div class="block">Create a StreamingContext using an existing SparkContext.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>sparkContext</code> - existing SparkContext</dd><dd><code>batchDuration</code> - the time interval at which streaming data will be divided into batches</dd></dl>
</li>
</ul>
<a name="StreamingContext(org.apache.spark.SparkConf, org.apache.spark.streaming.Duration)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>StreamingContext</h4>
<pre>public&nbsp;StreamingContext(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
                <a href="../../../../org/apache/spark/streaming/Duration.html" title="class in org.apache.spark.streaming">Duration</a>&nbsp;batchDuration)</pre>
<div class="block">Create a StreamingContext by providing the configuration necessary for a new SparkContext.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>conf</code> - a org.apache.spark.SparkConf object specifying Spark parameters</dd><dd><code>batchDuration</code> - the time interval at which streaming data will be divided into batches</dd></dl>
</li>
</ul>
<a name="StreamingContext(java.lang.String, java.lang.String, org.apache.spark.streaming.Duration, java.lang.String, scala.collection.Seq, scala.collection.Map)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>StreamingContext</h4>
<pre>public&nbsp;StreamingContext(String&nbsp;master,
                String&nbsp;appName,
                <a href="../../../../org/apache/spark/streaming/Duration.html" title="class in org.apache.spark.streaming">Duration</a>&nbsp;batchDuration,
                String&nbsp;sparkHome,
                scala.collection.Seq&lt;String&gt;&nbsp;jars,
                scala.collection.Map&lt;String,String&gt;&nbsp;environment)</pre>
<div class="block">Create a StreamingContext by providing the details necessary for creating a new SparkContext.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>master</code> - cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).</dd><dd><code>appName</code> - a name for your job, to display on the cluster web UI</dd><dd><code>batchDuration</code> - the time interval at which streaming data will be divided into batches</dd><dd><code>sparkHome</code> - (undocumented)</dd><dd><code>jars</code> - (undocumented)</dd><dd><code>environment</code> - (undocumented)</dd></dl>
</li>
</ul>
<a name="StreamingContext(java.lang.String, org.apache.hadoop.conf.Configuration)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>StreamingContext</h4>
<pre>public&nbsp;StreamingContext(String&nbsp;path,
                org.apache.hadoop.conf.Configuration&nbsp;hadoopConf)</pre>
<div class="block">Recreate a StreamingContext from a checkpoint file.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>path</code> - Path to the directory that was specified as the checkpoint directory</dd><dd><code>hadoopConf</code> - Optional, configuration object if necessary for reading from
                   HDFS compatible filesystems</dd></dl>
</li>
</ul>
<a name="StreamingContext(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>StreamingContext</h4>
<pre>public&nbsp;StreamingContext(String&nbsp;path)</pre>
<div class="block">Recreate a StreamingContext from a checkpoint file.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>path</code> - Path to the directory that was specified as the checkpoint directory</dd></dl>
</li>
</ul>
<a name="StreamingContext(java.lang.String, org.apache.spark.SparkContext)">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>StreamingContext</h4>
<pre>public&nbsp;StreamingContext(String&nbsp;path,
                <a href="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</a>&nbsp;sparkContext)</pre>
<div class="block">Recreate a StreamingContext from a checkpoint file using an existing SparkContext.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>path</code> - Path to the directory that was specified as the checkpoint directory</dd><dd><code>sparkContext</code> - Existing SparkContext</dd></dl>
</li>
</ul>
</li>
</ul>
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method_detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="getActive()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getActive</h4>
<pre>public static&nbsp;scala.Option&lt;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&gt;&nbsp;getActive()</pre>
<div class="block">:: Experimental ::
 <p>
 Get the currently active context, if there is one. Active means started but not stopped.</div>
<dl><dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="getActiveOrCreate(scala.Function0)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getActiveOrCreate</h4>
<pre>public static&nbsp;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&nbsp;getActiveOrCreate(scala.Function0&lt;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&gt;&nbsp;creatingFunc)</pre>
<div class="block">:: Experimental ::
 <p>
 Either return the "active" StreamingContext (that is, started but not stopped), or create a
 new StreamingContext that is</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>creatingFunc</code> - Function to create a new StreamingContext</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="getActiveOrCreate(java.lang.String, scala.Function0, org.apache.hadoop.conf.Configuration, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getActiveOrCreate</h4>
<pre>public static&nbsp;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&nbsp;getActiveOrCreate(String&nbsp;checkpointPath,
                                 scala.Function0&lt;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&gt;&nbsp;creatingFunc,
                                 org.apache.hadoop.conf.Configuration&nbsp;hadoopConf,
                                 boolean&nbsp;createOnError)</pre>
<div class="block">:: Experimental ::
 <p>
 Either get the currently active StreamingContext (that is, started but not stopped),
 OR recreate a StreamingContext from checkpoint data in the given path. If checkpoint data
 does not exist in the provided, then create a new StreamingContext by calling the provided
 <code>creatingFunc</code>.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>checkpointPath</code> - Checkpoint directory used in an earlier StreamingContext program</dd><dd><code>creatingFunc</code> - Function to create a new StreamingContext</dd><dd><code>hadoopConf</code> - Optional Hadoop configuration if necessary for reading from the
                       file system</dd><dd><code>createOnError</code> - Optional, whether to create a new StreamingContext if there is an
                       error in reading checkpoint data. By default, an exception will be
                       thrown on error.</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="getOrCreate(java.lang.String, scala.Function0, org.apache.hadoop.conf.Configuration, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getOrCreate</h4>
<pre>public static&nbsp;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&nbsp;getOrCreate(String&nbsp;checkpointPath,
                           scala.Function0&lt;<a href="../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</a>&gt;&nbsp;creatingFunc,
                           org.apache.hadoop.conf.Configuration&nbsp;hadoopConf,
                           boolean&nbsp;createOnError)</pre>
<div class="block">Either recreate a StreamingContext from checkpoint data or create a new StreamingContext.
 If checkpoint data exists in the provided <code>checkpointPath</code>, then StreamingContext will be
 recreated from the checkpoint data. If the data does not exist, then the StreamingContext
 will be created by called the provided <code>creatingFunc</code>.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>checkpointPath</code> - Checkpoint directory used in an earlier StreamingContext program</dd><dd><code>creatingFunc</code> - Function to create a new StreamingContext</dd><dd><code>hadoopConf</code> - Optional Hadoop configuration if necessary for reading from the
                       file system</dd><dd><code>createOnError</code> - Optional, whether to create a new StreamingContext if there is an
                       error in reading checkpoint data. By default, an exception will be
                       thrown on error.</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="jarOfClass(java.lang.Class)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>jarOfClass</h4>
<pre>public static&nbsp;scala.Option&lt;String&gt;&nbsp;jarOfClass(Class&lt;?&gt;&nbsp;cls)</pre>
<div class="block">Find the JAR from which a given class was loaded, to make it easy for users to pass
 their JARs to StreamingContext.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>cls</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="sparkContext()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sparkContext</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</a>&nbsp;sparkContext()</pre>
<div class="block">Return the associated Spark context</div>
<dl><dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="remember(org.apache.spark.streaming.Duration)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>remember</h4>
<pre>public&nbsp;void&nbsp;remember(<a href="../../../../org/apache/spark/streaming/Duration.html" title="class in org.apache.spark.streaming">Duration</a>&nbsp;duration)</pre>
<div class="block">Set each DStream in this context to remember RDDs it generated in the last given duration.
 DStreams remember RDDs only for a limited duration of time and release them for garbage
 collection. This method allows the developer to specify how long to remember the RDDs (
 if the developer wishes to query old data outside the DStream computation).</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>duration</code> - Minimum duration that each DStream should remember its RDDs</dd></dl>
</li>
</ul>
<a name="checkpoint(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>checkpoint</h4>
<pre>public&nbsp;void&nbsp;checkpoint(String&nbsp;directory)</pre>
<div class="block">Set the context to periodically checkpoint the DStream operations for driver
 fault-tolerance.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>directory</code> - HDFS-compatible directory where the checkpoint data will be reliably stored.
                  Note that this must be a fault-tolerant file system like HDFS.</dd></dl>
</li>
</ul>
<a name="receiverStream(org.apache.spark.streaming.receiver.Receiver, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>receiverStream</h4>
<pre>public&nbsp;&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</a>&lt;T&gt;&nbsp;receiverStream(<a href="../../../../org/apache/spark/streaming/receiver/Receiver.html" title="class in org.apache.spark.streaming.receiver">Receiver</a>&lt;T&gt;&nbsp;receiver,
                                         scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$1)</pre>
<div class="block">Create an input stream with any arbitrary user implemented receiver.
 Find more details at http://spark.apache.org/docs/latest/streaming-custom-receivers.html</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>receiver</code> - Custom implementation of Receiver</dd><dd><code>evidence$1</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="socketTextStream(java.lang.String, int, org.apache.spark.storage.StorageLevel)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>socketTextStream</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</a>&lt;String&gt;&nbsp;socketTextStream(String&nbsp;hostname,
                                            int&nbsp;port,
                                            <a href="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</a>&nbsp;storageLevel)</pre>
<div class="block">Creates an input stream from TCP source hostname:port. Data is received using
 a TCP socket and the receive bytes is interpreted as UTF8 encoded <code>\n</code> delimited
 lines.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>hostname</code> - Hostname to connect to for receiving data</dd><dd><code>port</code> - Port to connect to for receiving data</dd><dd><code>storageLevel</code> - Storage level to use for storing the received objects
                      (default: StorageLevel.MEMORY_AND_DISK_SER_2)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">See Also:</span></dt><dd><code>socketStream</code></dd></dl>
</li>
</ul>
<a name="socketStream(java.lang.String, int, scala.Function1, org.apache.spark.storage.StorageLevel, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>socketStream</h4>
<pre>public&nbsp;&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</a>&lt;T&gt;&nbsp;socketStream(String&nbsp;hostname,
                                       int&nbsp;port,
                                       scala.Function1&lt;java.io.InputStream,scala.collection.Iterator&lt;T&gt;&gt;&nbsp;converter,
                                       <a href="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</a>&nbsp;storageLevel,
                                       scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$2)</pre>
<div class="block">Creates an input stream from TCP source hostname:port. Data is received using
 a TCP socket and the receive bytes it interpreted as object using the given
 converter.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>hostname</code> - Hostname to connect to for receiving data</dd><dd><code>port</code> - Port to connect to for receiving data</dd><dd><code>converter</code> - Function to convert the byte stream to objects</dd><dd><code>storageLevel</code> - Storage level to use for storing the received objects</dd><dd><code>evidence$2</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="rawSocketStream(java.lang.String, int, org.apache.spark.storage.StorageLevel, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>rawSocketStream</h4>
<pre>public&nbsp;&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</a>&lt;T&gt;&nbsp;rawSocketStream(String&nbsp;hostname,
                                          int&nbsp;port,
                                          <a href="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</a>&nbsp;storageLevel,
                                          scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$3)</pre>
<div class="block">Create an input stream from network source hostname:port, where data is received
 as serialized blocks (serialized using the Spark's serializer) that can be directly
 pushed into the block manager without deserializing them. This is the most efficient
 way to receive data.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>hostname</code> - Hostname to connect to for receiving data</dd><dd><code>port</code> - Port to connect to for receiving data</dd><dd><code>storageLevel</code> - Storage level to use for storing the received objects
                      (default: StorageLevel.MEMORY_AND_DISK_SER_2)</dd><dd><code>evidence$3</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="fileStream(java.lang.String, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>fileStream</h4>
<pre>public&nbsp;&lt;K,V,F extends org.apache.hadoop.mapreduce.InputFormat&lt;K,V&gt;&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</a>&lt;scala.Tuple2&lt;K,V&gt;&gt;&nbsp;fileStream(String&nbsp;directory,
                                                                                                      scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$4,
                                                                                                      scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$5,
                                                                                                      scala.reflect.ClassTag&lt;F&gt;&nbsp;evidence$6)</pre>
<div class="block">Create an input stream that monitors a Hadoop-compatible filesystem
 for new files and reads them using the given key-value types and input format.
 Files must be written to the monitored directory by "moving" them from another
 location within the same file system. File names starting with . are ignored.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>directory</code> - HDFS directory to monitor for new file</dd><dd><code>evidence$4</code> - (undocumented)</dd><dd><code>evidence$5</code> - (undocumented)</dd><dd><code>evidence$6</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="fileStream(java.lang.String, scala.Function1, boolean, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>fileStream</h4>
<pre>public&nbsp;&lt;K,V,F extends org.apache.hadoop.mapreduce.InputFormat&lt;K,V&gt;&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</a>&lt;scala.Tuple2&lt;K,V&gt;&gt;&nbsp;fileStream(String&nbsp;directory,
                                                                                                      scala.Function1&lt;org.apache.hadoop.fs.Path,Object&gt;&nbsp;filter,
                                                                                                      boolean&nbsp;newFilesOnly,
                                                                                                      scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$7,
                                                                                                      scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$8,
                                                                                                      scala.reflect.ClassTag&lt;F&gt;&nbsp;evidence$9)</pre>
<div class="block">Create an input stream that monitors a Hadoop-compatible filesystem
 for new files and reads them using the given key-value types and input format.
 Files must be written to the monitored directory by "moving" them from another
 location within the same file system.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>directory</code> - HDFS directory to monitor for new file</dd><dd><code>filter</code> - Function to filter paths to process</dd><dd><code>newFilesOnly</code> - Should process only new files and ignore existing files in the directory</dd><dd><code>evidence$7</code> - (undocumented)</dd><dd><code>evidence$8</code> - (undocumented)</dd><dd><code>evidence$9</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="fileStream(java.lang.String, scala.Function1, boolean, org.apache.hadoop.conf.Configuration, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>fileStream</h4>
<pre>public&nbsp;&lt;K,V,F extends org.apache.hadoop.mapreduce.InputFormat&lt;K,V&gt;&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</a>&lt;scala.Tuple2&lt;K,V&gt;&gt;&nbsp;fileStream(String&nbsp;directory,
                                                                                                      scala.Function1&lt;org.apache.hadoop.fs.Path,Object&gt;&nbsp;filter,
                                                                                                      boolean&nbsp;newFilesOnly,
                                                                                                      org.apache.hadoop.conf.Configuration&nbsp;conf,
                                                                                                      scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$10,
                                                                                                      scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$11,
                                                                                                      scala.reflect.ClassTag&lt;F&gt;&nbsp;evidence$12)</pre>
<div class="block">Create an input stream that monitors a Hadoop-compatible filesystem
 for new files and reads them using the given key-value types and input format.
 Files must be written to the monitored directory by "moving" them from another
 location within the same file system. File names starting with . are ignored.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>directory</code> - HDFS directory to monitor for new file</dd><dd><code>filter</code> - Function to filter paths to process</dd><dd><code>newFilesOnly</code> - Should process only new files and ignore existing files in the directory</dd><dd><code>conf</code> - Hadoop configuration</dd><dd><code>evidence$10</code> - (undocumented)</dd><dd><code>evidence$11</code> - (undocumented)</dd><dd><code>evidence$12</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="textFileStream(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>textFileStream</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;String&gt;&nbsp;textFileStream(String&nbsp;directory)</pre>
<div class="block">Create an input stream that monitors a Hadoop-compatible filesystem
 for new files and reads them as text files (using key as LongWritable, value
 as Text and input format as TextInputFormat). Files must be written to the
 monitored directory by "moving" them from another location within the same
 file system. File names starting with . are ignored.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>directory</code> - HDFS directory to monitor for new file</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="binaryRecordsStream(java.lang.String, int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>binaryRecordsStream</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;byte[]&gt;&nbsp;binaryRecordsStream(String&nbsp;directory,
                                  int&nbsp;recordLength)</pre>
<div class="block">Create an input stream that monitors a Hadoop-compatible filesystem
 for new files and reads them as flat binary files, assuming a fixed length per record,
 generating one byte array per record. Files must be written to the monitored directory
 by "moving" them from another location within the same file system. File names
 starting with . are ignored.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>directory</code> - HDFS directory to monitor for new file</dd><dd><code>recordLength</code> - length of each record in bytes
 <p></dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Note:</span></dt>
  <dd>We ensure that the byte array for each record in the
 resulting RDDs of the DStream has the provided record length.</dd></dl>
</li>
</ul>
<a name="queueStream(scala.collection.mutable.Queue, boolean, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>queueStream</h4>
<pre>public&nbsp;&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</a>&lt;T&gt;&nbsp;queueStream(scala.collection.mutable.Queue&lt;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;T&gt;&gt;&nbsp;queue,
                              boolean&nbsp;oneAtATime,
                              scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$13)</pre>
<div class="block">Create an input stream from a queue of RDDs. In each batch,
 it will process either one or all of the RDDs returned by the queue.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>queue</code> - Queue of RDDs. Modifications to this data structure must be synchronized.</dd><dd><code>oneAtATime</code> - Whether only one RDD should be consumed from the queue in every interval</dd><dd><code>evidence$13</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Note:</span></dt>
  <dd>Arbitrary RDDs can be added to <code>queueStream</code>, there is no way to recover data of
 those RDDs, so <code>queueStream</code> doesn't support checkpointing.</dd></dl>
</li>
</ul>
<a name="queueStream(scala.collection.mutable.Queue, boolean, org.apache.spark.rdd.RDD, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>queueStream</h4>
<pre>public&nbsp;&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</a>&lt;T&gt;&nbsp;queueStream(scala.collection.mutable.Queue&lt;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;T&gt;&gt;&nbsp;queue,
                              boolean&nbsp;oneAtATime,
                              <a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;T&gt;&nbsp;defaultRDD,
                              scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$14)</pre>
<div class="block">Create an input stream from a queue of RDDs. In each batch,
 it will process either one or all of the RDDs returned by the queue.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>queue</code> - Queue of RDDs. Modifications to this data structure must be synchronized.</dd><dd><code>oneAtATime</code> - Whether only one RDD should be consumed from the queue in every interval</dd><dd><code>defaultRDD</code> - Default RDD is returned by the DStream when the queue is empty.
                   Set as null if no RDD should be returned when empty</dd><dd><code>evidence$14</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Note:</span></dt>
  <dd>Arbitrary RDDs can be added to <code>queueStream</code>, there is no way to recover data of
 those RDDs, so <code>queueStream</code> doesn't support checkpointing.</dd></dl>
</li>
</ul>
<a name="union(scala.collection.Seq, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>union</h4>
<pre>public&nbsp;&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;T&gt;&nbsp;union(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;T&gt;&gt;&nbsp;streams,
                   scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$15)</pre>
<div class="block">Create a unified DStream from multiple DStreams of the same type and same slide duration.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>streams</code> - (undocumented)</dd><dd><code>evidence$15</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="transform(scala.collection.Seq, scala.Function2, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>transform</h4>
<pre>public&nbsp;&lt;T&gt;&nbsp;<a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;T&gt;&nbsp;transform(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/streaming/dstream/DStream.html" title="class in org.apache.spark.streaming.dstream">DStream</a>&lt;?&gt;&gt;&nbsp;dstreams,
                       scala.Function2&lt;scala.collection.Seq&lt;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;?&gt;&gt;,<a href="../../../../org/apache/spark/streaming/Time.html" title="class in org.apache.spark.streaming">Time</a>,<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;T&gt;&gt;&nbsp;transformFunc,
                       scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$16)</pre>
<div class="block">Create a new DStream in which each RDD is generated by applying a function on RDDs of
 the DStreams.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>dstreams</code> - (undocumented)</dd><dd><code>transformFunc</code> - (undocumented)</dd><dd><code>evidence$16</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="addStreamingListener(org.apache.spark.streaming.scheduler.StreamingListener)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>addStreamingListener</h4>
<pre>public&nbsp;void&nbsp;addStreamingListener(<a href="../../../../org/apache/spark/streaming/scheduler/StreamingListener.html" title="interface in org.apache.spark.streaming.scheduler">StreamingListener</a>&nbsp;streamingListener)</pre>
<div class="block">Add a <a href="../../../../org/apache/spark/streaming/scheduler/StreamingListener.html" title="interface in org.apache.spark.streaming.scheduler"><code>StreamingListener</code></a> object for
 receiving system events related to streaming.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>streamingListener</code> - (undocumented)</dd></dl>
</li>
</ul>
<a name="getState()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getState</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/streaming/StreamingContextState.html" title="enum in org.apache.spark.streaming">StreamingContextState</a>&nbsp;getState()</pre>
<div class="block">:: DeveloperApi ::
 <p>
 Return the current state of the context. The context can be in three possible states -
 <p>
  - StreamingContextState.INITIALIZED - The context has been created, but not started yet.
    Input DStreams, transformations and output operations can be created on the context.
  - StreamingContextState.ACTIVE - The context has been started, and not stopped.
    Input DStreams, transformations and output operations cannot be created on the context.
  - StreamingContextState.STOPPED - The context has been stopped and cannot be used any more.</div>
<dl><dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd></dl>
</li>
</ul>
<a name="start()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>start</h4>
<pre>public&nbsp;void&nbsp;start()</pre>
<div class="block">Start the execution of the streams.
 <p></div>
<dl><dt><span class="strong">Throws:</span></dt>
<dd><code>IllegalStateException</code> - if the StreamingContext is already stopped.</dd></dl>
</li>
</ul>
<a name="awaitTermination()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>awaitTermination</h4>
<pre>public&nbsp;void&nbsp;awaitTermination()</pre>
<div class="block">Wait for the execution to stop. Any exceptions that occurs during the execution
 will be thrown in this thread.</div>
</li>
</ul>
<a name="awaitTerminationOrTimeout(long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>awaitTerminationOrTimeout</h4>
<pre>public&nbsp;boolean&nbsp;awaitTerminationOrTimeout(long&nbsp;timeout)</pre>
<div class="block">Wait for the execution to stop. Any exceptions that occurs during the execution
 will be thrown in this thread.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>timeout</code> - time to wait in milliseconds</dd>
<dt><span class="strong">Returns:</span></dt><dd><code>true</code> if it's stopped; or throw the reported error during the execution; or <code>false</code>
         if the waiting time elapsed before returning from the method.</dd></dl>
</li>
</ul>
<a name="stop(boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>stop</h4>
<pre>public&nbsp;void&nbsp;stop(boolean&nbsp;stopSparkContext)</pre>
<div class="block">Stop the execution of the streams immediately (does not wait for all received data
 to be processed). By default, if <code>stopSparkContext</code> is not specified, the underlying
 SparkContext will also be stopped. This implicit behavior can be configured using the
 SparkConf configuration spark.streaming.stopSparkContextByDefault.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>stopSparkContext</code> - If true, stops the associated SparkContext. The underlying SparkContext
                         will be stopped regardless of whether this StreamingContext has been
                         started.</dd></dl>
</li>
</ul>
<a name="stop(boolean, boolean)">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>stop</h4>
<pre>public&nbsp;void&nbsp;stop(boolean&nbsp;stopSparkContext,
        boolean&nbsp;stopGracefully)</pre>
<div class="block">Stop the execution of the streams, with option of ensuring all received data
 has been processed.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>stopSparkContext</code> - if true, stops the associated SparkContext. The underlying SparkContext
                         will be stopped regardless of whether this StreamingContext has been
                         started.</dd><dd><code>stopGracefully</code> - if true, stops gracefully by waiting for the processing of all
                       received data to be completed</dd></dl>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar_bottom">
<!--   -->
</a><a href="#skip-navbar_bottom" title="Skip navigation links"></a><a name="navbar_bottom_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/streaming/StateSpec.html" title="class in org.apache.spark.streaming"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../org/apache/spark/streaming/StreamingContextPythonHelper.html" title="class in org.apache.spark.streaming"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/streaming/StreamingContext.html" target="_top">Frames</a></li>
<li><a href="StreamingContext.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
<script defer="defer" type="text/javascript" src="../../../../lib/jquery.js"></script><script defer="defer" type="text/javascript" src="../../../../lib/api-javadocs.js"></script></body>
</html>
