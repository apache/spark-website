<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (1.7.0_151) on Mon Oct 02 14:47:15 PDT 2017 -->
<title>DataFrameWriter (Spark 2.1.2 JavaDoc)</title>
<meta name="date" content="2017-10-02">
<link rel="stylesheet" type="text/css" href="../../../../stylesheet.css" title="Style">
</head>
<body>
<script type="text/javascript"><!--
    try {
        if (location.href.indexOf('is-external=true') == -1) {
            parent.document.title="DataFrameWriter (Spark 2.1.2 JavaDoc)";
        }
    }
    catch(err) {
    }
//-->
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar_top">
<!--   -->
</a><a href="#skip-navbar_top" title="Skip navigation links"></a><a name="navbar_top_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/sql/DataFrameStatFunctions.html" title="class in org.apache.spark.sql"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../org/apache/spark/sql/Dataset.html" title="class in org.apache.spark.sql"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/sql/DataFrameWriter.html" target="_top">Frames</a></li>
<li><a href="DataFrameWriter.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.apache.spark.sql</div>
<h2 title="Class DataFrameWriter" class="title">Class DataFrameWriter&lt;T&gt;</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li>Object</li>
<li>
<ul class="inheritance">
<li>org.apache.spark.sql.DataFrameWriter&lt;T&gt;</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<hr>
<br>
<pre>public final class <span class="strong">DataFrameWriter&lt;T&gt;</span>
extends Object</pre>
<div class="block">Interface used to write a <a href="../../../../org/apache/spark/sql/Dataset.html" title="class in org.apache.spark.sql"><code>Dataset</code></a> to external storage systems (e.g. file systems,
 key-value stores, etc). Use <code>Dataset.write</code> to access this.
 <p></div>
<dl><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method_summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span>Methods</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#bucketBy(int,%20java.lang.String,%20scala.collection.Seq)">bucketBy</a></strong>(int&nbsp;numBuckets,
        String&nbsp;colName,
        scala.collection.Seq&lt;String&gt;&nbsp;colNames)</code>
<div class="block">Buckets the output by the given columns.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#bucketBy(int,%20java.lang.String,%20java.lang.String...)">bucketBy</a></strong>(int&nbsp;numBuckets,
        String&nbsp;colName,
        String...&nbsp;colNames)</code>
<div class="block">Buckets the output by the given columns.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#csv(java.lang.String)">csv</a></strong>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> in CSV format at the specified path.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#format(java.lang.String)">format</a></strong>(String&nbsp;source)</code>
<div class="block">Specifies the underlying output data source.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#insertInto(java.lang.String)">insertInto</a></strong>(String&nbsp;tableName)</code>
<div class="block">Inserts the content of the <code>DataFrame</code> to the specified table.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#jdbc(java.lang.String,%20java.lang.String,%20java.util.Properties)">jdbc</a></strong>(String&nbsp;url,
    String&nbsp;table,
    java.util.Properties&nbsp;connectionProperties)</code>
<div class="block">Saves the content of the <code>DataFrame</code> to an external database table via JDBC.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#json(java.lang.String)">json</a></strong>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> in JSON format (<a href="http://jsonlines.org/">
 JSON Lines text format or newline-delimited JSON</a>) at the specified path.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#mode(org.apache.spark.sql.SaveMode)">mode</a></strong>(<a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;saveMode)</code>
<div class="block">Specifies the behavior when data or table already exists.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#mode(java.lang.String)">mode</a></strong>(String&nbsp;saveMode)</code>
<div class="block">Specifies the behavior when data or table already exists.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#option(java.lang.String,%20boolean)">option</a></strong>(String&nbsp;key,
      boolean&nbsp;value)</code>
<div class="block">Adds an output option for the underlying data source.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#option(java.lang.String,%20double)">option</a></strong>(String&nbsp;key,
      double&nbsp;value)</code>
<div class="block">Adds an output option for the underlying data source.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#option(java.lang.String,%20long)">option</a></strong>(String&nbsp;key,
      long&nbsp;value)</code>
<div class="block">Adds an output option for the underlying data source.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#option(java.lang.String,%20java.lang.String)">option</a></strong>(String&nbsp;key,
      String&nbsp;value)</code>
<div class="block">Adds an output option for the underlying data source.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#options(scala.collection.Map)">options</a></strong>(scala.collection.Map&lt;String,String&gt;&nbsp;options)</code>
<div class="block">(Scala-specific) Adds output options for the underlying data source.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#options(java.util.Map)">options</a></strong>(java.util.Map&lt;String,String&gt;&nbsp;options)</code>
<div class="block">Adds output options for the underlying data source.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#orc(java.lang.String)">orc</a></strong>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> in ORC format at the specified path.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#parquet(java.lang.String)">parquet</a></strong>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> in Parquet format at the specified path.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#partitionBy(scala.collection.Seq)">partitionBy</a></strong>(scala.collection.Seq&lt;String&gt;&nbsp;colNames)</code>
<div class="block">Partitions the output by the given columns on the file system.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#partitionBy(java.lang.String...)">partitionBy</a></strong>(String...&nbsp;colNames)</code>
<div class="block">Partitions the output by the given columns on the file system.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#save()">save</a></strong>()</code>
<div class="block">Saves the content of the <code>DataFrame</code> as the specified table.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#save(java.lang.String)">save</a></strong>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> at the specified path.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#saveAsTable(java.lang.String)">saveAsTable</a></strong>(String&nbsp;tableName)</code>
<div class="block">Saves the content of the <code>DataFrame</code> as the specified table.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#sortBy(java.lang.String,%20scala.collection.Seq)">sortBy</a></strong>(String&nbsp;colName,
      scala.collection.Seq&lt;String&gt;&nbsp;colNames)</code>
<div class="block">Sorts the output in each bucket by the given columns.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#sortBy(java.lang.String,%20java.lang.String...)">sortBy</a></strong>(String&nbsp;colName,
      String...&nbsp;colNames)</code>
<div class="block">Sorts the output in each bucket by the given columns.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrameWriter.html#text(java.lang.String)">text</a></strong>(String&nbsp;path)</code>
<div class="block">Saves the content of the <code>DataFrame</code> in a text file at the specified path.</div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;Object</h3>
<code>equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method_detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="partitionBy(java.lang.String...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>partitionBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;partitionBy(String...&nbsp;colNames)</pre>
<div class="block">Partitions the output by the given columns on the file system. If specified, the output is
 laid out on the file system similar to Hive's partitioning scheme. As an example, when we
 partition a dataset by year and then month, the directory layout would look like:
 <p>
   - year=2016/month=01/
   - year=2016/month=02/
 <p>
 Partitioning is one of the most widely used techniques to optimize physical data layout.
 It provides a coarse-grained index for skipping unnecessary data reads when queries have
 predicates on the partitioned columns. In order for partitioning to work well, the number
 of distinct values in each column should typically be less than tens of thousands.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) staring Spark 2.1.0.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="bucketBy(int, java.lang.String, java.lang.String...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>bucketBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;bucketBy(int&nbsp;numBuckets,
                          String&nbsp;colName,
                          String...&nbsp;colNames)</pre>
<div class="block">Buckets the output by the given columns. If specified, the output is laid out on the file
 system similar to Hive's bucketing scheme.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) staring Spark 2.1.0.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>numBuckets</code> - (undocumented)</dd><dd><code>colName</code> - (undocumented)</dd><dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>2.0</dd></dl>
</li>
</ul>
<a name="sortBy(java.lang.String, java.lang.String...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sortBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;sortBy(String&nbsp;colName,
                        String...&nbsp;colNames)</pre>
<div class="block">Sorts the output in each bucket by the given columns.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) staring Spark 2.1.0.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>colName</code> - (undocumented)</dd><dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>2.0</dd></dl>
</li>
</ul>
<a name="mode(org.apache.spark.sql.SaveMode)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>mode</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;mode(<a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;saveMode)</pre>
<div class="block">Specifies the behavior when data or table already exists. Options include:
   - <code>SaveMode.Overwrite</code>: overwrite the existing data.
   - <code>SaveMode.Append</code>: append the data.
   - <code>SaveMode.Ignore</code>: ignore the operation (i.e. no-op).
   - <code>SaveMode.ErrorIfExists</code>: default option, throw an exception at runtime.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>saveMode</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="mode(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>mode</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;mode(String&nbsp;saveMode)</pre>
<div class="block">Specifies the behavior when data or table already exists. Options include:
   - <code>overwrite</code>: overwrite the existing data.
   - <code>append</code>: append the data.
   - <code>ignore</code>: ignore the operation (i.e. no-op).
   - <code>error</code>: default option, throw an exception at runtime.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>saveMode</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="format(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>format</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;format(String&nbsp;source)</pre>
<div class="block">Specifies the underlying output data source. Built-in options include "parquet", "json", etc.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>source</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="option(java.lang.String, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>option</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;option(String&nbsp;key,
                        String&nbsp;value)</pre>
<div class="block">Adds an output option for the underlying data source.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>key</code> - (undocumented)</dd><dd><code>value</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="option(java.lang.String, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>option</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;option(String&nbsp;key,
                        boolean&nbsp;value)</pre>
<div class="block">Adds an output option for the underlying data source.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>key</code> - (undocumented)</dd><dd><code>value</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>2.0.0</dd></dl>
</li>
</ul>
<a name="option(java.lang.String, long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>option</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;option(String&nbsp;key,
                        long&nbsp;value)</pre>
<div class="block">Adds an output option for the underlying data source.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>key</code> - (undocumented)</dd><dd><code>value</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>2.0.0</dd></dl>
</li>
</ul>
<a name="option(java.lang.String, double)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>option</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;option(String&nbsp;key,
                        double&nbsp;value)</pre>
<div class="block">Adds an output option for the underlying data source.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>key</code> - (undocumented)</dd><dd><code>value</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>2.0.0</dd></dl>
</li>
</ul>
<a name="options(scala.collection.Map)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>options</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;options(scala.collection.Map&lt;String,String&gt;&nbsp;options)</pre>
<div class="block">(Scala-specific) Adds output options for the underlying data source.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>options</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="options(java.util.Map)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>options</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;options(java.util.Map&lt;String,String&gt;&nbsp;options)</pre>
<div class="block">Adds output options for the underlying data source.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>options</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="partitionBy(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>partitionBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;partitionBy(scala.collection.Seq&lt;String&gt;&nbsp;colNames)</pre>
<div class="block">Partitions the output by the given columns on the file system. If specified, the output is
 laid out on the file system similar to Hive's partitioning scheme. As an example, when we
 partition a dataset by year and then month, the directory layout would look like:
 <p>
   - year=2016/month=01/
   - year=2016/month=02/
 <p>
 Partitioning is one of the most widely used techniques to optimize physical data layout.
 It provides a coarse-grained index for skipping unnecessary data reads when queries have
 predicates on the partitioned columns. In order for partitioning to work well, the number
 of distinct values in each column should typically be less than tens of thousands.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) staring Spark 2.1.0.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="bucketBy(int, java.lang.String, scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>bucketBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;bucketBy(int&nbsp;numBuckets,
                          String&nbsp;colName,
                          scala.collection.Seq&lt;String&gt;&nbsp;colNames)</pre>
<div class="block">Buckets the output by the given columns. If specified, the output is laid out on the file
 system similar to Hive's bucketing scheme.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) staring Spark 2.1.0.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>numBuckets</code> - (undocumented)</dd><dd><code>colName</code> - (undocumented)</dd><dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>2.0</dd></dl>
</li>
</ul>
<a name="sortBy(java.lang.String, scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sortBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</a>&lt;<a href="../../../../org/apache/spark/sql/DataFrameWriter.html" title="type parameter in DataFrameWriter">T</a>&gt;&nbsp;sortBy(String&nbsp;colName,
                        scala.collection.Seq&lt;String&gt;&nbsp;colNames)</pre>
<div class="block">Sorts the output in each bucket by the given columns.
 <p>
 This is applicable for all file-based data sources (e.g. Parquet, JSON) staring Spark 2.1.0.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>colName</code> - (undocumented)</dd><dd><code>colNames</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>2.0</dd></dl>
</li>
</ul>
<a name="save(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>save</h4>
<pre>public&nbsp;void&nbsp;save(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> at the specified path.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>path</code> - (undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="save()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>save</h4>
<pre>public&nbsp;void&nbsp;save()</pre>
<div class="block">Saves the content of the <code>DataFrame</code> as the specified table.
 <p></div>
<dl><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="insertInto(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>insertInto</h4>
<pre>public&nbsp;void&nbsp;insertInto(String&nbsp;tableName)</pre>
<div class="block">Inserts the content of the <code>DataFrame</code> to the specified table. It requires that
 the schema of the <code>DataFrame</code> is the same as the schema of the table.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>tableName</code> - (undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd>
<dt><span class="strong">Note:</span></dt>
  <dd>Unlike <code>saveAsTable</code>, <code>insertInto</code> ignores the column names and just uses position-based
 resolution. For example:
 <p>
 <pre><code>
    scala&gt; Seq((1, 2)).toDF("i", "j").write.mode("overwrite").saveAsTable("t1")
    scala&gt; Seq((3, 4)).toDF("j", "i").write.insertInto("t1")
    scala&gt; Seq((5, 6)).toDF("a", "b").write.insertInto("t1")
    scala&gt; sql("select * from t1").show
    +---+---+
    |  i|  j|
    +---+---+
    |  5|  6|
    |  3|  4|
    |  1|  2|
    +---+---+
 </code></pre>
 <p>
 Because it inserts data to an existing table, format or options will be ignored.
 <p></dd></dl>
</li>
</ul>
<a name="saveAsTable(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsTable</h4>
<pre>public&nbsp;void&nbsp;saveAsTable(String&nbsp;tableName)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> as the specified table.
 <p>
 In the case the table already exists, behavior of this function depends on the
 save mode, specified by the <code>mode</code> function (default to throwing an exception).
 When <code>mode</code> is <code>Overwrite</code>, the schema of the <code>DataFrame</code> does not need to be
 the same as that of the existing table.
 <p>
 When <code>mode</code> is <code>Append</code>, if there is an existing table, we will use the format and options of
 the existing table. The column order in the schema of the <code>DataFrame</code> doesn't need to be same
 as that of the existing table. Unlike <code>insertInto</code>, <code>saveAsTable</code> will use the column names to
 find the correct column positions. For example:
 <p>
 <pre><code>
    scala&gt; Seq((1, 2)).toDF("i", "j").write.mode("overwrite").saveAsTable("t1")
    scala&gt; Seq((3, 4)).toDF("j", "i").write.mode("append").saveAsTable("t1")
    scala&gt; sql("select * from t1").show
    +---+---+
    |  i|  j|
    +---+---+
    |  1|  2|
    |  4|  3|
    +---+---+
 </code></pre>
 <p>
 When the DataFrame is created from a non-partitioned <code>HadoopFsRelation</code> with a single input
 path, and the data source provider can be mapped to an existing Hive builtin SerDe (i.e. ORC
 and Parquet), the table is persisted in a Hive compatible format, which means other systems
 like Hive will be able to read this table. Otherwise, the table is persisted in a Spark SQL
 specific format.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>tableName</code> - (undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="jdbc(java.lang.String, java.lang.String, java.util.Properties)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>jdbc</h4>
<pre>public&nbsp;void&nbsp;jdbc(String&nbsp;url,
        String&nbsp;table,
        java.util.Properties&nbsp;connectionProperties)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> to an external database table via JDBC. In the case the
 table already exists in the external database, behavior of this function depends on the
 save mode, specified by the <code>mode</code> function (default to throwing an exception).
 <p>
 Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
 your external database systems.
 <p>
 You can set the following JDBC-specific option(s) for storing JDBC:
 <ul>
 <li><code>truncate</code> (default <code>false</code>): use <code>TRUNCATE TABLE</code> instead of <code>DROP TABLE</code>.</li>
 </ul>
 <p>
 In case of failures, users should turn off <code>truncate</code> option to use <code>DROP TABLE</code> again. Also,
 due to the different behavior of <code>TRUNCATE TABLE</code> among DBMS, it's not always safe to use this.
 MySQLDialect, DB2Dialect, MsSqlServerDialect, DerbyDialect, and OracleDialect supports this
 while PostgresDialect and default JDBCDirect doesn't. For unknown and unsupported JDBCDirect,
 the user option <code>truncate</code> is ignored.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>url</code> - JDBC database url of the form <code>jdbc:subprotocol:subname</code></dd><dd><code>table</code> - Name of the table in the external database.</dd><dd><code>connectionProperties</code> - JDBC database connection arguments, a list of arbitrary string
                             tag/value. Normally at least a "user" and "password" property
                             should be included. "batchsize" can be used to control the
                             number of rows per insert. "isolationLevel" can be one of
                             "NONE", "READ_COMMITTED", "READ_UNCOMMITTED", "REPEATABLE_READ",
                             or "SERIALIZABLE", corresponding to standard transaction
                             isolation levels defined by JDBC's Connection object, with default
                             of "READ_UNCOMMITTED".</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="json(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>json</h4>
<pre>public&nbsp;void&nbsp;json(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> in JSON format (<a href="http://jsonlines.org/">
 JSON Lines text format or newline-delimited JSON</a>) at the specified path.
 This is equivalent to:
 <pre><code>
   format("json").save(path)
 </code></pre>
 <p>
 You can set the following JSON-specific option(s) for writing JSON files:
 <ul>
 <li><code>compression</code> (default <code>null</code>): compression codec to use when saving to file. This can be
 one of the known case-insensitive shorten names (<code>none</code>, <code>bzip2</code>, <code>gzip</code>, <code>lz4</code>,
 <code>snappy</code> and <code>deflate</code>). </li>
 <li><code>dateFormat</code> (default <code>yyyy-MM-dd</code>): sets the string that indicates a date format.
 Custom date formats follow the formats at <code>java.text.SimpleDateFormat</code>. This applies to
 date type.</li>
 <li><code>timestampFormat</code> (default <code>yyyy-MM-dd'T'HH:mm:ss.SSSZZ</code>): sets the string that
 indicates a timestamp format. Custom date formats follow the formats at
 <code>java.text.SimpleDateFormat</code>. This applies to timestamp type.</li>
 </ul>
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>path</code> - (undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="parquet(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>parquet</h4>
<pre>public&nbsp;void&nbsp;parquet(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> in Parquet format at the specified path.
 This is equivalent to:
 <pre><code>
   format("parquet").save(path)
 </code></pre>
 <p>
 You can set the following Parquet-specific option(s) for writing Parquet files:
 <ul>
 <li><code>compression</code> (default is the value specified in <code>spark.sql.parquet.compression.codec</code>):
 compression codec to use when saving to file. This can be one of the known case-insensitive
 shorten names(none, <code>snappy</code>, <code>gzip</code>, and <code>lzo</code>). This will override
 <code>spark.sql.parquet.compression.codec</code>.</li>
 </ul>
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>path</code> - (undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="orc(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>orc</h4>
<pre>public&nbsp;void&nbsp;orc(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> in ORC format at the specified path.
 This is equivalent to:
 <pre><code>
   format("orc").save(path)
 </code></pre>
 <p>
 You can set the following ORC-specific option(s) for writing ORC files:
 <ul>
 <li><code>compression</code> (default <code>snappy</code>): compression codec to use when saving to file. This can be
 one of the known case-insensitive shorten names(<code>none</code>, <code>snappy</code>, <code>zlib</code>, and <code>lzo</code>).
 This will override <code>orc.compress</code>.</li>
 </ul>
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>path</code> - (undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.5.0</dd>
<dt><span class="strong">Note:</span></dt>
  <dd>Currently, this method can only be used after enabling Hive support</dd></dl>
</li>
</ul>
<a name="text(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>text</h4>
<pre>public&nbsp;void&nbsp;text(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> in a text file at the specified path.
 The DataFrame must have only one column that is of string type.
 Each row becomes a new line in the output file. For example:
 <pre><code>
   // Scala:
   df.write.text("/path/to/output")

   // Java:
   df.write().text("/path/to/output")
 </code></pre>
 <p>
 You can set the following option(s) for writing text files:
 <ul>
 <li><code>compression</code> (default <code>null</code>): compression codec to use when saving to file. This can be
 one of the known case-insensitive shorten names (<code>none</code>, <code>bzip2</code>, <code>gzip</code>, <code>lz4</code>,
 <code>snappy</code> and <code>deflate</code>). </li>
 </ul>
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>path</code> - (undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.6.0</dd></dl>
</li>
</ul>
<a name="csv(java.lang.String)">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>csv</h4>
<pre>public&nbsp;void&nbsp;csv(String&nbsp;path)</pre>
<div class="block">Saves the content of the <code>DataFrame</code> in CSV format at the specified path.
 This is equivalent to:
 <pre><code>
   format("csv").save(path)
 </code></pre>
 <p>
 You can set the following CSV-specific option(s) for writing CSV files:
 <ul>
 <li><code>sep</code> (default <code>,</code>): sets the single character as a separator for each
 field and value.</li>
 <li><code>quote</code> (default <code>"</code>): sets the single character used for escaping quoted values where
 the separator can be part of the value.</li>
 <li><code>escape</code> (default <code>\</code>): sets the single character used for escaping quotes inside
 an already quoted value.</li>
 <li><code>escapeQuotes</code> (default <code>true</code>): a flag indicating whether values containing
 quotes should always be enclosed in quotes. Default is to escape all values containing
 a quote character.</li>
 <li><code>quoteAll</code> (default <code>false</code>): A flag indicating whether all values should always be
 enclosed in quotes. Default is to only escape values containing a quote character.</li>
 <li><code>header</code> (default <code>false</code>): writes the names of columns as the first line.</li>
 <li><code>nullValue</code> (default empty string): sets the string representation of a null value.</li>
 <li><code>compression</code> (default <code>null</code>): compression codec to use when saving to file. This can be
 one of the known case-insensitive shorten names (<code>none</code>, <code>bzip2</code>, <code>gzip</code>, <code>lz4</code>,
 <code>snappy</code> and <code>deflate</code>). </li>
 <li><code>dateFormat</code> (default <code>yyyy-MM-dd</code>): sets the string that indicates a date format.
 Custom date formats follow the formats at <code>java.text.SimpleDateFormat</code>. This applies to
 date type.</li>
 <li><code>timestampFormat</code> (default <code>yyyy-MM-dd'T'HH:mm:ss.SSSZZ</code>): sets the string that
 indicates a timestamp format. Custom date formats follow the formats at
 <code>java.text.SimpleDateFormat</code>. This applies to timestamp type.</li>
 </ul>
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>path</code> - (undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>2.0.0</dd></dl>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar_bottom">
<!--   -->
</a><a href="#skip-navbar_bottom" title="Skip navigation links"></a><a name="navbar_bottom_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/sql/DataFrameStatFunctions.html" title="class in org.apache.spark.sql"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../org/apache/spark/sql/Dataset.html" title="class in org.apache.spark.sql"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/sql/DataFrameWriter.html" target="_top">Frames</a></li>
<li><a href="DataFrameWriter.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
<script defer="defer" type="text/javascript" src="../../../../lib/jquery.js"></script><script defer="defer" type="text/javascript" src="../../../../lib/api-javadocs.js"></script></body>
</html>
