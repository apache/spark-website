
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>MulticlassMetrics &#8212; PySpark 3.4.0 documentation</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/pyspark.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="canonical" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.evaluation.MulticlassMetrics.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="RankingMetrics" href="pyspark.mllib.evaluation.RankingMetrics.html" />
    <link rel="prev" title="RegressionMetrics" href="pyspark.mllib.evaluation.RegressionMetrics.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../index.html">
    
      <img src="../../_static/spark-logo-reverse.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../index.html">Overview</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../getting_started/index.html">Getting Started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../user_guide/index.html">User Guides</a>
        </li>
        
        <li class="nav-item active">
            <a class="nav-link" href="../index.html">API Reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../development/index.html">Development</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../migration_guide/index.html">Migration Guides</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
        
          
            
                <li class="">
                    <a href="../pyspark.sql/index.html">Spark SQL</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.pandas/index.html">Pandas API on Spark</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.ss/index.html">Structured Streaming</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.ml.html">MLlib (DataFrame-based)</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.streaming.html">Spark Streaming (Legacy)</a>
                </li>
            
          
            
                <li class="active">
                    <a href="../pyspark.mllib.html">MLlib (RDD-based)</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.html">Spark Core</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.resource.html">Resource Management</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.errors.html">Errors</a>
                </li>
            
          
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="multiclassmetrics">
<h1>MulticlassMetrics<a class="headerlink" href="#multiclassmetrics" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.mllib.evaluation.</code><code class="sig-name descname">MulticlassMetrics</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">predictionAndLabels</span><span class="p">:</span> <span class="n">pyspark.rdd.RDD<span class="p">[</span>Tuple<span class="p">[</span>float<span class="p">, </span>float<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/pyspark/mllib/evaluation.html#MulticlassMetrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluator for multiclass classification.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>predictionAndLabels</strong><span class="classifier"><a class="reference internal" href="pyspark.RDD.html#pyspark.RDD" title="pyspark.RDD"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.RDD</span></code></a></span></dt><dd><p>an RDD of prediction, label, optional weight and optional probability.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">predictionAndLabels</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span> <span class="o">=</span> <span class="n">MulticlassMetrics</span><span class="p">(</span><span class="n">predictionAndLabels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">confusionMatrix</span><span class="p">()</span><span class="o">.</span><span class="n">toArray</span><span class="p">()</span>
<span class="go">array([[ 2.,  1.,  1.],</span>
<span class="go">       [ 1.,  3.,  0.],</span>
<span class="go">       [ 0.,  0.,  1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">falsePositiveRate</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
<span class="go">0.2...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">precision</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="go">0.75...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">recall</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="go">1.0...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">fMeasure</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
<span class="go">0.52...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy</span>
<span class="go">0.66...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">weightedFalsePositiveRate</span>
<span class="go">0.19...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">weightedPrecision</span>
<span class="go">0.68...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">weightedRecall</span>
<span class="go">0.66...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">weightedFMeasure</span><span class="p">()</span>
<span class="go">0.66...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">weightedFMeasure</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="go">0.65...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predAndLabelsWithOptWeight</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
<span class="gp">... </span>     <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
<span class="gp">... </span>     <span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span> <span class="o">=</span> <span class="n">MulticlassMetrics</span><span class="p">(</span><span class="n">predAndLabelsWithOptWeight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">confusionMatrix</span><span class="p">()</span><span class="o">.</span><span class="n">toArray</span><span class="p">()</span>
<span class="go">array([[ 2.,  1.,  1.],</span>
<span class="go">       [ 1.,  3.,  0.],</span>
<span class="go">       [ 0.,  0.,  1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">falsePositiveRate</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
<span class="go">0.2...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">precision</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="go">0.75...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">recall</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="go">1.0...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">fMeasure</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
<span class="go">0.52...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy</span>
<span class="go">0.66...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">weightedFalsePositiveRate</span>
<span class="go">0.19...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">weightedPrecision</span>
<span class="go">0.68...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">weightedRecall</span>
<span class="go">0.66...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">weightedFMeasure</span><span class="p">()</span>
<span class="go">0.66...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">weightedFMeasure</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="go">0.65...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictionAndLabelsWithProbabilities</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span>
<span class="gp">... </span>     <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]),</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]),</span>
<span class="gp">... </span>     <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">])])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span> <span class="o">=</span> <span class="n">MulticlassMetrics</span><span class="p">(</span><span class="n">predictionAndLabelsWithProbabilities</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">logLoss</span><span class="p">()</span>
<span class="go">0.9682...</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.call" title="pyspark.mllib.evaluation.MulticlassMetrics.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(name, *a)</p></td>
<td><p>Call method of java_model</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.confusionMatrix" title="pyspark.mllib.evaluation.MulticlassMetrics.confusionMatrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">confusionMatrix</span></code></a>()</p></td>
<td><p>Returns confusion matrix: predicted classes are in columns, they are ordered by class label ascending, as in “labels”.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.fMeasure" title="pyspark.mllib.evaluation.MulticlassMetrics.fMeasure"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fMeasure</span></code></a>(label[, beta])</p></td>
<td><p>Returns f-measure.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.falsePositiveRate" title="pyspark.mllib.evaluation.MulticlassMetrics.falsePositiveRate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">falsePositiveRate</span></code></a>(label)</p></td>
<td><p>Returns false positive rate for a given label (category).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.logLoss" title="pyspark.mllib.evaluation.MulticlassMetrics.logLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logLoss</span></code></a>([eps])</p></td>
<td><p>Returns weighted logLoss.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.precision" title="pyspark.mllib.evaluation.MulticlassMetrics.precision"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision</span></code></a>(label)</p></td>
<td><p>Returns precision.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.recall" title="pyspark.mllib.evaluation.MulticlassMetrics.recall"><code class="xref py py-obj docutils literal notranslate"><span class="pre">recall</span></code></a>(label)</p></td>
<td><p>Returns recall.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.truePositiveRate" title="pyspark.mllib.evaluation.MulticlassMetrics.truePositiveRate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">truePositiveRate</span></code></a>(label)</p></td>
<td><p>Returns true positive rate for a given label (category).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.weightedFMeasure" title="pyspark.mllib.evaluation.MulticlassMetrics.weightedFMeasure"><code class="xref py py-obj docutils literal notranslate"><span class="pre">weightedFMeasure</span></code></a>([beta])</p></td>
<td><p>Returns weighted averaged f-measure.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.accuracy" title="pyspark.mllib.evaluation.MulticlassMetrics.accuracy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">accuracy</span></code></a></p></td>
<td><p>Returns accuracy (equals to the total number of correctly classified instances out of the total number of instances).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.weightedFalsePositiveRate" title="pyspark.mllib.evaluation.MulticlassMetrics.weightedFalsePositiveRate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">weightedFalsePositiveRate</span></code></a></p></td>
<td><p>Returns weighted false positive rate.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.weightedPrecision" title="pyspark.mllib.evaluation.MulticlassMetrics.weightedPrecision"><code class="xref py py-obj docutils literal notranslate"><span class="pre">weightedPrecision</span></code></a></p></td>
<td><p>Returns weighted averaged precision.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.weightedRecall" title="pyspark.mllib.evaluation.MulticlassMetrics.weightedRecall"><code class="xref py py-obj docutils literal notranslate"><span class="pre">weightedRecall</span></code></a></p></td>
<td><p>Returns weighted averaged recall.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pyspark.mllib.evaluation.MulticlassMetrics.weightedTruePositiveRate" title="pyspark.mllib.evaluation.MulticlassMetrics.weightedTruePositiveRate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">weightedTruePositiveRate</span></code></a></p></td>
<td><p>Returns weighted true positive rate.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Methods Documentation</p>
<dl class="py method">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">a</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; Any<a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Call method of java_model</p>
</dd></dl>

<dl class="py method">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.confusionMatrix">
<code class="sig-name descname">confusionMatrix</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="pyspark.mllib.linalg.Matrix.html#pyspark.mllib.linalg.Matrix" title="pyspark.mllib.linalg.Matrix">pyspark.mllib.linalg.Matrix</a><a class="reference internal" href="../../_modules/pyspark/mllib/evaluation.html#MulticlassMetrics.confusionMatrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.confusionMatrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns confusion matrix: predicted classes are in columns,
they are ordered by class label ascending, as in “labels”.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.fMeasure">
<code class="sig-name descname">fMeasure</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">beta</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../../_modules/pyspark/mllib/evaluation.html#MulticlassMetrics.fMeasure"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.fMeasure" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns f-measure.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.falsePositiveRate">
<code class="sig-name descname">falsePositiveRate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../../_modules/pyspark/mllib/evaluation.html#MulticlassMetrics.falsePositiveRate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.falsePositiveRate" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns false positive rate for a given label (category).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.logLoss">
<code class="sig-name descname">logLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">eps</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1e-15</span></em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../../_modules/pyspark/mllib/evaluation.html#MulticlassMetrics.logLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.logLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns weighted logLoss.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 3.0.0.</span></p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.precision">
<code class="sig-name descname">precision</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../../_modules/pyspark/mllib/evaluation.html#MulticlassMetrics.precision"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns precision.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.recall">
<code class="sig-name descname">recall</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../../_modules/pyspark/mllib/evaluation.html#MulticlassMetrics.recall"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns recall.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.truePositiveRate">
<code class="sig-name descname">truePositiveRate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../../_modules/pyspark/mllib/evaluation.html#MulticlassMetrics.truePositiveRate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.truePositiveRate" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns true positive rate for a given label (category).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.weightedFMeasure">
<code class="sig-name descname">weightedFMeasure</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">beta</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../../_modules/pyspark/mllib/evaluation.html#MulticlassMetrics.weightedFMeasure"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.weightedFMeasure" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns weighted averaged f-measure.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

<p class="rubric">Attributes Documentation</p>
<dl class="py attribute">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.accuracy">
<code class="sig-name descname">accuracy</code><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns accuracy (equals to the total number of correctly classified instances
out of the total number of instances).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.0.</span></p>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.weightedFalsePositiveRate">
<code class="sig-name descname">weightedFalsePositiveRate</code><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.weightedFalsePositiveRate" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns weighted false positive rate.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.weightedPrecision">
<code class="sig-name descname">weightedPrecision</code><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.weightedPrecision" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns weighted averaged precision.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.weightedRecall">
<code class="sig-name descname">weightedRecall</code><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.weightedRecall" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns weighted averaged recall.
(equals to precision, recall and f-measure)</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="pyspark.mllib.evaluation.MulticlassMetrics.weightedTruePositiveRate">
<code class="sig-name descname">weightedTruePositiveRate</code><a class="headerlink" href="#pyspark.mllib.evaluation.MulticlassMetrics.weightedTruePositiveRate" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns weighted true positive rate.
(equals to precision, recall and f-measure)</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.4.0.</span></p>
</div>
</dd></dl>

</dd></dl>

</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="pyspark.mllib.evaluation.RegressionMetrics.html" title="previous page">RegressionMetrics</a>
    <a class='right-next' id="next-link" href="pyspark.mllib.evaluation.RankingMetrics.html" title="next page">RankingMetrics</a>

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright .<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>