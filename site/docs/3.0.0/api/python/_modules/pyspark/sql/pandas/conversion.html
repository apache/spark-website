
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>pyspark.sql.pandas.conversion &#8212; PySpark 3.0.0 documentation</title>
    <link rel="stylesheet" href="../../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pyspark.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script type="text/javascript" src="../../../../_static/copybutton.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../../_static/pyspark.js"></script>
    <link rel="search" title="Search" href="../../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
    
        <li class="nav-item nav-item-0"><a href="../../../../index.html">PySpark 3.0.0 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="../../../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for pyspark.sql.pandas.conversion</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span> <span class="o">&gt;=</span> <span class="s1">&#39;3&#39;</span><span class="p">:</span>
    <span class="n">basestring</span> <span class="o">=</span> <span class="n">unicode</span> <span class="o">=</span> <span class="nb">str</span>
    <span class="n">xrange</span> <span class="o">=</span> <span class="nb">range</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">izip</span> <span class="k">as</span> <span class="nb">zip</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">since</span>
<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="kn">import</span> <span class="n">_load_from_socket</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.pandas.serializers</span> <span class="kn">import</span> <span class="n">ArrowCollectSerializer</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegralType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.traceback_utils</span> <span class="kn">import</span> <span class="n">SCCallSiteSync</span>
<span class="kn">from</span> <span class="nn">pyspark.util</span> <span class="kn">import</span> <span class="n">_exception_message</span>


<span class="k">class</span> <span class="nc">PandasConversionMixin</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Min-in for the conversion from Spark to pandas. Currently, only :class:`DataFrame`</span>
<span class="sd">    can use this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@since</span><span class="p">(</span><span class="mf">1.3</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">toPandas</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.</span>

<span class="sd">        This is only available if Pandas is installed and available.</span>

<span class="sd">        .. note:: This method should only be used if the resulting Pandas&#39;s :class:`DataFrame` is</span>
<span class="sd">            expected to be small, as all the data is loaded into the driver&#39;s memory.</span>

<span class="sd">        .. note:: Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.</span>

<span class="sd">        &gt;&gt;&gt; df.toPandas()  # doctest: +SKIP</span>
<span class="sd">           age   name</span>
<span class="sd">        0    2  Alice</span>
<span class="sd">        1    5    Bob</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pandas_version</span>
        <span class="n">require_minimum_pandas_version</span><span class="p">()</span>

        <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
        <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

        <span class="n">timezone</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sql_ctx</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">sessionLocalTimeZone</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sql_ctx</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">arrowPySparkEnabled</span><span class="p">():</span>
            <span class="n">use_arrow</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">to_arrow_schema</span>
                <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pyarrow_version</span>

                <span class="n">require_minimum_pyarrow_version</span><span class="p">()</span>
                <span class="n">to_arrow_schema</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sql_ctx</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">arrowPySparkFallbackEnabled</span><span class="p">():</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;toPandas attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true; however, &quot;</span>
                        <span class="s2">&quot;failed by the reason below:</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="s2">&quot;Attempting non-optimization as &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; is set to &quot;</span>
                        <span class="s2">&quot;true.&quot;</span> <span class="o">%</span> <span class="n">_exception_message</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="n">use_arrow</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;toPandas attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has &quot;</span>
                        <span class="s2">&quot;reached the error below and will not continue because automatic fallback &quot;</span>
                        <span class="s2">&quot;with &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; has been set to &quot;</span>
                        <span class="s2">&quot;false.</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">_exception_message</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="k">raise</span>

            <span class="c1"># Try to use Arrow optimization when the schema is supported and the required version</span>
            <span class="c1"># of PyArrow is found, if &#39;spark.sql.execution.arrow.pyspark.enabled&#39; is enabled.</span>
            <span class="k">if</span> <span class="n">use_arrow</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">_check_series_localize_timestamps</span>
                    <span class="kn">import</span> <span class="nn">pyarrow</span>
                    <span class="c1"># Rename columns to avoid duplicated column names.</span>
                    <span class="n">tmp_column_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;col_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">))]</span>
                    <span class="n">batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="o">*</span><span class="n">tmp_column_names</span><span class="p">)</span><span class="o">.</span><span class="n">_collect_as_arrow</span><span class="p">()</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">table</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">Table</span><span class="o">.</span><span class="n">from_batches</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
                        <span class="c1"># Pandas DataFrame created from PyArrow uses datetime64[ns] for date type</span>
                        <span class="c1"># values, but we should use datetime.date to match the behavior with when</span>
                        <span class="c1"># Arrow optimization is disabled.</span>
                        <span class="n">pdf</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">(</span><span class="n">date_as_object</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                        <span class="c1"># Rename back to the original column names.</span>
                        <span class="n">pdf</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">columns</span>
                        <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
                            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">):</span>
                                <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> \
                                    <span class="n">_check_series_localize_timestamps</span><span class="p">(</span><span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">],</span> <span class="n">timezone</span><span class="p">)</span>
                        <span class="k">return</span> <span class="n">pdf</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">([],</span> <span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="c1"># We might have to allow fallback here as well but multiple Spark jobs can</span>
                    <span class="c1"># be executed. So, simply fail in this case for now.</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;toPandas attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has &quot;</span>
                        <span class="s2">&quot;reached the error below and can not continue. Note that &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; does not have an &quot;</span>
                        <span class="s2">&quot;effect on failures in the middle of &quot;</span>
                        <span class="s2">&quot;computation.</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">_exception_message</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="k">raise</span>

        <span class="c1"># Below is toPandas without Arrow optimization.</span>
        <span class="n">pdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">collect</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">column_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">fieldIdx</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">):</span>
            <span class="c1"># For duplicate column name, we use `iloc` to access it.</span>
            <span class="k">if</span> <span class="n">column_counter</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">pandas_col</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">fieldIdx</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pandas_col</span> <span class="o">=</span> <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>

            <span class="n">pandas_type</span> <span class="o">=</span> <span class="n">PandasConversionMixin</span><span class="o">.</span><span class="n">_to_corrected_pandas_type</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">)</span>
            <span class="c1"># SPARK-21766: if an integer field is nullable and has null values, it can be</span>
            <span class="c1"># inferred by pandas as float column. Once we convert the column with NaN back</span>
            <span class="c1"># to integer type e.g., np.int16, we will hit exception. So we use the inferred</span>
            <span class="c1"># float type, not the corrected type from the schema in this case.</span>
            <span class="k">if</span> <span class="n">pandas_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> \
                <span class="ow">not</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">IntegralType</span><span class="p">)</span> <span class="ow">and</span> <span class="n">field</span><span class="o">.</span><span class="n">nullable</span> <span class="ow">and</span>
                    <span class="n">pandas_col</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">()):</span>
                <span class="n">dtype</span><span class="p">[</span><span class="n">fieldIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">pandas_type</span>
            <span class="c1"># Ensure we fall back to nullable numpy types, even when whole column is null:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">IntegralType</span><span class="p">)</span> <span class="ow">and</span> <span class="n">pandas_col</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">dtype</span><span class="p">[</span><span class="n">fieldIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">BooleanType</span><span class="p">)</span> <span class="ow">and</span> <span class="n">pandas_col</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">dtype</span><span class="p">[</span><span class="n">fieldIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">object</span>

        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
            <span class="n">column_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>

            <span class="c1"># For duplicate column name, we use `iloc` to access it.</span>
            <span class="k">if</span> <span class="n">column_counter</span><span class="p">[</span><span class="n">column_name</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">series</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">index</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">series</span> <span class="o">=</span> <span class="n">pdf</span><span class="p">[</span><span class="n">column_name</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">series</span> <span class="o">=</span> <span class="n">series</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="c1"># `insert` API makes copy of data, we only do it for Series of duplicate column names.</span>
            <span class="c1"># `pdf.iloc[:, index] = pdf.iloc[:, index]...` doesn&#39;t always work because `iloc` could</span>
            <span class="c1"># return a view or a copy depending by context.</span>
            <span class="k">if</span> <span class="n">column_counter</span><span class="p">[</span><span class="n">column_name</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">column_name</span><span class="p">,</span> <span class="n">series</span><span class="p">,</span> <span class="n">allow_duplicates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">df</span><span class="p">[</span><span class="n">column_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">series</span>

        <span class="n">pdf</span> <span class="o">=</span> <span class="n">df</span>

        <span class="k">if</span> <span class="n">timezone</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pdf</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">_check_series_convert_timestamps_local_tz</span>
            <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
                <span class="c1"># TODO: handle nested timestamps, such as ArrayType(TimestampType())?</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">):</span>
                    <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> \
                        <span class="n">_check_series_convert_timestamps_local_tz</span><span class="p">(</span><span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">],</span> <span class="n">timezone</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pdf</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_to_corrected_pandas_type</span><span class="p">(</span><span class="n">dt</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        When converting Spark SQL records to Pandas :class:`DataFrame`, the inferred data type</span>
<span class="sd">        may be wrong. This method gets the corrected data type for Pandas if that type may be</span>
<span class="sd">        inferred incorrectly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">ByteType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">int8</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">ShortType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">int16</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">IntegerType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">LongType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">FloatType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">DoubleType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">BooleanType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">bool</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">TimestampType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">datetime64</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_collect_as_arrow</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns all records as a list of ArrowRecordBatches, pyarrow must be installed</span>
<span class="sd">        and available on driver and worker Python environments.</span>

<span class="sd">        .. note:: Experimental.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">SCCallSiteSync</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sc</span><span class="p">):</span>
            <span class="n">port</span><span class="p">,</span> <span class="n">auth_secret</span><span class="p">,</span> <span class="n">jsocket_auth_server</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jdf</span><span class="o">.</span><span class="n">collectAsArrowToPython</span><span class="p">()</span>

        <span class="c1"># Collect list of un-ordered batches where last element is a list of correct order indices</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">_load_from_socket</span><span class="p">((</span><span class="n">port</span><span class="p">,</span> <span class="n">auth_secret</span><span class="p">),</span> <span class="n">ArrowCollectSerializer</span><span class="p">()))</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># Join serving thread and raise any exceptions from collectAsArrowToPython</span>
            <span class="n">jsocket_auth_server</span><span class="o">.</span><span class="n">getResult</span><span class="p">()</span>

        <span class="c1"># Separate RecordBatches from batch order indices in results</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="n">results</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">batch_order</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Re-order the batch list using the correct order</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">batches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">batch_order</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">SparkConversionMixin</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Min-in for the conversion from pandas to Spark. Currently, only :class:`SparkSession`</span>
<span class="sd">    can use this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">createDataFrame</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">samplingRatio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verifySchema</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pandas_version</span>
        <span class="n">require_minimum_pandas_version</span><span class="p">()</span>

        <span class="n">timezone</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">sessionLocalTimeZone</span><span class="p">()</span>

        <span class="c1"># If no schema supplied by user then get the names of columns only</span>
        <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">schema</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">basestring</span><span class="p">)</span> <span class="k">else</span>
                      <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">)</span>
                      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">arrowPySparkEnabled</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_from_pandas_with_arrow</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">timezone</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">pyspark.util</span> <span class="kn">import</span> <span class="n">_exception_message</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">arrowPySparkFallbackEnabled</span><span class="p">():</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;createDataFrame attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true; however, &quot;</span>
                        <span class="s2">&quot;failed by the reason below:</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="s2">&quot;Attempting non-optimization as &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; is set to &quot;</span>
                        <span class="s2">&quot;true.&quot;</span> <span class="o">%</span> <span class="n">_exception_message</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;createDataFrame attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has &quot;</span>
                        <span class="s2">&quot;reached the error below and will not continue because automatic &quot;</span>
                        <span class="s2">&quot;fallback with &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; &quot;</span>
                        <span class="s2">&quot;has been set to false.</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">_exception_message</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="k">raise</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_from_pandas</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">timezone</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_dataframe</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">samplingRatio</span><span class="p">,</span> <span class="n">verifySchema</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert_from_pandas</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pdf</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">timezone</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Convert a pandas.DataFrame to list of records that can be used to make a DataFrame</span>
<span class="sd">         :return list of records</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">timezone</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">_check_series_convert_timestamps_tz_local</span>
            <span class="n">copied</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">StructType</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">schema</span><span class="p">:</span>
                    <span class="c1"># TODO: handle nested timestamps, such as ArrayType(TimestampType())?</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">):</span>
                        <span class="n">s</span> <span class="o">=</span> <span class="n">_check_series_convert_timestamps_tz_local</span><span class="p">(</span><span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">],</span> <span class="n">timezone</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]:</span>
                            <span class="k">if</span> <span class="ow">not</span> <span class="n">copied</span><span class="p">:</span>
                                <span class="c1"># Copy once if the series is modified to prevent the original</span>
                                <span class="c1"># Pandas DataFrame from being updated</span>
                                <span class="n">pdf</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                                <span class="n">copied</span> <span class="o">=</span> <span class="kc">True</span>
                            <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">series</span> <span class="ow">in</span> <span class="n">pdf</span><span class="o">.</span><span class="n">iteritems</span><span class="p">():</span>
                    <span class="n">s</span> <span class="o">=</span> <span class="n">_check_series_convert_timestamps_tz_local</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">timezone</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">series</span><span class="p">:</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">copied</span><span class="p">:</span>
                            <span class="c1"># Copy once if the series is modified to prevent the original</span>
                            <span class="c1"># Pandas DataFrame from being updated</span>
                            <span class="n">pdf</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                            <span class="n">copied</span> <span class="o">=</span> <span class="kc">True</span>
                        <span class="n">pdf</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

        <span class="c1"># Convert pandas.DataFrame to list of numpy records</span>
        <span class="n">np_records</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">to_records</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Check if any columns need to be fixed for Spark to infer properly</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">np_records</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">record_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_numpy_record_dtype</span><span class="p">(</span><span class="n">np_records</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">record_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">record_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">np_records</span><span class="p">]</span>

        <span class="c1"># Convert list of numpy records to python lists</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">np_records</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_get_numpy_record_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rec</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Used when converting a pandas.DataFrame to Spark using to_records(), this will correct</span>
<span class="sd">        the dtypes of fields in a record so they can be properly loaded into Spark.</span>
<span class="sd">        :param rec: a numpy record to check field dtypes</span>
<span class="sd">        :return corrected dtype for a numpy.record or None if no correction needed</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
        <span class="n">cur_dtypes</span> <span class="o">=</span> <span class="n">rec</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">col_names</span> <span class="o">=</span> <span class="n">cur_dtypes</span><span class="o">.</span><span class="n">names</span>
        <span class="n">record_type_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">has_rec_fix</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cur_dtypes</span><span class="p">)):</span>
            <span class="n">curr_type</span> <span class="o">=</span> <span class="n">cur_dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># If type is a datetime64 timestamp, convert to microseconds</span>
            <span class="c1"># NOTE: if dtype is datetime[ns] then np.record.tolist() will output values as longs,</span>
            <span class="c1"># conversion from [us] or lower will lead to py datetime objects, see SPARK-22417</span>
            <span class="k">if</span> <span class="n">curr_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;datetime64[ns]&#39;</span><span class="p">):</span>
                <span class="n">curr_type</span> <span class="o">=</span> <span class="s1">&#39;datetime64[us]&#39;</span>
                <span class="n">has_rec_fix</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">record_type_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">str</span><span class="p">(</span><span class="n">col_names</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">curr_type</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">record_type_list</span><span class="p">)</span> <span class="k">if</span> <span class="n">has_rec_fix</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_create_from_pandas_with_arrow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pdf</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">timezone</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a DataFrame from a given pandas.DataFrame by slicing it into partitions, converting</span>
<span class="sd">        to Arrow data, then sending to the JVM to parallelize. If a schema is passed in, the</span>
<span class="sd">        data types will be used to coerce the data in Pandas to Arrow conversion.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.serializers</span> <span class="kn">import</span> <span class="n">ArrowStreamPandasSerializer</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">TimestampType</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">from_arrow_type</span><span class="p">,</span> <span class="n">to_arrow_type</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pandas_version</span><span class="p">,</span> \
            <span class="n">require_minimum_pyarrow_version</span>

        <span class="n">require_minimum_pandas_version</span><span class="p">()</span>
        <span class="n">require_minimum_pyarrow_version</span><span class="p">()</span>

        <span class="kn">from</span> <span class="nn">pandas.api.types</span> <span class="kn">import</span> <span class="n">is_datetime64_dtype</span><span class="p">,</span> <span class="n">is_datetime64tz_dtype</span>
        <span class="kn">import</span> <span class="nn">pyarrow</span> <span class="k">as</span> <span class="nn">pa</span>

        <span class="c1"># Create the Spark schema from list of names passed in with Arrow types</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">arrow_schema</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">Schema</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="n">preserve_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">struct</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">arrow_schema</span><span class="p">):</span>
                <span class="n">struct</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">from_arrow_type</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">type</span><span class="p">),</span> <span class="n">nullable</span><span class="o">=</span><span class="n">field</span><span class="o">.</span><span class="n">nullable</span><span class="p">)</span>
            <span class="n">schema</span> <span class="o">=</span> <span class="n">struct</span>

        <span class="c1"># Determine arrow types to coerce data when creating batches</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">StructType</span><span class="p">):</span>
            <span class="n">arrow_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">to_arrow_type</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">dataType</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">schema</span><span class="o">.</span><span class="n">fields</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">DataType</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Single data type </span><span class="si">%s</span><span class="s2"> is not supported with Arrow&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">schema</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Any timestamps must be coerced to be compatible with Spark</span>
            <span class="n">arrow_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">to_arrow_type</span><span class="p">(</span><span class="n">TimestampType</span><span class="p">())</span>
                           <span class="k">if</span> <span class="n">is_datetime64_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_datetime64tz_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
                           <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">pdf</span><span class="o">.</span><span class="n">dtypes</span><span class="p">]</span>

        <span class="c1"># Slice the DataFrame to be batched</span>
        <span class="n">step</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">defaultParallelism</span><span class="p">)</span>  <span class="c1"># round int up</span>
        <span class="n">pdf_slices</span> <span class="o">=</span> <span class="p">(</span><span class="n">pdf</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">start</span> <span class="o">+</span> <span class="n">step</span><span class="p">]</span> <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pdf</span><span class="p">),</span> <span class="n">step</span><span class="p">))</span>

        <span class="c1"># Create list of Arrow (columns, type) for serializer dump_stream</span>
        <span class="n">arrow_data</span> <span class="o">=</span> <span class="p">[[(</span><span class="n">c</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">c</span><span class="p">),</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pdf_slice</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(),</span> <span class="n">arrow_types</span><span class="p">)]</span>
                      <span class="k">for</span> <span class="n">pdf_slice</span> <span class="ow">in</span> <span class="n">pdf_slices</span><span class="p">]</span>

        <span class="n">jsqlContext</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="o">.</span><span class="n">_jsqlContext</span>

        <span class="n">safecheck</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">arrowSafeTypeConversion</span><span class="p">()</span>
        <span class="n">col_by_name</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># col by name only applies to StructType columns, can&#39;t happen here</span>
        <span class="n">ser</span> <span class="o">=</span> <span class="n">ArrowStreamPandasSerializer</span><span class="p">(</span><span class="n">timezone</span><span class="p">,</span> <span class="n">safecheck</span><span class="p">,</span> <span class="n">col_by_name</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">reader_func</span><span class="p">(</span><span class="n">temp_filename</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">PythonSQLUtils</span><span class="o">.</span><span class="n">readArrowStreamFromFile</span><span class="p">(</span><span class="n">jsqlContext</span><span class="p">,</span> <span class="n">temp_filename</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">create_RDD_server</span><span class="p">():</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">ArrowRDDServer</span><span class="p">(</span><span class="n">jsqlContext</span><span class="p">)</span>

        <span class="c1"># Create Spark DataFrame from Arrow stream file, using one batch per partition</span>
        <span class="n">jrdd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_serialize_to_jvm</span><span class="p">(</span><span class="n">arrow_data</span><span class="p">,</span> <span class="n">ser</span><span class="p">,</span> <span class="n">reader_func</span><span class="p">,</span> <span class="n">create_RDD_server</span><span class="p">)</span>
        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">PythonSQLUtils</span><span class="o">.</span><span class="n">toDataFrame</span><span class="p">(</span><span class="n">jrdd</span><span class="p">,</span> <span class="n">schema</span><span class="o">.</span><span class="n">json</span><span class="p">(),</span> <span class="n">jsqlContext</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="p">)</span>
        <span class="n">df</span><span class="o">.</span><span class="n">_schema</span> <span class="o">=</span> <span class="n">schema</span>
        <span class="k">return</span> <span class="n">df</span>


<span class="k">def</span> <span class="nf">_test</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
    <span class="kn">import</span> <span class="nn">pyspark.sql.pandas.conversion</span>
    <span class="n">globs</span> <span class="o">=</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">pandas</span><span class="o">.</span><span class="n">conversion</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span>\
        <span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[4]&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;sql.pandas.conversion tests&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;spark&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spark</span>
    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span>
        <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">pandas</span><span class="o">.</span><span class="n">conversion</span><span class="p">,</span> <span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
        <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">NORMALIZE_WHITESPACE</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">REPORT_NDIFF</span><span class="p">)</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../../index.html">
              <img class="logo" src="../../../../_static/spark-logo-hd.png" alt="Logo"/>
            </a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
    
        <li class="nav-item nav-item-0"><a href="../../../../index.html">PySpark 3.0.0 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="../../../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.3.1.
    </div>
  </body>
</html>