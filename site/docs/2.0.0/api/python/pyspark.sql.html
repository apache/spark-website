<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>pyspark.sql module &mdash; PySpark 2.0.0 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/pyspark.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '2.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/pyspark.js"></script>
    <link rel="top" title="PySpark 2.0.0 documentation" href="index.html" />
    <link rel="next" title="pyspark.streaming module" href="pyspark.streaming.html" />
    <link rel="prev" title="pyspark.mllib package" href="pyspark.mllib.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="pyspark.streaming.html" title="pyspark.streaming module"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="pyspark.mllib.html" title="pyspark.mllib package"
             accesskey="P">previous</a> |</li>
    
        <li><a href="index.html">PySpark 2.0.0 documentation</a> &raquo;</li>
 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="pyspark-sql-module">
<h1>pyspark.sql module<a class="headerlink" href="#pyspark-sql-module" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-pyspark.sql">
<span id="module-context"></span><h2>Module Context<a class="headerlink" href="#module-pyspark.sql" title="Permalink to this headline">¶</a></h2>
<p>Important classes of Spark SQL and DataFrames:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.SQLContext</span></tt></a>
Main entry point for <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> and SQL functionality.</li>
<li><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataFrame</span></tt></a>
A distributed collection of data grouped into named columns.</li>
<li><a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.Column</span></tt></a>
A column expression in a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</li>
<li><a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.Row</span></tt></a>
A row of data in a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</li>
<li><a class="reference internal" href="#pyspark.sql.HiveContext" title="pyspark.sql.HiveContext"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.HiveContext</span></tt></a>
Main entry point for accessing data stored in Apache Hive.</li>
<li><a class="reference internal" href="#pyspark.sql.GroupedData" title="pyspark.sql.GroupedData"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.GroupedData</span></tt></a>
Aggregation methods, returned by <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.groupBy()</span></tt></a>.</li>
<li><a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions" title="pyspark.sql.DataFrameNaFunctions"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataFrameNaFunctions</span></tt></a>
Methods for handling missing data (null values).</li>
<li><a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions" title="pyspark.sql.DataFrameStatFunctions"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataFrameStatFunctions</span></tt></a>
Methods for statistics functionality.</li>
<li><a class="reference internal" href="#module-pyspark.sql.functions" title="pyspark.sql.functions"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.functions</span></tt></a>
List of built-in functions available for <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</li>
<li><a class="reference internal" href="#module-pyspark.sql.types" title="pyspark.sql.types"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.types</span></tt></a>
List of data types available.</li>
<li><a class="reference internal" href="#pyspark.sql.Window" title="pyspark.sql.Window"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.Window</span></tt></a>
For working with window functions.</li>
</ul>
</div></blockquote>
<dl class="class">
<dt id="pyspark.sql.SparkSession">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">SparkSession</tt><big>(</big><em>sparkContext</em>, <em>jsparkSession=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession" title="Permalink to this definition">¶</a></dt>
<dd><p>The entry point to programming Spark with the Dataset and DataFrame API.</p>
<p>A SparkSession can be used create <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>, register <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as
tables, execute SQL over tables, cache tables, and read parquet files.
To create a SparkSession, use the following builder pattern:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s">&quot;local&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s">&quot;Word Count&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s">&quot;spark.some.config.option&quot;</span><span class="p">,</span> <span class="s">&quot;some-value&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>
</div>
<dl class="class">
<dt id="pyspark.sql.SparkSession.Builder">
<em class="property">class </em><tt class="descname">Builder</tt><a class="headerlink" href="#pyspark.sql.SparkSession.Builder" title="Permalink to this definition">¶</a></dt>
<dd><p>Builder for <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><tt class="xref py py-class docutils literal"><span class="pre">SparkSession</span></tt></a>.</p>
<dl class="method">
<dt id="pyspark.sql.SparkSession.Builder.appName">
<tt class="descname">appName</tt><big>(</big><em>name</em><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.Builder.appName" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets a name for the application, which will be shown in the Spark web UI.</p>
<p>If no application name is set, a randomly generated name will be used.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>name</strong> &#8211; an application name</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.Builder.config">
<tt class="descname">config</tt><big>(</big><em>key=None</em>, <em>value=None</em>, <em>conf=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.Builder.config" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets a config option. Options set using this method are automatically propagated to
both <tt class="xref py py-class docutils literal"><span class="pre">SparkConf</span></tt> and <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><tt class="xref py py-class docutils literal"><span class="pre">SparkSession</span></tt></a>&#8216;s own configuration.</p>
<p>For an existing SparkConf, use <cite>conf</cite> parameter.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.conf</span> <span class="kn">import</span> <span class="n">SparkConf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">SparkConf</span><span class="p">())</span>
<span class="go">&lt;pyspark.sql.session...</span>
</pre></div>
</div>
<p>For a (key, value) pair, you can omit parameter names.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s">&quot;spark.some.config.option&quot;</span><span class="p">,</span> <span class="s">&quot;some-value&quot;</span><span class="p">)</span>
<span class="go">&lt;pyspark.sql.session...</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>key</strong> &#8211; a key name string for configuration property</li>
<li><strong>value</strong> &#8211; a value for configuration property</li>
<li><strong>conf</strong> &#8211; an instance of <tt class="xref py py-class docutils literal"><span class="pre">SparkConf</span></tt></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.Builder.enableHiveSupport">
<tt class="descname">enableHiveSupport</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.Builder.enableHiveSupport" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables Hive support, including connectivity to a persistent Hive metastore, support
for Hive serdes, and Hive user-defined functions.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.Builder.getOrCreate">
<tt class="descname">getOrCreate</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.Builder.getOrCreate" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets an existing <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><tt class="xref py py-class docutils literal"><span class="pre">SparkSession</span></tt></a> or, if there is no existing one, creates a
new one based on the options set in this builder.</p>
<p>This method first checks whether there is a valid global default SparkSession, and if
yes, return that one. If no valid global default SparkSession exists, the method
creates a new SparkSession and assigns the newly created SparkSession as the global
default.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s">&quot;k1&quot;</span><span class="p">,</span> <span class="s">&quot;v1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;k1&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">s1</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;k1&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s">&quot;v1&quot;</span>
<span class="go">True</span>
</pre></div>
</div>
<p>In case an existing SparkSession is returned, the config options specified
in this builder will be applied to the existing SparkSession.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">s2</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s">&quot;k2&quot;</span><span class="p">,</span> <span class="s">&quot;v2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;k1&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">s2</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;k1&quot;</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;k2&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">s2</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;k2&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.Builder.master">
<tt class="descname">master</tt><big>(</big><em>master</em><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.Builder.master" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the Spark master URL to connect to, such as &#8220;local&#8221; to run locally, &#8220;local[4]&#8221;
to run locally with 4 cores, or &#8220;spark://master:7077&#8221; to run on a Spark standalone
cluster.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>master</strong> &#8211; a url for spark master</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SparkSession.builder">
<tt class="descclassname">SparkSession.</tt><tt class="descname">builder</tt><em class="property"> = &lt;pyspark.sql.session.Builder object at 0x7f8396ea2cd0&gt;</em><a class="headerlink" href="#pyspark.sql.SparkSession.builder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SparkSession.catalog">
<tt class="descclassname">SparkSession.</tt><tt class="descname">catalog</tt><a class="headerlink" href="#pyspark.sql.SparkSession.catalog" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface through which the user may create, drop, alter or query underlying
databases, tables, functions etc.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SparkSession.conf">
<tt class="descclassname">SparkSession.</tt><tt class="descname">conf</tt><a class="headerlink" href="#pyspark.sql.SparkSession.conf" title="Permalink to this definition">¶</a></dt>
<dd><p>Runtime configuration interface for Spark.</p>
<p>This is the interface through which the user can get and set all Spark and Hadoop
configurations that are relevant to Spark SQL. When getting the value of a config,
this defaults to the value set in the underlying <tt class="xref py py-class docutils literal"><span class="pre">SparkContext</span></tt>, if any.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.createDataFrame">
<tt class="descclassname">SparkSession.</tt><tt class="descname">createDataFrame</tt><big>(</big><em>data</em>, <em>schema=None</em>, <em>samplingRatio=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.createDataFrame" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> from an <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt>, a list or a <tt class="xref py py-class docutils literal"><span class="pre">pandas.DataFrame</span></tt>.</p>
<p>When <tt class="docutils literal"><span class="pre">schema</span></tt> is a list of column names, the type of each column
will be inferred from <tt class="docutils literal"><span class="pre">data</span></tt>.</p>
<p>When <tt class="docutils literal"><span class="pre">schema</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt>, it will try to infer the schema (column names and types)
from <tt class="docutils literal"><span class="pre">data</span></tt>, which should be an RDD of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>,
or <tt class="xref py py-class docutils literal"><span class="pre">namedtuple</span></tt>, or <tt class="xref py py-class docutils literal"><span class="pre">dict</span></tt>.</p>
<p>When <tt class="docutils literal"><span class="pre">schema</span></tt> is <tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt> or datatype string, it must match the real data, or
exception will be thrown at runtime. If the given schema is not StructType, it will be
wrapped into a StructType as its only field, and the field name will be &#8220;value&#8221;, each record
will also be wrapped into a tuple, which can be converted to row later.</p>
<p>If schema inference is needed, <tt class="docutils literal"><span class="pre">samplingRatio</span></tt> is used to determined the ratio of
rows used for schema inference. The first row will be used if <tt class="docutils literal"><span class="pre">samplingRatio</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>data</strong> &#8211; an RDD of any kind of SQL data representation(e.g. row, tuple, int, boolean,
etc.), or <tt class="xref py py-class docutils literal"><span class="pre">list</span></tt>, or <tt class="xref py py-class docutils literal"><span class="pre">pandas.DataFrame</span></tt>.</li>
<li><strong>schema</strong> &#8211; a <tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt> or a datatype string or a list of column names, default
is None.  The data type string format equals to <cite>DataType.simpleString</cite>, except that
top level struct type can omit the <cite>struct&lt;&gt;</cite> and atomic types use <cite>typeName()</cite> as
their format, e.g. use <cite>byte</cite> instead of <cite>tinyint</cite> for ByteType. We can also use <cite>int</cite>
as a short name for IntegerType.</li>
<li><strong>samplingRatio</strong> &#8211; the sample ratio of rows used for inferring</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></p>
</td>
</tr>
</tbody>
</table>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 2.0: </span>The schema parameter can be a DataType or a datatype string after 2.0. If it&#8217;s not a
StructType, it will be wrapped into a StructType and each record will also be wrapped
into a tuple.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="p">[(</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=u&#39;Alice&#39;, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="p">[{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=1, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=u&#39;Alice&#39;, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">person</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">Person</span><span class="p">(</span><span class="o">*</span><span class="n">r</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">person</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>   <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="gp">... </span>   <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(0=1, 1=2)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s">&quot;a: string, b: int&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(a=u&#39;Alice&#39;, b=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s">&quot;int&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(value=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s">&quot;boolean&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span> 
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">Py4JJavaError</span>: <span class="n">...</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.newSession">
<tt class="descclassname">SparkSession.</tt><tt class="descname">newSession</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.newSession" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new SparkSession as new session, that has separate SQLConf,
registered temporary views and UDFs, but shared SparkContext and
table cache.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.range">
<tt class="descclassname">SparkSession.</tt><tt class="descname">range</tt><big>(</big><em>start</em>, <em>end=None</em>, <em>step=1</em>, <em>numPartitions=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.range" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with single LongType column named <cite>id</cite>,
containing elements in a range from <cite>start</cite> to <cite>end</cite> (exclusive) with
step value <cite>step</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>start</strong> &#8211; the start value</li>
<li><strong>end</strong> &#8211; the end value (exclusive)</li>
<li><strong>step</strong> &#8211; the incremental step (default: 1)</li>
<li><strong>numPartitions</strong> &#8211; the number of partitions of the DataFrame</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></p>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=1), Row(id=3), Row(id=5)]</span>
</pre></div>
</div>
<p>If only one argument is specified, it will be used as the end value.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=0), Row(id=1), Row(id=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SparkSession.read">
<tt class="descclassname">SparkSession.</tt><tt class="descname">read</tt><a class="headerlink" href="#pyspark.sql.SparkSession.read" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameReader" title="pyspark.sql.DataFrameReader"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameReader</span></tt></a> that can be used to read data
in as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrameReader" title="pyspark.sql.DataFrameReader"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameReader</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SparkSession.readStream">
<tt class="descclassname">SparkSession.</tt><tt class="descname">readStream</tt><a class="headerlink" href="#pyspark.sql.SparkSession.readStream" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">DataStreamReader</span></tt> that can be used to read data streams
as a streaming <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><tt class="xref py py-class docutils literal"><span class="pre">DataStreamReader</span></tt></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SparkSession.sparkContext">
<tt class="descclassname">SparkSession.</tt><tt class="descname">sparkContext</tt><a class="headerlink" href="#pyspark.sql.SparkSession.sparkContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the underlying <tt class="xref py py-class docutils literal"><span class="pre">SparkContext</span></tt>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.sql">
<tt class="descclassname">SparkSession.</tt><tt class="descname">sql</tt><big>(</big><em>sqlQuery</em><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.sql" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> representing the result of the given query.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT field1 AS f1, field2 as f2 from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=1, f2=u&#39;row1&#39;), Row(f1=2, f2=u&#39;row2&#39;), Row(f1=3, f2=u&#39;row3&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.stop">
<tt class="descclassname">SparkSession.</tt><tt class="descname">stop</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stop the underlying <tt class="xref py py-class docutils literal"><span class="pre">SparkContext</span></tt>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SparkSession.streams">
<tt class="descclassname">SparkSession.</tt><tt class="descname">streams</tt><a class="headerlink" href="#pyspark.sql.SparkSession.streams" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">StreamingQueryManager</span></tt> that allows managing all the
<tt class="xref py py-class docutils literal"><span class="pre">StreamingQuery</span></tt> StreamingQueries active on <cite>this</cite> context.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><tt class="xref py py-class docutils literal"><span class="pre">StreamingQueryManager</span></tt></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SparkSession.table">
<tt class="descclassname">SparkSession.</tt><tt class="descname">table</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.SparkSession.table" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the specified table as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SparkSession.udf">
<tt class="descclassname">SparkSession.</tt><tt class="descname">udf</tt><a class="headerlink" href="#pyspark.sql.SparkSession.udf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">UDFRegistration</span></tt> for UDF registration.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><tt class="xref py py-class docutils literal"><span class="pre">UDFRegistration</span></tt></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SparkSession.version">
<tt class="descclassname">SparkSession.</tt><tt class="descname">version</tt><a class="headerlink" href="#pyspark.sql.SparkSession.version" title="Permalink to this definition">¶</a></dt>
<dd><p>The version of Spark on which this application is running.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.SQLContext">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">SQLContext</tt><big>(</big><em>sparkContext</em>, <em>sparkSession=None</em>, <em>jsqlContext=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext" title="Permalink to this definition">¶</a></dt>
<dd><p>The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.</p>
<p>As of Spark 2.0, this is replaced by <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><tt class="xref py py-class docutils literal"><span class="pre">SparkSession</span></tt></a>. However, we are keeping the class
here for backward compatibility.</p>
<p>A SQLContext can be used create <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>, register <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as
tables, execute SQL over tables, cache tables, and read parquet files.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sparkContext</strong> &#8211; The <tt class="xref py py-class docutils literal"><span class="pre">SparkContext</span></tt> backing this SQLContext.</li>
<li><strong>sparkSession</strong> &#8211; The <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><tt class="xref py py-class docutils literal"><span class="pre">SparkSession</span></tt></a> around which this SQLContext wraps.</li>
<li><strong>jsqlContext</strong> &#8211; An optional JVM Scala SQLContext. If set, we do not instantiate a new
SQLContext in the JVM, instead we make all calls to this object.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="pyspark.sql.SQLContext.cacheTable">
<tt class="descname">cacheTable</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.cacheTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Caches the specified table in-memory.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.clearCache">
<tt class="descname">clearCache</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.clearCache" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes all cached tables from the in-memory cache.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.createDataFrame">
<tt class="descname">createDataFrame</tt><big>(</big><em>data</em>, <em>schema=None</em>, <em>samplingRatio=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.createDataFrame" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> from an <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt>, a list or a <tt class="xref py py-class docutils literal"><span class="pre">pandas.DataFrame</span></tt>.</p>
<p>When <tt class="docutils literal"><span class="pre">schema</span></tt> is a list of column names, the type of each column
will be inferred from <tt class="docutils literal"><span class="pre">data</span></tt>.</p>
<p>When <tt class="docutils literal"><span class="pre">schema</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt>, it will try to infer the schema (column names and types)
from <tt class="docutils literal"><span class="pre">data</span></tt>, which should be an RDD of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>,
or <tt class="xref py py-class docutils literal"><span class="pre">namedtuple</span></tt>, or <tt class="xref py py-class docutils literal"><span class="pre">dict</span></tt>.</p>
<p>When <tt class="docutils literal"><span class="pre">schema</span></tt> is <tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt> or datatype string, it must match the real data, or
exception will be thrown at runtime. If the given schema is not StructType, it will be
wrapped into a StructType as its only field, and the field name will be &#8220;value&#8221;, each record
will also be wrapped into a tuple, which can be converted to row later.</p>
<p>If schema inference is needed, <tt class="docutils literal"><span class="pre">samplingRatio</span></tt> is used to determined the ratio of
rows used for schema inference. The first row will be used if <tt class="docutils literal"><span class="pre">samplingRatio</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>data</strong> &#8211; an RDD of any kind of SQL data representation(e.g. row, tuple, int, boolean,
etc.), or <tt class="xref py py-class docutils literal"><span class="pre">list</span></tt>, or <tt class="xref py py-class docutils literal"><span class="pre">pandas.DataFrame</span></tt>.</li>
<li><strong>schema</strong> &#8211; a <tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt> or a datatype string or a list of column names, default
is None.  The data type string format equals to <cite>DataType.simpleString</cite>, except that
top level struct type can omit the <cite>struct&lt;&gt;</cite> and atomic types use <cite>typeName()</cite> as
their format, e.g. use <cite>byte</cite> instead of <cite>tinyint</cite> for ByteType. We can also use <cite>int</cite>
as a short name for IntegerType.</li>
<li><strong>samplingRatio</strong> &#8211; the sample ratio of rows used for inferring</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></p>
</td>
</tr>
</tbody>
</table>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 2.0: </span>The schema parameter can be a DataType or a datatype string after 2.0. If it&#8217;s not a
StructType, it will be wrapped into a StructType and each record will also be wrapped
into a tuple.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="p">[(</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=u&#39;Alice&#39;, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="p">[{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=1, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=u&#39;Alice&#39;, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">person</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">Person</span><span class="p">(</span><span class="o">*</span><span class="n">r</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">person</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>   <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="gp">... </span>   <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(0=1, 1=2)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s">&quot;a: string, b: int&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(a=u&#39;Alice&#39;, b=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s">&quot;int&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(value=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="s">&quot;boolean&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span> 
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">Py4JJavaError</span>: <span class="n">...</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.createExternalTable">
<tt class="descname">createExternalTable</tt><big>(</big><em>tableName</em>, <em>path=None</em>, <em>source=None</em>, <em>schema=None</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.createExternalTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates an external table based on the dataset in a data source.</p>
<p>It returns the DataFrame associated with the external table.</p>
<p>The data source is specified by the <tt class="docutils literal"><span class="pre">source</span></tt> and a set of <tt class="docutils literal"><span class="pre">options</span></tt>.
If <tt class="docutils literal"><span class="pre">source</span></tt> is not specified, the default data source configured by
<tt class="docutils literal"><span class="pre">spark.sql.sources.default</span></tt> will be used.</p>
<p>Optionally, a schema can be provided as the schema of the returned <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> and
created external table.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.dropTempTable">
<tt class="descname">dropTempTable</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.dropTempTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove the temp table from catalog.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">dropTempTable</span><span class="p">(</span><span class="s">&quot;table1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.getConf">
<tt class="descname">getConf</tt><big>(</big><em>key</em>, <em>defaultValue=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.getConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of Spark SQL configuration property for the given key.</p>
<p>If the key is not set and defaultValue is not None, return
defaultValue. If the key is not set and defaultValue is None, return
the system default value.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">(</span><span class="s">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">)</span>
<span class="go">u&#39;200&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">(</span><span class="s">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="s">u&quot;10&quot;</span><span class="p">)</span>
<span class="go">u&#39;10&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="p">(</span><span class="s">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="s">u&quot;50&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">(</span><span class="s">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="s">u&quot;10&quot;</span><span class="p">)</span>
<span class="go">u&#39;50&#39;</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="classmethod">
<dt id="pyspark.sql.SQLContext.getOrCreate">
<em class="property">classmethod </em><tt class="descname">getOrCreate</tt><big>(</big><em>sc</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.getOrCreate" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the existing SQLContext or create a new one with given SparkContext.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sc</strong> &#8211; SparkContext</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.newSession">
<tt class="descname">newSession</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.newSession" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new SQLContext as new session, that has separate SQLConf,
registered temporary views and UDFs, but shared SparkContext and
table cache.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.range">
<tt class="descname">range</tt><big>(</big><em>start</em>, <em>end=None</em>, <em>step=1</em>, <em>numPartitions=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.range" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with single LongType column named <cite>id</cite>,
containing elements in a range from <cite>start</cite> to <cite>end</cite> (exclusive) with
step value <cite>step</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>start</strong> &#8211; the start value</li>
<li><strong>end</strong> &#8211; the end value (exclusive)</li>
<li><strong>step</strong> &#8211; the incremental step (default: 1)</li>
<li><strong>numPartitions</strong> &#8211; the number of partitions of the DataFrame</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></p>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=1), Row(id=3), Row(id=5)]</span>
</pre></div>
</div>
<p>If only one argument is specified, it will be used as the end value.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=0), Row(id=1), Row(id=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SQLContext.read">
<tt class="descname">read</tt><a class="headerlink" href="#pyspark.sql.SQLContext.read" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameReader" title="pyspark.sql.DataFrameReader"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameReader</span></tt></a> that can be used to read data
in as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrameReader" title="pyspark.sql.DataFrameReader"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameReader</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SQLContext.readStream">
<tt class="descname">readStream</tt><a class="headerlink" href="#pyspark.sql.SQLContext.readStream" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">DataStreamReader</span></tt> that can be used to read data streams
as a streaming <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><tt class="xref py py-class docutils literal"><span class="pre">DataStreamReader</span></tt></td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">text_sdf</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.registerDataFrameAsTable">
<tt class="descname">registerDataFrameAsTable</tt><big>(</big><em>df</em>, <em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.registerDataFrameAsTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers the given <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as a temporary table in the catalog.</p>
<p>Temporary tables exist only during the lifetime of this instance of <a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">SQLContext</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.registerFunction">
<tt class="descname">registerFunction</tt><big>(</big><em>name</em>, <em>f</em>, <em>returnType=StringType</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.registerFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a python function (including lambda function) as a UDF
so it can be used in SQL statements.</p>
<p>In addition to a name and the function itself, the return type can be optionally specified.
When the return type is not given it default to a string and conversion will automatically
be done.  For any other return type, the produced object must match the specified type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> &#8211; name of the UDF</li>
<li><strong>f</strong> &#8211; python function</li>
<li><strong>returnType</strong> &#8211; a <tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt> object</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerFunction</span><span class="p">(</span><span class="s">&quot;stringLengthString&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT stringLengthString(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(stringLengthString(test)=u&#39;4&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerFunction</span><span class="p">(</span><span class="s">&quot;stringLengthInt&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT stringLengthInt(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(stringLengthInt(test)=4)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s">&quot;stringLengthInt&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT stringLengthInt(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(stringLengthInt(test)=4)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.2.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.setConf">
<tt class="descname">setConf</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.setConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the given Spark SQL configuration property.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.sql">
<tt class="descname">sql</tt><big>(</big><em>sqlQuery</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.sql" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> representing the result of the given query.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT field1 AS f1, field2 as f2 from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=1, f2=u&#39;row1&#39;), Row(f1=2, f2=u&#39;row2&#39;), Row(f1=3, f2=u&#39;row3&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SQLContext.streams">
<tt class="descname">streams</tt><a class="headerlink" href="#pyspark.sql.SQLContext.streams" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">StreamingQueryManager</span></tt> that allows managing all the
<tt class="xref py py-class docutils literal"><span class="pre">StreamingQuery</span></tt> StreamingQueries active on <cite>this</cite> context.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.table">
<tt class="descname">table</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.table" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the specified table as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.tableNames">
<tt class="descname">tableNames</tt><big>(</big><em>dbName=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.tableNames" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of names of tables in the database <tt class="docutils literal"><span class="pre">dbName</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dbName</strong> &#8211; string, name of the database to use. Default to the current database.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of table names, in string</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s">&quot;table1&quot;</span> <span class="ow">in</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">tableNames</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s">&quot;table1&quot;</span> <span class="ow">in</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">tableNames</span><span class="p">(</span><span class="s">&quot;default&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.tables">
<tt class="descname">tables</tt><big>(</big><em>dbName=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.tables" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing names of tables in the given database.</p>
<p>If <tt class="docutils literal"><span class="pre">dbName</span></tt> is not specified, the current database will be used.</p>
<p>The returned DataFrame has two columns: <tt class="docutils literal"><span class="pre">tableName</span></tt> and <tt class="docutils literal"><span class="pre">isTemporary</span></tt>
(a column with <tt class="xref py py-class docutils literal"><span class="pre">BooleanType</span></tt> indicating if a table is a temporary one or not).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dbName</strong> &#8211; string, name of the database to use.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">tables</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s">&quot;tableName = &#39;table1&#39;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">Row(tableName=u&#39;table1&#39;, isTemporary=True)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SQLContext.udf">
<tt class="descname">udf</tt><a class="headerlink" href="#pyspark.sql.SQLContext.udf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">UDFRegistration</span></tt> for UDF registration.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><tt class="xref py py-class docutils literal"><span class="pre">UDFRegistration</span></tt></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.uncacheTable">
<tt class="descname">uncacheTable</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.uncacheTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the specified table from the in-memory cache.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.HiveContext">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">HiveContext</tt><big>(</big><em>sparkContext</em>, <em>jhiveContext=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.HiveContext" title="Permalink to this definition">¶</a></dt>
<dd><p>A variant of Spark SQL that integrates with data stored in Hive.</p>
<p>Configuration for Hive is read from <tt class="docutils literal"><span class="pre">hive-site.xml</span></tt> on the classpath.
It supports running both SQL and HiveQL commands.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sparkContext</strong> &#8211; The SparkContext to wrap.</li>
<li><strong>jhiveContext</strong> &#8211; An optional JVM Scala HiveContext. If set, we do not instantiate a new
<a class="reference internal" href="#pyspark.sql.HiveContext" title="pyspark.sql.HiveContext"><tt class="xref py py-class docutils literal"><span class="pre">HiveContext</span></tt></a> in the JVM, instead we make all calls to this object.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 2.0.0. Use SparkSession.builder.enableHiveSupport().getOrCreate().</p>
</div>
<dl class="method">
<dt id="pyspark.sql.HiveContext.refreshTable">
<tt class="descname">refreshTable</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.HiveContext.refreshTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Invalidate and refresh all the cached the metadata of the given
table. For performance reasons, Spark SQL or the external data source
library it uses might cache certain metadata about a table, such as the
location of blocks. When those change outside of Spark SQL, users should
call this function to invalidate the cache.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrame">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DataFrame</tt><big>(</big><em>jdf</em>, <em>sql_ctx</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame" title="Permalink to this definition">¶</a></dt>
<dd><p>A distributed collection of data grouped into named columns.</p>
<p>A <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> is equivalent to a relational table in Spark SQL,
and can be created using various functions in <a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">SQLContext</span></tt></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">people</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">&quot;...&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Once created, it can be manipulated using the various domain-specific-language
(DSL) functions defined in: <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>, <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a>.</p>
<p>To select a column from the data frame, use the apply method:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ageCol</span> <span class="o">=</span> <span class="n">people</span><span class="o">.</span><span class="n">age</span>
</pre></div>
</div>
<p>A more concrete example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># To create DataFrame using SQLContext</span>
<span class="n">people</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">&quot;...&quot;</span><span class="p">)</span>
<span class="n">department</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">&quot;...&quot;</span><span class="p">)</span>

<span class="n">people</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">people</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">)</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">department</span><span class="p">,</span> <span class="n">people</span><span class="o">.</span><span class="n">deptId</span> <span class="o">==</span> <span class="n">department</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>          <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">department</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s">&quot;salary&quot;</span><span class="p">:</span> <span class="s">&quot;avg&quot;</span><span class="p">,</span> <span class="s">&quot;age&quot;</span><span class="p">:</span> <span class="s">&quot;max&quot;</span><span class="p">})</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrame.agg">
<tt class="descname">agg</tt><big>(</big><em>*exprs</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.agg" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate on the entire <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> without groups
(shorthand for <tt class="docutils literal"><span class="pre">df.groupBy.agg()</span></tt>).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s">&quot;age&quot;</span><span class="p">:</span> <span class="s">&quot;max&quot;</span><span class="p">})</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(max(age)=5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(min(age)=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.alias">
<tt class="descname">alias</tt><big>(</big><em>alias</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.alias" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with an alias set.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_as1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;df_as1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_as2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;df_as2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">joined_df</span> <span class="o">=</span> <span class="n">df_as1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df_as2</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">&quot;df_as1.name&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">col</span><span class="p">(</span><span class="s">&quot;df_as2.name&quot;</span><span class="p">),</span> <span class="s">&#39;inner&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">joined_df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&quot;df_as1.name&quot;</span><span class="p">,</span> <span class="s">&quot;df_as2.name&quot;</span><span class="p">,</span> <span class="s">&quot;df_as2.age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Bob&#39;, name=u&#39;Bob&#39;, age=5), Row(name=u&#39;Alice&#39;, name=u&#39;Alice&#39;, age=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.approxQuantile">
<tt class="descname">approxQuantile</tt><big>(</big><em>col</em>, <em>probabilities</em>, <em>relativeError</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.approxQuantile" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the approximate quantiles of a numerical column of a
DataFrame.</p>
<p>The result of this algorithm has the following deterministic bound:
If the DataFrame has N elements and if we request the quantile at
probability <cite>p</cite> up to error <cite>err</cite>, then the algorithm will return
a sample <cite>x</cite> from the DataFrame so that the <em>exact</em> rank of <cite>x</cite> is
close to (p * N). More precisely,</p>
<blockquote>
<div>floor((p - err) * N) &lt;= rank(x) &lt;= ceil((p + err) * N).</div></blockquote>
<p>This method implements a variation of the Greenwald-Khanna
algorithm (with some speed optimizations). The algorithm was first
present in [[<a class="reference external" href="http://dx.doi.org/10.1145/375663.375670">http://dx.doi.org/10.1145/375663.375670</a>
Space-efficient Online Computation of Quantile Summaries]]
by Greenwald and Khanna.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>col</strong> &#8211; the name of the numerical column</li>
<li><strong>probabilities</strong> &#8211; a list of quantile probabilities
Each number must belong to [0, 1].
For example 0 is the minimum, 0.5 is the median, 1 is the maximum.</li>
<li><strong>relativeError</strong> &#8211; The relative target precision to achieve
(&gt;= 0). If set to zero, the exact quantiles are computed, which
could be very expensive. Note that values greater than 1 are
accepted but give the same result as 1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">the approximate quantiles at the given probabilities</p>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.cache">
<tt class="descname">cache</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.cache" title="Permalink to this definition">¶</a></dt>
<dd><p>Persists with the default storage level (<tt class="xref py py-class docutils literal"><span class="pre">MEMORY_ONLY</span></tt>).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.coalesce">
<tt class="descname">coalesce</tt><big>(</big><em>numPartitions</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.coalesce" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> that has exactly <cite>numPartitions</cite> partitions.</p>
<p>Similar to coalesce defined on an <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt>, this operation results in a
narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,
there will not be a shuffle, instead each of the 100 new partitions will
claim 10 of the current partitions.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.collect">
<tt class="descname">collect</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.collect" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all the records as a list of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.columns">
<tt class="descname">columns</tt><a class="headerlink" href="#pyspark.sql.DataFrame.columns" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all column names as a list.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">columns</span>
<span class="go">[&#39;age&#39;, &#39;name&#39;]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.corr">
<tt class="descname">corr</tt><big>(</big><em>col1</em>, <em>col2</em>, <em>method=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.corr" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the correlation of two columns of a DataFrame as a double value.
Currently only supports the Pearson Correlation Coefficient.
<a class="reference internal" href="#pyspark.sql.DataFrame.corr" title="pyspark.sql.DataFrame.corr"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.corr()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.corr" title="pyspark.sql.DataFrameStatFunctions.corr"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.corr()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column</li>
<li><strong>col2</strong> &#8211; The name of the second column</li>
<li><strong>method</strong> &#8211; The correlation method. Currently only supports &#8220;pearson&#8221;</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.count">
<tt class="descname">count</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of rows in this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">2</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.cov">
<tt class="descname">cov</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.cov" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the sample covariance for the given columns, specified by their names, as a
double value. <a class="reference internal" href="#pyspark.sql.DataFrame.cov" title="pyspark.sql.DataFrame.cov"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.cov()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.cov" title="pyspark.sql.DataFrameStatFunctions.cov"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.cov()</span></tt></a> are aliases.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column</li>
<li><strong>col2</strong> &#8211; The name of the second column</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.createOrReplaceTempView">
<tt class="descname">createOrReplaceTempView</tt><big>(</big><em>name</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.createOrReplaceTempView" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates or replaces a temporary view with this DataFrame.</p>
<p>The lifetime of this temporary table is tied to the <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><tt class="xref py py-class docutils literal"><span class="pre">SparkSession</span></tt></a>
that was used to create this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;select * from people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df3</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">dropTempView</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.createTempView">
<tt class="descname">createTempView</tt><big>(</big><em>name</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.createTempView" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a temporary view with this DataFrame.</p>
<p>The lifetime of this temporary table is tied to the <a class="reference internal" href="#pyspark.sql.SparkSession" title="pyspark.sql.SparkSession"><tt class="xref py py-class docutils literal"><span class="pre">SparkSession</span></tt></a>
that was used to create this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.
throws <tt class="xref py py-class docutils literal"><span class="pre">TempTableAlreadyExistsException</span></tt>, if the view name already exists in the
catalog.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createTempView</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;select * from people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createTempView</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>  
<span class="gt">Traceback (most recent call last):</span>
<span class="c">...</span>
<span class="gr">AnalysisException</span>: <span class="n">u&quot;Temporary table &#39;people&#39; already exists;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">dropTempView</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.crosstab">
<tt class="descname">crosstab</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.crosstab" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a pair-wise frequency table of the given columns. Also known as a contingency
table. The number of distinct values for each column should be less than 1e4. At most 1e6
non-zero pair frequencies will be returned.
The first column of each row will be the distinct values of <cite>col1</cite> and the column names
will be the distinct values of <cite>col2</cite>. The name of the first column will be <cite>$col1_$col2</cite>.
Pairs that have no occurrences will have zero as their counts.
<a class="reference internal" href="#pyspark.sql.DataFrame.crosstab" title="pyspark.sql.DataFrame.crosstab"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.crosstab()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.crosstab" title="pyspark.sql.DataFrameStatFunctions.crosstab"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.crosstab()</span></tt></a> are aliases.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column. Distinct items will make the first item of
each row.</li>
<li><strong>col2</strong> &#8211; The name of the second column. Distinct items will make the column names
of the DataFrame.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.cube">
<tt class="descname">cube</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.cube" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a multi-dimensional cube for the current <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> using
the specified columns, so we can run aggregation on them.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">cube</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="s">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| name| age|count|</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| null|null|    2|</span>
<span class="go">| null|   2|    1|</span>
<span class="go">| null|   5|    1|</span>
<span class="go">|Alice|null|    1|</span>
<span class="go">|Alice|   2|    1|</span>
<span class="go">|  Bob|null|    1|</span>
<span class="go">|  Bob|   5|    1|</span>
<span class="go">+-----+----+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.describe">
<tt class="descname">describe</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.describe" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes statistics for numeric columns.</p>
<p>This include count, mean, stddev, min, and max. If no columns are
given, this function computes statistics for all numerical columns.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is meant for exploratory data analysis, as we make no         guarantee about the backward compatibility of the schema of the resulting DataFrame.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------+------------------+</span>
<span class="go">|summary|               age|</span>
<span class="go">+-------+------------------+</span>
<span class="go">|  count|                 2|</span>
<span class="go">|   mean|               3.5|</span>
<span class="go">| stddev|2.1213203435596424|</span>
<span class="go">|    min|                 2|</span>
<span class="go">|    max|                 5|</span>
<span class="go">+-------+------------------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">([</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------+------------------+-----+</span>
<span class="go">|summary|               age| name|</span>
<span class="go">+-------+------------------+-----+</span>
<span class="go">|  count|                 2|    2|</span>
<span class="go">|   mean|               3.5| null|</span>
<span class="go">| stddev|2.1213203435596424| null|</span>
<span class="go">|    min|                 2|Alice|</span>
<span class="go">|    max|                 5|  Bob|</span>
<span class="go">+-------+------------------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.distinct">
<tt class="descname">distinct</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.distinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing the distinct rows in this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">2</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.drop">
<tt class="descname">drop</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.drop" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> that drops the specified column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>col</strong> &#8211; a string name of the column to drop, or a
<a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> to drop.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;), Row(name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;), Row(name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s">&#39;inner&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, height=85, name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s">&#39;inner&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;, height=85)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.dropDuplicates">
<tt class="descname">dropDuplicates</tt><big>(</big><em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.dropDuplicates" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with duplicate rows removed,
optionally only considering certain columns.</p>
<p><a class="reference internal" href="#pyspark.sql.DataFrame.drop_duplicates" title="pyspark.sql.DataFrame.drop_duplicates"><tt class="xref py py-func docutils literal"><span class="pre">drop_duplicates()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.dropDuplicates" title="pyspark.sql.DataFrame.dropDuplicates"><tt class="xref py py-func docutils literal"><span class="pre">dropDuplicates()</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span> \
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">),</span> \
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">),</span> \
<span class="gp">... </span>    <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">)])</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">|  5|    80|Alice|</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">([</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">|  5|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.drop_duplicates">
<tt class="descname">drop_duplicates</tt><big>(</big><em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.drop_duplicates" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#pyspark.sql.DataFrame.drop_duplicates" title="pyspark.sql.DataFrame.drop_duplicates"><tt class="xref py py-func docutils literal"><span class="pre">drop_duplicates()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.dropDuplicates" title="pyspark.sql.DataFrame.dropDuplicates"><tt class="xref py py-func docutils literal"><span class="pre">dropDuplicates()</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.dropna">
<tt class="descname">dropna</tt><big>(</big><em>how='any'</em>, <em>thresh=None</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.dropna" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> omitting rows with null values.
<a class="reference internal" href="#pyspark.sql.DataFrame.dropna" title="pyspark.sql.DataFrame.dropna"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.dropna()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.drop" title="pyspark.sql.DataFrameNaFunctions.drop"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.drop()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>how</strong> &#8211; &#8216;any&#8217; or &#8216;all&#8217;.
If &#8216;any&#8217;, drop a row if it contains any nulls.
If &#8216;all&#8217;, drop a row only if all its values are null.</li>
<li><strong>thresh</strong> &#8211; int, default None
If specified, drop rows that have less than <cite>thresh</cite> non-null values.
This overwrites the <cite>how</cite> parameter.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">drop</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.dtypes">
<tt class="descname">dtypes</tt><a class="headerlink" href="#pyspark.sql.DataFrame.dtypes" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all column names and their data types as a list.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;int&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.explain">
<tt class="descname">explain</tt><big>(</big><em>extended=False</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.explain" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the (logical and physical) plans to the console for debugging purpose.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>extended</strong> &#8211; boolean, default <tt class="docutils literal"><span class="pre">False</span></tt>. If <tt class="docutils literal"><span class="pre">False</span></tt>, prints only the physical plan.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">explain</span><span class="p">()</span>
<span class="go">== Physical Plan ==</span>
<span class="go">Scan ExistingRDD[age#0,name#1]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="go">== Parsed Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Analyzed Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Optimized Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Physical Plan ==</span>
<span class="gp">...</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.fillna">
<tt class="descname">fillna</tt><big>(</big><em>value</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.fillna" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace null values, alias for <tt class="docutils literal"><span class="pre">na.fill()</span></tt>.
<a class="reference internal" href="#pyspark.sql.DataFrame.fillna" title="pyspark.sql.DataFrame.fillna"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.fillna()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.fill" title="pyspark.sql.DataFrameNaFunctions.fill"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.fill()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>value</strong> &#8211; int, long, float, string, or dict.
Value to replace null values with.
If the value is a dict, then <cite>subset</cite> is ignored and <cite>value</cite> must be a mapping
from column name (string) to replacement value. The replacement value must be
an int, long, float, or string.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">|  5|    50|  Bob|</span>
<span class="go">| 50|    50|  Tom|</span>
<span class="go">| 50|    50| null|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">({</span><span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;unknown&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-------+</span>
<span class="go">|age|height|   name|</span>
<span class="go">+---+------+-------+</span>
<span class="go">| 10|    80|  Alice|</span>
<span class="go">|  5|  null|    Bob|</span>
<span class="go">| 50|  null|    Tom|</span>
<span class="go">| 50|  null|unknown|</span>
<span class="go">+---+------+-------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.filter">
<tt class="descname">filter</tt><big>(</big><em>condition</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.filter" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters rows using the given condition.</p>
<p><a class="reference internal" href="#pyspark.sql.DataFrame.where" title="pyspark.sql.DataFrame.where"><tt class="xref py py-func docutils literal"><span class="pre">where()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.filter" title="pyspark.sql.DataFrame.filter"><tt class="xref py py-func docutils literal"><span class="pre">filter()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>condition</strong> &#8211; a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> of <a class="reference internal" href="#pyspark.sql.types.BooleanType" title="pyspark.sql.types.BooleanType"><tt class="xref py py-class docutils literal"><span class="pre">types.BooleanType</span></tt></a>
or a string of SQL expression.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s">&quot;age &gt; 3&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s">&quot;age = 2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.first">
<tt class="descname">first</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.first" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first row as a <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">Row(age=2, name=u&#39;Alice&#39;)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.foreach">
<tt class="descname">foreach</tt><big>(</big><em>f</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.foreach" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the <tt class="docutils literal"><span class="pre">f</span></tt> function to all <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a> of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>This is a shorthand for <tt class="docutils literal"><span class="pre">df.rdd.foreach()</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">person</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="n">person</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.foreachPartition">
<tt class="descname">foreachPartition</tt><big>(</big><em>f</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.foreachPartition" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the <tt class="docutils literal"><span class="pre">f</span></tt> function to each partition of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>This a shorthand for <tt class="docutils literal"><span class="pre">df.rdd.foreachPartition()</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">people</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">person</span> <span class="ow">in</span> <span class="n">people</span><span class="p">:</span>
<span class="gp">... </span>        <span class="k">print</span><span class="p">(</span><span class="n">person</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">foreachPartition</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.freqItems">
<tt class="descname">freqItems</tt><big>(</big><em>cols</em>, <em>support=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.freqItems" title="Permalink to this definition">¶</a></dt>
<dd><p>Finding frequent items for columns, possibly with false positives. Using the
frequent element count algorithm described in
&#8220;<a class="reference external" href="http://dx.doi.org/10.1145/762471.762473">http://dx.doi.org/10.1145/762471.762473</a>, proposed by Karp, Schenker, and Papadimitriou&#8221;.
<a class="reference internal" href="#pyspark.sql.DataFrame.freqItems" title="pyspark.sql.DataFrame.freqItems"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.freqItems()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.freqItems" title="pyspark.sql.DataFrameStatFunctions.freqItems"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.freqItems()</span></tt></a> are aliases.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is meant for exploratory data analysis, as we make no         guarantee about the backward compatibility of the schema of the resulting DataFrame.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>cols</strong> &#8211; Names of the columns to calculate frequent items for as a list or tuple of
strings.</li>
<li><strong>support</strong> &#8211; The frequency with which to consider an item &#8216;frequent&#8217;. Default is 1%.
The support must be greater than 1e-4.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.groupBy">
<tt class="descname">groupBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.groupBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Groups the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> using the specified columns,
so we can run aggregation on them. See <a class="reference internal" href="#pyspark.sql.GroupedData" title="pyspark.sql.GroupedData"><tt class="xref py py-class docutils literal"><span class="pre">GroupedData</span></tt></a>
for all the available aggregate functions.</p>
<p><a class="reference internal" href="#pyspark.sql.DataFrame.groupby" title="pyspark.sql.DataFrame.groupby"><tt class="xref py py-func docutils literal"><span class="pre">groupby()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><tt class="xref py py-func docutils literal"><span class="pre">groupBy()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of columns to group by.
Each element should be a column name (string) or an expression (<a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a>).</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">avg</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(avg(age)=3.5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="s">&#39;mean&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(name=u&#39;Alice&#39;, avg(age)=2.0), Row(name=u&#39;Bob&#39;, avg(age)=5.0)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">avg</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(name=u&#39;Alice&#39;, avg(age)=2.0), Row(name=u&#39;Bob&#39;, avg(age)=5.0)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">([</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">])</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=2, count=1), Row(name=u&#39;Bob&#39;, age=5, count=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.groupby">
<tt class="descname">groupby</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.groupby" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#pyspark.sql.DataFrame.groupby" title="pyspark.sql.DataFrame.groupby"><tt class="xref py py-func docutils literal"><span class="pre">groupby()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><tt class="xref py py-func docutils literal"><span class="pre">groupBy()</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.head">
<tt class="descname">head</tt><big>(</big><em>n=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.head" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first <tt class="docutils literal"><span class="pre">n</span></tt> rows.</p>
<p>Note that this method should only be used if the resulting array is expected
to be small, as all the data is loaded into the driver&#8217;s memory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>n</strong> &#8211; int, default 1. Number of rows to return.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">If n is greater than 1, return a list of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.
If n is 1, return a single Row.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="go">Row(age=2, name=u&#39;Alice&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.intersect">
<tt class="descname">intersect</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.intersect" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing rows only in
both this frame and another frame.</p>
<p>This is equivalent to <cite>INTERSECT</cite> in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.isLocal">
<tt class="descname">isLocal</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.isLocal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <tt class="docutils literal"><span class="pre">True</span></tt> if the <a class="reference internal" href="#pyspark.sql.DataFrame.collect" title="pyspark.sql.DataFrame.collect"><tt class="xref py py-func docutils literal"><span class="pre">collect()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrame.take" title="pyspark.sql.DataFrame.take"><tt class="xref py py-func docutils literal"><span class="pre">take()</span></tt></a> methods can be run locally
(without any Spark executors).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.isStreaming">
<tt class="descname">isStreaming</tt><a class="headerlink" href="#pyspark.sql.DataFrame.isStreaming" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns true if this <tt class="xref py py-class docutils literal"><span class="pre">Dataset</span></tt> contains one or more sources that continuously
return data as it arrives. A <tt class="xref py py-class docutils literal"><span class="pre">Dataset</span></tt> that reads data from a streaming source
must be executed as a <tt class="xref py py-class docutils literal"><span class="pre">StreamingQuery</span></tt> using the <tt class="xref py py-func docutils literal"><span class="pre">start()</span></tt> method in
<tt class="xref py py-class docutils literal"><span class="pre">DataStreamWriter</span></tt>.  Methods that return a single answer, (e.g., <a class="reference internal" href="#pyspark.sql.DataFrame.count" title="pyspark.sql.DataFrame.count"><tt class="xref py py-func docutils literal"><span class="pre">count()</span></tt></a> or
<a class="reference internal" href="#pyspark.sql.DataFrame.collect" title="pyspark.sql.DataFrame.collect"><tt class="xref py py-func docutils literal"><span class="pre">collect()</span></tt></a>) will throw an <tt class="xref py py-class docutils literal"><span class="pre">AnalysisException</span></tt> when there is a streaming
source present.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.join">
<tt class="descname">join</tt><big>(</big><em>other</em>, <em>on=None</em>, <em>how=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Joins with another <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>, using the given join expression.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>other</strong> &#8211; Right side of the join</li>
<li><strong>on</strong> &#8211; a string for the join column name, a list of column names,
a join expression (Column), or a list of Columns.
If <cite>on</cite> is a string or a list of strings indicating the name of the join column(s),
the column(s) must exist on both sides, and this performs an equi-join.</li>
<li><strong>how</strong> &#8211; str, default &#8216;inner&#8217;.
One of <cite>inner</cite>, <cite>outer</cite>, <cite>left_outer</cite>, <cite>right_outer</cite>, <cite>leftsemi</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The following performs a full outer join between <tt class="docutils literal"><span class="pre">df1</span></tt> and <tt class="docutils literal"><span class="pre">df2</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s">&#39;outer&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">height</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=None, height=80), Row(name=u&#39;Bob&#39;, height=85), Row(name=u&#39;Alice&#39;, height=None)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;outer&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Tom&#39;, height=80), Row(name=u&#39;Bob&#39;, height=85), Row(name=u&#39;Alice&#39;, height=None)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cond</span> <span class="o">=</span> <span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df3</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">==</span> <span class="n">df3</span><span class="o">.</span><span class="n">age</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df3</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="s">&#39;outer&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df3</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=2), Row(name=u&#39;Bob&#39;, age=5)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">height</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Bob&#39;, height=85)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df4</span><span class="p">,</span> <span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Bob&#39;, age=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.limit">
<tt class="descname">limit</tt><big>(</big><em>num</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.limit" title="Permalink to this definition">¶</a></dt>
<dd><p>Limits the result count to the number specified.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.na">
<tt class="descname">na</tt><a class="headerlink" href="#pyspark.sql.DataFrame.na" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions" title="pyspark.sql.DataFrameNaFunctions"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameNaFunctions</span></tt></a> for handling missing values.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.orderBy">
<tt class="descname">orderBy</tt><big>(</big><em>*cols</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.orderBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> sorted by the specified column(s).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>cols</strong> &#8211; list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> or column names to sort by.</li>
<li><strong>ascending</strong> &#8211; boolean or list of boolean (default True).
Sort ascending vs. descending. Specify list for multiple sort orders.
If a list is specified, length of the list must equal length of the <cite>cols</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">asc</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">desc</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">),</span> <span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">([</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.persist">
<tt class="descname">persist</tt><big>(</big><em>storageLevel=StorageLevel(False</em>, <em>True</em>, <em>False</em>, <em>False</em>, <em>1)</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.persist" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the storage level to persist its values across operations
after the first time it is computed. This can only be used to assign
a new storage level if the RDD does not have a storage level set yet.
If no storage level is specified defaults to (<tt class="xref py py-class docutils literal"><span class="pre">MEMORY_ONLY</span></tt>).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.printSchema">
<tt class="descname">printSchema</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.printSchema" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints out the schema in the tree format.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="go">root</span>
<span class="go"> |-- age: integer (nullable = true)</span>
<span class="go"> |-- name: string (nullable = true)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.randomSplit">
<tt class="descname">randomSplit</tt><big>(</big><em>weights</em>, <em>seed=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.randomSplit" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly splits this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with the provided weights.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weights</strong> &#8211; list of doubles as weights with which to split the DataFrame. Weights will
be normalized if they don&#8217;t sum up to 1.0.</li>
<li><strong>seed</strong> &#8211; The seed for sampling.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span> <span class="o">=</span> <span class="n">df4</span><span class="o">.</span><span class="n">randomSplit</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="mi">24</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.rdd">
<tt class="descname">rdd</tt><a class="headerlink" href="#pyspark.sql.DataFrame.rdd" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the content as an <a class="reference internal" href="pyspark.html#pyspark.RDD" title="pyspark.RDD"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.RDD</span></tt></a> of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.registerTempTable">
<tt class="descname">registerTempTable</tt><big>(</big><em>name</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.registerTempTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers this RDD as a temporary table using the given name.</p>
<p>The lifetime of this temporary table is tied to the <a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">SQLContext</span></tt></a>
that was used to create this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;select * from people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">dropTempView</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 2.0, use createOrReplaceTempView instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.repartition">
<tt class="descname">repartition</tt><big>(</big><em>numPartitions</em>, <em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.repartition" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> partitioned by the given partitioning expressions. The
resulting DataFrame is hash partitioned.</p>
<p><tt class="docutils literal"><span class="pre">numPartitions</span></tt> can be an int to specify the target number of partitions or a Column.
If it is a Column, it will be used as the first partitioning column. If not specified,
the default number of partitions is used.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 1.6: </span>Added optional arguments to specify the partitioning columns. Also made numPartitions
optional if partitioning columns are specified.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
<span class="go">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  2|Alice|</span>
<span class="go">+---+-----+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="s">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  2|Alice|</span>
<span class="go">+---+-----+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
<span class="go">7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="s">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  2|Alice|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.replace">
<tt class="descname">replace</tt><big>(</big><em>to_replace</em>, <em>value</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> replacing a value with another value.
<a class="reference internal" href="#pyspark.sql.DataFrame.replace" title="pyspark.sql.DataFrame.replace"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.replace()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.replace" title="pyspark.sql.DataFrameNaFunctions.replace"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.replace()</span></tt></a> are
aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>to_replace</strong> &#8211; int, long, float, string, or list.
Value to be replaced.
If the value is a dict, then <cite>value</cite> is ignored and <cite>to_replace</cite> must be a
mapping from column name (string) to replacement value. The value to be
replaced must be an int, long, float, or string.</li>
<li><strong>value</strong> &#8211; int, long, float, string, or list.
Value to use to replace holes.
The replacement value must be an int, long, float, or string. If <cite>value</cite> is a
list or tuple, <cite>value</cite> should be of the same length with <cite>to_replace</cite>.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+-----+</span>
<span class="go">| age|height| name|</span>
<span class="go">+----+------+-----+</span>
<span class="go">|  20|    80|Alice|</span>
<span class="go">|   5|  null|  Bob|</span>
<span class="go">|null|  null|  Tom|</span>
<span class="go">|null|  null| null|</span>
<span class="go">+----+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="s">&#39;Bob&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;A&#39;</span><span class="p">,</span> <span class="s">&#39;B&#39;</span><span class="p">],</span> <span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+----+</span>
<span class="go">| age|height|name|</span>
<span class="go">+----+------+----+</span>
<span class="go">|  10|    80|   A|</span>
<span class="go">|   5|  null|   B|</span>
<span class="go">|null|  null| Tom|</span>
<span class="go">|null|  null|null|</span>
<span class="go">+----+------+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.rollup">
<tt class="descname">rollup</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.rollup" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a multi-dimensional rollup for the current <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> using
the specified columns, so we can run aggregation on them.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">rollup</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="s">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| name| age|count|</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| null|null|    2|</span>
<span class="go">|Alice|null|    1|</span>
<span class="go">|Alice|   2|    1|</span>
<span class="go">|  Bob|null|    1|</span>
<span class="go">|  Bob|   5|    1|</span>
<span class="go">+-----+----+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.sample">
<tt class="descname">sample</tt><big>(</big><em>withReplacement</em>, <em>fraction</em>, <em>seed=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sampled subset of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">False</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">2</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.sampleBy">
<tt class="descname">sampleBy</tt><big>(</big><em>col</em>, <em>fractions</em>, <em>seed=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.sampleBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a stratified sample without replacement based on the
fraction given on each stratum.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>col</strong> &#8211; column that defines strata</li>
<li><strong>fractions</strong> &#8211; sampling fraction for each stratum. If a stratum is not
specified, we treat its fraction as zero.</li>
<li><strong>seed</strong> &#8211; random seed</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a new DataFrame that represents the stratified sample</p>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">((</span><span class="n">col</span><span class="p">(</span><span class="s">&quot;id&quot;</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sampled</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampleBy</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">,</span> <span class="n">fractions</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">},</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sampled</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|key|count|</span>
<span class="go">+---+-----+</span>
<span class="go">|  0|    5|</span>
<span class="go">|  1|    9|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.schema">
<tt class="descname">schema</tt><a class="headerlink" href="#pyspark.sql.DataFrame.schema" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the schema of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as a <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><tt class="xref py py-class docutils literal"><span class="pre">types.StructType</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">schema</span>
<span class="go">StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.select">
<tt class="descname">select</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Projects a set of expressions and returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string) or expressions (<a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a>).
If one of the column names is &#8216;*&#8217;, that column is expanded to include all columns
in the current DataFrame.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&#39;*&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=2), Row(name=u&#39;Bob&#39;, age=5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">+</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=12), Row(name=u&#39;Bob&#39;, age=15)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.selectExpr">
<tt class="descname">selectExpr</tt><big>(</big><em>*expr</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.selectExpr" title="Permalink to this definition">¶</a></dt>
<dd><p>Projects a set of SQL expressions and returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>This is a variant of <a class="reference internal" href="#pyspark.sql.DataFrame.select" title="pyspark.sql.DataFrame.select"><tt class="xref py py-func docutils literal"><span class="pre">select()</span></tt></a> that accepts SQL expressions.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">&quot;age * 2&quot;</span><span class="p">,</span> <span class="s">&quot;abs(age)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.show">
<tt class="descname">show</tt><big>(</big><em>n=20</em>, <em>truncate=True</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.show" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the first <tt class="docutils literal"><span class="pre">n</span></tt> rows to the console.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n</strong> &#8211; Number of rows to show.</li>
<li><strong>truncate</strong> &#8211; Whether truncate long strings and align cells right.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span>
<span class="go">DataFrame[age: int, name: string]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.sort">
<tt class="descname">sort</tt><big>(</big><em>*cols</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.sort" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> sorted by the specified column(s).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>cols</strong> &#8211; list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> or column names to sort by.</li>
<li><strong>ascending</strong> &#8211; boolean or list of boolean (default True).
Sort ascending vs. descending. Specify list for multiple sort orders.
If a list is specified, length of the list must equal length of the <cite>cols</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">asc</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">desc</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">),</span> <span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">([</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.sortWithinPartitions">
<tt class="descname">sortWithinPartitions</tt><big>(</big><em>*cols</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.sortWithinPartitions" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with each partition sorted by the specified column(s).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>cols</strong> &#8211; list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> or column names to sort by.</li>
<li><strong>ascending</strong> &#8211; boolean or list of boolean (default True).
Sort ascending vs. descending. Specify list for multiple sort orders.
If a list is specified, length of the list must equal length of the <cite>cols</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sortWithinPartitions</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.stat">
<tt class="descname">stat</tt><a class="headerlink" href="#pyspark.sql.DataFrame.stat" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions" title="pyspark.sql.DataFrameStatFunctions"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameStatFunctions</span></tt></a> for statistic functions.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.subtract">
<tt class="descname">subtract</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.subtract" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing rows in this frame
but not in another frame.</p>
<p>This is equivalent to <cite>EXCEPT</cite> in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.take">
<tt class="descname">take</tt><big>(</big><em>num</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.take" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first <tt class="docutils literal"><span class="pre">num</span></tt> rows as a <tt class="xref py py-class docutils literal"><span class="pre">list</span></tt> of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.toDF">
<tt class="descname">toDF</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.toDF" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new class:<cite>DataFrame</cite> that with new specified column names</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of new column names (string)</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s">&#39;f1&#39;</span><span class="p">,</span> <span class="s">&#39;f2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=2, f2=u&#39;Alice&#39;), Row(f1=5, f2=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.toJSON">
<tt class="descname">toJSON</tt><big>(</big><em>use_unicode=True</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.toJSON" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> into a <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt> of string.</p>
<p>Each row is turned into a JSON document as one element in the returned RDD.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">toJSON</span><span class="p">()</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">u&#39;{&quot;age&quot;:2,&quot;name&quot;:&quot;Alice&quot;}&#39;</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.toLocalIterator">
<tt class="descname">toLocalIterator</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.toLocalIterator" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator that contains all of the rows in this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.
The iterator will consume as much memory as the largest partition in this DataFrame.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">toLocalIterator</span><span class="p">())</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.toPandas">
<tt class="descname">toPandas</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.toPandas" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as Pandas <tt class="docutils literal"><span class="pre">pandas.DataFrame</span></tt>.</p>
<p>Note that this method should only be used if the resulting Pandas&#8217;s DataFrame is expected
to be small, as all the data is loaded into the driver&#8217;s memory.</p>
<p>This is only available if Pandas is installed and available.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>  
<span class="go">   age   name</span>
<span class="go">0    2  Alice</span>
<span class="go">1    5    Bob</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.union">
<tt class="descname">union</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.union" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing union of rows in this
frame and another frame.</p>
<p>This is equivalent to <cite>UNION ALL</cite> in SQL. To do a SQL-style set union
(that does deduplication of elements), use this function followed by a distinct.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.unionAll">
<tt class="descname">unionAll</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.unionAll" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing union of rows in this
frame and another frame.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 2.0, use union instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.unpersist">
<tt class="descname">unpersist</tt><big>(</big><em>blocking=False</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.unpersist" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as non-persistent, and remove all blocks for it from
memory and disk.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><cite>blocking</cite> default has changed to False to match Scala in 2.0.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.where">
<tt class="descname">where</tt><big>(</big><em>condition</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.where" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#pyspark.sql.DataFrame.where" title="pyspark.sql.DataFrame.where"><tt class="xref py py-func docutils literal"><span class="pre">where()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.filter" title="pyspark.sql.DataFrame.filter"><tt class="xref py py-func docutils literal"><span class="pre">filter()</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.withColumn">
<tt class="descname">withColumn</tt><big>(</big><em>colName</em>, <em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.withColumn" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> by adding a column or replacing the
existing column that has the same name.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>colName</strong> &#8211; string, name of the new column.</li>
<li><strong>col</strong> &#8211; a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> expression for the new column.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">&#39;age2&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;, age2=4), Row(age=5, name=u&#39;Bob&#39;, age2=7)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.withColumnRenamed">
<tt class="descname">withColumnRenamed</tt><big>(</big><em>existing</em>, <em>new</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.withColumnRenamed" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> by renaming an existing column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>existing</strong> &#8211; string, name of the existing column to rename.</li>
<li><strong>col</strong> &#8211; string, new name of the column.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;age2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age2=2, name=u&#39;Alice&#39;), Row(age2=5, name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.write">
<tt class="descname">write</tt><a class="headerlink" href="#pyspark.sql.DataFrame.write" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface for saving the content of the non-streaming <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> out into external
storage.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrameWriter" title="pyspark.sql.DataFrameWriter"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameWriter</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.writeStream">
<tt class="descname">writeStream</tt><a class="headerlink" href="#pyspark.sql.DataFrame.writeStream" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface for saving the content of the streaming <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> out into external
storage.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><tt class="xref py py-class docutils literal"><span class="pre">DataStreamWriter</span></tt></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.GroupedData">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">GroupedData</tt><big>(</big><em>jgd</em>, <em>sql_ctx</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of methods for aggregations on a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>,
created by <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.groupBy()</span></tt></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.GroupedData.agg">
<tt class="descname">agg</tt><big>(</big><em>*exprs</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.agg" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute aggregates and returns the result as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>The available aggregate functions are <cite>avg</cite>, <cite>max</cite>, <cite>min</cite>, <cite>sum</cite>, <cite>count</cite>.</p>
<p>If <tt class="docutils literal"><span class="pre">exprs</span></tt> is a single <tt class="xref py py-class docutils literal"><span class="pre">dict</span></tt> mapping from string to string, then the key
is the column to perform aggregation on, and the value is the aggregate function.</p>
<p>Alternatively, <tt class="docutils literal"><span class="pre">exprs</span></tt> can also be a list of aggregate <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> expressions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>exprs</strong> &#8211; a dict mapping from column name (string) to aggregate functions (string),
or a list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a>.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">gdf</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">gdf</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s">&quot;*&quot;</span><span class="p">:</span> <span class="s">&quot;count&quot;</span><span class="p">})</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(name=u&#39;Alice&#39;, count(1)=1), Row(name=u&#39;Bob&#39;, count(1)=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">gdf</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(name=u&#39;Alice&#39;, min(age)=2), Row(name=u&#39;Bob&#39;, min(age)=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.avg">
<tt class="descname">avg</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.avg" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes average values for each numeric columns for each group.</p>
<p><a class="reference internal" href="#pyspark.sql.GroupedData.mean" title="pyspark.sql.GroupedData.mean"><tt class="xref py py-func docutils literal"><span class="pre">mean()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.GroupedData.avg" title="pyspark.sql.GroupedData.avg"><tt class="xref py py-func docutils literal"><span class="pre">avg()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string). Non-numeric columns are ignored.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">avg</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(avg(age)=3.5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">avg</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(avg(age)=3.5, avg(height)=82.5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.count">
<tt class="descname">count</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Counts the number of records for each group.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[Row(age=2, count=1), Row(age=5, count=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.max">
<tt class="descname">max</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the max value for each numeric columns for each group.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(max(age)=5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(max(age)=5, max(height)=85)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.mean">
<tt class="descname">mean</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes average values for each numeric columns for each group.</p>
<p><a class="reference internal" href="#pyspark.sql.GroupedData.mean" title="pyspark.sql.GroupedData.mean"><tt class="xref py py-func docutils literal"><span class="pre">mean()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.GroupedData.avg" title="pyspark.sql.GroupedData.avg"><tt class="xref py py-func docutils literal"><span class="pre">avg()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string). Non-numeric columns are ignored.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(avg(age)=3.5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(avg(age)=3.5, avg(height)=82.5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.min">
<tt class="descname">min</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.min" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the min value for each numeric column for each group.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string). Non-numeric columns are ignored.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(min(age)=2)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(min(age)=2, min(height)=80)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.pivot">
<tt class="descname">pivot</tt><big>(</big><em>pivot_col</em>, <em>values=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.pivot" title="Permalink to this definition">¶</a></dt>
<dd><p>Pivots a column of the current [[DataFrame]] and perform the specified aggregation.
There are two versions of pivot function: one that requires the caller to specify the list
of distinct values to pivot on, and one that does not. The latter is more concise but less
efficient, because Spark needs to first compute the list of distinct values internally.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pivot_col</strong> &#8211; Name of the column to pivot.</li>
<li><strong>values</strong> &#8211; List of values that will be translated to columns in the output DataFrame.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p># Compute the sum of earnings for each year by course with each course as a separate column</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">&quot;year&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="s">&quot;course&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s">&quot;dotNET&quot;</span><span class="p">,</span> <span class="s">&quot;Java&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s">&quot;earnings&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(year=2012, dotNET=15000, Java=20000), Row(year=2013, dotNET=48000, Java=30000)]</span>
</pre></div>
</div>
<p># Or without specifying column values (less efficient)</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">&quot;year&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="s">&quot;course&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s">&quot;earnings&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.sum">
<tt class="descname">sum</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the sum for each numeric columns for each group.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string). Non-numeric columns are ignored.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(sum(age)=7)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(sum(age)=7, sum(height)=165)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Column">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">Column</tt><big>(</big><em>jc</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column" title="Permalink to this definition">¶</a></dt>
<dd><p>A column in a DataFrame.</p>
<p><a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> instances can be created by:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># 1. Select a column out of a DataFrame</span>

<span class="n">df</span><span class="o">.</span><span class="n">colName</span>
<span class="n">df</span><span class="p">[</span><span class="s">&quot;colName&quot;</span><span class="p">]</span>

<span class="c"># 2. Create from an expression</span>
<span class="n">df</span><span class="o">.</span><span class="n">colName</span> <span class="o">+</span> <span class="mi">1</span>
<span class="mi">1</span> <span class="o">/</span> <span class="n">df</span><span class="o">.</span><span class="n">colName</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.Column.alias">
<tt class="descname">alias</tt><big>(</big><em>*alias</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.alias" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this column aliased with a new name or names (in the case of expressions that
return more than one column, such as explode).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;age2&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age2=2), Row(age2=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.asc">
<tt class="descname">asc</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.Column.asc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the ascending order of the given column name.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.astype">
<tt class="descname">astype</tt><big>(</big><em>dataType</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.astype" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#pyspark.sql.Column.astype" title="pyspark.sql.Column.astype"><tt class="xref py py-func docutils literal"><span class="pre">astype()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.Column.cast" title="pyspark.sql.Column.cast"><tt class="xref py py-func docutils literal"><span class="pre">cast()</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.between">
<tt class="descname">between</tt><big>(</big><em>lowerBound</em>, <em>upperBound</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.between" title="Permalink to this definition">¶</a></dt>
<dd><p>A boolean expression that is evaluated to true if the value of this
expression is between the given columns.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+---------------------------+</span>
<span class="go">| name|((age &gt;= 2) AND (age &lt;= 4))|</span>
<span class="go">+-----+---------------------------+</span>
<span class="go">|Alice|                       true|</span>
<span class="go">|  Bob|                      false|</span>
<span class="go">+-----+---------------------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.bitwiseAND">
<tt class="descname">bitwiseAND</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.bitwiseAND" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.bitwiseOR">
<tt class="descname">bitwiseOR</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.bitwiseOR" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.bitwiseXOR">
<tt class="descname">bitwiseXOR</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.bitwiseXOR" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.cast">
<tt class="descname">cast</tt><big>(</big><em>dataType</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.cast" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the column into type <tt class="docutils literal"><span class="pre">dataType</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s">&quot;string&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;ages&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(ages=u&#39;2&#39;), Row(ages=u&#39;5&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">StringType</span><span class="p">())</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;ages&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(ages=u&#39;2&#39;), Row(ages=u&#39;5&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.desc">
<tt class="descname">desc</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.Column.desc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the descending order of the given column name.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.endswith">
<tt class="descname">endswith</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.endswith" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.getField">
<tt class="descname">getField</tt><big>(</big><em>name</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.getField" title="Permalink to this definition">¶</a></dt>
<dd><p>An expression that gets a field by name in a StructField.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="s">&quot;b&quot;</span><span class="p">))])</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">getField</span><span class="p">(</span><span class="s">&quot;b&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+</span>
<span class="go">|r.b|</span>
<span class="go">+---+</span>
<span class="go">|  b|</span>
<span class="go">+---+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+</span>
<span class="go">|r.a|</span>
<span class="go">+---+</span>
<span class="go">|  1|</span>
<span class="go">+---+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.getItem">
<tt class="descname">getItem</tt><big>(</big><em>key</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.getItem" title="Permalink to this definition">¶</a></dt>
<dd><p>An expression that gets an item at position <tt class="docutils literal"><span class="pre">ordinal</span></tt> out of a list,
or gets an item by key out of a dict.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">{</span><span class="s">&quot;key&quot;</span><span class="p">:</span> <span class="s">&quot;value&quot;</span><span class="p">})])</span><span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="s">&quot;d&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">l</span><span class="o">.</span><span class="n">getItem</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">getItem</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+</span>
<span class="go">|l[0]|d[key]|</span>
<span class="go">+----+------+</span>
<span class="go">|   1| value|</span>
<span class="go">+----+------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="s">&quot;key&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+</span>
<span class="go">|l[0]|d[key]|</span>
<span class="go">+----+------+</span>
<span class="go">|   1| value|</span>
<span class="go">+----+------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.isNotNull">
<tt class="descname">isNotNull</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.Column.isNotNull" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the current expression is not null.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.isNull">
<tt class="descname">isNull</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.Column.isNull" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the current expression is null.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.isin">
<tt class="descname">isin</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.isin" title="Permalink to this definition">¶</a></dt>
<dd><p>A boolean expression that is evaluated to true if the value of this
expression is contained by the evaluated values of the arguments.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="s">&quot;Bob&quot;</span><span class="p">,</span> <span class="s">&quot;Mike&quot;</span><span class="p">)]</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])]</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.like">
<tt class="descname">like</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.like" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.name">
<tt class="descname">name</tt><big>(</big><em>*alias</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.name" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#pyspark.sql.Column.name" title="pyspark.sql.Column.name"><tt class="xref py py-func docutils literal"><span class="pre">name()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.Column.alias" title="pyspark.sql.Column.alias"><tt class="xref py py-func docutils literal"><span class="pre">alias()</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.otherwise">
<tt class="descname">otherwise</tt><big>(</big><em>value</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.otherwise" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions.
If <a class="reference internal" href="#pyspark.sql.Column.otherwise" title="pyspark.sql.Column.otherwise"><tt class="xref py py-func docutils literal"><span class="pre">Column.otherwise()</span></tt></a> is not invoked, None is returned for unmatched conditions.</p>
<p>See <a class="reference internal" href="#pyspark.sql.functions.when" title="pyspark.sql.functions.when"><tt class="xref py py-func docutils literal"><span class="pre">pyspark.sql.functions.when()</span></tt></a> for example usage.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>value</strong> &#8211; a literal value, or a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> expression.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+-------------------------------------+</span>
<span class="go">| name|CASE WHEN (age &gt; 3) THEN 1 ELSE 0 END|</span>
<span class="go">+-----+-------------------------------------+</span>
<span class="go">|Alice|                                    0|</span>
<span class="go">|  Bob|                                    1|</span>
<span class="go">+-----+-------------------------------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.over">
<tt class="descname">over</tt><big>(</big><em>window</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.over" title="Permalink to this definition">¶</a></dt>
<dd><p>Define a windowing column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>window</strong> &#8211; a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a></td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a Column</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rowsBetween</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">rank</span><span class="p">,</span> <span class="nb">min</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># df.select(rank().over(window), min(&#39;age&#39;).over(window))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.rlike">
<tt class="descname">rlike</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.rlike" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.startswith">
<tt class="descname">startswith</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.startswith" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.substr">
<tt class="descname">substr</tt><big>(</big><em>startPos</em>, <em>length</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.substr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> which is a substring of the column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>startPos</strong> &#8211; start position (int or Column)</li>
<li><strong>length</strong> &#8211; length of the substring (int or Column)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">substr</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;col&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(col=u&#39;Ali&#39;), Row(col=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.when">
<tt class="descname">when</tt><big>(</big><em>condition</em>, <em>value</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.when" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions.
If <a class="reference internal" href="#pyspark.sql.Column.otherwise" title="pyspark.sql.Column.otherwise"><tt class="xref py py-func docutils literal"><span class="pre">Column.otherwise()</span></tt></a> is not invoked, None is returned for unmatched conditions.</p>
<p>See <a class="reference internal" href="#pyspark.sql.functions.when" title="pyspark.sql.functions.when"><tt class="xref py py-func docutils literal"><span class="pre">pyspark.sql.functions.when()</span></tt></a> for example usage.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>condition</strong> &#8211; a boolean <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> expression.</li>
<li><strong>value</strong> &#8211; a literal value, or a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> expression.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+------------------------------------------------------------+</span>
<span class="go">| name|CASE WHEN (age &gt; 4) THEN 1 WHEN (age &lt; 3) THEN -1 ELSE 0 END|</span>
<span class="go">+-----+------------------------------------------------------------+</span>
<span class="go">|Alice|                                                          -1|</span>
<span class="go">|  Bob|                                                           1|</span>
<span class="go">+-----+------------------------------------------------------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Row">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">Row</tt><a class="headerlink" href="#pyspark.sql.Row" title="Permalink to this definition">¶</a></dt>
<dd><p>A row in <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.
The fields in it can be accessed:</p>
<ul class="simple">
<li>like attributes (<tt class="docutils literal"><span class="pre">row.key</span></tt>)</li>
<li>like dictionary values (<tt class="docutils literal"><span class="pre">row[key]</span></tt>)</li>
</ul>
<p><tt class="docutils literal"><span class="pre">key</span> <span class="pre">in</span> <span class="pre">row</span></tt> will search through row keys.</p>
<p>Row can be used to create a row object by using named arguments,
the fields will be sorted by names.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">row</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&quot;Alice&quot;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span>
<span class="go">Row(age=11, name=&#39;Alice&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s">&#39;age&#39;</span><span class="p">]</span>
<span class="go">(&#39;Alice&#39;, 11)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">row</span><span class="o">.</span><span class="n">age</span>
<span class="go">(&#39;Alice&#39;, 11)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;name&#39;</span> <span class="ow">in</span> <span class="n">row</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;wrong_key&#39;</span> <span class="ow">in</span> <span class="n">row</span>
<span class="go">False</span>
</pre></div>
</div>
<p>Row also can be used to create another Row like class, then it
could be used to create Row objects, such as</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="s">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span>
<span class="go">&lt;Row(name, age)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;name&#39;</span> <span class="ow">in</span> <span class="n">Person</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;wrong_key&#39;</span> <span class="ow">in</span> <span class="n">Person</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span><span class="p">(</span><span class="s">&quot;Alice&quot;</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="go">Row(name=&#39;Alice&#39;, age=11)</span>
</pre></div>
</div>
<dl class="method">
<dt id="pyspark.sql.Row.asDict">
<tt class="descname">asDict</tt><big>(</big><em>recursive=False</em><big>)</big><a class="headerlink" href="#pyspark.sql.Row.asDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return as an dict</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>recursive</strong> &#8211; turns the nested Row as dict (default: False).</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&quot;Alice&quot;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span><span class="o">.</span><span class="n">asDict</span><span class="p">()</span> <span class="o">==</span> <span class="p">{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">11</span><span class="p">}</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span><span class="o">.</span><span class="n">asDict</span><span class="p">()</span> <span class="o">==</span> <span class="p">{</span><span class="s">&#39;key&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">&#39;value&#39;</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">age</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;a&#39;</span><span class="p">)}</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span><span class="o">.</span><span class="n">asDict</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> <span class="o">==</span> <span class="p">{</span><span class="s">&#39;key&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">&#39;value&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}}</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameNaFunctions">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DataFrameNaFunctions</tt><big>(</big><em>df</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions" title="Permalink to this definition">¶</a></dt>
<dd><p>Functionality for working with missing data in <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameNaFunctions.drop">
<tt class="descname">drop</tt><big>(</big><em>how='any'</em>, <em>thresh=None</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions.drop" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> omitting rows with null values.
<a class="reference internal" href="#pyspark.sql.DataFrame.dropna" title="pyspark.sql.DataFrame.dropna"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.dropna()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.drop" title="pyspark.sql.DataFrameNaFunctions.drop"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.drop()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>how</strong> &#8211; &#8216;any&#8217; or &#8216;all&#8217;.
If &#8216;any&#8217;, drop a row if it contains any nulls.
If &#8216;all&#8217;, drop a row only if all its values are null.</li>
<li><strong>thresh</strong> &#8211; int, default None
If specified, drop rows that have less than <cite>thresh</cite> non-null values.
This overwrites the <cite>how</cite> parameter.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">drop</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameNaFunctions.fill">
<tt class="descname">fill</tt><big>(</big><em>value</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions.fill" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace null values, alias for <tt class="docutils literal"><span class="pre">na.fill()</span></tt>.
<a class="reference internal" href="#pyspark.sql.DataFrame.fillna" title="pyspark.sql.DataFrame.fillna"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.fillna()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.fill" title="pyspark.sql.DataFrameNaFunctions.fill"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.fill()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>value</strong> &#8211; int, long, float, string, or dict.
Value to replace null values with.
If the value is a dict, then <cite>subset</cite> is ignored and <cite>value</cite> must be a mapping
from column name (string) to replacement value. The replacement value must be
an int, long, float, or string.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">|  5|    50|  Bob|</span>
<span class="go">| 50|    50|  Tom|</span>
<span class="go">| 50|    50| null|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">({</span><span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;unknown&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-------+</span>
<span class="go">|age|height|   name|</span>
<span class="go">+---+------+-------+</span>
<span class="go">| 10|    80|  Alice|</span>
<span class="go">|  5|  null|    Bob|</span>
<span class="go">| 50|  null|    Tom|</span>
<span class="go">| 50|  null|unknown|</span>
<span class="go">+---+------+-------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameNaFunctions.replace">
<tt class="descname">replace</tt><big>(</big><em>to_replace</em>, <em>value</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> replacing a value with another value.
<a class="reference internal" href="#pyspark.sql.DataFrame.replace" title="pyspark.sql.DataFrame.replace"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.replace()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.replace" title="pyspark.sql.DataFrameNaFunctions.replace"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.replace()</span></tt></a> are
aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>to_replace</strong> &#8211; int, long, float, string, or list.
Value to be replaced.
If the value is a dict, then <cite>value</cite> is ignored and <cite>to_replace</cite> must be a
mapping from column name (string) to replacement value. The value to be
replaced must be an int, long, float, or string.</li>
<li><strong>value</strong> &#8211; int, long, float, string, or list.
Value to use to replace holes.
The replacement value must be an int, long, float, or string. If <cite>value</cite> is a
list or tuple, <cite>value</cite> should be of the same length with <cite>to_replace</cite>.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+-----+</span>
<span class="go">| age|height| name|</span>
<span class="go">+----+------+-----+</span>
<span class="go">|  20|    80|Alice|</span>
<span class="go">|   5|  null|  Bob|</span>
<span class="go">|null|  null|  Tom|</span>
<span class="go">|null|  null| null|</span>
<span class="go">+----+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="s">&#39;Bob&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;A&#39;</span><span class="p">,</span> <span class="s">&#39;B&#39;</span><span class="p">],</span> <span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+----+</span>
<span class="go">| age|height|name|</span>
<span class="go">+----+------+----+</span>
<span class="go">|  10|    80|   A|</span>
<span class="go">|   5|  null|   B|</span>
<span class="go">|null|  null| Tom|</span>
<span class="go">|null|  null|null|</span>
<span class="go">+----+------+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameStatFunctions">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DataFrameStatFunctions</tt><big>(</big><em>df</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions" title="Permalink to this definition">¶</a></dt>
<dd><p>Functionality for statistic functions with <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.approxQuantile">
<tt class="descname">approxQuantile</tt><big>(</big><em>col</em>, <em>probabilities</em>, <em>relativeError</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.approxQuantile" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the approximate quantiles of a numerical column of a
DataFrame.</p>
<p>The result of this algorithm has the following deterministic bound:
If the DataFrame has N elements and if we request the quantile at
probability <cite>p</cite> up to error <cite>err</cite>, then the algorithm will return
a sample <cite>x</cite> from the DataFrame so that the <em>exact</em> rank of <cite>x</cite> is
close to (p * N). More precisely,</p>
<blockquote>
<div>floor((p - err) * N) &lt;= rank(x) &lt;= ceil((p + err) * N).</div></blockquote>
<p>This method implements a variation of the Greenwald-Khanna
algorithm (with some speed optimizations). The algorithm was first
present in [[<a class="reference external" href="http://dx.doi.org/10.1145/375663.375670">http://dx.doi.org/10.1145/375663.375670</a>
Space-efficient Online Computation of Quantile Summaries]]
by Greenwald and Khanna.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>col</strong> &#8211; the name of the numerical column</li>
<li><strong>probabilities</strong> &#8211; a list of quantile probabilities
Each number must belong to [0, 1].
For example 0 is the minimum, 0.5 is the median, 1 is the maximum.</li>
<li><strong>relativeError</strong> &#8211; The relative target precision to achieve
(&gt;= 0). If set to zero, the exact quantiles are computed, which
could be very expensive. Note that values greater than 1 are
accepted but give the same result as 1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">the approximate quantiles at the given probabilities</p>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.corr">
<tt class="descname">corr</tt><big>(</big><em>col1</em>, <em>col2</em>, <em>method=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.corr" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the correlation of two columns of a DataFrame as a double value.
Currently only supports the Pearson Correlation Coefficient.
<a class="reference internal" href="#pyspark.sql.DataFrame.corr" title="pyspark.sql.DataFrame.corr"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.corr()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.corr" title="pyspark.sql.DataFrameStatFunctions.corr"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.corr()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column</li>
<li><strong>col2</strong> &#8211; The name of the second column</li>
<li><strong>method</strong> &#8211; The correlation method. Currently only supports &#8220;pearson&#8221;</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.cov">
<tt class="descname">cov</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.cov" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the sample covariance for the given columns, specified by their names, as a
double value. <a class="reference internal" href="#pyspark.sql.DataFrame.cov" title="pyspark.sql.DataFrame.cov"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.cov()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.cov" title="pyspark.sql.DataFrameStatFunctions.cov"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.cov()</span></tt></a> are aliases.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column</li>
<li><strong>col2</strong> &#8211; The name of the second column</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.crosstab">
<tt class="descname">crosstab</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.crosstab" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a pair-wise frequency table of the given columns. Also known as a contingency
table. The number of distinct values for each column should be less than 1e4. At most 1e6
non-zero pair frequencies will be returned.
The first column of each row will be the distinct values of <cite>col1</cite> and the column names
will be the distinct values of <cite>col2</cite>. The name of the first column will be <cite>$col1_$col2</cite>.
Pairs that have no occurrences will have zero as their counts.
<a class="reference internal" href="#pyspark.sql.DataFrame.crosstab" title="pyspark.sql.DataFrame.crosstab"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.crosstab()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.crosstab" title="pyspark.sql.DataFrameStatFunctions.crosstab"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.crosstab()</span></tt></a> are aliases.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column. Distinct items will make the first item of
each row.</li>
<li><strong>col2</strong> &#8211; The name of the second column. Distinct items will make the column names
of the DataFrame.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.freqItems">
<tt class="descname">freqItems</tt><big>(</big><em>cols</em>, <em>support=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.freqItems" title="Permalink to this definition">¶</a></dt>
<dd><p>Finding frequent items for columns, possibly with false positives. Using the
frequent element count algorithm described in
&#8220;<a class="reference external" href="http://dx.doi.org/10.1145/762471.762473">http://dx.doi.org/10.1145/762471.762473</a>, proposed by Karp, Schenker, and Papadimitriou&#8221;.
<a class="reference internal" href="#pyspark.sql.DataFrame.freqItems" title="pyspark.sql.DataFrame.freqItems"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.freqItems()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.freqItems" title="pyspark.sql.DataFrameStatFunctions.freqItems"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.freqItems()</span></tt></a> are aliases.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is meant for exploratory data analysis, as we make no         guarantee about the backward compatibility of the schema of the resulting DataFrame.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>cols</strong> &#8211; Names of the columns to calculate frequent items for as a list or tuple of
strings.</li>
<li><strong>support</strong> &#8211; The frequency with which to consider an item &#8216;frequent&#8217;. Default is 1%.
The support must be greater than 1e-4.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.sampleBy">
<tt class="descname">sampleBy</tt><big>(</big><em>col</em>, <em>fractions</em>, <em>seed=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.sampleBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a stratified sample without replacement based on the
fraction given on each stratum.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>col</strong> &#8211; column that defines strata</li>
<li><strong>fractions</strong> &#8211; sampling fraction for each stratum. If a stratum is not
specified, we treat its fraction as zero.</li>
<li><strong>seed</strong> &#8211; random seed</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a new DataFrame that represents the stratified sample</p>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">((</span><span class="n">col</span><span class="p">(</span><span class="s">&quot;id&quot;</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sampled</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampleBy</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">,</span> <span class="n">fractions</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">},</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sampled</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|key|count|</span>
<span class="go">+---+-----+</span>
<span class="go">|  0|    5|</span>
<span class="go">|  1|    9|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Window">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">Window</tt><a class="headerlink" href="#pyspark.sql.Window" title="Permalink to this definition">¶</a></dt>
<dd><p>Utility functions for defining window in DataFrames.</p>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># PARTITION BY country ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s">&quot;country&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rowsBetween</span><span class="p">(</span><span class="o">-</span><span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># PARTITION BY country ORDER BY date RANGE BETWEEN 3 PRECEDING AND 3 FOLLOWING</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s">&quot;country&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rangeBetween</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="staticmethod">
<dt id="pyspark.sql.Window.orderBy">
<em class="property">static </em><tt class="descname">orderBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.Window.orderBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a> with the ordering defined.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="staticmethod">
<dt id="pyspark.sql.Window.partitionBy">
<em class="property">static </em><tt class="descname">partitionBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.Window.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a> with the partitioning defined.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.WindowSpec">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">WindowSpec</tt><big>(</big><em>jspec</em><big>)</big><a class="headerlink" href="#pyspark.sql.WindowSpec" title="Permalink to this definition">¶</a></dt>
<dd><p>A window specification that defines the partitioning, ordering,
and frame boundaries.</p>
<p>Use the static methods in <a class="reference internal" href="#pyspark.sql.Window" title="pyspark.sql.Window"><tt class="xref py py-class docutils literal"><span class="pre">Window</span></tt></a> to create a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.WindowSpec.orderBy">
<tt class="descname">orderBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.WindowSpec.orderBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the ordering columns in a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; names of columns or expressions</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.WindowSpec.partitionBy">
<tt class="descname">partitionBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.WindowSpec.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the partitioning columns in a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; names of columns or expressions</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.WindowSpec.rangeBetween">
<tt class="descname">rangeBetween</tt><big>(</big><em>start</em>, <em>end</em><big>)</big><a class="headerlink" href="#pyspark.sql.WindowSpec.rangeBetween" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the frame boundaries, from <cite>start</cite> (inclusive) to <cite>end</cite> (inclusive).</p>
<p>Both <cite>start</cite> and <cite>end</cite> are relative from the current row. For example,
&#8220;0&#8221; means &#8220;current row&#8221;, while &#8220;-1&#8221; means one off before the current row,
and &#8220;5&#8221; means the five off after the current row.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start</strong> &#8211; boundary start, inclusive.
The frame is unbounded if this is <tt class="docutils literal"><span class="pre">-sys.maxsize</span></tt> (or lower).</li>
<li><strong>end</strong> &#8211; boundary end, inclusive.
The frame is unbounded if this is <tt class="docutils literal"><span class="pre">sys.maxsize</span></tt> (or higher).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.WindowSpec.rowsBetween">
<tt class="descname">rowsBetween</tt><big>(</big><em>start</em>, <em>end</em><big>)</big><a class="headerlink" href="#pyspark.sql.WindowSpec.rowsBetween" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the frame boundaries, from <cite>start</cite> (inclusive) to <cite>end</cite> (inclusive).</p>
<p>Both <cite>start</cite> and <cite>end</cite> are relative positions from the current row.
For example, &#8220;0&#8221; means &#8220;current row&#8221;, while &#8220;-1&#8221; means the row before
the current row, and &#8220;5&#8221; means the fifth row after the current row.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start</strong> &#8211; boundary start, inclusive.
The frame is unbounded if this is <tt class="docutils literal"><span class="pre">-sys.maxsize</span></tt> (or lower).</li>
<li><strong>end</strong> &#8211; boundary end, inclusive.
The frame is unbounded if this is <tt class="docutils literal"><span class="pre">sys.maxsize</span></tt> (or higher).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameReader">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DataFrameReader</tt><big>(</big><em>spark</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface used to load a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> from external storage systems
(e.g. file systems, key-value stores, etc). Use <tt class="xref py py-func docutils literal"><span class="pre">spark.read()</span></tt>
to access this.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameReader.csv">
<tt class="descname">csv</tt><big>(</big><em>path</em>, <em>schema=None</em>, <em>sep=None</em>, <em>encoding=None</em>, <em>quote=None</em>, <em>escape=None</em>, <em>comment=None</em>, <em>header=None</em>, <em>inferSchema=None</em>, <em>ignoreLeadingWhiteSpace=None</em>, <em>ignoreTrailingWhiteSpace=None</em>, <em>nullValue=None</em>, <em>nanValue=None</em>, <em>positiveInf=None</em>, <em>negativeInf=None</em>, <em>dateFormat=None</em>, <em>maxColumns=None</em>, <em>maxCharsPerColumn=None</em>, <em>maxMalformedLogPerPartition=None</em>, <em>mode=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.csv" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a CSV file and returns the result as a  <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>This function will go through the input once to determine the input schema if
<tt class="docutils literal"><span class="pre">inferSchema</span></tt> is enabled. To avoid going through the entire data once, disable
<tt class="docutils literal"><span class="pre">inferSchema</span></tt> option or specify the schema explicitly using <tt class="docutils literal"><span class="pre">schema</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; string, or list of strings, for input path(s).</li>
<li><strong>schema</strong> &#8211; an optional <tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt> for the input schema.</li>
<li><strong>sep</strong> &#8211; sets the single character as a separator for each field and value.
If None is set, it uses the default value, <tt class="docutils literal"><span class="pre">,</span></tt>.</li>
<li><strong>encoding</strong> &#8211; decodes the CSV files by the given encoding type. If None is set,
it uses the default value, <tt class="docutils literal"><span class="pre">UTF-8</span></tt>.</li>
<li><strong>quote</strong> &#8211; sets the single character used for escaping quoted values where the
separator can be part of the value. If None is set, it uses the default
value, <tt class="docutils literal"><span class="pre">&quot;</span></tt>. If you would like to turn off quotations, you need to set an
empty string.</li>
<li><strong>escape</strong> &#8211; sets the single character used for escaping quotes inside an already
quoted value. If None is set, it uses the default value, <tt class="docutils literal"><span class="pre">\</span></tt>.</li>
<li><strong>comment</strong> &#8211; sets the single character used for skipping lines beginning with this
character. By default (None), it is disabled.</li>
<li><strong>header</strong> &#8211; uses the first line as names of columns. If None is set, it uses the
default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>inferSchema</strong> &#8211; infers the input schema automatically from data. It requires one extra
pass over the data. If None is set, it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>ignoreLeadingWhiteSpace</strong> &#8211; defines whether or not leading whitespaces from values
being read should be skipped. If None is set, it uses
the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>ignoreTrailingWhiteSpace</strong> &#8211; defines whether or not trailing whitespaces from values
being read should be skipped. If None is set, it uses
the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>nullValue</strong> &#8211; sets the string representation of a null value. If None is set, it uses
the default value, empty string.</li>
<li><strong>nanValue</strong> &#8211; sets the string representation of a non-number value. If None is set, it
uses the default value, <tt class="docutils literal"><span class="pre">NaN</span></tt>.</li>
<li><strong>positiveInf</strong> &#8211; sets the string representation of a positive infinity value. If None
is set, it uses the default value, <tt class="docutils literal"><span class="pre">Inf</span></tt>.</li>
<li><strong>negativeInf</strong> &#8211; sets the string representation of a negative infinity value. If None
is set, it uses the default value, <tt class="docutils literal"><span class="pre">Inf</span></tt>.</li>
<li><strong>dateFormat</strong> &#8211; sets the string that indicates a date format. Custom date formats
follow the formats at <tt class="docutils literal"><span class="pre">java.text.SimpleDateFormat</span></tt>. This
applies to both date type and timestamp type. By default, it is None
which means trying to parse times and date by
<tt class="docutils literal"><span class="pre">java.sql.Timestamp.valueOf()</span></tt> and <tt class="docutils literal"><span class="pre">java.sql.Date.valueOf()</span></tt>.</li>
<li><strong>maxColumns</strong> &#8211; defines a hard limit of how many columns a record can have. If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">20480</span></tt>.</li>
<li><strong>maxCharsPerColumn</strong> &#8211; defines the maximum number of characters allowed for any given
value being read. If None is set, it uses the default value,
<tt class="docutils literal"><span class="pre">1000000</span></tt>.</li>
<li><strong>maxMalformedLogPerPartition</strong> &#8211; sets the maximum number of malformed rows Spark will
log for each partition. Malformed records beyond this
number will be ignored. If None is set, it
uses the default value, <tt class="docutils literal"><span class="pre">10</span></tt>.</li>
<li><strong>mode</strong> &#8211; <dl class="docutils">
<dt>allows a mode for dealing with corrupt records during parsing. If None is</dt>
<dd>set, it uses the default value, <tt class="docutils literal"><span class="pre">PERMISSIVE</span></tt>.</dd>
</dl>
<ul>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">PERMISSIVE</span></tt> <span class="classifier-delimiter">:</span> <span class="classifier">sets other fields to <tt class="docutils literal"><span class="pre">null</span></tt> when it meets a corrupted record.</span></dt>
<dd>When a schema is set by user, it sets <tt class="docutils literal"><span class="pre">null</span></tt> for extra fields.</dd>
</dl>
</li>
<li><tt class="docutils literal"><span class="pre">DROPMALFORMED</span></tt> : ignores the whole corrupted records.</li>
<li><tt class="docutils literal"><span class="pre">FAILFAST</span></tt> : throws an exception when it meets corrupted records.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/ages.csv&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;_c0&#39;, &#39;string&#39;), (&#39;_c1&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.format">
<tt class="descname">format</tt><big>(</big><em>source</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the input data source format.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>source</strong> &#8211; string, name of the data source, e.g. &#8216;json&#8217;, &#8216;parquet&#8217;.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&#39;json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.jdbc">
<tt class="descname">jdbc</tt><big>(</big><em>url</em>, <em>table</em>, <em>column=None</em>, <em>lowerBound=None</em>, <em>upperBound=None</em>, <em>numPartitions=None</em>, <em>predicates=None</em>, <em>properties=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.jdbc" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> representing the database table named <tt class="docutils literal"><span class="pre">table</span></tt>
accessible via JDBC URL <tt class="docutils literal"><span class="pre">url</span></tt> and connection <tt class="docutils literal"><span class="pre">properties</span></tt>.</p>
<p>Partitions of the table will be retrieved in parallel if either <tt class="docutils literal"><span class="pre">column</span></tt> or
<tt class="docutils literal"><span class="pre">predicates</span></tt> is specified.</p>
<p>If both <tt class="docutils literal"><span class="pre">column</span></tt> and <tt class="docutils literal"><span class="pre">predicates</span></tt> are specified, <tt class="docutils literal"><span class="pre">column</span></tt> will be used.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Don&#8217;t create too many partitions in parallel on a large cluster;         otherwise Spark might crash your external database systems.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>url</strong> &#8211; a JDBC URL of the form <tt class="docutils literal"><span class="pre">jdbc:subprotocol:subname</span></tt></li>
<li><strong>table</strong> &#8211; the name of the table</li>
<li><strong>column</strong> &#8211; the name of an integer column that will be used for partitioning;
if this parameter is specified, then <tt class="docutils literal"><span class="pre">numPartitions</span></tt>, <tt class="docutils literal"><span class="pre">lowerBound</span></tt>
(inclusive), and <tt class="docutils literal"><span class="pre">upperBound</span></tt> (exclusive) will form partition strides
for generated WHERE clause expressions used to split the column
<tt class="docutils literal"><span class="pre">column</span></tt> evenly</li>
<li><strong>lowerBound</strong> &#8211; the minimum value of <tt class="docutils literal"><span class="pre">column</span></tt> used to decide partition stride</li>
<li><strong>upperBound</strong> &#8211; the maximum value of <tt class="docutils literal"><span class="pre">column</span></tt> used to decide partition stride</li>
<li><strong>numPartitions</strong> &#8211; the number of partitions</li>
<li><strong>predicates</strong> &#8211; a list of expressions suitable for inclusion in WHERE clauses;
each one defines one partition of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></li>
<li><strong>properties</strong> &#8211; a dictionary of JDBC database connection arguments; normally,
at least a &#8220;user&#8221; and &#8220;password&#8221; property should be included</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a DataFrame</p>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.json">
<tt class="descname">json</tt><big>(</big><em>path</em>, <em>schema=None</em>, <em>primitivesAsString=None</em>, <em>prefersDecimal=None</em>, <em>allowComments=None</em>, <em>allowUnquotedFieldNames=None</em>, <em>allowSingleQuotes=None</em>, <em>allowNumericLeadingZero=None</em>, <em>allowBackslashEscapingAnyCharacter=None</em>, <em>mode=None</em>, <em>columnNameOfCorruptRecord=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.json" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a JSON file (one object per line) or an RDD of Strings storing JSON objects
(one object per record) and returns the result as a :class`DataFrame`.</p>
<p>If the <tt class="docutils literal"><span class="pre">schema</span></tt> parameter is not specified, this function goes
through the input once to determine the input schema.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; string represents path to the JSON dataset,
or RDD of Strings storing JSON objects.</li>
<li><strong>schema</strong> &#8211; an optional <tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt> for the input schema.</li>
<li><strong>primitivesAsString</strong> &#8211; infers all primitive values as a string type. If None is set,
it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>prefersDecimal</strong> &#8211; infers all floating-point values as a decimal type. If the values
do not fit in decimal, then it infers them as doubles. If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>allowComments</strong> &#8211; ignores Java/C++ style comment in JSON records. If None is set,
it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>allowUnquotedFieldNames</strong> &#8211; allows unquoted JSON field names. If None is set,
it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>allowSingleQuotes</strong> &#8211; allows single quotes in addition to double quotes. If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">true</span></tt>.</li>
<li><strong>allowNumericLeadingZero</strong> &#8211; allows leading zeros in numbers (e.g. 00012). If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>allowBackslashEscapingAnyCharacter</strong> &#8211; allows accepting quoting of all character
using backslash quoting mechanism. If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>mode</strong> &#8211; <dl class="docutils">
<dt>allows a mode for dealing with corrupt records during parsing. If None is</dt>
<dd>set, it uses the default value, <tt class="docutils literal"><span class="pre">PERMISSIVE</span></tt>.</dd>
</dl>
<ul>
<li><tt class="docutils literal"><span class="pre">PERMISSIVE</span></tt> : sets other fields to <tt class="docutils literal"><span class="pre">null</span></tt> when it meets a corrupted                   record and puts the malformed string into a new field configured by                  <tt class="docutils literal"><span class="pre">columnNameOfCorruptRecord</span></tt>. When a schema is set by user, it sets                  <tt class="docutils literal"><span class="pre">null</span></tt> for extra fields.</li>
<li><tt class="docutils literal"><span class="pre">DROPMALFORMED</span></tt> : ignores the whole corrupted records.</li>
<li><tt class="docutils literal"><span class="pre">FAILFAST</span></tt> : throws an exception when it meets corrupted records.</li>
</ul>
</li>
<li><strong>columnNameOfCorruptRecord</strong> &#8211; allows renaming the new field having malformed string
created by <tt class="docutils literal"><span class="pre">PERMISSIVE</span></tt> mode. This overrides
<tt class="docutils literal"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></tt>. If None is set,
it uses the value specified in
<tt class="docutils literal"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></tt>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.load">
<tt class="descname">load</tt><big>(</big><em>path=None</em>, <em>format=None</em>, <em>schema=None</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads data from a data source and returns it as a :class`DataFrame`.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; optional string or a list of string for file-system backed data sources.</li>
<li><strong>format</strong> &#8211; optional string for format of the data source. Default to &#8216;parquet&#8217;.</li>
<li><strong>schema</strong> &#8211; optional <tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt> for the input schema.</li>
<li><strong>options</strong> &#8211; all other string options</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">,</span> <span class="n">opt1</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">opt2</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">opt3</span><span class="o">=</span><span class="s">&#39;str&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;name&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;month&#39;, &#39;int&#39;), (&#39;day&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&#39;json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">([</span><span class="s">&#39;python/test_support/sql/people.json&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s">&#39;python/test_support/sql/people1.json&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;aka&#39;, &#39;string&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.option">
<tt class="descname">option</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.option" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds an input option for the underlying data source.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.options">
<tt class="descname">options</tt><big>(</big><em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.options" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds input options for the underlying data source.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.orc">
<tt class="descname">orc</tt><big>(</big><em>path</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.orc" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads an ORC file, returning the result as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Currently ORC support is only available together with Hive support.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/orc_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;a&#39;, &#39;bigint&#39;), (&#39;b&#39;, &#39;int&#39;), (&#39;c&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.parquet">
<tt class="descname">parquet</tt><big>(</big><em>*paths</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.parquet" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a Parquet file, returning the result as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<dl class="docutils">
<dt>You can set the following Parquet-specific option(s) for reading Parquet files:</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">mergeSchema</span></tt>: sets whether we should merge schemas collected from all                 Parquet part-files. This will override <tt class="docutils literal"><span class="pre">spark.sql.parquet.mergeSchema</span></tt>.                 The default value is specified in <tt class="docutils literal"><span class="pre">spark.sql.parquet.mergeSchema</span></tt>.</li>
</ul>
</dd>
</dl>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;name&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;month&#39;, &#39;int&#39;), (&#39;day&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.schema">
<tt class="descname">schema</tt><big>(</big><em>schema</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.schema" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the input schema.</p>
<p>Some data sources (e.g. JSON) can infer the input schema automatically from data.
By specifying the schema here, the underlying data source can skip the schema
inference step, and thus speed up data loading.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>schema</strong> &#8211; a StructType object</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.table">
<tt class="descname">table</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.table" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the specified table as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tableName</strong> &#8211; string, name of the table.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s">&#39;tmpTable&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s">&#39;tmpTable&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;name&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;month&#39;, &#39;int&#39;), (&#39;day&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.text">
<tt class="descname">text</tt><big>(</big><em>paths</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads text files and returns a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> whose schema starts with a
string column named &#8220;value&#8221;, and followed by partitioned columns if there
are any.</p>
<p>Each line in the text file is a new row in the resulting DataFrame.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>paths</strong> &#8211; string, or list of strings, for input path(s).</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/text-test.txt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(value=u&#39;hello&#39;), Row(value=u&#39;this&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameWriter">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DataFrameWriter</tt><big>(</big><em>df</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface used to write a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to external storage systems
(e.g. file systems, key-value stores, etc). Use <a class="reference internal" href="#pyspark.sql.DataFrame.write" title="pyspark.sql.DataFrame.write"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.write()</span></tt></a>
to access this.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.csv">
<tt class="descname">csv</tt><big>(</big><em>path</em>, <em>mode=None</em>, <em>compression=None</em>, <em>sep=None</em>, <em>quote=None</em>, <em>escape=None</em>, <em>header=None</em>, <em>nullValue=None</em>, <em>escapeQuotes=None</em>, <em>quoteAll=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.csv" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> in CSV format at the specified path.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; the path in any Hadoop supported file system</li>
<li><strong>mode</strong> &#8211; <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
<li><strong>compression</strong> &#8211; compression codec to use when saving to file. This can be one of the
known case-insensitive shorten names (none, bzip2, gzip, lz4,
snappy and deflate).</li>
<li><strong>sep</strong> &#8211; sets the single character as a separator for each field and value. If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">,</span></tt>.</li>
<li><strong>quote</strong> &#8211; sets the single character used for escaping quoted values where the
separator can be part of the value. If None is set, it uses the default
value, <tt class="docutils literal"><span class="pre">&quot;</span></tt>. If you would like to turn off quotations, you need to set an
empty string.</li>
<li><strong>escape</strong> &#8211; sets the single character used for escaping quotes inside an already
quoted value. If None is set, it uses the default value, <tt class="docutils literal"><span class="pre">\</span></tt></li>
<li><strong>escapeQuotes</strong> &#8211; A flag indicating whether values containing quotes should always
be enclosed in quotes. If None is set, it uses the default value
<tt class="docutils literal"><span class="pre">true</span></tt>, escaping all values containing a quote character.</li>
<li><strong>quoteAll</strong> &#8211; A flag indicating whether all values should always be enclosed in
quotes. If None is set, it uses the default value <tt class="docutils literal"><span class="pre">false</span></tt>,
only escaping values containing a quote character.</li>
<li><strong>header</strong> &#8211; writes the names of columns as the first line. If None is set, it uses
the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>nullValue</strong> &#8211; sets the string representation of a null value. If None is set, it uses
the default value, empty string.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.format">
<tt class="descname">format</tt><big>(</big><em>source</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the underlying output data source.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>source</strong> &#8211; string, name of the data source, e.g. &#8216;json&#8217;, &#8216;parquet&#8217;.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&#39;json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.insertInto">
<tt class="descname">insertInto</tt><big>(</big><em>tableName</em>, <em>overwrite=False</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.insertInto" title="Permalink to this definition">¶</a></dt>
<dd><p>Inserts the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to the specified table.</p>
<p>It requires that the schema of the class:<cite>DataFrame</cite> is the same as the
schema of the table.</p>
<p>Optionally overwriting any existing data.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.jdbc">
<tt class="descname">jdbc</tt><big>(</big><em>url</em>, <em>table</em>, <em>mode=None</em>, <em>properties=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.jdbc" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to an external database table via JDBC.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Don&#8217;t create too many partitions in parallel on a large cluster;         otherwise Spark might crash your external database systems.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> &#8211; a JDBC URL of the form <tt class="docutils literal"><span class="pre">jdbc:subprotocol:subname</span></tt></li>
<li><strong>table</strong> &#8211; Name of the table in the external database.</li>
<li><strong>mode</strong> &#8211; <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
<li><strong>properties</strong> &#8211; JDBC database connection arguments, a list of
arbitrary string tag/value. Normally at least a
&#8220;user&#8221; and &#8220;password&#8221; property should be included.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.json">
<tt class="descname">json</tt><big>(</big><em>path</em>, <em>mode=None</em>, <em>compression=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.json" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> in JSON format at the specified path.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; the path in any Hadoop supported file system</li>
<li><strong>mode</strong> &#8211; <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
<li><strong>compression</strong> &#8211; compression codec to use when saving to file. This can be one of the
known case-insensitive shorten names (none, bzip2, gzip, lz4,
snappy and deflate).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.mode">
<tt class="descname">mode</tt><big>(</big><em>saveMode</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.mode" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the behavior when data or table already exists.</p>
<p>Options include:</p>
<ul class="simple">
<li><cite>append</cite>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><cite>overwrite</cite>: Overwrite existing data.</li>
<li><cite>error</cite>: Throw an exception if data already exists.</li>
<li><cite>ignore</cite>: Silently ignore this operation if data already exists.</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s">&#39;append&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.option">
<tt class="descname">option</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.option" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds an output option for the underlying data source.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.options">
<tt class="descname">options</tt><big>(</big><em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.options" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds output options for the underlying data source.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.orc">
<tt class="descname">orc</tt><big>(</big><em>path</em>, <em>mode=None</em>, <em>partitionBy=None</em>, <em>compression=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.orc" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> in ORC format at the specified path.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Currently ORC support is only available together with Hive support.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; the path in any Hadoop supported file system</li>
<li><strong>mode</strong> &#8211; <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
<li><strong>partitionBy</strong> &#8211; names of partitioning columns</li>
<li><strong>compression</strong> &#8211; compression codec to use when saving to file. This can be one of the
known case-insensitive shorten names (none, snappy, zlib, and lzo).
This will override <tt class="docutils literal"><span class="pre">orc.compress</span></tt>. If None is set, it uses the
default value, <tt class="docutils literal"><span class="pre">snappy</span></tt>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">orc_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/orc_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">orc_df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.parquet">
<tt class="descname">parquet</tt><big>(</big><em>path</em>, <em>mode=None</em>, <em>partitionBy=None</em>, <em>compression=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.parquet" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> in Parquet format at the specified path.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; the path in any Hadoop supported file system</li>
<li><strong>mode</strong> &#8211; <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
<li><strong>partitionBy</strong> &#8211; names of partitioning columns</li>
<li><strong>compression</strong> &#8211; compression codec to use when saving to file. This can be one of the
known case-insensitive shorten names (none, snappy, gzip, and lzo).
This will override <tt class="docutils literal"><span class="pre">spark.sql.parquet.compression.codec</span></tt>. If None
is set, it uses the value specified in
<tt class="docutils literal"><span class="pre">spark.sql.parquet.compression.codec</span></tt>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.partitionBy">
<tt class="descname">partitionBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Partitions the output by the given columns on the file system.</p>
<p>If specified, the output is laid out on the file system similar
to Hive&#8217;s partitioning scheme.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; name of columns</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s">&#39;year&#39;</span><span class="p">,</span> <span class="s">&#39;month&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.save">
<tt class="descname">save</tt><big>(</big><em>path=None</em>, <em>format=None</em>, <em>mode=None</em>, <em>partitionBy=None</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the contents of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to a data source.</p>
<p>The data source is specified by the <tt class="docutils literal"><span class="pre">format</span></tt> and a set of <tt class="docutils literal"><span class="pre">options</span></tt>.
If <tt class="docutils literal"><span class="pre">format</span></tt> is not specified, the default data source configured by
<tt class="docutils literal"><span class="pre">spark.sql.sources.default</span></tt> will be used.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; the path in a Hadoop supported file system</li>
<li><strong>format</strong> &#8211; the format used to save</li>
<li><strong>mode</strong> &#8211; <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
<li><strong>partitionBy</strong> &#8211; names of partitioning columns</li>
<li><strong>options</strong> &#8211; all other string options</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s">&#39;append&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.saveAsTable">
<tt class="descname">saveAsTable</tt><big>(</big><em>name</em>, <em>format=None</em>, <em>mode=None</em>, <em>partitionBy=None</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.saveAsTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as the specified table.</p>
<p>In the case the table already exists, behavior of this function depends on the
save mode, specified by the <cite>mode</cite> function (default to throwing an exception).
When <cite>mode</cite> is <cite>Overwrite</cite>, the schema of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> does not need to be
the same as that of the existing table.</p>
<ul class="simple">
<li><cite>append</cite>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><cite>overwrite</cite>: Overwrite existing data.</li>
<li><cite>error</cite>: Throw an exception if data already exists.</li>
<li><cite>ignore</cite>: Silently ignore this operation if data already exists.</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> &#8211; the table name</li>
<li><strong>format</strong> &#8211; the format used to save</li>
<li><strong>mode</strong> &#8211; one of <cite>append</cite>, <cite>overwrite</cite>, <cite>error</cite>, <cite>ignore</cite> (default: error)</li>
<li><strong>partitionBy</strong> &#8211; names of partitioning columns</li>
<li><strong>options</strong> &#8211; all other string options</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.text">
<tt class="descname">text</tt><big>(</big><em>path</em>, <em>compression=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the DataFrame in a text file at the specified path.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; the path in any Hadoop supported file system</li>
<li><strong>compression</strong> &#8211; compression codec to use when saving to file. This can be one of the
known case-insensitive shorten names (none, bzip2, gzip, lz4,
snappy and deflate).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The DataFrame must have only one column that is of string type.
Each row becomes a new line in the output file.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pyspark.sql.types">
<span id="pyspark-sql-types-module"></span><h2>pyspark.sql.types module<a class="headerlink" href="#module-pyspark.sql.types" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pyspark.sql.types.DataType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">DataType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for data types.</p>
<dl class="method">
<dt id="pyspark.sql.types.DataType.fromInternal">
<tt class="descname">fromInternal</tt><big>(</big><em>obj</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an internal SQL object into a native Python object.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.json">
<tt class="descname">json</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.json" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.needConversion">
<tt class="descname">needConversion</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd><p>Does this type need to conversion between Python object and internal SQL object.</p>
<p>This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.toInternal">
<tt class="descname">toInternal</tt><big>(</big><em>obj</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a Python object into an internal SQL object.</p>
</dd></dl>

<dl class="classmethod">
<dt id="pyspark.sql.types.DataType.typeName">
<em class="property">classmethod </em><tt class="descname">typeName</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.typeName"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.typeName" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.NullType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">NullType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#NullType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.NullType" title="Permalink to this definition">¶</a></dt>
<dd><p>Null type.</p>
<p>The data type representing None, used for the types that cannot be inferred.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.StringType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">StringType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#StringType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StringType" title="Permalink to this definition">¶</a></dt>
<dd><p>String data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.BinaryType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">BinaryType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#BinaryType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.BinaryType" title="Permalink to this definition">¶</a></dt>
<dd><p>Binary (byte array) data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.BooleanType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">BooleanType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#BooleanType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.BooleanType" title="Permalink to this definition">¶</a></dt>
<dd><p>Boolean data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.DateType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">DateType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#DateType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DateType" title="Permalink to this definition">¶</a></dt>
<dd><p>Date (datetime.date) data type.</p>
<dl class="attribute">
<dt id="pyspark.sql.types.DateType.EPOCH_ORDINAL">
<tt class="descname">EPOCH_ORDINAL</tt><em class="property"> = 719163</em><a class="headerlink" href="#pyspark.sql.types.DateType.EPOCH_ORDINAL" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DateType.fromInternal">
<tt class="descname">fromInternal</tt><big>(</big><em>v</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DateType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DateType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DateType.needConversion">
<tt class="descname">needConversion</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DateType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DateType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DateType.toInternal">
<tt class="descname">toInternal</tt><big>(</big><em>d</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DateType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DateType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.TimestampType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">TimestampType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#TimestampType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.TimestampType" title="Permalink to this definition">¶</a></dt>
<dd><p>Timestamp (datetime.datetime) data type.</p>
<dl class="method">
<dt id="pyspark.sql.types.TimestampType.fromInternal">
<tt class="descname">fromInternal</tt><big>(</big><em>ts</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#TimestampType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.TimestampType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.TimestampType.needConversion">
<tt class="descname">needConversion</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#TimestampType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.TimestampType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.TimestampType.toInternal">
<tt class="descname">toInternal</tt><big>(</big><em>dt</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#TimestampType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.TimestampType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.DecimalType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">DecimalType</tt><big>(</big><em>precision=10</em>, <em>scale=0</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DecimalType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DecimalType" title="Permalink to this definition">¶</a></dt>
<dd><p>Decimal (decimal.Decimal) data type.</p>
<p>The DecimalType must have fixed precision (the maximum total number of digits)
and scale (the number of digits on the right of dot). For example, (5, 2) can
support the value from [-999.99 to 999.99].</p>
<p>The precision can be up to 38, the scale must less or equal to precision.</p>
<p>When create a DecimalType, the default precision and scale is (10, 0). When infer
schema from decimal.Decimal objects, it will be DecimalType(38, 18).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>precision</strong> &#8211; the maximum total number of digits (default: 10)</li>
<li><strong>scale</strong> &#8211; the number of digits on right side of dot. (default: 0)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="pyspark.sql.types.DecimalType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DecimalType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DecimalType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DecimalType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DecimalType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DecimalType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.DoubleType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">DoubleType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#DoubleType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DoubleType" title="Permalink to this definition">¶</a></dt>
<dd><p>Double data type, representing double precision floats.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.FloatType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">FloatType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#FloatType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.FloatType" title="Permalink to this definition">¶</a></dt>
<dd><p>Float data type, representing single precision floats.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.ByteType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">ByteType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#ByteType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ByteType" title="Permalink to this definition">¶</a></dt>
<dd><p>Byte data type, i.e. a signed integer in a single byte.</p>
<dl class="method">
<dt id="pyspark.sql.types.ByteType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ByteType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ByteType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.IntegerType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">IntegerType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#IntegerType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.IntegerType" title="Permalink to this definition">¶</a></dt>
<dd><p>Int data type, i.e. a signed 32-bit integer.</p>
<dl class="method">
<dt id="pyspark.sql.types.IntegerType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#IntegerType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.IntegerType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.LongType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">LongType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#LongType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.LongType" title="Permalink to this definition">¶</a></dt>
<dd><p>Long data type, i.e. a signed 64-bit integer.</p>
<p>If the values are beyond the range of [-9223372036854775808, 9223372036854775807],
please use <a class="reference internal" href="#pyspark.sql.types.DecimalType" title="pyspark.sql.types.DecimalType"><tt class="xref py py-class docutils literal"><span class="pre">DecimalType</span></tt></a>.</p>
<dl class="method">
<dt id="pyspark.sql.types.LongType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#LongType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.LongType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.ShortType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">ShortType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#ShortType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ShortType" title="Permalink to this definition">¶</a></dt>
<dd><p>Short data type, i.e. a signed 16-bit integer.</p>
<dl class="method">
<dt id="pyspark.sql.types.ShortType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ShortType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ShortType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.ArrayType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">ArrayType</tt><big>(</big><em>elementType</em>, <em>containsNull=True</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType" title="Permalink to this definition">¶</a></dt>
<dd><p>Array data type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>elementType</strong> &#8211; <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt></a> of each element in the array.</li>
<li><strong>containsNull</strong> &#8211; boolean, whether the array can contain null (None) values.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="pyspark.sql.types.ArrayType.fromInternal">
<tt class="descname">fromInternal</tt><big>(</big><em>obj</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="classmethod">
<dt id="pyspark.sql.types.ArrayType.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.needConversion">
<tt class="descname">needConversion</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.toInternal">
<tt class="descname">toInternal</tt><big>(</big><em>obj</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.MapType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">MapType</tt><big>(</big><em>keyType</em>, <em>valueType</em>, <em>valueContainsNull=True</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType" title="Permalink to this definition">¶</a></dt>
<dd><p>Map data type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>keyType</strong> &#8211; <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt></a> of the keys in the map.</li>
<li><strong>valueType</strong> &#8211; <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt></a> of the values in the map.</li>
<li><strong>valueContainsNull</strong> &#8211; indicates whether values can contain null (None) values.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Keys in a map data type are not allowed to be null (None).</p>
<dl class="method">
<dt id="pyspark.sql.types.MapType.fromInternal">
<tt class="descname">fromInternal</tt><big>(</big><em>obj</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="classmethod">
<dt id="pyspark.sql.types.MapType.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.needConversion">
<tt class="descname">needConversion</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.toInternal">
<tt class="descname">toInternal</tt><big>(</big><em>obj</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.StructField">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">StructField</tt><big>(</big><em>name</em>, <em>dataType</em>, <em>nullable=True</em>, <em>metadata=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField" title="Permalink to this definition">¶</a></dt>
<dd><p>A field in <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> &#8211; string, name of the field.</li>
<li><strong>dataType</strong> &#8211; <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt></a> of the field.</li>
<li><strong>nullable</strong> &#8211; boolean, whether the field can be null (None) or not.</li>
<li><strong>metadata</strong> &#8211; a dict from string to simple type that can be toInternald to JSON automatically</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="pyspark.sql.types.StructField.fromInternal">
<tt class="descname">fromInternal</tt><big>(</big><em>obj</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="classmethod">
<dt id="pyspark.sql.types.StructField.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.needConversion">
<tt class="descname">needConversion</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.needConversion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.toInternal">
<tt class="descname">toInternal</tt><big>(</big><em>obj</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.toInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.StructType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">StructType</tt><big>(</big><em>fields=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType" title="Permalink to this definition">¶</a></dt>
<dd><p>Struct type, consisting of a list of <a class="reference internal" href="#pyspark.sql.types.StructField" title="pyspark.sql.types.StructField"><tt class="xref py py-class docutils literal"><span class="pre">StructField</span></tt></a>.</p>
<p>This is the data type representing a <tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt>.</p>
<p>Iterating a <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt></a> will iterate its <tt class="xref py py-class docutils literal"><span class="pre">StructField`s.</span>
<span class="pre">A</span> <span class="pre">contained</span> <span class="pre">:class:`StructField</span></tt> can be accessed by name or position.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span><span class="p">[</span><span class="s">&quot;f1&quot;</span><span class="p">]</span>
<span class="go">StructField(f1,StringType,true)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">StructField(f1,StringType,true)</span>
</pre></div>
</div>
<dl class="method">
<dt id="pyspark.sql.types.StructType.add">
<tt class="descname">add</tt><big>(</big><em>field</em>, <em>data_type=None</em>, <em>nullable=True</em>, <em>metadata=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a StructType by adding new elements to it to define the schema. The method accepts
either:</p>
<blockquote>
<div><ol class="loweralpha simple">
<li>A single parameter which is a StructField object.</li>
<li>Between 2 and 4 parameters as (name, data_type, nullable (optional),
metadata(optional). The data_type parameter may be either a String or a
DataType object.</li>
</ol>
</div></blockquote>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&quot;f2&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct2</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span> \
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;f2&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">None</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">==</span> <span class="n">struct2</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct2</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">==</span> <span class="n">struct2</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&quot;f1&quot;</span><span class="p">,</span> <span class="s">&quot;string&quot;</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct2</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;f1&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">struct1</span> <span class="o">==</span> <span class="n">struct2</span>
<span class="go">True</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>field</strong> &#8211; Either the name of the field or a StructField object</li>
<li><strong>data_type</strong> &#8211; If present, the DataType of the StructField to create</li>
<li><strong>nullable</strong> &#8211; Whether the field to add should be nullable (default True)</li>
<li><strong>metadata</strong> &#8211; Any additional metadata (default None)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a new updated StructType</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.fromInternal">
<tt class="descname">fromInternal</tt><big>(</big><em>obj</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.fromInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.fromInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="classmethod">
<dt id="pyspark.sql.types.StructType.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.needConversion">
<tt class="descname">needConversion</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.needConversion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.needConversion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.toInternal">
<tt class="descname">toInternal</tt><big>(</big><em>obj</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.toInternal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.toInternal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pyspark.sql.functions">
<span id="pyspark-sql-functions-module"></span><h2>pyspark.sql.functions module<a class="headerlink" href="#module-pyspark.sql.functions" title="Permalink to this headline">¶</a></h2>
<p>A collections of builtin functions</p>
<dl class="function">
<dt id="pyspark.sql.functions.abs">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">abs</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the absolute value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.acos">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">acos</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.acos" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the cosine inverse of the given value; the returned angle is in the range0.0 through pi.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.add_months">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">add_months</tt><big>(</big><em>start</em>, <em>months</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#add_months"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.add_months" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the date that is <cite>months</cite> months after <cite>start</cite></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">add_months</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=datetime.date(2015, 5, 8))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.approxCountDistinct">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">approxCountDistinct</tt><big>(</big><em>col</em>, <em>rsd=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#approxCountDistinct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.approxCountDistinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> for approximate distinct count of <tt class="docutils literal"><span class="pre">col</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">approxCountDistinct</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">array</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new array column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string) or list of <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expressions that have
the same data type.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;arr&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(arr=[2, 2]), Row(arr=[5, 5])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">])</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;arr&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(arr=[2, 2]), Row(arr=[5, 5])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array_contains">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">array_contains</tt><big>(</big><em>col</em>, <em>value</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#array_contains"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array_contains" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns True if the array contains the given value. The collection
elements and value must be of the same type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col</strong> &#8211; name of column containing array</li>
<li><strong>value</strong> &#8211; value to check for in array</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">,</span> <span class="s">&quot;c&quot;</span><span class="p">],),</span> <span class="p">([],)],</span> <span class="p">[</span><span class="s">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array_contains</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s">&quot;a&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.asc">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">asc</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.asc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the ascending order of the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.ascii">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">ascii</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.ascii" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the numeric value of the first character of the string column.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.asin">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">asin</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.asin" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sine inverse of the given value; the returned angle is in the range-pi/2 through pi/2.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.atan">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">atan</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.atan" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the tangent inverse of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.atan2">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">atan2</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.atan2" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the angle theta from the conversion of rectangular coordinates (x, y) topolar coordinates (r, theta).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.avg">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">avg</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.avg" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the average of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.base64">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">base64</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.base64" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the BASE64 encoding of a binary column and returns it as a string column.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.bin">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">bin</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#bin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.bin" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the string representation of the binary value of the given column.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">bin</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=u&#39;10&#39;), Row(c=u&#39;101&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.bitwiseNOT">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">bitwiseNOT</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.bitwiseNOT" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes bitwise not.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.broadcast">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">broadcast</tt><big>(</big><em>df</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#broadcast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks a DataFrame as small enough for use in broadcast joins.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.bround">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">bround</tt><big>(</big><em>col</em>, <em>scale=0</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#bround"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.bround" title="Permalink to this definition">¶</a></dt>
<dd><p>Round the given value to <cite>scale</cite> decimal places using HALF_EVEN rounding mode if <cite>scale</cite> &gt;= 0
or at integral part when <cite>scale</cite> &lt; 0.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mf">2.5</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">bround</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=2.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cbrt">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">cbrt</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.cbrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the cube-root of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.ceil">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">ceil</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.ceil" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the ceiling of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.coalesce">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">coalesce</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#coalesce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.coalesce" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first column that is not null.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> <span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+----+</span>
<span class="go">|   a|   b|</span>
<span class="go">+----+----+</span>
<span class="go">|null|null|</span>
<span class="go">|   1|null|</span>
<span class="go">|null|   2|</span>
<span class="go">+----+----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">coalesce</span><span class="p">(</span><span class="n">cDf</span><span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">],</span> <span class="n">cDf</span><span class="p">[</span><span class="s">&quot;b&quot;</span><span class="p">]))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+--------------+</span>
<span class="go">|coalesce(a, b)|</span>
<span class="go">+--------------+</span>
<span class="go">|          null|</span>
<span class="go">|             1|</span>
<span class="go">|             2|</span>
<span class="go">+--------------+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&#39;*&#39;</span><span class="p">,</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">cDf</span><span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">],</span> <span class="n">lit</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+----+----------------+</span>
<span class="go">|   a|   b|coalesce(a, 0.0)|</span>
<span class="go">+----+----+----------------+</span>
<span class="go">|null|null|             0.0|</span>
<span class="go">|   1|null|             1.0|</span>
<span class="go">|null|   2|             0.0|</span>
<span class="go">+----+----+----------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.col">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">col</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.col" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> based on the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.collect_list">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">collect_list</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.collect_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns a list of objects with duplicates.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.collect_set">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">collect_set</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.collect_set" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns a set of objects with duplicate elements eliminated.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.column">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">column</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.column" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> based on the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.concat">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">concat</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#concat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.concat" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates multiple input string columns together into a single string column.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;abcd&#39;</span><span class="p">,</span><span class="s">&#39;123&#39;</span><span class="p">)],</span> <span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">,</span> <span class="s">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">concat</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=u&#39;abcd123&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.concat_ws">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">concat_ws</tt><big>(</big><em>sep</em>, <em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#concat_ws"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.concat_ws" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates multiple input string columns together into a single string column,
using the given separator.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;abcd&#39;</span><span class="p">,</span><span class="s">&#39;123&#39;</span><span class="p">)],</span> <span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">,</span> <span class="s">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">concat_ws</span><span class="p">(</span><span class="s">&#39;-&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=u&#39;abcd-123&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.conv">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">conv</tt><big>(</big><em>col</em>, <em>fromBase</em>, <em>toBase</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#conv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.conv" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a number in a string column from one base to another.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&quot;010101&quot;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;n&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">conv</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;hex&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hex=u&#39;15&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.corr">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">corr</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#corr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.corr" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> for the Pearson Correlation Coefficient for <tt class="docutils literal"><span class="pre">col1</span></tt>
and <tt class="docutils literal"><span class="pre">col2</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">corr</span><span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=1.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cos">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">cos</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.cos" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the cosine of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cosh">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">cosh</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.cosh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the hyperbolic cosine of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.count">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">count</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the number of items in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.countDistinct">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">countDistinct</tt><big>(</big><em>col</em>, <em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#countDistinct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.countDistinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> for distinct count of <tt class="docutils literal"><span class="pre">col</span></tt> or <tt class="docutils literal"><span class="pre">cols</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">countDistinct</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=2)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">countDistinct</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.covar_pop">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">covar_pop</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#covar_pop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.covar_pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> for the population covariance of <tt class="docutils literal"><span class="pre">col1</span></tt>
and <tt class="docutils literal"><span class="pre">col2</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">covar_pop</span><span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=0.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.covar_samp">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">covar_samp</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#covar_samp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.covar_samp" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> for the sample covariance of <tt class="docutils literal"><span class="pre">col1</span></tt>
and <tt class="docutils literal"><span class="pre">col2</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">covar_samp</span><span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=0.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.crc32">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">crc32</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#crc32"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.crc32" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the cyclic redundancy check value  (CRC32) of a binary column and
returns the value as a bigint.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;ABC&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">crc32</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;crc32&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(crc32=2743272264)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.create_map">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">create_map</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#create_map"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.create_map" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new map column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string) or list of <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expressions that grouped
as key-value pairs, e.g. (key1, value1, key2, value2, ...).</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">create_map</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;map&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(map={u&#39;Alice&#39;: 2}), Row(map={u&#39;Bob&#39;: 5})]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">create_map</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">])</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;map&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(map={u&#39;Alice&#39;: 2}), Row(map={u&#39;Bob&#39;: 5})]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cume_dist">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">cume_dist</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.functions.cume_dist" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the cumulative distribution of values within a window partition,
i.e. the fraction of rows that are below the current row.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.current_date">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">current_date</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#current_date"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.current_date" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current date as a date column.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.current_timestamp">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">current_timestamp</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#current_timestamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.current_timestamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current timestamp as a timestamp column.</p>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.date_add">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">date_add</tt><big>(</big><em>start</em>, <em>days</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#date_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.date_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the date that is <cite>days</cite> days after <cite>start</cite></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">date_add</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=datetime.date(2015, 4, 9))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.date_format">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">date_format</tt><big>(</big><em>date</em>, <em>format</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#date_format"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.date_format" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a date/timestamp/string to a value of string in the format specified by the date
format given by the second argument.</p>
<p>A pattern could be for instance <cite>dd.MM.yyyy</cite> and could return a string like &#8216;18.03.1993&#8217;. All
pattern letters of the Java class <cite>java.text.SimpleDateFormat</cite> can be used.</p>
<p>NOTE: Use when ever possible specialized functions like <cite>year</cite>. These benefit from a
specialized implementation.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">date_format</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="s">&#39;MM/dd/yyy&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(date=u&#39;04/08/2015&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.date_sub">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">date_sub</tt><big>(</big><em>start</em>, <em>days</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#date_sub"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.date_sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the date that is <cite>days</cite> days before <cite>start</cite></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">date_sub</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=datetime.date(2015, 4, 7))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.datediff">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">datediff</tt><big>(</big><em>end</em>, <em>start</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#datediff"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.datediff" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of days from <cite>start</cite> to <cite>end</cite>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,</span><span class="s">&#39;2015-05-10&#39;</span><span class="p">)],</span> <span class="p">[</span><span class="s">&#39;d1&#39;</span><span class="p">,</span> <span class="s">&#39;d2&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">datediff</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">d1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;diff&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(diff=32)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.dayofmonth">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">dayofmonth</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#dayofmonth"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.dayofmonth" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the day of the month of a given date as integer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">dayofmonth</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;day&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(day=8)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.dayofyear">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">dayofyear</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#dayofyear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.dayofyear" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the day of the year of a given date as integer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">dayofyear</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;day&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(day=98)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.decode">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">decode</tt><big>(</big><em>col</em>, <em>charset</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the first argument into a string from a binary using the provided character set
(one of &#8216;US-ASCII&#8217;, &#8216;ISO-8859-1&#8217;, &#8216;UTF-8&#8217;, &#8216;UTF-16BE&#8217;, &#8216;UTF-16LE&#8217;, &#8216;UTF-16&#8217;).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.dense_rank">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">dense_rank</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.functions.dense_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the rank of rows within a window partition, without any gaps.</p>
<p>The difference between rank and denseRank is that denseRank leaves no gaps in ranking
sequence when there are ties. That is, if you were ranking a competition using denseRank
and had three people tie for second place, you would say that all three were in second
place and that the next person came in third.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.desc">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">desc</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.desc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the descending order of the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.encode">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">encode</tt><big>(</big><em>col</em>, <em>charset</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the first argument into a binary from a string using the provided character set
(one of &#8216;US-ASCII&#8217;, &#8216;ISO-8859-1&#8217;, &#8216;UTF-8&#8217;, &#8216;UTF-16BE&#8217;, &#8216;UTF-16LE&#8217;, &#8216;UTF-16&#8217;).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.exp">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">exp</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the exponential of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.explode">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">explode</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#explode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.explode" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new row for each element in the given array or map.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">intlist</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">mapfield</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;a&quot;</span><span class="p">:</span> <span class="s">&quot;b&quot;</span><span class="p">})])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">explode</span><span class="p">(</span><span class="n">eDF</span><span class="o">.</span><span class="n">intlist</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;anInt&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(anInt=1), Row(anInt=2), Row(anInt=3)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">explode</span><span class="p">(</span><span class="n">eDF</span><span class="o">.</span><span class="n">mapfield</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">,</span> <span class="s">&quot;value&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|key|value|</span>
<span class="go">+---+-----+</span>
<span class="go">|  a|    b|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.expm1">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">expm1</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.expm1" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the exponential of the given value minus one.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.expr">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">expr</tt><big>(</big><em>str</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#expr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.expr" title="Permalink to this definition">¶</a></dt>
<dd><p>Parses the expression string into the column that it represents</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">expr</span><span class="p">(</span><span class="s">&quot;length(name)&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(length(name)=5), Row(length(name)=3)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.factorial">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">factorial</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#factorial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.factorial" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the factorial of the given value.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">5</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;n&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">factorial</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;f&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f=120)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.first">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">first</tt><big>(</big><em>col</em>, <em>ignorenulls=False</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#first"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.first" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the first value in a group.</p>
<p>The function by default returns the first values it sees. It will return the first non-null
value it sees when ignoreNulls is set to true. If all values are null, then null is returned.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.floor">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">floor</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.floor" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the floor of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.format_number">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">format_number</tt><big>(</big><em>col</em>, <em>d</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#format_number"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.format_number" title="Permalink to this definition">¶</a></dt>
<dd><p>Formats the number X to a format like &#8216;#,&#8211;#,&#8211;#.&#8211;&#8217;, rounded to d decimal places,
and returns the result as a string.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col</strong> &#8211; the column name of the numeric value to be formatted</li>
<li><strong>d</strong> &#8211; the N decimal places</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">5</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">format_number</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;v&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(v=u&#39;5.0000&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.format_string">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">format_string</tt><big>(</big><em>format</em>, <em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#format_string"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.format_string" title="Permalink to this definition">¶</a></dt>
<dd><p>Formats the arguments in printf-style and returns the result as a string column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col</strong> &#8211; the column name of the numeric value to be formatted</li>
<li><strong>d</strong> &#8211; the N decimal places</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">5</span><span class="p">,</span> <span class="s">&quot;hello&quot;</span><span class="p">)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">format_string</span><span class="p">(</span><span class="s">&#39;</span><span class="si">%d</span><span class="s"> </span><span class="si">%s</span><span class="s">&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;v&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(v=u&#39;5 hello&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.from_unixtime">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">from_unixtime</tt><big>(</big><em>timestamp</em>, <em>format='yyyy-MM-dd HH:mm:ss'</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#from_unixtime"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.from_unixtime" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string
representing the timestamp of that moment in the current system time zone in the given
format.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.from_utc_timestamp">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">from_utc_timestamp</tt><big>(</big><em>timestamp</em>, <em>tz</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#from_utc_timestamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.from_utc_timestamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Assumes given timestamp is UTC and converts to given timezone.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;t&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_utc_timestamp</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="s">&quot;PST&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;t&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(t=datetime.datetime(1997, 2, 28, 2, 30))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.get_json_object">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">get_json_object</tt><big>(</big><em>col</em>, <em>path</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#get_json_object"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.get_json_object" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts json object from a json string based on json path specified, and returns json string
of the extracted json object. It will return null if the input json string is invalid.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col</strong> &#8211; string column in json format</li>
<li><strong>path</strong> &#8211; path to the json object to extract</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="s">&quot;1&quot;</span><span class="p">,</span> <span class="s">&#39;&#39;&#39;{&quot;f1&quot;: &quot;value1&quot;, &quot;f2&quot;: &quot;value2&quot;}&#39;&#39;&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s">&quot;2&quot;</span><span class="p">,</span> <span class="s">&#39;&#39;&#39;{&quot;f1&quot;: &quot;value12&quot;}&#39;&#39;&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">,</span> <span class="s">&quot;jstring&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">get_json_object</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">jstring</span><span class="p">,</span> <span class="s">&#39;$.f1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;c0&quot;</span><span class="p">),</span> \
<span class="gp">... </span>                  <span class="n">get_json_object</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">jstring</span><span class="p">,</span> <span class="s">&#39;$.f2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;c1&quot;</span><span class="p">)</span> <span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(key=u&#39;1&#39;, c0=u&#39;value1&#39;, c1=u&#39;value2&#39;), Row(key=u&#39;2&#39;, c0=u&#39;value12&#39;, c1=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.greatest">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">greatest</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#greatest"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.greatest" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the greatest value of the list of column names, skipping null values.
This function takes at least 2 parameters. It will return null iff all parameters are null.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="s">&#39;c&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">greatest</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;greatest&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(greatest=4)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.grouping">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">grouping</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#grouping"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.grouping" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated
or not, returns 1 for aggregated or 0 for not aggregated in the result set.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">cube</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">grouping</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+--------------+--------+</span>
<span class="go">| name|grouping(name)|sum(age)|</span>
<span class="go">+-----+--------------+--------+</span>
<span class="go">| null|             1|       7|</span>
<span class="go">|Alice|             0|       2|</span>
<span class="go">|  Bob|             0|       5|</span>
<span class="go">+-----+--------------+--------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.grouping_id">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">grouping_id</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#grouping_id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.grouping_id" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the level of grouping, equals to</p>
<blockquote>
<div>(grouping(c1) &lt;&lt; (n-1)) + (grouping(c2) &lt;&lt; (n-2)) + ... + grouping(cn)</div></blockquote>
<p>Note: the list of columns should match with grouping columns exactly, or empty (means all the
grouping columns).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">cube</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">grouping_id</span><span class="p">(),</span> <span class="nb">sum</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+-------------+--------+</span>
<span class="go">| name|grouping_id()|sum(age)|</span>
<span class="go">+-----+-------------+--------+</span>
<span class="go">| null|            1|       7|</span>
<span class="go">|Alice|            0|       2|</span>
<span class="go">|  Bob|            0|       5|</span>
<span class="go">+-----+-------------+--------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.hash">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">hash</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#hash"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.hash" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the hash code of given columns, and returns the result as an int column.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;ABC&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">hash</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;hash&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hash=-757602832)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.hex">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">hex</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#hex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.hex" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes hex value of the given column, which could be StringType,
BinaryType, IntegerType or LongType.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;ABC&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">hex</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">),</span> <span class="nb">hex</span><span class="p">(</span><span class="s">&#39;b&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hex(a)=u&#39;414243&#39;, hex(b)=u&#39;3&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.hour">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">hour</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#hour"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.hour" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the hours of a given date as integer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08 13:08:15&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">hour</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;hour&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hour=13)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.hypot">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">hypot</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.hypot" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <cite>sqrt(a^2 + b^2)</cite> without intermediate overflow or underflow.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.initcap">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">initcap</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#initcap"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.initcap" title="Permalink to this definition">¶</a></dt>
<dd><p>Translate the first letter of each word to upper case in the sentence.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;ab cd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">initcap</span><span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;v&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(v=u&#39;Ab Cd&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.input_file_name">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">input_file_name</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#input_file_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.input_file_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a string column for the file name of the current Spark task.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.instr">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">instr</tt><big>(</big><em>str</em>, <em>substr</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#instr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.instr" title="Permalink to this definition">¶</a></dt>
<dd><p>Locate the position of the first occurrence of substr column in the given string.
Returns null if either of the arguments are null.</p>
<p>NOTE: The position is not zero based, but 1 based index, returns 0 if substr
could not be found in str.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;abcd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">instr</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.isnan">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">isnan</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#isnan"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.isnan" title="Permalink to this definition">¶</a></dt>
<dd><p>An expression that returns true iff the column is NaN.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">&#39;nan&#39;</span><span class="p">)),</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s">&#39;nan&#39;</span><span class="p">),</span> <span class="mf">2.0</span><span class="p">)],</span> <span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">isnan</span><span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;r1&quot;</span><span class="p">),</span> <span class="n">isnan</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;r2&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r1=False, r2=False), Row(r1=True, r2=True)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.isnull">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">isnull</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#isnull"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.isnull" title="Permalink to this definition">¶</a></dt>
<dd><p>An expression that returns true iff the column is null.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> <span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">isnull</span><span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;r1&quot;</span><span class="p">),</span> <span class="n">isnull</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;r2&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r1=False, r2=False), Row(r1=True, r2=True)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.json_tuple">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">json_tuple</tt><big>(</big><em>col</em>, <em>*fields</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#json_tuple"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.json_tuple" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new row for a json column according to the given field names.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col</strong> &#8211; string column in json format</li>
<li><strong>fields</strong> &#8211; list of fields to extract</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="s">&quot;1&quot;</span><span class="p">,</span> <span class="s">&#39;&#39;&#39;{&quot;f1&quot;: &quot;value1&quot;, &quot;f2&quot;: &quot;value2&quot;}&#39;&#39;&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s">&quot;2&quot;</span><span class="p">,</span> <span class="s">&#39;&#39;&#39;{&quot;f1&quot;: &quot;value12&quot;}&#39;&#39;&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">,</span> <span class="s">&quot;jstring&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">json_tuple</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">jstring</span><span class="p">,</span> <span class="s">&#39;f1&#39;</span><span class="p">,</span> <span class="s">&#39;f2&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(key=u&#39;1&#39;, c0=u&#39;value1&#39;, c1=u&#39;value2&#39;), Row(key=u&#39;2&#39;, c0=u&#39;value12&#39;, c1=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.kurtosis">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">kurtosis</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.kurtosis" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the kurtosis of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lag">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">lag</tt><big>(</big><em>col</em>, <em>count=1</em>, <em>default=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#lag"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.lag" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the value that is <cite>offset</cite> rows before the current row, and
<cite>defaultValue</cite> if there is less than <cite>offset</cite> rows before the current row. For example,
an <cite>offset</cite> of one will return the previous row at any given point in the window partition.</p>
<p>This is equivalent to the LAG function in SQL.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col</strong> &#8211; name of column or expression</li>
<li><strong>count</strong> &#8211; number of row to extend</li>
<li><strong>default</strong> &#8211; default value</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.last">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">last</tt><big>(</big><em>col</em>, <em>ignorenulls=False</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#last"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.last" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the last value in a group.</p>
<p>The function by default returns the last values it sees. It will return the last non-null
value it sees when ignoreNulls is set to true. If all values are null, then null is returned.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.last_day">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">last_day</tt><big>(</big><em>date</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#last_day"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.last_day" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the last day of the month which the given date belongs to.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;1997-02-10&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">last_day</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(date=datetime.date(1997, 2, 28))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lead">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">lead</tt><big>(</big><em>col</em>, <em>count=1</em>, <em>default=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#lead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.lead" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the value that is <cite>offset</cite> rows after the current row, and
<cite>defaultValue</cite> if there is less than <cite>offset</cite> rows after the current row. For example,
an <cite>offset</cite> of one will return the next row at any given point in the window partition.</p>
<p>This is equivalent to the LEAD function in SQL.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col</strong> &#8211; name of column or expression</li>
<li><strong>count</strong> &#8211; number of row to extend</li>
<li><strong>default</strong> &#8211; default value</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.least">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">least</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#least"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.least" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the least value of the list of column names, skipping null values.
This function takes at least 2 parameters. It will return null iff all parameters are null.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="s">&#39;c&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">least</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;least&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(least=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.length">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">length</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#length"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.length" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the length of a string or binary expression.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;ABC&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;length&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(length=3)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.levenshtein">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">levenshtein</tt><big>(</big><em>left</em>, <em>right</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#levenshtein"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.levenshtein" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Levenshtein distance of the two given strings.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df0</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;kitten&#39;</span><span class="p">,</span> <span class="s">&#39;sitting&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;l&#39;</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df0</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">levenshtein</span><span class="p">(</span><span class="s">&#39;l&#39;</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=3)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lit">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">lit</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.lit" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> of literal value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.locate">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">locate</tt><big>(</big><em>substr</em>, <em>str</em>, <em>pos=1</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#locate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.locate" title="Permalink to this definition">¶</a></dt>
<dd><p>Locate the position of the first occurrence of substr in a string column, after position pos.</p>
<p>NOTE: The position is not zero based, but 1 based index. returns 0 if substr
could not be found in str.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>substr</strong> &#8211; a string</li>
<li><strong>str</strong> &#8211; a Column of StringType</li>
<li><strong>pos</strong> &#8211; start position (zero based)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;abcd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">locate</span><span class="p">(</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">log</tt><big>(</big><em>arg1</em>, <em>arg2=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#log"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first argument-based logarithm of the second argument.</p>
<p>If there is only one argument, then this takes the natural logarithm of the argument.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;ten&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">ten</span><span class="p">)[:</span><span class="mi">7</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[&#39;0.30102&#39;, &#39;0.69897&#39;]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;e&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">e</span><span class="p">)[:</span><span class="mi">7</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[&#39;0.69314&#39;, &#39;1.60943&#39;]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log10">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">log10</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.log10" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the logarithm of the given value in Base 10.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log1p">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">log1p</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.log1p" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the natural logarithm of the given value plus one.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log2">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">log2</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#log2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.log2" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the base-2 logarithm of the argument.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">4</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">log2</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;log2&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(log2=2.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lower">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">lower</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.lower" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a string column to lower case.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lpad">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">lpad</tt><big>(</big><em>col</em>, <em>len</em>, <em>pad</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#lpad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.lpad" title="Permalink to this definition">¶</a></dt>
<dd><p>Left-pad the string column to width <cite>len</cite> with <cite>pad</cite>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;abcd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">lpad</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s">&#39;#&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=u&#39;##abcd&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.ltrim">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">ltrim</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.ltrim" title="Permalink to this definition">¶</a></dt>
<dd><p>Trim the spaces from left end for the specified string value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.max">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">max</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the maximum value of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.md5">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">md5</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#md5"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.md5" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the MD5 digest and returns the value as a 32 character hex string.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;ABC&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">md5</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;hash&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hash=u&#39;902fbdd2b1df0c4f70b4a5d23525e932&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.mean">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">mean</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the average of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.min">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">min</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.min" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the minimum value of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.minute">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">minute</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#minute"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.minute" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the minutes of a given date as integer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08 13:08:15&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">minute</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;minute&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(minute=8)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.monotonically_increasing_id">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">monotonically_increasing_id</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#monotonically_increasing_id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.monotonically_increasing_id" title="Permalink to this definition">¶</a></dt>
<dd><p>A column that generates monotonically increasing 64-bit integers.</p>
<p>The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.
The current implementation puts the partition ID in the upper 31 bits, and the record number
within each partition in the lower 33 bits. The assumption is that the data frame has
less than 1 billion partitions, and each partition has less than 8 billion records.</p>
<p>As an example, consider a <tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt> with two partitions, each with 3 records.
This expression would return the following IDs:
0, 1, 2, 8589934592 (1L &lt;&lt; 33), 8589934593, 8589934594.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df0</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)])</span><span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="s">&#39;col1&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df0</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">monotonically_increasing_id</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;id&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.month">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">month</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#month"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.month" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Extract the month of a given date as integer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">month</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;month&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(month=4)]</span>
</pre></div>
</div>
</div></blockquote>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.months_between">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">months_between</tt><big>(</big><em>date1</em>, <em>date2</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#months_between"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.months_between" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of months between date1 and date2.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,</span> <span class="s">&#39;1996-10-30&#39;</span><span class="p">)],</span> <span class="p">[</span><span class="s">&#39;t&#39;</span><span class="p">,</span> <span class="s">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">months_between</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;months&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(months=3.9495967...)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.nanvl">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">nanvl</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#nanvl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.nanvl" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns col1 if it is not NaN, or col2 if col1 is NaN.</p>
<p>Both inputs should be floating point columns (DoubleType or FloatType).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">&#39;nan&#39;</span><span class="p">)),</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s">&#39;nan&#39;</span><span class="p">),</span> <span class="mf">2.0</span><span class="p">)],</span> <span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">nanvl</span><span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;r1&quot;</span><span class="p">),</span> <span class="n">nanvl</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;r2&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.next_day">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">next_day</tt><big>(</big><em>date</em>, <em>dayOfWeek</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#next_day"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.next_day" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first date which is later than the value of the date column.</p>
<dl class="docutils">
<dt>Day of the week parameter is case insensitive, and accepts:</dt>
<dd>&#8220;Mon&#8221;, &#8220;Tue&#8221;, &#8220;Wed&#8221;, &#8220;Thu&#8221;, &#8220;Fri&#8221;, &#8220;Sat&#8221;, &#8220;Sun&#8221;.</dd>
</dl>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-07-27&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">next_day</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="s">&#39;Sun&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(date=datetime.date(2015, 8, 2))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.ntile">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">ntile</tt><big>(</big><em>n</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#ntile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.ntile" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the ntile group id (from 1 to <cite>n</cite> inclusive)
in an ordered window partition. For example, if <cite>n</cite> is 4, the first
quarter of the rows will get value 1, the second quarter will get 2,
the third quarter will get 3, and the last quarter will get 4.</p>
<p>This is equivalent to the NTILE function in SQL.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>n</strong> &#8211; an integer</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.percent_rank">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">percent_rank</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.functions.percent_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the relative rank (i.e. percentile) of rows within a window partition.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.posexplode">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">posexplode</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#posexplode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.posexplode" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new row for each element with position in the given array or map.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">intlist</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">mapfield</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;a&quot;</span><span class="p">:</span> <span class="s">&quot;b&quot;</span><span class="p">})])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">posexplode</span><span class="p">(</span><span class="n">eDF</span><span class="o">.</span><span class="n">intlist</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">posexplode</span><span class="p">(</span><span class="n">eDF</span><span class="o">.</span><span class="n">mapfield</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+---+-----+</span>
<span class="go">|pos|key|value|</span>
<span class="go">+---+---+-----+</span>
<span class="go">|  0|  a|    b|</span>
<span class="go">+---+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.1.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.pow">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">pow</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.pow" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of the first argument raised to the power of the second argument.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.quarter">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">quarter</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#quarter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.quarter" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the quarter of a given date as integer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">quarter</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;quarter&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(quarter=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rand">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">rand</tt><big>(</big><em>seed=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#rand"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.rand" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a random column with i.i.d. samples from U[0.0, 1.0].</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.randn">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">randn</tt><big>(</big><em>seed=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#randn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.randn" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a column with i.i.d. samples from the standard normal distribution.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rank">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">rank</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.functions.rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the rank of rows within a window partition.</p>
<p>The difference between rank and denseRank is that denseRank leaves no gaps in ranking
sequence when there are ties. That is, if you were ranking a competition using denseRank
and had three people tie for second place, you would say that all three were in second
place and that the next person came in third.</p>
<p>This is equivalent to the RANK function in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.regexp_extract">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">regexp_extract</tt><big>(</big><em>str</em>, <em>pattern</em>, <em>idx</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#regexp_extract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.regexp_extract" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract a specific(idx) group identified by a java regex, from the specified string column.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;100-200&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;str&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">regexp_extract</span><span class="p">(</span><span class="s">&#39;str&#39;</span><span class="p">,</span> <span class="s">&#39;(\d+)-(\d+)&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=u&#39;100&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.regexp_replace">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">regexp_replace</tt><big>(</big><em>str</em>, <em>pattern</em>, <em>replacement</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#regexp_replace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.regexp_replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace all substrings of the specified string value that match regexp with rep.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;100-200&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;str&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">regexp_replace</span><span class="p">(</span><span class="s">&#39;str&#39;</span><span class="p">,</span> <span class="s">&#39;(\d+)&#39;</span><span class="p">,</span> <span class="s">&#39;--&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;d&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(d=u&#39;-----&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.repeat">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">repeat</tt><big>(</big><em>col</em>, <em>n</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#repeat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.repeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeats a string column n times, and returns it as a new string column.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;ab&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">repeat</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=u&#39;ababab&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.reverse">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">reverse</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.reverse" title="Permalink to this definition">¶</a></dt>
<dd><p>Reverses the string column and returns it as a new string column.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rint">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">rint</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.rint" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the double value that is closest in value to the argument and is equal to a mathematical integer.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.round">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">round</tt><big>(</big><em>col</em>, <em>scale=0</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#round"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.round" title="Permalink to this definition">¶</a></dt>
<dd><p>Round the given value to <cite>scale</cite> decimal places using HALF_UP rounding mode if <cite>scale</cite> &gt;= 0
or at integral part when <cite>scale</cite> &lt; 0.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mf">2.5</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=3.0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.row_number">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">row_number</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.functions.row_number" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns a sequential number starting at 1 within a window partition.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rpad">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">rpad</tt><big>(</big><em>col</em>, <em>len</em>, <em>pad</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#rpad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.rpad" title="Permalink to this definition">¶</a></dt>
<dd><p>Right-pad the string column to width <cite>len</cite> with <cite>pad</cite>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;abcd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">rpad</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s">&#39;#&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=u&#39;abcd##&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rtrim">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">rtrim</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.rtrim" title="Permalink to this definition">¶</a></dt>
<dd><p>Trim the spaces from right end for the specified string value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.second">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">second</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#second"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.second" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the seconds of a given date as integer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08 13:08:15&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">second</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;second&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(second=15)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sha1">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sha1</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#sha1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.sha1" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the hex string result of SHA-1.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;ABC&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sha1</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;hash&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(hash=u&#39;3c01bdbb26f358bab27f267924aa2c9a03fcfdb8&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sha2">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sha2</tt><big>(</big><em>col</em>, <em>numBits</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#sha2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.sha2" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,
and SHA-512). The numBits indicates the desired bit length of the result, which must have a
value of 224, 256, 384, 512, or 0 (which is equivalent to 256).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">digests</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sha2</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">digests</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">Row(s=u&#39;3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">digests</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">Row(s=u&#39;cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961&#39;)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.shiftLeft">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">shiftLeft</tt><big>(</big><em>col</em>, <em>numBits</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#shiftLeft"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.shiftLeft" title="Permalink to this definition">¶</a></dt>
<dd><p>Shift the given value numBits left.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">21</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">shiftLeft</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=42)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.shiftRight">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">shiftRight</tt><big>(</big><em>col</em>, <em>numBits</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#shiftRight"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.shiftRight" title="Permalink to this definition">¶</a></dt>
<dd><p>Shift the given value numBits right.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">42</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">shiftRight</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=21)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.shiftRightUnsigned">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">shiftRightUnsigned</tt><big>(</big><em>col</em>, <em>numBits</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#shiftRightUnsigned"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.shiftRightUnsigned" title="Permalink to this definition">¶</a></dt>
<dd><p>Unsigned shift the given value numBits right.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="o">-</span><span class="mi">42</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">shiftRightUnsigned</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=9223372036854775787)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.signum">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">signum</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.signum" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the signum of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sin">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sin</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.sin" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sine of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sinh">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sinh</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.sinh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the hyperbolic sine of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.size">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">size</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.size" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: returns the length of the array or map stored in the column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>col</strong> &#8211; name of column or expression</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],),([</span><span class="mi">1</span><span class="p">],),([],)],</span> <span class="p">[</span><span class="s">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.skewness">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">skewness</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.skewness" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the skewness of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sort_array">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sort_array</tt><big>(</big><em>col</em>, <em>asc=True</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#sort_array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.sort_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Collection function: sorts the input array for the given column in ascending order.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>col</strong> &#8211; name of column or expression</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],),([</span><span class="mi">1</span><span class="p">],),([],)],</span> <span class="p">[</span><span class="s">&#39;data&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sort_array</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=[1, 2, 3]), Row(r=[1]), Row(r=[])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sort_array</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">asc</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=[3, 2, 1]), Row(r=[1]), Row(r=[])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.soundex">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">soundex</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#soundex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.soundex" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the SoundEx encoding for a string</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&quot;Peters&quot;</span><span class="p">,),(</span><span class="s">&quot;Uhrbach&quot;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">soundex</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;soundex&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(soundex=u&#39;P362&#39;), Row(soundex=u&#39;U612&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.spark_partition_id">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">spark_partition_id</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#spark_partition_id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.spark_partition_id" title="Permalink to this definition">¶</a></dt>
<dd><p>A column for partition ID of the Spark task.</p>
<p>Note that this is indeterministic because it depends on data partitioning and task scheduling.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">spark_partition_id</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;pid&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(pid=0), Row(pid=0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.split">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">split</tt><big>(</big><em>str</em>, <em>pattern</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#split"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits str around pattern (pattern is a regular expression).</p>
<p>NOTE: pattern is a string represent the regular expression.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;ab12cd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">split</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="s">&#39;[0-9]+&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=[u&#39;ab&#39;, u&#39;cd&#39;])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sqrt">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sqrt</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the square root of the specified float value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.stddev">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">stddev</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.stddev" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the unbiased sample standard deviation of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.stddev_pop">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">stddev_pop</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.stddev_pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns population standard deviation of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.stddev_samp">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">stddev_samp</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.stddev_samp" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the unbiased sample standard deviation of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.struct">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">struct</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#struct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.struct" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new struct column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string) or list of <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expressions</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">struct</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;struct&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(struct=Row(age=2, name=u&#39;Alice&#39;)), Row(struct=Row(age=5, name=u&#39;Bob&#39;))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">struct</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">])</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;struct&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(struct=Row(age=2, name=u&#39;Alice&#39;)), Row(struct=Row(age=5, name=u&#39;Bob&#39;))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.substring">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">substring</tt><big>(</big><em>str</em>, <em>pos</em>, <em>len</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#substring"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.substring" title="Permalink to this definition">¶</a></dt>
<dd><p>Substring starts at <cite>pos</cite> and is of length <cite>len</cite> when str is String type or
returns the slice of byte array that starts at <cite>pos</cite> in byte and is of length <cite>len</cite>
when str is Binary type</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;abcd&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">substring</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=u&#39;ab&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.substring_index">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">substring_index</tt><big>(</big><em>str</em>, <em>delim</em>, <em>count</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#substring_index"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.substring_index" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the substring from string str before count occurrences of the delimiter delim.
If count is positive, everything the left of the final delimiter (counting from left) is
returned. If count is negative, every to the right of the final delimiter (counting from the
right) is returned. substring_index performs a case-sensitive match when searching for delim.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;a.b.c.d&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">substring_index</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="s">&#39;.&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=u&#39;a.b&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">substring_index</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="s">&#39;.&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(s=u&#39;b.c.d&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sum">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sum</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the sum of all values in the expression.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sumDistinct">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sumDistinct</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.sumDistinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the sum of distinct values in the expression.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.tan">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">tan</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.tan" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the tangent of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.tanh">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">tanh</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the hyperbolic tangent of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.toDegrees">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">toDegrees</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.toDegrees" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an angle measured in radians to an approximately equivalent angle measured in degrees.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.toRadians">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">toRadians</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.toRadians" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an angle measured in degrees to an approximately equivalent angle measured in radians.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.to_date">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">to_date</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#to_date"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.to_date" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the column of StringType or TimestampType into DateType.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;t&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_date</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;date&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(date=datetime.date(1997, 2, 28))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.to_utc_timestamp">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">to_utc_timestamp</tt><big>(</big><em>timestamp</em>, <em>tz</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#to_utc_timestamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.to_utc_timestamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Assumes given timestamp is in given timezone and converts to UTC.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;1997-02-28 10:30:00&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;t&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">to_utc_timestamp</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="s">&quot;PST&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;t&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(t=datetime.datetime(1997, 2, 28, 18, 30))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.translate">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">translate</tt><big>(</big><em>srcCol</em>, <em>matching</em>, <em>replace</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#translate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.translate" title="Permalink to this definition">¶</a></dt>
<dd><p>A function translate any character in the <cite>srcCol</cite> by a character in <cite>matching</cite>.
The characters in <cite>replace</cite> is corresponding to the characters in <cite>matching</cite>.
The translate will happen when any character in the string matching with the character
in the <cite>matching</cite>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;translate&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">translate</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">,</span> <span class="s">&quot;rnlt&quot;</span><span class="p">,</span> <span class="s">&quot;123&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;r&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(r=u&#39;1a2s3ae&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.trim">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">trim</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.trim" title="Permalink to this definition">¶</a></dt>
<dd><p>Trim the spaces from both ends for the specified string column.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.trunc">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">trunc</tt><big>(</big><em>date</em>, <em>format</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#trunc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.trunc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns date truncated to the unit specified by the format.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>format</strong> &#8211; &#8216;year&#8217;, &#8216;YYYY&#8217;, &#8216;yy&#8217; or &#8216;month&#8217;, &#8216;mon&#8217;, &#8216;mm&#8217;</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;1997-02-28&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;d&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">trunc</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="s">&#39;year&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;year&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(year=datetime.date(1997, 1, 1))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">trunc</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="s">&#39;mon&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;month&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(month=datetime.date(1997, 2, 1))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.udf">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">udf</tt><big>(</big><em>f</em>, <em>returnType=StringType</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#udf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.udf" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expression representing a user defined function (UDF).
Note that the user-defined functions must be deterministic. Due to optimization,
duplicate invocations may be eliminated or the function may even be invoked more times than
it is present in the query.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slen</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">slen</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;slen&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(slen=5), Row(slen=3)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.unbase64">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">unbase64</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.unbase64" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes a BASE64 encoded string column and returns it as a binary column.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.unhex">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">unhex</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#unhex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.unhex" title="Permalink to this definition">¶</a></dt>
<dd><p>Inverse of hex. Interprets each pair of characters as a hexadecimal number
and converts to the byte representation of number.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;414243&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">unhex</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(unhex(a)=bytearray(b&#39;ABC&#39;))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.unix_timestamp">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">unix_timestamp</tt><big>(</big><em>timestamp=None</em>, <em>format='yyyy-MM-dd HH:mm:ss'</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#unix_timestamp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.unix_timestamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert time string with given pattern (&#8216;yyyy-MM-dd HH:mm:ss&#8217;, by default)
to Unix time stamp (in seconds), using the default timezone and the default
locale, return null if fail.</p>
<p>if <cite>timestamp</cite> is None, then it returns current timestamp.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.upper">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">upper</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.upper" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a string column to upper case.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.var_pop">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">var_pop</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.var_pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the population variance of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.var_samp">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">var_samp</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.var_samp" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the unbiased variance of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.variance">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">variance</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.variance" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the population variance of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.6.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.weekofyear">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">weekofyear</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#weekofyear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.weekofyear" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the week number of a given date as integer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">weekofyear</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;week&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(week=15)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.when">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">when</tt><big>(</big><em>condition</em>, <em>value</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#when"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.when" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions.
If <tt class="xref py py-func docutils literal"><span class="pre">Column.otherwise()</span></tt> is not invoked, None is returned for unmatched conditions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>condition</strong> &#8211; a boolean <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expression.</li>
<li><strong>value</strong> &#8211; a literal value, or a <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expression.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;age&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=3), Row(age=4)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=3), Row(age=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.window">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">window</tt><big>(</big><em>timeColumn</em>, <em>windowDuration</em>, <em>slideDuration=None</em>, <em>startTime=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#window"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.window" title="Permalink to this definition">¶</a></dt>
<dd><p>Bucketize rows into one or more time windows given a timestamp specifying column. Window
starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
[12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
the order of months are not supported.</p>
<p>The time column must be of TimestampType.</p>
<p>Durations are provided as strings, e.g. &#8216;1 second&#8217;, &#8216;1 day 12 hours&#8217;, &#8216;2 minutes&#8217;. Valid
interval strings are &#8216;week&#8217;, &#8216;day&#8217;, &#8216;hour&#8217;, &#8216;minute&#8217;, &#8216;second&#8217;, &#8216;millisecond&#8217;, &#8216;microsecond&#8217;.
If the <cite>slideDuration</cite> is not provided, the windows will be tumbling windows.</p>
<p>The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start
window intervals. For example, in order to have hourly tumbling windows that start 15 minutes
past the hour, e.g. 12:15-13:15, 13:15-14:15... provide <cite>startTime</cite> as <cite>15 minutes</cite>.</p>
<p>The output column will be a struct called &#8216;window&#8217; by default with the nested columns &#8216;start&#8217;
and &#8216;end&#8217;, where &#8216;start&#8217; and &#8216;end&#8217; will be of <cite>TimestampType</cite>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&quot;2016-03-11 09:00:07&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s">&quot;date&quot;</span><span class="p">,</span> <span class="s">&quot;val&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">window</span><span class="p">(</span><span class="s">&quot;date&quot;</span><span class="p">,</span> <span class="s">&quot;5 seconds&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="s">&quot;val&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;sum&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">window</span><span class="o">.</span><span class="n">start</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s">&quot;string&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;start&quot;</span><span class="p">),</span>
<span class="gp">... </span>         <span class="n">w</span><span class="o">.</span><span class="n">window</span><span class="o">.</span><span class="n">end</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s">&quot;string&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;end&quot;</span><span class="p">),</span> <span class="s">&quot;sum&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(start=u&#39;2016-03-11 09:00:05&#39;, end=u&#39;2016-03-11 09:00:10&#39;, sum=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.year">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">year</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#year"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.year" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the year of a given date as integer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s">&#39;2015-04-08&#39;</span><span class="p">,)],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">year</span><span class="p">(</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;year&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(year=2015)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.5.</span></p>
</div>
</dd></dl>

</div>
<div class="section" id="module-pyspark.sql.streaming">
<span id="pyspark-sql-streaming-module"></span><h2>pyspark.sql.streaming module<a class="headerlink" href="#module-pyspark.sql.streaming" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pyspark.sql.streaming.StreamingQuery">
<em class="property">class </em><tt class="descclassname">pyspark.sql.streaming.</tt><tt class="descname">StreamingQuery</tt><big>(</big><em>jsq</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery" title="Permalink to this definition">¶</a></dt>
<dd><p>A handle to a query that is executing continuously in the background as new data arrives.
All these methods are thread-safe.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.awaitTermination">
<tt class="descname">awaitTermination</tt><big>(</big><em>timeout=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.awaitTermination"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.awaitTermination" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for the termination of <cite>this</cite> query, either by <tt class="xref py py-func docutils literal"><span class="pre">query.stop()</span></tt> or by an
exception. If the query has terminated with an exception, then the exception will be thrown.
If <cite>timeout</cite> is set, it returns whether the query has terminated or not within the
<cite>timeout</cite> seconds.</p>
<p>If the query has terminated, then all subsequent calls to this method will either return
immediately (if the query was terminated by <a class="reference internal" href="#pyspark.sql.streaming.StreamingQuery.stop" title="pyspark.sql.streaming.StreamingQuery.stop"><tt class="xref py py-func docutils literal"><span class="pre">stop()</span></tt></a>), or throw the exception
immediately (if the query has terminated with exception).</p>
<p>throws <tt class="xref py py-class docutils literal"><span class="pre">StreamingQueryException</span></tt>, if <cite>this</cite> query has terminated with an exception</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.streaming.StreamingQuery.id">
<tt class="descname">id</tt><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.id" title="Permalink to this definition">¶</a></dt>
<dd><p>The id of the streaming query. This id is unique across all queries that have been
started in the current process.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.streaming.StreamingQuery.isActive">
<tt class="descname">isActive</tt><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.isActive"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.isActive" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether this streaming query is currently active or not.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.streaming.StreamingQuery.name">
<tt class="descname">name</tt><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.name" title="Permalink to this definition">¶</a></dt>
<dd><p>The name of the streaming query. This name is unique across all active queries.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.processAllAvailable">
<tt class="descname">processAllAvailable</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.processAllAvailable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.processAllAvailable" title="Permalink to this definition">¶</a></dt>
<dd><p>Blocks until all available data in the source has been processed and committed to the
sink. This method is intended for testing. Note that in the case of continually arriving
data, this method may block forever. Additionally, this method is only guaranteed to block
until data that has been synchronously appended data to a stream source prior to invocation.
(i.e. <cite>getOffset</cite> must immediately reflect the addition).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQuery.stop">
<tt class="descname">stop</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQuery.stop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQuery.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stop this streaming query.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.streaming.StreamingQueryManager">
<em class="property">class </em><tt class="descclassname">pyspark.sql.streaming.</tt><tt class="descname">StreamingQueryManager</tt><big>(</big><em>jsqm</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQueryManager"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQueryManager" title="Permalink to this definition">¶</a></dt>
<dd><p>A class to manage all the <a class="reference internal" href="#pyspark.sql.streaming.StreamingQuery" title="pyspark.sql.streaming.StreamingQuery"><tt class="xref py py-class docutils literal"><span class="pre">StreamingQuery</span></tt></a> StreamingQueries active.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
<dl class="attribute">
<dt id="pyspark.sql.streaming.StreamingQueryManager.active">
<tt class="descname">active</tt><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQueryManager.active"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQueryManager.active" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of active queries associated with this SQLContext</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&#39;memory&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="s">&#39;this_query&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqm</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">streams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># get the list of active streaming queries</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">q</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">sqm</span><span class="o">.</span><span class="n">active</span><span class="p">]</span>
<span class="go">[u&#39;this_query&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination">
<tt class="descname">awaitAnyTermination</tt><big>(</big><em>timeout=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQueryManager.awaitAnyTermination"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination" title="Permalink to this definition">¶</a></dt>
<dd><p>Wait until any of the queries on the associated SQLContext has terminated since the
creation of the context, or since <a class="reference internal" href="#pyspark.sql.streaming.StreamingQueryManager.resetTerminated" title="pyspark.sql.streaming.StreamingQueryManager.resetTerminated"><tt class="xref py py-func docutils literal"><span class="pre">resetTerminated()</span></tt></a> was called. If any query was
terminated with an exception, then the exception will be thrown.
If <cite>timeout</cite> is set, it returns whether the query has terminated or not within the
<cite>timeout</cite> seconds.</p>
<p>If a query has terminated, then subsequent calls to <a class="reference internal" href="#pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination" title="pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination"><tt class="xref py py-func docutils literal"><span class="pre">awaitAnyTermination()</span></tt></a> will
either return immediately (if the query was terminated by <tt class="xref py py-func docutils literal"><span class="pre">query.stop()</span></tt>),
or throw the exception immediately (if the query was terminated with exception). Use
<a class="reference internal" href="#pyspark.sql.streaming.StreamingQueryManager.resetTerminated" title="pyspark.sql.streaming.StreamingQueryManager.resetTerminated"><tt class="xref py py-func docutils literal"><span class="pre">resetTerminated()</span></tt></a> to clear past terminations and wait for new terminations.</p>
<p>In the case where multiple queries have terminated since <tt class="xref py py-func docutils literal"><span class="pre">resetTermination()</span></tt>
was called, if any query has terminated with exception, then <a class="reference internal" href="#pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination" title="pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination"><tt class="xref py py-func docutils literal"><span class="pre">awaitAnyTermination()</span></tt></a>
will throw any of the exception. For correctly documenting exceptions across multiple
queries, users need to stop all of them after any of them terminates with exception, and
then check the <cite>query.exception()</cite> for each query.</p>
<p>throws <tt class="xref py py-class docutils literal"><span class="pre">StreamingQueryException</span></tt>, if <cite>this</cite> query has terminated with an exception</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQueryManager.get">
<tt class="descname">get</tt><big>(</big><em>id</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQueryManager.get"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQueryManager.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an active query from this SQLContext or throws exception if an active query
with this name doesn&#8217;t exist.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&#39;memory&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="s">&#39;this_query&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">name</span>
<span class="go">u&#39;this_query&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">sq</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">isActive</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">sq</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">isActive</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.StreamingQueryManager.resetTerminated">
<tt class="descname">resetTerminated</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#StreamingQueryManager.resetTerminated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.StreamingQueryManager.resetTerminated" title="Permalink to this definition">¶</a></dt>
<dd><p>Forget about past terminated queries so that <a class="reference internal" href="#pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination" title="pyspark.sql.streaming.StreamingQueryManager.awaitAnyTermination"><tt class="xref py py-func docutils literal"><span class="pre">awaitAnyTermination()</span></tt></a> can be used
again to wait for new terminations.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">resetTerminated</span><span class="p">()</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.streaming.DataStreamReader">
<em class="property">class </em><tt class="descclassname">pyspark.sql.streaming.</tt><tt class="descname">DataStreamReader</tt><big>(</big><em>spark</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface used to load a streaming <tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt> from external storage systems
(e.g. file systems, key-value stores, etc). Use <tt class="xref py py-func docutils literal"><span class="pre">spark.readStream()</span></tt>
to access this.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.csv">
<tt class="descname">csv</tt><big>(</big><em>path</em>, <em>schema=None</em>, <em>sep=None</em>, <em>encoding=None</em>, <em>quote=None</em>, <em>escape=None</em>, <em>comment=None</em>, <em>header=None</em>, <em>inferSchema=None</em>, <em>ignoreLeadingWhiteSpace=None</em>, <em>ignoreTrailingWhiteSpace=None</em>, <em>nullValue=None</em>, <em>nanValue=None</em>, <em>positiveInf=None</em>, <em>negativeInf=None</em>, <em>dateFormat=None</em>, <em>maxColumns=None</em>, <em>maxCharsPerColumn=None</em>, <em>maxMalformedLogPerPartition=None</em>, <em>mode=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.csv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.csv" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a CSV file stream and returns the result as a  <tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt>.</p>
<p>This function will go through the input once to determine the input schema if
<tt class="docutils literal"><span class="pre">inferSchema</span></tt> is enabled. To avoid going through the entire data once, disable
<tt class="docutils literal"><span class="pre">inferSchema</span></tt> option or specify the schema explicitly using <tt class="docutils literal"><span class="pre">schema</span></tt>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; string, or list of strings, for input path(s).</li>
<li><strong>schema</strong> &#8211; an optional <tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt> for the input schema.</li>
<li><strong>sep</strong> &#8211; sets the single character as a separator for each field and value.
If None is set, it uses the default value, <tt class="docutils literal"><span class="pre">,</span></tt>.</li>
<li><strong>encoding</strong> &#8211; decodes the CSV files by the given encoding type. If None is set,
it uses the default value, <tt class="docutils literal"><span class="pre">UTF-8</span></tt>.</li>
<li><strong>quote</strong> &#8211; sets the single character used for escaping quoted values where the
separator can be part of the value. If None is set, it uses the default
value, <tt class="docutils literal"><span class="pre">&quot;</span></tt>. If you would like to turn off quotations, you need to set an
empty string.</li>
<li><strong>escape</strong> &#8211; sets the single character used for escaping quotes inside an already
quoted value. If None is set, it uses the default value, <tt class="docutils literal"><span class="pre">\</span></tt>.</li>
<li><strong>comment</strong> &#8211; sets the single character used for skipping lines beginning with this
character. By default (None), it is disabled.</li>
<li><strong>header</strong> &#8211; uses the first line as names of columns. If None is set, it uses the
default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>inferSchema</strong> &#8211; infers the input schema automatically from data. It requires one extra
pass over the data. If None is set, it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>ignoreLeadingWhiteSpace</strong> &#8211; defines whether or not leading whitespaces from values
being read should be skipped. If None is set, it uses
the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>ignoreTrailingWhiteSpace</strong> &#8211; defines whether or not trailing whitespaces from values
being read should be skipped. If None is set, it uses
the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>nullValue</strong> &#8211; sets the string representation of a null value. If None is set, it uses
the default value, empty string.</li>
<li><strong>nanValue</strong> &#8211; sets the string representation of a non-number value. If None is set, it
uses the default value, <tt class="docutils literal"><span class="pre">NaN</span></tt>.</li>
<li><strong>positiveInf</strong> &#8211; sets the string representation of a positive infinity value. If None
is set, it uses the default value, <tt class="docutils literal"><span class="pre">Inf</span></tt>.</li>
<li><strong>negativeInf</strong> &#8211; sets the string representation of a negative infinity value. If None
is set, it uses the default value, <tt class="docutils literal"><span class="pre">Inf</span></tt>.</li>
<li><strong>dateFormat</strong> &#8211; sets the string that indicates a date format. Custom date formats
follow the formats at <tt class="docutils literal"><span class="pre">java.text.SimpleDateFormat</span></tt>. This
applies to both date type and timestamp type. By default, it is None
which means trying to parse times and date by
<tt class="docutils literal"><span class="pre">java.sql.Timestamp.valueOf()</span></tt> and <tt class="docutils literal"><span class="pre">java.sql.Date.valueOf()</span></tt>.</li>
<li><strong>maxColumns</strong> &#8211; defines a hard limit of how many columns a record can have. If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">20480</span></tt>.</li>
<li><strong>maxCharsPerColumn</strong> &#8211; defines the maximum number of characters allowed for any given
value being read. If None is set, it uses the default value,
<tt class="docutils literal"><span class="pre">1000000</span></tt>.</li>
<li><strong>mode</strong> &#8211; <dl class="docutils">
<dt>allows a mode for dealing with corrupt records during parsing. If None is</dt>
<dd>set, it uses the default value, <tt class="docutils literal"><span class="pre">PERMISSIVE</span></tt>.</dd>
</dl>
<ul>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">PERMISSIVE</span></tt> <span class="classifier-delimiter">:</span> <span class="classifier">sets other fields to <tt class="docutils literal"><span class="pre">null</span></tt> when it meets a corrupted record.</span></dt>
<dd>When a schema is set by user, it sets <tt class="docutils literal"><span class="pre">null</span></tt> for extra fields.</dd>
</dl>
</li>
<li><tt class="docutils literal"><span class="pre">DROPMALFORMED</span></tt> : ignores the whole corrupted records.</li>
<li><tt class="docutils literal"><span class="pre">FAILFAST</span></tt> : throws an exception when it meets corrupted records.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">csv_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="n">schema</span> <span class="o">=</span> <span class="n">sdf_schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csv_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csv_sdf</span><span class="o">.</span><span class="n">schema</span> <span class="o">==</span> <span class="n">sdf_schema</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.format">
<tt class="descname">format</tt><big>(</big><em>source</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.format"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the input data source format.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>source</strong> &#8211; string, name of the data source, e.g. &#8216;json&#8217;, &#8216;parquet&#8217;.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;text&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.json">
<tt class="descname">json</tt><big>(</big><em>path</em>, <em>schema=None</em>, <em>primitivesAsString=None</em>, <em>prefersDecimal=None</em>, <em>allowComments=None</em>, <em>allowUnquotedFieldNames=None</em>, <em>allowSingleQuotes=None</em>, <em>allowNumericLeadingZero=None</em>, <em>allowBackslashEscapingAnyCharacter=None</em>, <em>mode=None</em>, <em>columnNameOfCorruptRecord=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.json" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a JSON file stream (one object per line) and returns a :class`DataFrame`.</p>
<p>If the <tt class="docutils literal"><span class="pre">schema</span></tt> parameter is not specified, this function goes
through the input once to determine the input schema.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; string represents path to the JSON dataset,
or RDD of Strings storing JSON objects.</li>
<li><strong>schema</strong> &#8211; an optional <tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt> for the input schema.</li>
<li><strong>primitivesAsString</strong> &#8211; infers all primitive values as a string type. If None is set,
it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>prefersDecimal</strong> &#8211; infers all floating-point values as a decimal type. If the values
do not fit in decimal, then it infers them as doubles. If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>allowComments</strong> &#8211; ignores Java/C++ style comment in JSON records. If None is set,
it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>allowUnquotedFieldNames</strong> &#8211; allows unquoted JSON field names. If None is set,
it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>allowSingleQuotes</strong> &#8211; allows single quotes in addition to double quotes. If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">true</span></tt>.</li>
<li><strong>allowNumericLeadingZero</strong> &#8211; allows leading zeros in numbers (e.g. 00012). If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>allowBackslashEscapingAnyCharacter</strong> &#8211; allows accepting quoting of all character
using backslash quoting mechanism. If None is
set, it uses the default value, <tt class="docutils literal"><span class="pre">false</span></tt>.</li>
<li><strong>mode</strong> &#8211; <dl class="docutils">
<dt>allows a mode for dealing with corrupt records during parsing. If None is</dt>
<dd>set, it uses the default value, <tt class="docutils literal"><span class="pre">PERMISSIVE</span></tt>.</dd>
</dl>
<ul>
<li><tt class="docutils literal"><span class="pre">PERMISSIVE</span></tt> : sets other fields to <tt class="docutils literal"><span class="pre">null</span></tt> when it meets a corrupted                   record and puts the malformed string into a new field configured by                  <tt class="docutils literal"><span class="pre">columnNameOfCorruptRecord</span></tt>. When a schema is set by user, it sets                  <tt class="docutils literal"><span class="pre">null</span></tt> for extra fields.</li>
<li><tt class="docutils literal"><span class="pre">DROPMALFORMED</span></tt> : ignores the whole corrupted records.</li>
<li><tt class="docutils literal"><span class="pre">FAILFAST</span></tt> : throws an exception when it meets corrupted records.</li>
</ul>
</li>
<li><strong>columnNameOfCorruptRecord</strong> &#8211; allows renaming the new field having malformed string
created by <tt class="docutils literal"><span class="pre">PERMISSIVE</span></tt> mode. This overrides
<tt class="docutils literal"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></tt>. If None is set,
it uses the value specified in
<tt class="docutils literal"><span class="pre">spark.sql.columnNameOfCorruptRecord</span></tt>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="n">schema</span> <span class="o">=</span> <span class="n">sdf_schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span><span class="o">.</span><span class="n">schema</span> <span class="o">==</span> <span class="n">sdf_schema</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.load">
<tt class="descname">load</tt><big>(</big><em>path=None</em>, <em>format=None</em>, <em>schema=None</em>, <em>**options</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.load"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a data stream from a data source and returns it as a :class`DataFrame`.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; optional string for file-system backed data sources.</li>
<li><strong>format</strong> &#8211; optional string for format of the data source. Default to &#8216;parquet&#8217;.</li>
<li><strong>schema</strong> &#8211; optional <tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt> for the input schema.</li>
<li><strong>options</strong> &#8211; all other string options</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;json&quot;</span><span class="p">)</span>                                       <span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sdf_schema</span><span class="p">)</span>                                       <span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json_sdf</span><span class="o">.</span><span class="n">schema</span> <span class="o">==</span> <span class="n">sdf_schema</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.option">
<tt class="descname">option</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.option"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.option" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds an input option for the underlying data source.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;x&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.options">
<tt class="descname">options</tt><big>(</big><em>**options</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.options"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.options" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds input options for the underlying data source.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">options</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">&quot;1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.parquet">
<tt class="descname">parquet</tt><big>(</big><em>path</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.parquet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.parquet" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a Parquet file stream, returning the result as a <tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt>.</p>
<dl class="docutils">
<dt>You can set the following Parquet-specific option(s) for reading Parquet files:</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">mergeSchema</span></tt>: sets whether we should merge schemas collected from all                 Parquet part-files. This will override <tt class="docutils literal"><span class="pre">spark.sql.parquet.mergeSchema</span></tt>.                 The default value is specified in <tt class="docutils literal"><span class="pre">spark.sql.parquet.mergeSchema</span></tt>.</li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">parquet_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sdf_schema</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parquet_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parquet_sdf</span><span class="o">.</span><span class="n">schema</span> <span class="o">==</span> <span class="n">sdf_schema</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.schema">
<tt class="descname">schema</tt><big>(</big><em>schema</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.schema"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.schema" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the input schema.</p>
<p>Some data sources (e.g. JSON) can infer the input schema automatically from data.
By specifying the schema here, the underlying data source can skip the schema
inference step, and thus speed up data loading.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>schema</strong> &#8211; a StructType object</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">sdf_schema</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamReader.text">
<tt class="descname">text</tt><big>(</big><em>path</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamReader.text"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamReader.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a text file stream and returns a <tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt> whose schema starts with a
string column named &#8220;value&#8221;, and followed by partitioned columns if there
are any.</p>
<p>Each line in the text file is a new row in the resulting DataFrame.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>paths</strong> &#8211; string, or list of strings, for input path(s).</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">text_sdf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_sdf</span><span class="o">.</span><span class="n">isStreaming</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s">&quot;value&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">text_sdf</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.streaming.DataStreamWriter">
<em class="property">class </em><tt class="descclassname">pyspark.sql.streaming.</tt><tt class="descname">DataStreamWriter</tt><big>(</big><em>df</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface used to write a streaming <tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt> to external storage systems
(e.g. file systems, key-value stores, etc). Use <tt class="xref py py-func docutils literal"><span class="pre">DataFrame.writeStream()</span></tt>
to access this.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.format">
<tt class="descname">format</tt><big>(</big><em>source</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.format"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the underlying output data source.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>source</strong> &#8211; string, name of the data source, e.g. &#8216;json&#8217;, &#8216;parquet&#8217;.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&#39;json&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.option">
<tt class="descname">option</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.option"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.option" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds an output option for the underlying data source.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.options">
<tt class="descname">options</tt><big>(</big><em>**options</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.options"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.options" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds output options for the underlying data source.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.outputMode">
<tt class="descname">outputMode</tt><big>(</big><em>outputMode</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.outputMode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.outputMode" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.</p>
<blockquote>
<div><p>Options include:</p>
<ul>
<li><dl class="first docutils">
<dt><cite>append</cite>:Only the new rows in the streaming DataFrame/Dataset will be written to</dt>
<dd><p class="first last">the sink</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>complete</cite>:All the rows in the streaming DataFrame/Dataset will be written to the sink</dt>
<dd><p class="first last">every time these is some updates</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Experimental.</p>
<div class="last highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s">&#39;append&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.partitionBy">
<tt class="descname">partitionBy</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.partitionBy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Partitions the output by the given columns on the file system.</p>
<p>If specified, the output is laid out on the file system similar
to Hive&#8217;s partitioning scheme.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; name of columns</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.queryName">
<tt class="descname">queryName</tt><big>(</big><em>queryName</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.queryName"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.queryName" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the name of the <a class="reference internal" href="#pyspark.sql.streaming.StreamingQuery" title="pyspark.sql.streaming.StreamingQuery"><tt class="xref py py-class docutils literal"><span class="pre">StreamingQuery</span></tt></a> that can be started with
<a class="reference internal" href="#pyspark.sql.streaming.DataStreamWriter.start" title="pyspark.sql.streaming.DataStreamWriter.start"><tt class="xref py py-func docutils literal"><span class="pre">start()</span></tt></a>. This name must be unique among all the currently active queries
in the associated SparkSession.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>queryName</strong> &#8211; unique name for the query</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="s">&#39;streaming_query&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.start">
<tt class="descname">start</tt><big>(</big><em>path=None</em>, <em>format=None</em>, <em>partitionBy=None</em>, <em>queryName=None</em>, <em>**options</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.start"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.start" title="Permalink to this definition">¶</a></dt>
<dd><p>Streams the contents of the <tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt> to a data source.</p>
<p>The data source is specified by the <tt class="docutils literal"><span class="pre">format</span></tt> and a set of <tt class="docutils literal"><span class="pre">options</span></tt>.
If <tt class="docutils literal"><span class="pre">format</span></tt> is not specified, the default data source configured by
<tt class="docutils literal"><span class="pre">spark.sql.sources.default</span></tt> will be used.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; the path in a Hadoop supported file system</li>
<li><strong>format</strong> &#8211; <p>the format used to save</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
<li><strong>partitionBy</strong> &#8211; names of partitioning columns</li>
<li><strong>queryName</strong> &#8211; unique name for the query</li>
<li><strong>options</strong> &#8211; All other string options. You may want to provide a <cite>checkpointLocation</cite>
for most streams, however it is not required for a <cite>memory</cite> stream.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&#39;memory&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="s">&#39;this_query&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">isActive</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">name</span>
<span class="go">u&#39;this_query&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">isActive</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">processingTime</span><span class="o">=</span><span class="s">&#39;5 seconds&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">queryName</span><span class="o">=</span><span class="s">&#39;that_query&#39;</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s">&#39;memory&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">name</span>
<span class="go">u&#39;that_query&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">isActive</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sq</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.streaming.DataStreamWriter.trigger">
<tt class="descname">trigger</tt><big>(</big><em>*args</em>, <em>**kwargs</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/streaming.html#DataStreamWriter.trigger"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.trigger" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the trigger for the stream query. If this is not set it will run the query as fast
as possible, which is equivalent to setting the trigger to <tt class="docutils literal"><span class="pre">processingTime='0</span> <span class="pre">seconds'</span></tt>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>processingTime</strong> &#8211; a processing time interval as a string, e.g. &#8216;5 seconds&#8217;, &#8216;1 minute&#8217;.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># trigger the query for execution every 5 seconds</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">processingTime</span><span class="o">=</span><span class="s">&#39;5 seconds&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/spark-logo-hd.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">pyspark.sql module</a><ul>
<li><a class="reference internal" href="#module-pyspark.sql">Module Context</a></li>
<li><a class="reference internal" href="#module-pyspark.sql.types">pyspark.sql.types module</a></li>
<li><a class="reference internal" href="#module-pyspark.sql.functions">pyspark.sql.functions module</a></li>
<li><a class="reference internal" href="#module-pyspark.sql.streaming">pyspark.sql.streaming module</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="pyspark.mllib.html"
                        title="previous chapter">pyspark.mllib package</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="pyspark.streaming.html"
                        title="next chapter">pyspark.streaming module</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/pyspark.sql.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="pyspark.streaming.html" title="pyspark.streaming module"
             >next</a></li>
        <li class="right" >
          <a href="pyspark.mllib.html" title="pyspark.mllib package"
             >previous</a> |</li>
    
        <li><a href="index.html">PySpark 2.0.0 documentation</a> &raquo;</li>
 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>