
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>pyspark.sql.pandas.functions &#8212; PySpark 3.0.1 documentation</title>
    <link rel="stylesheet" href="../../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pyspark.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script type="text/javascript" src="../../../../_static/copybutton.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../../_static/pyspark.js"></script>
    <link rel="search" title="Search" href="../../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
    
        <li class="nav-item nav-item-0"><a href="../../../../index.html">PySpark 3.0.1 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="../../../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for pyspark.sql.pandas.functions</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>

<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">since</span>
<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="kn">import</span> <span class="n">PythonEvalType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.pandas.typehints</span> <span class="kn">import</span> <span class="n">infer_eval_type</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pandas_version</span><span class="p">,</span> <span class="n">require_minimum_pyarrow_version</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">DataType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.udf</span> <span class="kn">import</span> <span class="n">_create_udf</span>
<span class="kn">from</span> <span class="nn">pyspark.util</span> <span class="kn">import</span> <span class="n">_get_argspec</span>


<div class="viewcode-block" id="PandasUDFType"><a class="viewcode-back" href="../../../../pyspark.sql.html#pyspark.sql.functions.PandasUDFType">[docs]</a><span class="k">class</span> <span class="nc">PandasUDFType</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pandas UDF Types. See :meth:`pyspark.sql.functions.pandas_udf`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">SCALAR</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span>

    <span class="n">SCALAR_ITER</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_ITER_UDF</span>

    <span class="n">GROUPED_MAP</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span>

    <span class="n">GROUPED_AGG</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_AGG_PANDAS_UDF</span></div>


<div class="viewcode-block" id="pandas_udf"><a class="viewcode-back" href="../../../../pyspark.sql.html#pyspark.sql.functions.pandas_udf">[docs]</a><span class="nd">@since</span><span class="p">(</span><span class="mf">2.3</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">pandas_udf</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">functionType</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a pandas user defined function (a.k.a. vectorized user defined function).</span>

<span class="sd">    Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer</span>
<span class="sd">    data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF</span>
<span class="sd">    is defined using the `pandas_udf` as a decorator or to wrap the function, and no</span>
<span class="sd">    additional configuration is required. A Pandas UDF behaves as a regular PySpark function</span>
<span class="sd">    API in general.</span>

<span class="sd">    :param f: user-defined function. A python function if used as a standalone function</span>
<span class="sd">    :param returnType: the return type of the user-defined function. The value can be either a</span>
<span class="sd">        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>
<span class="sd">    :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.</span>
<span class="sd">        Default: SCALAR.</span>

<span class="sd">        .. note:: This parameter exists for compatibility. Using Python type hints is encouraged.</span>

<span class="sd">    In order to use this API, customarily the below are imported:</span>

<span class="sd">    &gt;&gt;&gt; import pandas as pd</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf</span>

<span class="sd">    From Spark 3.0 with Python 3.6+, `Python type hints &lt;https://www.python.org/dev/peps/pep-0484&gt;`_</span>
<span class="sd">    detect the function types as below:</span>

<span class="sd">    &gt;&gt;&gt; @pandas_udf(IntegerType())</span>
<span class="sd">    ... def slen(s: pd.Series) -&gt; pd.Series:</span>
<span class="sd">    ...     return s.str.len()</span>

<span class="sd">    Prior to Spark 3.0, the pandas UDF used `functionType` to decide the execution type as below:</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql.functions import PandasUDFType</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.sql.types import IntegerType</span>
<span class="sd">    &gt;&gt;&gt; @pandas_udf(IntegerType(), PandasUDFType.SCALAR)</span>
<span class="sd">    ... def slen(s):</span>
<span class="sd">    ...     return s.str.len()</span>

<span class="sd">    It is preferred to specify type hints for the pandas UDF instead of specifying pandas UDF</span>
<span class="sd">    type via `functionType` which will be deprecated in the future releases.</span>

<span class="sd">    Note that the type hint should use `pandas.Series` in all cases but there is one variant</span>
<span class="sd">    that `pandas.DataFrame` should be used for its input or output type hint instead when the input</span>
<span class="sd">    or output column is of :class:`pyspark.sql.types.StructType`. The following example shows</span>
<span class="sd">    a Pandas UDF which takes long column, string column and struct column, and outputs a struct</span>
<span class="sd">    column. It requires the function to specify the type hints of `pandas.Series` and</span>
<span class="sd">    `pandas.DataFrame` as below:</span>

<span class="sd">    &gt;&gt;&gt; @pandas_udf(&quot;col1 string, col2 long&quot;)</span>
<span class="sd">    &gt;&gt;&gt; def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -&gt; pd.DataFrame:</span>
<span class="sd">    ...     s3[&#39;col2&#39;] = s1 + s2.str.len()</span>
<span class="sd">    ...     return s3</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; # Create a Spark DataFrame that has three columns including a sturct column.</span>
<span class="sd">    ... df = spark.createDataFrame(</span>
<span class="sd">    ...     [[1, &quot;a string&quot;, (&quot;a nested string&quot;,)]],</span>
<span class="sd">    ...     &quot;long_col long, string_col string, struct_col struct&lt;col1:string&gt;&quot;)</span>
<span class="sd">    &gt;&gt;&gt; df.printSchema()</span>
<span class="sd">    root</span>
<span class="sd">    |-- long_column: long (nullable = true)</span>
<span class="sd">    |-- string_column: string (nullable = true)</span>
<span class="sd">    |-- struct_column: struct (nullable = true)</span>
<span class="sd">    |    |-- col1: string (nullable = true)</span>
<span class="sd">    &gt;&gt;&gt; df.select(func(&quot;long_col&quot;, &quot;string_col&quot;, &quot;struct_col&quot;)).printSchema()</span>
<span class="sd">    |-- func(long_col, string_col, struct_col): struct (nullable = true)</span>
<span class="sd">    |    |-- col1: string (nullable = true)</span>
<span class="sd">    |    |-- col2: long (nullable = true)</span>

<span class="sd">    In the following sections, it describes the cominations of the supported type hints. For</span>
<span class="sd">    simplicity, `pandas.DataFrame` variant is omitted.</span>

<span class="sd">    * Series to Series</span>
<span class="sd">        `pandas.Series`, ... -&gt; `pandas.Series`</span>

<span class="sd">        The function takes one or more `pandas.Series` and outputs one `pandas.Series`.</span>
<span class="sd">        The output of the function should always be of the same length as the input.</span>

<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;string&quot;)</span>
<span class="sd">        ... def to_upper(s: pd.Series) -&gt; pd.Series:</span>
<span class="sd">        ...     return s.str.upper()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([(&quot;John Doe&quot;,)], (&quot;name&quot;,))</span>
<span class="sd">        &gt;&gt;&gt; df.select(to_upper(&quot;name&quot;)).show()</span>
<span class="sd">        +--------------+</span>
<span class="sd">        |to_upper(name)|</span>
<span class="sd">        +--------------+</span>
<span class="sd">        |      JOHN DOE|</span>
<span class="sd">        +--------------+</span>

<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;first string, last string&quot;)</span>
<span class="sd">        ... def split_expand(s: pd.Series) -&gt; pd.DataFrame:</span>
<span class="sd">        ...     return s.str.split(expand=True)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([(&quot;John Doe&quot;,)], (&quot;name&quot;,))</span>
<span class="sd">        &gt;&gt;&gt; df.select(split_expand(&quot;name&quot;)).show()</span>
<span class="sd">        +------------------+</span>
<span class="sd">        |split_expand(name)|</span>
<span class="sd">        +------------------+</span>
<span class="sd">        |       [John, Doe]|</span>
<span class="sd">        +------------------+</span>

<span class="sd">        .. note:: The length of the input is not that of the whole input column, but is the</span>
<span class="sd">            length of an internal batch used for each call to the function.</span>

<span class="sd">    * Iterator of Series to Iterator of Series</span>
<span class="sd">        `Iterator[pandas.Series]` -&gt; `Iterator[pandas.Series]`</span>

<span class="sd">        The function takes an iterator of `pandas.Series` and outputs an iterator of</span>
<span class="sd">        `pandas.Series`. In this case, the created pandas UDF instance requires one input</span>
<span class="sd">        column when this is called as a PySpark column. The length of the entire output from</span>
<span class="sd">        the function should be the same length of the entire input; therefore, it can</span>
<span class="sd">        prefetch the data from the input iterator as long as the lengths are the same.</span>

<span class="sd">        It is also useful when the UDF execution</span>
<span class="sd">        requires initializing some states although internally it works identically as</span>
<span class="sd">        Series to Series case. The pseudocode below illustrates the example.</span>

<span class="sd">        .. highlight:: python</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            @pandas_udf(&quot;long&quot;)</span>
<span class="sd">            def calculate(iterator: Iterator[pd.Series]) -&gt; Iterator[pd.Series]:</span>
<span class="sd">                # Do some expensive initialization with a state</span>
<span class="sd">                state = very_expensive_initialization()</span>
<span class="sd">                for x in iterator:</span>
<span class="sd">                    # Use that state for whole iterator.</span>
<span class="sd">                    yield calculate_with_state(x, state)</span>

<span class="sd">            df.select(calculate(&quot;value&quot;)).show()</span>

<span class="sd">        &gt;&gt;&gt; from typing import Iterator</span>
<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;long&quot;)</span>
<span class="sd">        ... def plus_one(iterator: Iterator[pd.Series]) -&gt; Iterator[pd.Series]:</span>
<span class="sd">        ...     for s in iterator:</span>
<span class="sd">        ...         yield s + 1</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[&quot;v&quot;]))</span>
<span class="sd">        &gt;&gt;&gt; df.select(plus_one(df.v)).show()</span>
<span class="sd">        +-----------+</span>
<span class="sd">        |plus_one(v)|</span>
<span class="sd">        +-----------+</span>
<span class="sd">        |          2|</span>
<span class="sd">        |          3|</span>
<span class="sd">        |          4|</span>
<span class="sd">        +-----------+</span>

<span class="sd">        .. note:: The length of each series is the length of a batch internally used.</span>

<span class="sd">    * Iterator of Multiple Series to Iterator of Series</span>
<span class="sd">        `Iterator[Tuple[pandas.Series, ...]]` -&gt; `Iterator[pandas.Series]`</span>

<span class="sd">        The function takes an iterator of a tuple of multiple `pandas.Series` and outputs an</span>
<span class="sd">        iterator of `pandas.Series`. In this case, the created pandas UDF instance requires</span>
<span class="sd">        input columns as many as the series when this is called as a PySpark column.</span>
<span class="sd">        Otherwise, it has the same characteristics and restrictions as Iterator of Series</span>
<span class="sd">        to Iterator of Series case.</span>

<span class="sd">        &gt;&gt;&gt; from typing import Iterator, Tuple</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.functions import struct, col</span>
<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;long&quot;)</span>
<span class="sd">        ... def multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -&gt; Iterator[pd.Series]:</span>
<span class="sd">        ...     for s1, df in iterator:</span>
<span class="sd">        ...         yield s1 * df.v</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[&quot;v&quot;]))</span>
<span class="sd">        &gt;&gt;&gt; df.withColumn(&#39;output&#39;, multiply(col(&quot;v&quot;), struct(col(&quot;v&quot;)))).show()</span>
<span class="sd">        +---+------+</span>
<span class="sd">        |  v|output|</span>
<span class="sd">        +---+------+</span>
<span class="sd">        |  1|     1|</span>
<span class="sd">        |  2|     4|</span>
<span class="sd">        |  3|     9|</span>
<span class="sd">        +---+------+</span>

<span class="sd">        .. note:: The length of each series is the length of a batch internally used.</span>

<span class="sd">    * Series to Scalar</span>
<span class="sd">        `pandas.Series`, ... -&gt; `Any`</span>

<span class="sd">        The function takes `pandas.Series` and returns a scalar value. The `returnType`</span>
<span class="sd">        should be a primitive data type, and the returned scalar can be either a python primitive</span>
<span class="sd">        type, e.g., int or float or a numpy data type, e.g., numpy.int64 or numpy.float64.</span>
<span class="sd">        `Any` should ideally be a specific scalar type accordingly.</span>

<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;double&quot;)</span>
<span class="sd">        ... def mean_udf(v: pd.Series) -&gt; float:</span>
<span class="sd">        ...     return v.mean()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (&quot;id&quot;, &quot;v&quot;))</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&quot;id&quot;).agg(mean_udf(df[&#39;v&#39;])).show()</span>
<span class="sd">        +---+-----------+</span>
<span class="sd">        | id|mean_udf(v)|</span>
<span class="sd">        +---+-----------+</span>
<span class="sd">        |  1|        1.5|</span>
<span class="sd">        |  2|        6.0|</span>
<span class="sd">        +---+-----------+</span>

<span class="sd">        This UDF can also be used as window functions as below:</span>

<span class="sd">        &gt;&gt;&gt; from pyspark.sql import Window</span>
<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;double&quot;)</span>
<span class="sd">        ... def mean_udf(v: pd.Series) -&gt; float:</span>
<span class="sd">        ...     return v.mean()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (&quot;id&quot;, &quot;v&quot;))</span>
<span class="sd">        &gt;&gt;&gt; w = Window.partitionBy(&#39;id&#39;).orderBy(&#39;v&#39;).rowsBetween(-1, 0)</span>
<span class="sd">        &gt;&gt;&gt; df.withColumn(&#39;mean_v&#39;, mean_udf(&quot;v&quot;).over(w)).show()</span>
<span class="sd">        +---+----+------+</span>
<span class="sd">        | id|   v|mean_v|</span>
<span class="sd">        +---+----+------+</span>
<span class="sd">        |  1| 1.0|   1.0|</span>
<span class="sd">        |  1| 2.0|   1.5|</span>
<span class="sd">        |  2| 3.0|   3.0|</span>
<span class="sd">        |  2| 5.0|   4.0|</span>
<span class="sd">        |  2|10.0|   7.5|</span>
<span class="sd">        +---+----+------+</span>

<span class="sd">        .. note:: For performance reasons, the input series to window functions are not copied.</span>
<span class="sd">            Therefore, mutating the input series is not allowed and will cause incorrect results.</span>
<span class="sd">            For the same reason, users should also not rely on the index of the input series.</span>

<span class="sd">        .. seealso:: :meth:`pyspark.sql.GroupedData.agg` and :class:`pyspark.sql.Window`</span>

<span class="sd">    .. note:: The user-defined functions do not support conditional expressions or short circuiting</span>
<span class="sd">        in boolean expressions and it ends up with being executed all internally. If the functions</span>
<span class="sd">        can fail on special rows, the workaround is to incorporate the condition into the functions.</span>

<span class="sd">    .. note:: The user-defined functions do not take keyword arguments on the calling side.</span>

<span class="sd">    .. note:: The data type of returned `pandas.Series` from the user-defined functions should be</span>
<span class="sd">        matched with defined `returnType` (see :meth:`types.to_arrow_type` and</span>
<span class="sd">        :meth:`types.from_arrow_type`). When there is mismatch between them, Spark might do</span>
<span class="sd">        conversion on returned data. The conversion is not guaranteed to be correct and results</span>
<span class="sd">        should be checked for accuracy by users.</span>

<span class="sd">    .. note:: Currently,</span>
<span class="sd">        :class:`pyspark.sql.types.MapType`,</span>
<span class="sd">        :class:`pyspark.sql.types.ArrayType` of :class:`pyspark.sql.types.TimestampType` and</span>
<span class="sd">        nested :class:`pyspark.sql.types.StructType`</span>
<span class="sd">        are currently not supported as output types.</span>

<span class="sd">    .. seealso:: :meth:`pyspark.sql.DataFrame.mapInPandas`</span>
<span class="sd">    .. seealso:: :meth:`pyspark.sql.GroupedData.applyInPandas`</span>
<span class="sd">    .. seealso:: :meth:`pyspark.sql.PandasCogroupedOps.applyInPandas`</span>
<span class="sd">    .. seealso:: :meth:`pyspark.sql.UDFRegistration.register`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># The following table shows most of Pandas data and SQL type conversions in Pandas UDFs that</span>
    <span class="c1"># are not yet visible to the user. Some of behaviors are buggy and might be changed in the near</span>
    <span class="c1"># future. The table might have to be eventually documented externally.</span>
    <span class="c1"># Please see SPARK-28132&#39;s PR to see the codes in order to generate the table below.</span>
    <span class="c1">#</span>
    <span class="c1"># +-----------------------------+----------------------+------------------+------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+------------------+--------------+--------------+--------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+--------------+-----------------+------------------+-----------+--------------------------------+  # noqa</span>
    <span class="c1"># |SQL Type \ Pandas Value(Type)|None(object(NoneType))|        True(bool)|           1(int8)|          1(int16)|            1(int32)|            1(int64)|          1(uint8)|         1(uint16)|         1(uint32)|         1(uint64)|  1.0(float16)|  1.0(float32)|  1.0(float64)|1970-01-01 00:00:00(datetime64[ns])|1970-01-01 00:00:00-05:00(datetime64[ns, US/Eastern])|a(object(string))|  1(object(Decimal))|[1 2 3](object(array[int32]))| 1.0(float128)|(1+0j)(complex64)|(1+0j)(complex128)|A(category)|1 days 00:00:00(timedelta64[ns])|  # noqa</span>
    <span class="c1"># +-----------------------------+----------------------+------------------+------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+------------------+--------------+--------------+--------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+--------------+-----------------+------------------+-----------+--------------------------------+  # noqa</span>
    <span class="c1"># |                      boolean|                  None|              True|              True|              True|                True|                True|              True|              True|              True|              True|          True|          True|          True|                                  X|                                                    X|                X|                   X|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |                      tinyint|                  None|                 1|                 1|                 1|                   1|                   1|                 1|                 1|                 1|                 1|             1|             1|             1|                                  X|                                                    X|                X|                   1|                            X|             X|                X|                 X|          0|                               X|  # noqa</span>
    <span class="c1"># |                     smallint|                  None|                 1|                 1|                 1|                   1|                   1|                 1|                 1|                 1|                 1|             1|             1|             1|                                  X|                                                    X|                X|                   1|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |                          int|                  None|                 1|                 1|                 1|                   1|                   1|                 1|                 1|                 1|                 1|             1|             1|             1|                                  X|                                                    X|                X|                   1|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |                       bigint|                  None|                 1|                 1|                 1|                   1|                   1|                 1|                 1|                 1|                 1|             1|             1|             1|                                  0|                                       18000000000000|                X|                   1|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |                        float|                  None|               1.0|               1.0|               1.0|                 1.0|                 1.0|               1.0|               1.0|               1.0|               1.0|           1.0|           1.0|           1.0|                                  X|                                                    X|                X|                   X|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |                       double|                  None|               1.0|               1.0|               1.0|                 1.0|                 1.0|               1.0|               1.0|               1.0|               1.0|           1.0|           1.0|           1.0|                                  X|                                                    X|                X|                   X|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |                         date|                  None|                 X|                 X|                 X|datetime.date(197...|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|               datetime.date(197...|                                 datetime.date(197...|                X|datetime.date(197...|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |                    timestamp|                  None|                 X|                 X|                 X|                   X|datetime.datetime...|                 X|                 X|                 X|                 X|             X|             X|             X|               datetime.datetime...|                                 datetime.datetime...|                X|datetime.datetime...|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |                       string|                  None|                &#39;&#39;|                &#39;&#39;|                &#39;&#39;|              &#39;\x01&#39;|              &#39;\x01&#39;|                &#39;&#39;|                &#39;&#39;|            &#39;\x01&#39;|            &#39;\x01&#39;|            &#39;&#39;|            &#39;&#39;|            &#39;&#39;|                                  X|                                                    X|              &#39;a&#39;|                   X|                            X|            &#39;&#39;|                X|                &#39;&#39;|          X|                               X|  # noqa</span>
    <span class="c1"># |                decimal(10,0)|                  None|                 X|                 X|                 X|                   X|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|                                  X|                                                    X|                X|        Decimal(&#39;1&#39;)|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |                   array&lt;int&gt;|                  None|                 X|                 X|                 X|                   X|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|                                  X|                                                    X|                X|                   X|                    [1, 2, 3]|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |              map&lt;string,int&gt;|                     X|                 X|                 X|                 X|                   X|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|                                  X|                                                    X|                X|                   X|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |               struct&lt;_1:int&gt;|                     X|                 X|                 X|                 X|                   X|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|                                  X|                                                    X|                X|                   X|                            X|             X|                X|                 X|          X|                               X|  # noqa</span>
    <span class="c1"># |                       binary|                  None|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|  bytearray(b&#39;\x01&#39;)|  bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;&#39;)|bytearray(b&#39;&#39;)|bytearray(b&#39;&#39;)|                     bytearray(b&#39;&#39;)|                                       bytearray(b&#39;&#39;)|  bytearray(b&#39;a&#39;)|                   X|                            X|bytearray(b&#39;&#39;)|   bytearray(b&#39;&#39;)|    bytearray(b&#39;&#39;)|          X|                  bytearray(b&#39;&#39;)|  # noqa</span>
    <span class="c1"># +-----------------------------+----------------------+------------------+------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+------------------+--------------+--------------+--------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+--------------+-----------------+------------------+-----------+--------------------------------+  # noqa</span>
    <span class="c1">#</span>
    <span class="c1"># Note: DDL formatted string is used for &#39;SQL Type&#39; for simplicity. This string can be</span>
    <span class="c1">#       used in `returnType`.</span>
    <span class="c1"># Note: The values inside of the table are generated by `repr`.</span>
    <span class="c1"># Note: Python 3.7.3, Pandas 0.24.2 and PyArrow 0.13.0 are used.</span>
    <span class="c1"># Note: Timezone is KST.</span>
    <span class="c1"># Note: &#39;X&#39; means it throws an exception during the conversion.</span>
    <span class="n">require_minimum_pandas_version</span><span class="p">()</span>
    <span class="n">require_minimum_pyarrow_version</span><span class="p">()</span>

    <span class="c1"># decorator @pandas_udf(returnType, functionType)</span>
    <span class="n">is_decorator</span> <span class="o">=</span> <span class="n">f</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">DataType</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">is_decorator</span><span class="p">:</span>
        <span class="c1"># If DataType has been passed as a positional argument</span>
        <span class="c1"># for decorator use it as a returnType</span>
        <span class="n">return_type</span> <span class="o">=</span> <span class="n">f</span> <span class="ow">or</span> <span class="n">returnType</span>

        <span class="k">if</span> <span class="n">functionType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># @pandas_udf(dataType, functionType=functionType)</span>
            <span class="c1"># @pandas_udf(returnType=dataType, functionType=functionType)</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">functionType</span>
        <span class="k">elif</span> <span class="n">returnType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">returnType</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="c1"># @pandas_udf(dataType, functionType)</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">returnType</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># @pandas_udf(dataType) or @pandas_udf(returnType=dataType)</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">return_type</span> <span class="o">=</span> <span class="n">returnType</span>

        <span class="k">if</span> <span class="n">functionType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">functionType</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">return_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid return type: returnType can not be None&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">eval_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span><span class="p">,</span>
                         <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_ITER_UDF</span><span class="p">,</span>
                         <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span><span class="p">,</span>
                         <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_AGG_PANDAS_UDF</span><span class="p">,</span>
                         <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_MAP_PANDAS_ITER_UDF</span><span class="p">,</span>
                         <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_COGROUPED_MAP_PANDAS_UDF</span><span class="p">,</span>
                         <span class="kc">None</span><span class="p">]:</span>  <span class="c1"># None means it should infer the type from type hints.</span>

        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid function type: &quot;</span>
                         <span class="s2">&quot;functionType must be one the values from PandasUDFType&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_decorator</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_create_pandas_udf</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">return_type</span><span class="p">,</span> <span class="n">evalType</span><span class="o">=</span><span class="n">eval_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_create_pandas_udf</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">return_type</span><span class="p">,</span> <span class="n">evalType</span><span class="o">=</span><span class="n">eval_type</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_create_pandas_udf</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">returnType</span><span class="p">,</span> <span class="n">evalType</span><span class="p">):</span>
    <span class="n">argspec</span> <span class="o">=</span> <span class="n">_get_argspec</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="c1"># pandas UDF by type hints.</span>
    <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">inspect</span> <span class="kn">import</span> <span class="n">signature</span>

        <span class="k">if</span> <span class="n">evalType</span> <span class="ow">in</span> <span class="p">[</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span><span class="p">,</span>
                        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_ITER_UDF</span><span class="p">,</span>
                        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_AGG_PANDAS_UDF</span><span class="p">]:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for &quot;</span>
                <span class="s2">&quot;pandas UDF instead of specifying pandas UDF type which will be deprecated &quot;</span>
                <span class="s2">&quot;in the future releases. See SPARK-28264 for more details.&quot;</span><span class="p">,</span> <span class="ne">UserWarning</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">evalType</span> <span class="ow">in</span> <span class="p">[</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span><span class="p">,</span>
                          <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_MAP_PANDAS_ITER_UDF</span><span class="p">,</span>
                          <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_COGROUPED_MAP_PANDAS_UDF</span><span class="p">]:</span>
            <span class="c1"># In case of &#39;SQL_GROUPED_MAP_PANDAS_UDF&#39;,  deprecation warning is being triggered</span>
            <span class="c1"># at `apply` instead.</span>
            <span class="c1"># In case of &#39;SQL_MAP_PANDAS_ITER_UDF&#39; and &#39;SQL_COGROUPED_MAP_PANDAS_UDF&#39;, the</span>
            <span class="c1"># evaluation type will always be set.</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">argspec</span><span class="o">.</span><span class="n">annotations</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">evalType</span> <span class="o">=</span> <span class="n">infer_eval_type</span><span class="p">(</span><span class="n">signature</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
            <span class="k">assert</span> <span class="n">evalType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">evalType</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Set default is scalar UDF.</span>
        <span class="n">evalType</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">evalType</span> <span class="o">==</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span> <span class="ow">or</span>
            <span class="n">evalType</span> <span class="o">==</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_ITER_UDF</span><span class="p">)</span> <span class="ow">and</span> \
            <span class="nb">len</span><span class="p">(</span><span class="n">argspec</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> \
            <span class="n">argspec</span><span class="o">.</span><span class="n">varargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function: 0-arg pandas_udfs are not supported. &quot;</span>
            <span class="s2">&quot;Instead, create a 1-arg pandas_udf and ignore the arg in your function.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">evalType</span> <span class="o">==</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span> \
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">argspec</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function: pandas_udf with function type GROUPED_MAP or &quot;</span>
            <span class="s2">&quot;the function in groupby.applyInPandas &quot;</span>
            <span class="s2">&quot;must take either one argument (data) or two arguments (key, data).&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">evalType</span> <span class="o">==</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_COGROUPED_MAP_PANDAS_UDF</span> \
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">argspec</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function: the function in cogroup.applyInPandas &quot;</span>
            <span class="s2">&quot;must take either two arguments (left, right) &quot;</span>
            <span class="s2">&quot;or three arguments (key, left, right).&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_create_udf</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">returnType</span><span class="p">,</span> <span class="n">evalType</span><span class="p">)</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../../index.html">
              <img class="logo" src="../../../../_static/spark-logo-hd.png" alt="Logo"/>
            </a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
    
        <li class="nav-item nav-item-0"><a href="../../../../index.html">PySpark 3.0.1 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="../../../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.3.1.
    </div>
  </body>
</html>