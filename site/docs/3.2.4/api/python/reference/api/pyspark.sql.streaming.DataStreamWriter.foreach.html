
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>pyspark.sql.streaming.DataStreamWriter.foreach &#8212; PySpark 3.2.4 documentation</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/pyspark.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="canonical" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.foreach.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="pyspark.sql.streaming.DataStreamWriter.foreachBatch" href="pyspark.sql.streaming.DataStreamWriter.foreachBatch.html" />
    <link rel="prev" title="pyspark.sql.streaming.DataStreamReader.text" href="pyspark.sql.streaming.DataStreamReader.text.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    <!-- Matomo -->
    <script type="text/javascript">
        var _paq = window._paq = window._paq || [];
        /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
        _paq.push(["disableCookies"]);
        _paq.push(['trackPageView']);
        _paq.push(['enableLinkTracking']);
        (function() {
            var u="https://analytics.apache.org/";
            _paq.push(['setTrackerUrl', u+'matomo.php']);
            _paq.push(['setSiteId', '40']);
            var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
            g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
        })();
    </script>
    <!-- End Matomo Code -->
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../index.html">
    
      <img src="../../_static/spark-logo-reverse.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../getting_started/index.html">Getting Started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../user_guide/index.html">User Guide</a>
        </li>
        
        <li class="nav-item active">
            <a class="nav-link" href="../index.html">API Reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../development/index.html">Development</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../migration_guide/index.html">Migration Guide</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
          
            
                <li class="">
                    <a href="../pyspark.sql.html">Spark SQL</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.pandas/index.html">Pandas API on Spark</a>
                </li>
            
          
            
                <li class="active">
                    <a href="../pyspark.ss.html">Structured Streaming</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.ml.html">MLlib (DataFrame-based)</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.streaming.html">Spark Streaming</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.mllib.html">MLlib (RDD-based)</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.html">Spark Core</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.resource.html">Resource Management</a>
                </li>
            
          
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="pyspark-sql-streaming-datastreamwriter-foreach">
<h1>pyspark.sql.streaming.DataStreamWriter.foreach<a class="headerlink" href="#pyspark-sql-streaming-datastreamwriter-foreach" title="Permalink to this headline">¶</a></h1>
<dl class="py method">
<dt id="pyspark.sql.streaming.DataStreamWriter.foreach">
<code class="sig-prename descclassname">DataStreamWriter.</code><code class="sig-name descname">foreach</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/pyspark/sql/streaming.html#DataStreamWriter.foreach"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.streaming.DataStreamWriter.foreach" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the output of the streaming query to be processed using the provided writer <code class="docutils literal notranslate"><span class="pre">f</span></code>.
This is often used to write the output of a streaming query to arbitrary storage systems.
The processing logic can be specified in two ways.</p>
<ol class="arabic">
<li><dl class="simple">
<dt>A <strong>function</strong> that takes a row as input.</dt><dd><p>This is a simple way to express your processing logic. Note that this does
not allow you to deduplicate generated data when failures cause reprocessing of
some input data. That would require you to specify the processing logic in the next
way.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>An <strong>object</strong> with a <code class="docutils literal notranslate"><span class="pre">process</span></code> method and optional <code class="docutils literal notranslate"><span class="pre">open</span></code> and <code class="docutils literal notranslate"><span class="pre">close</span></code> methods.</dt><dd><p>The object can have the following methods.</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">open(partition_id,</span> <span class="pre">epoch_id)</span></code>: <em>Optional</em> method that initializes the processing</dt><dd><p>(for example, open a connection, start a transaction, etc). Additionally, you can
use the <cite>partition_id</cite> and <cite>epoch_id</cite> to deduplicate regenerated data
(discussed later).</p>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">process(row)</span></code>: <em>Non-optional</em> method that processes each <code class="xref py py-class docutils literal notranslate"><span class="pre">Row</span></code>.</p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">close(error)</span></code>: <em>Optional</em> method that finalizes and cleans up (for example,</dt><dd><p>close connection, commit transaction, etc.) after all rows have been processed.</p>
</dd>
</dl>
</li>
</ul>
<p>The object will be used by Spark in the following way.</p>
<ul>
<li><dl class="simple">
<dt>A single copy of this object is responsible of all the data generated by a</dt><dd><p>single task in a query. In other words, one instance is responsible for
processing one partition of the data generated in a distributed manner.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>This object must be serializable because each task will get a fresh</dt><dd><p>serialized-deserialized copy of the provided object. Hence, it is strongly
recommended that any initialization for writing data (e.g. opening a
connection or starting a transaction) is done after the <cite>open(…)</cite>
method has been called, which signifies that the task is ready to generate data.</p>
</dd>
</dl>
</li>
<li><p>The lifecycle of the methods are as follows.</p>
<blockquote>
<div><p>For each partition with <code class="docutils literal notranslate"><span class="pre">partition_id</span></code>:</p>
<p>… For each batch/epoch of streaming data with <code class="docutils literal notranslate"><span class="pre">epoch_id</span></code>:</p>
<p>……. Method <code class="docutils literal notranslate"><span class="pre">open(partitionId,</span> <span class="pre">epochId)</span></code> is called.</p>
<dl class="simple">
<dt>……. If <code class="docutils literal notranslate"><span class="pre">open(...)</span></code> returns true, for each row in the partition and</dt><dd><p>batch/epoch, method <code class="docutils literal notranslate"><span class="pre">process(row)</span></code> is called.</p>
</dd>
<dt>……. Method <code class="docutils literal notranslate"><span class="pre">close(errorOrNull)</span></code> is called with error (if any) seen while</dt><dd><p>processing rows.</p>
</dd>
</dl>
</div></blockquote>
</li>
</ul>
<p>Important points to note:</p>
<ul class="simple">
<li><dl class="simple">
<dt>The <cite>partitionId</cite> and <cite>epochId</cite> can be used to deduplicate generated data when</dt><dd><p>failures cause reprocessing of some input data. This depends on the execution
mode of the query. If the streaming query is being executed in the micro-batch
mode, then every partition represented by a unique tuple (partition_id, epoch_id)
is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used
to deduplicate and/or transactionally commit data and achieve exactly-once
guarantees. However, if the streaming query is being executed in the continuous
mode, then this guarantee does not hold and therefore should not be used for
deduplication.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>The <code class="docutils literal notranslate"><span class="pre">close()</span></code> method (if exists) will be called if <cite>open()</cite> method exists and</dt><dd><p>returns successfully (irrespective of the return value), except if the Python
crashes in the middle.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ol>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.0.</span></p>
</div>
<p class="rubric">Notes</p>
<p>This API is evolving.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print every row using a function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">print_row</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">print_row</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print every row using a object with process() method</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">RowPrinter</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">open</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition_id</span><span class="p">,</span> <span class="n">epoch_id</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Opened </span><span class="si">%d</span><span class="s2">, </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">partition_id</span><span class="p">,</span> <span class="n">epoch_id</span><span class="p">))</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="kc">True</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">print</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Closed with error: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">writer</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">RowPrinter</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="pyspark.sql.streaming.DataStreamReader.text.html" title="previous page">pyspark.sql.streaming.DataStreamReader.text</a>
    <a class='right-next' id="next-link" href="pyspark.sql.streaming.DataStreamWriter.foreachBatch.html" title="next page">pyspark.sql.streaming.DataStreamWriter.foreachBatch</a>

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright .<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>