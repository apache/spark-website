
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>pyspark.sql.pandas.group_ops &#8212; PySpark 3.2.4 documentation</title>
    
  <link rel="stylesheet" href="../../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/pyspark.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="canonical" href="https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/pandas/group_ops.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../../../index.html">
    
      <img src="../../../../_static/spark-logo-reverse.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../getting_started/index.html">Getting Started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../user_guide/index.html">User Guide</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../reference/index.html">API Reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../development/index.html">Development</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../migration_guide/index.html">Migration Guide</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for pyspark.sql.pandas.group_ops</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="kn">import</span> <span class="n">PythonEvalType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.column</span> <span class="kn">import</span> <span class="n">Column</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>


<span class="k">class</span> <span class="nc">PandasGroupedOpsMixin</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Min-in for pandas grouped operations. Currently, only :class:`GroupedData`</span>
<span class="sd">    can use this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">udf</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        It is an alias of :meth:`pyspark.sql.GroupedData.applyInPandas`; however, it takes a</span>
<span class="sd">        :meth:`pyspark.sql.functions.pandas_udf` whereas</span>
<span class="sd">        :meth:`pyspark.sql.GroupedData.applyInPandas` takes a Python native function.</span>

<span class="sd">        .. versionadded:: 2.3.0</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        udf : :func:`pyspark.sql.functions.pandas_udf`</span>
<span class="sd">            a grouped map user-defined function returned by</span>
<span class="sd">            :func:`pyspark.sql.functions.pandas_udf`.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        It is preferred to use :meth:`pyspark.sql.GroupedData.applyInPandas` over this</span>
<span class="sd">        API. This API will be deprecated in the future releases.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf, PandasUDFType</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">        ...     (&quot;id&quot;, &quot;v&quot;))</span>
<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;id long, v double&quot;, PandasUDFType.GROUPED_MAP)  # doctest: +SKIP</span>
<span class="sd">        ... def normalize(pdf):</span>
<span class="sd">        ...     v = pdf.v</span>
<span class="sd">        ...     return pdf.assign(v=(v - v.mean()) / v.std())</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&quot;id&quot;).apply(normalize).show()  # doctest: +SKIP</span>
<span class="sd">        +---+-------------------+</span>
<span class="sd">        | id|                  v|</span>
<span class="sd">        +---+-------------------+</span>
<span class="sd">        |  1|-0.7071067811865475|</span>
<span class="sd">        |  1| 0.7071067811865475|</span>
<span class="sd">        |  2|-0.8320502943378437|</span>
<span class="sd">        |  2|-0.2773500981126146|</span>
<span class="sd">        |  2| 1.1094003924504583|</span>
<span class="sd">        +---+-------------------+</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        pyspark.sql.functions.pandas_udf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Columns are special because hasattr always return True</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">udf</span><span class="p">,</span> <span class="n">Column</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">udf</span><span class="p">,</span> <span class="s1">&#39;func&#39;</span><span class="p">)</span> \
                <span class="ow">or</span> <span class="n">udf</span><span class="o">.</span><span class="n">evalType</span> <span class="o">!=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid udf: the udf argument must be a pandas_udf of type &quot;</span>
                             <span class="s2">&quot;GROUPED_MAP.&quot;</span><span class="p">)</span>

        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;It is preferred to use &#39;applyInPandas&#39; over this &quot;</span>
            <span class="s2">&quot;API. This API will be deprecated in the future releases. See SPARK-28264 for &quot;</span>
            <span class="s2">&quot;more details.&quot;</span><span class="p">,</span> <span class="ne">UserWarning</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">applyInPandas</span><span class="p">(</span><span class="n">udf</span><span class="o">.</span><span class="n">func</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">udf</span><span class="o">.</span><span class="n">returnType</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">applyInPandas</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">schema</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Maps each group of the current :class:`DataFrame` using a pandas udf and returns the result</span>
<span class="sd">        as a `DataFrame`.</span>

<span class="sd">        The function should take a `pandas.DataFrame` and return another</span>
<span class="sd">        `pandas.DataFrame`. For each group, all columns are passed together as a `pandas.DataFrame`</span>
<span class="sd">        to the user-function and the returned `pandas.DataFrame` are combined as a</span>
<span class="sd">        :class:`DataFrame`.</span>

<span class="sd">        The `schema` should be a :class:`StructType` describing the schema of the returned</span>
<span class="sd">        `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match</span>
<span class="sd">        the field names in the defined schema if specified as strings, or match the</span>
<span class="sd">        field data types by position if not strings, e.g. integer indices.</span>
<span class="sd">        The length of the returned `pandas.DataFrame` can be arbitrary.</span>

<span class="sd">        .. versionadded:: 3.0.0</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        func : function</span>
<span class="sd">            a Python native function that takes a `pandas.DataFrame`, and outputs a</span>
<span class="sd">            `pandas.DataFrame`.</span>
<span class="sd">        schema : :class:`pyspark.sql.types.DataType` or str</span>
<span class="sd">            the return type of the `func` in PySpark. The value can be either a</span>
<span class="sd">            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import pandas as pd  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf, ceil</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">        ...     (&quot;id&quot;, &quot;v&quot;))  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; def normalize(pdf):</span>
<span class="sd">        ...     v = pdf.v</span>
<span class="sd">        ...     return pdf.assign(v=(v - v.mean()) / v.std())</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&quot;id&quot;).applyInPandas(</span>
<span class="sd">        ...     normalize, schema=&quot;id long, v double&quot;).show()  # doctest: +SKIP</span>
<span class="sd">        +---+-------------------+</span>
<span class="sd">        | id|                  v|</span>
<span class="sd">        +---+-------------------+</span>
<span class="sd">        |  1|-0.7071067811865475|</span>
<span class="sd">        |  1| 0.7071067811865475|</span>
<span class="sd">        |  2|-0.8320502943378437|</span>
<span class="sd">        |  2|-0.2773500981126146|</span>
<span class="sd">        |  2| 1.1094003924504583|</span>
<span class="sd">        +---+-------------------+</span>

<span class="sd">        Alternatively, the user can pass a function that takes two arguments.</span>
<span class="sd">        In this case, the grouping key(s) will be passed as the first argument and the data will</span>
<span class="sd">        be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy</span>
<span class="sd">        data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in</span>
<span class="sd">        as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.</span>
<span class="sd">        This is useful when the user does not want to hardcode grouping key(s) in the function.</span>

<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">        ...     (&quot;id&quot;, &quot;v&quot;))  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; def mean_func(key, pdf):</span>
<span class="sd">        ...     # key is a tuple of one numpy.int64, which is the value</span>
<span class="sd">        ...     # of &#39;id&#39; for the current group</span>
<span class="sd">        ...     return pd.DataFrame([key + (pdf.v.mean(),)])</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&#39;id&#39;).applyInPandas(</span>
<span class="sd">        ...     mean_func, schema=&quot;id long, v double&quot;).show()  # doctest: +SKIP</span>
<span class="sd">        +---+---+</span>
<span class="sd">        | id|  v|</span>
<span class="sd">        +---+---+</span>
<span class="sd">        |  1|1.5|</span>
<span class="sd">        |  2|6.0|</span>
<span class="sd">        +---+---+</span>

<span class="sd">        &gt;&gt;&gt; def sum_func(key, pdf):</span>
<span class="sd">        ...     # key is a tuple of two numpy.int64s, which is the values</span>
<span class="sd">        ...     # of &#39;id&#39; and &#39;ceil(df.v / 2)&#39; for the current group</span>
<span class="sd">        ...     return pd.DataFrame([key + (pdf.v.sum(),)])</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(df.id, ceil(df.v / 2)).applyInPandas(</span>
<span class="sd">        ...     sum_func, schema=&quot;id long, `ceil(v / 2)` long, v double&quot;).show()  # doctest: +SKIP</span>
<span class="sd">        +---+-----------+----+</span>
<span class="sd">        | id|ceil(v / 2)|   v|</span>
<span class="sd">        +---+-----------+----+</span>
<span class="sd">        |  2|          5|10.0|</span>
<span class="sd">        |  1|          1| 3.0|</span>
<span class="sd">        |  2|          3| 5.0|</span>
<span class="sd">        |  2|          2| 3.0|</span>
<span class="sd">        +---+-----------+----+</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function requires a full shuffle. All the data of a group will be loaded</span>
<span class="sd">        into memory, so the user should be aware of the potential OOM risk if data is skewed</span>
<span class="sd">        and certain groups are too large to fit in memory.</span>

<span class="sd">        If returning a new `pandas.DataFrame` constructed with a dictionary, it is</span>
<span class="sd">        recommended to explicitly index the columns by name to ensure the positions are correct,</span>
<span class="sd">        or alternatively use an `OrderedDict`.</span>
<span class="sd">        For example, `pd.DataFrame({&#39;id&#39;: ids, &#39;a&#39;: data}, columns=[&#39;id&#39;, &#39;a&#39;])` or</span>
<span class="sd">        `pd.DataFrame(OrderedDict([(&#39;id&#39;, ids), (&#39;a&#39;, data)]))`.</span>

<span class="sd">        This API is experimental.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        pyspark.sql.functions.pandas_udf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">GroupedData</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">GroupedData</span><span class="p">)</span>

        <span class="n">udf</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">functionType</span><span class="o">=</span><span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_MAP</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span>
        <span class="n">udf_column</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">])</span>
        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jgd</span><span class="o">.</span><span class="n">flatMapGroupsInPandas</span><span class="p">(</span><span class="n">udf_column</span><span class="o">.</span><span class="n">_jc</span><span class="o">.</span><span class="n">expr</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sql_ctx</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">cogroup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Cogroups this group with another group so that we can run cogrouped operations.</span>

<span class="sd">        .. versionadded:: 3.0.0</span>

<span class="sd">        See :class:`PandasCogroupedOps` for the operations that can be run.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">GroupedData</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">GroupedData</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">PandasCogroupedOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<div class="viewcode-block" id="PandasCogroupedOps"><a class="viewcode-back" href="../../../../reference/api/pyspark.sql.PandasCogroupedOps.html#pyspark.sql.PandasCogroupedOps">[docs]</a><span class="k">class</span> <span class="nc">PandasCogroupedOps</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A logical grouping of two :class:`GroupedData`,</span>
<span class="sd">    created by :func:`GroupedData.cogroup`.</span>

<span class="sd">    .. versionadded:: 3.0.0</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This API is experimental.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gd1</span><span class="p">,</span> <span class="n">gd2</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gd1</span> <span class="o">=</span> <span class="n">gd1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gd2</span> <span class="o">=</span> <span class="n">gd2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sql_ctx</span> <span class="o">=</span> <span class="n">gd1</span><span class="o">.</span><span class="n">sql_ctx</span>

<div class="viewcode-block" id="PandasCogroupedOps.applyInPandas"><a class="viewcode-back" href="../../../../reference/api/pyspark.sql.PandasCogroupedOps.applyInPandas.html#pyspark.sql.PandasCogroupedOps.applyInPandas">[docs]</a>    <span class="k">def</span> <span class="nf">applyInPandas</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">schema</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies a function to each cogroup using pandas and returns the result</span>
<span class="sd">        as a `DataFrame`.</span>

<span class="sd">        The function should take two `pandas.DataFrame`\\s and return another</span>
<span class="sd">        `pandas.DataFrame`.  For each side of the cogroup, all columns are passed together as a</span>
<span class="sd">        `pandas.DataFrame` to the user-function and the returned `pandas.DataFrame` are combined as</span>
<span class="sd">        a :class:`DataFrame`.</span>

<span class="sd">        The `schema` should be a :class:`StructType` describing the schema of the returned</span>
<span class="sd">        `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match</span>
<span class="sd">        the field names in the defined schema if specified as strings, or match the</span>
<span class="sd">        field data types by position if not strings, e.g. integer indices.</span>
<span class="sd">        The length of the returned `pandas.DataFrame` can be arbitrary.</span>

<span class="sd">        .. versionadded:: 3.0.0</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        func : function</span>
<span class="sd">            a Python native function that takes two `pandas.DataFrame`\\s, and</span>
<span class="sd">            outputs a `pandas.DataFrame`, or that takes one tuple (grouping keys) and two</span>
<span class="sd">            pandas ``DataFrame``\\s, and outputs a pandas ``DataFrame``.</span>
<span class="sd">        schema : :class:`pyspark.sql.types.DataType` or str</span>
<span class="sd">            the return type of the `func` in PySpark. The value can be either a</span>
<span class="sd">            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf</span>
<span class="sd">        &gt;&gt;&gt; df1 = spark.createDataFrame(</span>
<span class="sd">        ...     [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],</span>
<span class="sd">        ...     (&quot;time&quot;, &quot;id&quot;, &quot;v1&quot;))</span>
<span class="sd">        &gt;&gt;&gt; df2 = spark.createDataFrame(</span>
<span class="sd">        ...     [(20000101, 1, &quot;x&quot;), (20000101, 2, &quot;y&quot;)],</span>
<span class="sd">        ...     (&quot;time&quot;, &quot;id&quot;, &quot;v2&quot;))</span>
<span class="sd">        &gt;&gt;&gt; def asof_join(l, r):</span>
<span class="sd">        ...     return pd.merge_asof(l, r, on=&quot;time&quot;, by=&quot;id&quot;)</span>
<span class="sd">        &gt;&gt;&gt; df1.groupby(&quot;id&quot;).cogroup(df2.groupby(&quot;id&quot;)).applyInPandas(</span>
<span class="sd">        ...     asof_join, schema=&quot;time int, id int, v1 double, v2 string&quot;</span>
<span class="sd">        ... ).show()  # doctest: +SKIP</span>
<span class="sd">        +--------+---+---+---+</span>
<span class="sd">        |    time| id| v1| v2|</span>
<span class="sd">        +--------+---+---+---+</span>
<span class="sd">        |20000101|  1|1.0|  x|</span>
<span class="sd">        |20000102|  1|3.0|  x|</span>
<span class="sd">        |20000101|  2|2.0|  y|</span>
<span class="sd">        |20000102|  2|4.0|  y|</span>
<span class="sd">        +--------+---+---+---+</span>

<span class="sd">        Alternatively, the user can define a function that takes three arguments.  In this case,</span>
<span class="sd">        the grouping key(s) will be passed as the first argument and the data will be passed as the</span>
<span class="sd">        second and third arguments.  The grouping key(s) will be passed as a tuple of numpy data</span>
<span class="sd">        types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in as two</span>
<span class="sd">        `pandas.DataFrame` containing all columns from the original Spark DataFrames.</span>

<span class="sd">        &gt;&gt;&gt; def asof_join(k, l, r):</span>
<span class="sd">        ...     if k == (1,):</span>
<span class="sd">        ...         return pd.merge_asof(l, r, on=&quot;time&quot;, by=&quot;id&quot;)</span>
<span class="sd">        ...     else:</span>
<span class="sd">        ...         return pd.DataFrame(columns=[&#39;time&#39;, &#39;id&#39;, &#39;v1&#39;, &#39;v2&#39;])</span>
<span class="sd">        &gt;&gt;&gt; df1.groupby(&quot;id&quot;).cogroup(df2.groupby(&quot;id&quot;)).applyInPandas(</span>
<span class="sd">        ...     asof_join, &quot;time int, id int, v1 double, v2 string&quot;).show()  # doctest: +SKIP</span>
<span class="sd">        +--------+---+---+---+</span>
<span class="sd">        |    time| id| v1| v2|</span>
<span class="sd">        +--------+---+---+---+</span>
<span class="sd">        |20000101|  1|1.0|  x|</span>
<span class="sd">        |20000102|  1|3.0|  x|</span>
<span class="sd">        +--------+---+---+---+</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function requires a full shuffle. All the data of a cogroup will be loaded</span>
<span class="sd">        into memory, so the user should be aware of the potential OOM risk if data is skewed</span>
<span class="sd">        and certain groups are too large to fit in memory.</span>

<span class="sd">        If returning a new `pandas.DataFrame` constructed with a dictionary, it is</span>
<span class="sd">        recommended to explicitly index the columns by name to ensure the positions are correct,</span>
<span class="sd">        or alternatively use an `OrderedDict`.</span>
<span class="sd">        For example, `pd.DataFrame({&#39;id&#39;: ids, &#39;a&#39;: data}, columns=[&#39;id&#39;, &#39;a&#39;])` or</span>
<span class="sd">        `pd.DataFrame(OrderedDict([(&#39;id&#39;, ids), (&#39;a&#39;, data)]))`.</span>

<span class="sd">        This API is experimental.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        pyspark.sql.functions.pandas_udf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span>

        <span class="n">udf</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">functionType</span><span class="o">=</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_COGROUPED_MAP_PANDAS_UDF</span><span class="p">)</span>
        <span class="n">all_cols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_cols</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gd1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_cols</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gd2</span><span class="p">)</span>
        <span class="n">udf_column</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="o">*</span><span class="n">all_cols</span><span class="p">)</span>
        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gd1</span><span class="o">.</span><span class="n">_jgd</span><span class="o">.</span><span class="n">flatMapCoGroupsInPandas</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gd2</span><span class="o">.</span><span class="n">_jgd</span><span class="p">,</span> <span class="n">udf_column</span><span class="o">.</span><span class="n">_jc</span><span class="o">.</span><span class="n">expr</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sql_ctx</span><span class="p">)</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_extract_cols</span><span class="p">(</span><span class="n">gd</span><span class="p">):</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">_df</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span></div>


<span class="k">def</span> <span class="nf">_test</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
    <span class="kn">import</span> <span class="nn">pyspark.sql.pandas.group_ops</span>
    <span class="n">globs</span> <span class="o">=</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">pandas</span><span class="o">.</span><span class="n">group_ops</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span>\
        <span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[4]&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;sql.pandas.group tests&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;spark&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spark</span>
    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span>
        <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">pandas</span><span class="o">.</span><span class="n">group_ops</span><span class="p">,</span> <span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
        <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">NORMALIZE_WHITESPACE</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">REPORT_NDIFF</span><span class="p">)</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

              </div>
              
              
              <div class='prev-next-bottom'>
                

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright .<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>