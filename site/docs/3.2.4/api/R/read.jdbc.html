<!DOCTYPE html><html><head><title>R: Create a SparkDataFrame representing the database table...</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R.css" />

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.3/styles/github.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.3/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.3/languages/r.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</head><body><div class="container">

<table style="width: 100%;"><tr><td>read.jdbc {SparkR}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Create a SparkDataFrame representing the database table accessible via JDBC URL</h2>

<h3>Description</h3>

<p>Additional JDBC database connection properties can be set (...)
You can find the JDBC-specific option and parameter documentation for reading tables via JDBC in
<a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option</a>
Data Source Option in the version you use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.jdbc(
  url,
  tableName,
  partitionColumn = NULL,
  lowerBound = NULL,
  upperBound = NULL,
  numPartitions = 0L,
  predicates = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;"><td><code>url</code></td>
<td>
<p>JDBC database url of the form <code>jdbc:subprotocol:subname</code></p>
</td></tr>
<tr style="vertical-align: top;"><td><code>tableName</code></td>
<td>
<p>the name of the table in the external database</p>
</td></tr>
<tr style="vertical-align: top;"><td><code>partitionColumn</code></td>
<td>
<p>the name of a column of numeric, date, or timestamp type
that will be used for partitioning.</p>
</td></tr>
<tr style="vertical-align: top;"><td><code>lowerBound</code></td>
<td>
<p>the minimum value of <code>partitionColumn</code> used to decide partition stride</p>
</td></tr>
<tr style="vertical-align: top;"><td><code>upperBound</code></td>
<td>
<p>the maximum value of <code>partitionColumn</code> used to decide partition stride</p>
</td></tr>
<tr style="vertical-align: top;"><td><code>numPartitions</code></td>
<td>
<p>the number of partitions, This, along with <code>lowerBound</code> (inclusive),
<code>upperBound</code> (exclusive), form partition strides for generated WHERE
clause expressions used to split the column <code>partitionColumn</code> evenly.
This defaults to SparkContext.defaultParallelism when unset.</p>
</td></tr>
<tr style="vertical-align: top;"><td><code>predicates</code></td>
<td>
<p>a list of conditions in the where clause; each one defines one partition</p>
</td></tr>
<tr style="vertical-align: top;"><td><code>...</code></td>
<td>
<p>additional JDBC database connection named properties.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only one of partitionColumn or predicates should be set. Partitions of the table will be
retrieved in parallel based on the <code>numPartitions</code> or by the predicates.
</p>
<p>Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
your external database systems.
</p>


<h3>Value</h3>

<p>SparkDataFrame
</p>


<h3>Note</h3>

<p>read.jdbc since 2.0.0
</p>


<h3>Examples</h3>

<pre><code class="r">

## Not run: 
##D sparkR.session()
##D jdbcUrl <- "jdbc:mysql://localhost:3306/databasename"
##D df <- read.jdbc(jdbcUrl, "table", predicates = list("field<=123"), user = "username")
##D df2 <- read.jdbc(jdbcUrl, "table2", partitionColumn = "index", lowerBound = 0,
##D                  upperBound = 10000, user = "username", password = "password")
## End(Not run)



</code></pre>

<hr /><div style="text-align: center;">[Package <em>SparkR</em> version 3.2.4 <a href="00Index.html">Index</a>]</div>
</div>
</body></html>
