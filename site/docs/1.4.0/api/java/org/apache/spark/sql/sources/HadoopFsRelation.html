<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (version 1.7.0_60) on Wed Jun 10 22:11:16 PDT 2015 -->
<title>HadoopFsRelation (Spark 1.4.0 JavaDoc)</title>
<meta name="date" content="2015-06-10">
<link rel="stylesheet" type="text/css" href="../../../../../stylesheet.css" title="Style">
</head>
<body>
<script type="text/javascript"><!--
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="HadoopFsRelation (Spark 1.4.0 JavaDoc)";
    }
//-->
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar_top">
<!--   -->
</a><a href="#skip-navbar_top" title="Skip navigation links"></a><a name="navbar_top_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../../index-all.html">Index</a></li>
<li><a href="../../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../../org/apache/spark/sql/sources/GreaterThanOrEqual.html" title="class in org.apache.spark.sql.sources"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelationProvider.html" title="interface in org.apache.spark.sql.sources"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../../index.html?org/apache/spark/sql/sources/HadoopFsRelation.html" target="_top">Frames</a></li>
<li><a href="HadoopFsRelation.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.apache.spark.sql.sources</div>
<h2 title="Class HadoopFsRelation" class="title">Class HadoopFsRelation</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li>Object</li>
<li>
<ul class="inheritance">
<li><a href="../../../../../org/apache/spark/sql/sources/BaseRelation.html" title="class in org.apache.spark.sql.sources">org.apache.spark.sql.sources.BaseRelation</a></li>
<li>
<ul class="inheritance">
<li>org.apache.spark.sql.sources.HadoopFsRelation</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<hr>
<br>
<pre>public abstract class <span class="strong">HadoopFsRelation</span>
extends <a href="../../../../../org/apache/spark/sql/sources/BaseRelation.html" title="class in org.apache.spark.sql.sources">BaseRelation</a></pre>
<div class="block">::Experimental::
 A <a href="../../../../../org/apache/spark/sql/sources/BaseRelation.html" title="class in org.apache.spark.sql.sources"><code>BaseRelation</code></a> that provides much of the common code required for formats that store their
 data to an HDFS compatible filesystem.
 <p>
 For the read path, similar to <a href="../../../../../org/apache/spark/sql/sources/PrunedFilteredScan.html" title="interface in org.apache.spark.sql.sources"><code>PrunedFilteredScan</code></a>, it can eliminate unneeded columns and
 filter using selected predicates before producing an RDD containing all matching tuples as
 <code>Row</code> objects. In addition, when reading from Hive style partitioned tables stored in file
 systems, it's able to discover partitioning information from the paths of input directories, and
 perform partition pruning before start reading the data. Subclasses of <a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#HadoopFsRelation()"><code>HadoopFsRelation()</code></a>
 must override one of the three <code>buildScan</code> methods to implement the read path.
 <p>
 For the write path, it provides the ability to write to both non-partitioned and partitioned
 tables.  Directory layout of the partitioned tables is compatible with Hive.
 <p></div>
<dl><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_summary">
<!--   -->
</a>
<h3>Constructor Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Constructor Summary table, listing constructors, and an explanation">
<caption><span>Constructors</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Constructor and Description</th>
</tr>
<tr class="altColor">
<td class="colOne"><code><strong><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#HadoopFsRelation()">HadoopFsRelation</a></strong>()</code>&nbsp;</td>
</tr>
</table>
</li>
</ul>
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method_summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span>Methods</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;<a href="../../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#buildScan(org.apache.hadoop.fs.FileStatus[])">buildScan</a></strong>(org.apache.hadoop.fs.FileStatus[]&nbsp;inputFiles)</code>
<div class="block">For a non-partitioned relation, this method builds an <code>RDD[Row]</code> containing all rows within
 this relation.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;<a href="../../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#buildScan(java.lang.String[],%20org.apache.hadoop.fs.FileStatus[])">buildScan</a></strong>(String[]&nbsp;requiredColumns,
         org.apache.hadoop.fs.FileStatus[]&nbsp;inputFiles)</code>
<div class="block">For a non-partitioned relation, this method builds an <code>RDD[Row]</code> containing all rows within
 this relation.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;<a href="../../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#buildScan(java.lang.String[],%20org.apache.spark.sql.sources.Filter[],%20org.apache.hadoop.fs.FileStatus[])">buildScan</a></strong>(String[]&nbsp;requiredColumns,
         <a href="../../../../../org/apache/spark/sql/sources/Filter.html" title="class in org.apache.spark.sql.sources">Filter</a>[]&nbsp;filters,
         org.apache.hadoop.fs.FileStatus[]&nbsp;inputFiles)</code>
<div class="block">For a non-partitioned relation, this method builds an <code>RDD[Row]</code> containing all rows within
 this relation.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>abstract <a href="../../../../../org/apache/spark/sql/types/StructType.html" title="class in org.apache.spark.sql.types">StructType</a></code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#dataSchema()">dataSchema</a></strong>()</code>
<div class="block">Specifies schema of actual data files.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../../org/apache/spark/sql/types/StructType.html" title="class in org.apache.spark.sql.types">StructType</a></code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#partitionColumns()">partitionColumns</a></strong>()</code>
<div class="block">Partition columns.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>abstract String[]</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#paths()">paths</a></strong>()</code>
<div class="block">Base paths of this relation.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>abstract <a href="../../../../../org/apache/spark/sql/sources/OutputWriterFactory.html" title="class in org.apache.spark.sql.sources">OutputWriterFactory</a></code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#prepareJobForWrite(org.apache.hadoop.mapreduce.Job)">prepareJobForWrite</a></strong>(org.apache.hadoop.mapreduce.Job&nbsp;job)</code>
<div class="block">Prepares a write job and returns an <a href="../../../../../org/apache/spark/sql/sources/OutputWriterFactory.html" title="class in org.apache.spark.sql.sources"><code>OutputWriterFactory</code></a>.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/apache/spark/sql/types/StructType.html" title="class in org.apache.spark.sql.types">StructType</a></code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#schema()">schema</a></strong>()</code>
<div class="block">Schema of this relation.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>scala.Option&lt;<a href="../../../../../org/apache/spark/sql/types/StructType.html" title="class in org.apache.spark.sql.types">StructType</a>&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelation.html#userDefinedPartitionColumns()">userDefinedPartitionColumns</a></strong>()</code>
<div class="block">Optional user defined partition columns.</div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_org.apache.spark.sql.sources.BaseRelation">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;org.apache.spark.sql.sources.<a href="../../../../../org/apache/spark/sql/sources/BaseRelation.html" title="class in org.apache.spark.sql.sources">BaseRelation</a></h3>
<code><a href="../../../../../org/apache/spark/sql/sources/BaseRelation.html#needConversion()">needConversion</a>, <a href="../../../../../org/apache/spark/sql/sources/BaseRelation.html#sizeInBytes()">sizeInBytes</a>, <a href="../../../../../org/apache/spark/sql/sources/BaseRelation.html#sqlContext()">sqlContext</a></code></li>
</ul>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;Object</h3>
<code>equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_detail">
<!--   -->
</a>
<h3>Constructor Detail</h3>
<a name="HadoopFsRelation()">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>HadoopFsRelation</h4>
<pre>public&nbsp;HadoopFsRelation()</pre>
</li>
</ul>
</li>
</ul>
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method_detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="paths()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>paths</h4>
<pre>public abstract&nbsp;String[]&nbsp;paths()</pre>
<div class="block">Base paths of this relation.  For partitioned relations, it should be either root directories
 of all partition directories.
 <p></div>
<dl><dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="partitionColumns()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>partitionColumns</h4>
<pre>public final&nbsp;<a href="../../../../../org/apache/spark/sql/types/StructType.html" title="class in org.apache.spark.sql.types">StructType</a>&nbsp;partitionColumns()</pre>
<div class="block">Partition columns.  Can be either defined by <code>userDefinedPartitionColumns</code> or automatically
 discovered.  Note that they should always be nullable.
 <p></div>
<dl><dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="userDefinedPartitionColumns()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>userDefinedPartitionColumns</h4>
<pre>public&nbsp;scala.Option&lt;<a href="../../../../../org/apache/spark/sql/types/StructType.html" title="class in org.apache.spark.sql.types">StructType</a>&gt;&nbsp;userDefinedPartitionColumns()</pre>
<div class="block">Optional user defined partition columns.
 <p></div>
<dl><dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="schema()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>schema</h4>
<pre>public&nbsp;<a href="../../../../../org/apache/spark/sql/types/StructType.html" title="class in org.apache.spark.sql.types">StructType</a>&nbsp;schema()</pre>
<div class="block">Schema of this relation.  It consists of columns appearing in <code>dataSchema</code> and all partition
 columns not appearing in <code>dataSchema</code>.
 <p></div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../../org/apache/spark/sql/sources/BaseRelation.html#schema()">schema</a></code>&nbsp;in class&nbsp;<code><a href="../../../../../org/apache/spark/sql/sources/BaseRelation.html" title="class in org.apache.spark.sql.sources">BaseRelation</a></code></dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="dataSchema()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>dataSchema</h4>
<pre>public abstract&nbsp;<a href="../../../../../org/apache/spark/sql/types/StructType.html" title="class in org.apache.spark.sql.types">StructType</a>&nbsp;dataSchema()</pre>
<div class="block">Specifies schema of actual data files.  For partitioned relations, if one or more partitioned
 columns are contained in the data files, they should also appear in <code>dataSchema</code>.
 <p></div>
<dl><dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="buildScan(org.apache.hadoop.fs.FileStatus[])">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>buildScan</h4>
<pre>public&nbsp;<a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;<a href="../../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</a>&gt;&nbsp;buildScan(org.apache.hadoop.fs.FileStatus[]&nbsp;inputFiles)</pre>
<div class="block">For a non-partitioned relation, this method builds an <code>RDD[Row]</code> containing all rows within
 this relation. For partitioned relations, this method is called for each selected partition,
 and builds an <code>RDD[Row]</code> containing all rows within that single partition.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>inputFiles</code> - For a non-partitioned relation, it contains paths of all data files in the
        relation. For a partitioned relation, it contains paths of all data files in a single
        selected partition.
 <p></dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="buildScan(java.lang.String[], org.apache.hadoop.fs.FileStatus[])">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>buildScan</h4>
<pre>public&nbsp;<a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;<a href="../../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</a>&gt;&nbsp;buildScan(String[]&nbsp;requiredColumns,
                 org.apache.hadoop.fs.FileStatus[]&nbsp;inputFiles)</pre>
<div class="block">For a non-partitioned relation, this method builds an <code>RDD[Row]</code> containing all rows within
 this relation. For partitioned relations, this method is called for each selected partition,
 and builds an <code>RDD[Row]</code> containing all rows within that single partition.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>requiredColumns</code> - Required columns.</dd><dd><code>inputFiles</code> - For a non-partitioned relation, it contains paths of all data files in the
        relation. For a partitioned relation, it contains paths of all data files in a single
        selected partition.
 <p></dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="buildScan(java.lang.String[], org.apache.spark.sql.sources.Filter[], org.apache.hadoop.fs.FileStatus[])">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>buildScan</h4>
<pre>public&nbsp;<a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;<a href="../../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</a>&gt;&nbsp;buildScan(String[]&nbsp;requiredColumns,
                 <a href="../../../../../org/apache/spark/sql/sources/Filter.html" title="class in org.apache.spark.sql.sources">Filter</a>[]&nbsp;filters,
                 org.apache.hadoop.fs.FileStatus[]&nbsp;inputFiles)</pre>
<div class="block">For a non-partitioned relation, this method builds an <code>RDD[Row]</code> containing all rows within
 this relation. For partitioned relations, this method is called for each selected partition,
 and builds an <code>RDD[Row]</code> containing all rows within that single partition.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>requiredColumns</code> - Required columns.</dd><dd><code>filters</code> - Candidate filters to be pushed down. The actual filter should be the conjunction
        of all <code>filters</code>.  The pushed down filters are currently purely an optimization as they
        will all be evaluated again. This means it is safe to use them with methods that produce
        false positives such as filtering partitions based on a bloom filter.</dd><dd><code>inputFiles</code> - For a non-partitioned relation, it contains paths of all data files in the
        relation. For a partitioned relation, it contains paths of all data files in a single
        selected partition.
 <p></dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
<a name="prepareJobForWrite(org.apache.hadoop.mapreduce.Job)">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>prepareJobForWrite</h4>
<pre>public abstract&nbsp;<a href="../../../../../org/apache/spark/sql/sources/OutputWriterFactory.html" title="class in org.apache.spark.sql.sources">OutputWriterFactory</a>&nbsp;prepareJobForWrite(org.apache.hadoop.mapreduce.Job&nbsp;job)</pre>
<div class="block">Prepares a write job and returns an <a href="../../../../../org/apache/spark/sql/sources/OutputWriterFactory.html" title="class in org.apache.spark.sql.sources"><code>OutputWriterFactory</code></a>.  Client side job preparation can
 be put here.  For example, user defined output committer can be configured here
 by setting the output committer class in the conf of spark.sql.sources.outputCommitterClass.
 <p>
 Note that the only side effect expected here is mutating <code>job</code> via its setters.  Especially,
 Spark SQL caches <a href="../../../../../org/apache/spark/sql/sources/BaseRelation.html" title="class in org.apache.spark.sql.sources"><code>BaseRelation</code></a> instances for performance, mutating relation internal states
 may cause unexpected behaviors.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>job</code> - (undocumented)</dd>
<dt><span class="strong">Returns:</span></dt><dd>(undocumented)</dd><dt><span class="strong">Since:</span></dt>
  <dd>1.4.0</dd></dl>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar_bottom">
<!--   -->
</a><a href="#skip-navbar_bottom" title="Skip navigation links"></a><a name="navbar_bottom_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../../index-all.html">Index</a></li>
<li><a href="../../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../../org/apache/spark/sql/sources/GreaterThanOrEqual.html" title="class in org.apache.spark.sql.sources"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../../org/apache/spark/sql/sources/HadoopFsRelationProvider.html" title="interface in org.apache.spark.sql.sources"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../../index.html?org/apache/spark/sql/sources/HadoopFsRelation.html" target="_top">Frames</a></li>
<li><a href="HadoopFsRelation.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
</body>
</html>
