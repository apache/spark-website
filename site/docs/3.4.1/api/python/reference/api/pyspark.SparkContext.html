
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>pyspark.SparkContext &#8212; PySpark 3.4.1 documentation</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/pyspark.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="canonical" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="pyspark.RDD" href="pyspark.RDD.html" />
    <link rel="prev" title="Spark Core" href="../pyspark.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    <!-- Matomo -->
    <script type="text/javascript">
        var _paq = window._paq = window._paq || [];
        /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
        _paq.push(["disableCookies"]);
        _paq.push(['trackPageView']);
        _paq.push(['enableLinkTracking']);
        (function() {
            var u="https://analytics.apache.org/";
            _paq.push(['setTrackerUrl', u+'matomo.php']);
            _paq.push(['setSiteId', '40']);
            var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
            g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
        })();
    </script>
    <!-- End Matomo Code -->
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../index.html">
    
      <img src="../../_static/spark-logo-reverse.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../index.html">Overview</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../getting_started/index.html">Getting Started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../user_guide/index.html">User Guides</a>
        </li>
        
        <li class="nav-item active">
            <a class="nav-link" href="../index.html">API Reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../development/index.html">Development</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../migration_guide/index.html">Migration Guides</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
        
          
            
                <li class="">
                    <a href="../pyspark.sql/index.html">Spark SQL</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.pandas/index.html">Pandas API on Spark</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.ss/index.html">Structured Streaming</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.ml.html">MLlib (DataFrame-based)</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.streaming.html">Spark Streaming (Legacy)</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.mllib.html">MLlib (RDD-based)</a>
                </li>
            
          
            
                <li class="active">
                    <a href="../pyspark.html">Spark Core</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.resource.html">Resource Management</a>
                </li>
            
          
            
                <li class="">
                    <a href="../pyspark.errors.html">Errors</a>
                </li>
            
          
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="pyspark-sparkcontext">
<h1>pyspark.SparkContext<a class="headerlink" href="#pyspark-sparkcontext" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt id="pyspark.SparkContext">
<em class="property">class </em><code class="sig-prename descclassname">pyspark.</code><code class="sig-name descname">SparkContext</code><span class="sig-paren">(</span><em class="sig-param">master: Optional[str] = None</em>, <em class="sig-param">appName: Optional[str] = None</em>, <em class="sig-param">sparkHome: Optional[str] = None</em>, <em class="sig-param">pyFiles: Optional[List[str]] = None</em>, <em class="sig-param">environment: Optional[Dict[str</em>, <em class="sig-param">Any]] = None</em>, <em class="sig-param">batchSize: int = 0</em>, <em class="sig-param">serializer: pyspark.serializers.Serializer = CloudPickleSerializer()</em>, <em class="sig-param">conf: Optional[pyspark.conf.SparkConf] = None</em>, <em class="sig-param">gateway: Optional[py4j.java_gateway.JavaGateway] = None</em>, <em class="sig-param">jsc: Optional[py4j.java_gateway.JavaObject] = None</em>, <em class="sig-param">profiler_cls: Type[pyspark.profiler.BasicProfiler] = &lt;class 'pyspark.profiler.BasicProfiler'&gt;</em>, <em class="sig-param">udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = &lt;class 'pyspark.profiler.UDFBasicProfiler'&gt;</em>, <em class="sig-param">memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = &lt;class 'pyspark.profiler.MemoryProfiler'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/pyspark/context.html#SparkContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.SparkContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Main entry point for Spark functionality. A SparkContext represents the
connection to a Spark cluster, and can be used to create <a class="reference internal" href="pyspark.RDD.html#pyspark.RDD" title="pyspark.RDD"><code class="xref py py-class docutils literal notranslate"><span class="pre">RDD</span></code></a> and
broadcast variables on that cluster.</p>
<p>When you create a new SparkContext, at least the master and app name should
be set, either through the named parameters here or through <cite>conf</cite>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>master</strong><span class="classifier">str, optional</span></dt><dd><p>Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).</p>
</dd>
<dt><strong>appName</strong><span class="classifier">str, optional</span></dt><dd><p>A name for your job, to display on the cluster web UI.</p>
</dd>
<dt><strong>sparkHome</strong><span class="classifier">str, optional</span></dt><dd><p>Location where Spark is installed on cluster nodes.</p>
</dd>
<dt><strong>pyFiles</strong><span class="classifier">list, optional</span></dt><dd><p>Collection of .zip or .py files to send to the cluster
and add to PYTHONPATH.  These can be paths on the local file
system or HDFS, HTTP, HTTPS, or FTP URLs.</p>
</dd>
<dt><strong>environment</strong><span class="classifier">dict, optional</span></dt><dd><p>A dictionary of environment variables to set on
worker nodes.</p>
</dd>
<dt><strong>batchSize</strong><span class="classifier">int, optional, default 0</span></dt><dd><p>The number of Python objects represented as a single
Java object. Set 1 to disable batching, 0 to automatically choose
the batch size based on object sizes, or -1 to use an unlimited
batch size</p>
</dd>
<dt><strong>serializer</strong><span class="classifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">Serializer</span></code>, optional, default <code class="xref py py-class docutils literal notranslate"><span class="pre">CPickleSerializer</span></code></span></dt><dd><p>The serializer for RDDs.</p>
</dd>
<dt><strong>conf</strong><span class="classifier"><a class="reference internal" href="pyspark.SparkConf.html#pyspark.SparkConf" title="pyspark.SparkConf"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkConf</span></code></a>, optional</span></dt><dd><p>An object setting Spark properties.</p>
</dd>
<dt><strong>gateway</strong><span class="classifier">class:<cite>py4j.java_gateway.JavaGateway</cite>,  optional</span></dt><dd><p>Use an existing gateway and JVM, otherwise a new JVM
will be instantiated. This is only used internally.</p>
</dd>
<dt><strong>jsc</strong><span class="classifier">class:<cite>py4j.java_gateway.JavaObject</cite>, optional</span></dt><dd><p>The JavaSparkContext instance. This is only used internally.</p>
</dd>
<dt><strong>profiler_cls</strong><span class="classifier">type, optional, default <code class="xref py py-class docutils literal notranslate"><span class="pre">BasicProfiler</span></code></span></dt><dd><p>A class of custom Profiler used to do profiling</p>
</dd>
<dt><strong>udf_profiler_cls</strong><span class="classifier">type, optional, default <code class="xref py py-class docutils literal notranslate"><span class="pre">UDFBasicProfiler</span></code></span></dt><dd><p>A class of custom Profiler used to do udf profiling</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Only one <a class="reference internal" href="#pyspark.SparkContext" title="pyspark.SparkContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code></a> should be active per JVM. You must <cite>stop()</cite>
the active <a class="reference internal" href="#pyspark.SparkContext" title="pyspark.SparkContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code></a> before creating a new one.</p>
<p><a class="reference internal" href="#pyspark.SparkContext" title="pyspark.SparkContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code></a> instance is not supported to share across multiple
processes out of the box, and PySpark does not guarantee multi-processing execution.
Use threads instead for concurrent processing purpose.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.context</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="s1">&#39;local&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sc2</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="s1">&#39;local&#39;</span><span class="p">,</span> <span class="s1">&#39;test2&#39;</span><span class="p">)</span> 
<span class="gt">Traceback (most recent call last):</span>
<span class="w">    </span><span class="o">...</span>
<span class="gr">ValueError</span>: <span class="n">...</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.accumulator.html#pyspark.SparkContext.accumulator" title="pyspark.SparkContext.accumulator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">accumulator</span></code></a>(value[, accum_param])</p></td>
<td><p>Create an <a class="reference internal" href="pyspark.Accumulator.html#pyspark.Accumulator" title="pyspark.Accumulator"><code class="xref py py-class docutils literal notranslate"><span class="pre">Accumulator</span></code></a> with the given initial value, using a given <a class="reference internal" href="pyspark.AccumulatorParam.html#pyspark.AccumulatorParam" title="pyspark.AccumulatorParam"><code class="xref py py-class docutils literal notranslate"><span class="pre">AccumulatorParam</span></code></a> helper object to define how to add values of the data type if provided.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.addArchive.html#pyspark.SparkContext.addArchive" title="pyspark.SparkContext.addArchive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addArchive</span></code></a>(path)</p></td>
<td><p>Add an archive to be downloaded with this Spark job on every node.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.addFile.html#pyspark.SparkContext.addFile" title="pyspark.SparkContext.addFile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addFile</span></code></a>(path[, recursive])</p></td>
<td><p>Add a file to be downloaded with this Spark job on every node.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.addPyFile.html#pyspark.SparkContext.addPyFile" title="pyspark.SparkContext.addPyFile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addPyFile</span></code></a>(path)</p></td>
<td><p>Add a .py or .zip dependency for all tasks to be executed on this SparkContext in the future.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.binaryFiles.html#pyspark.SparkContext.binaryFiles" title="pyspark.SparkContext.binaryFiles"><code class="xref py py-obj docutils literal notranslate"><span class="pre">binaryFiles</span></code></a>(path[, minPartitions])</p></td>
<td><p>Read a directory of binary files from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI as a byte array.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.binaryRecords.html#pyspark.SparkContext.binaryRecords" title="pyspark.SparkContext.binaryRecords"><code class="xref py py-obj docutils literal notranslate"><span class="pre">binaryRecords</span></code></a>(path, recordLength)</p></td>
<td><p>Load data from a flat binary file, assuming each record is a set of numbers with the specified numerical format (see ByteBuffer), and the number of bytes per record is constant.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.broadcast.html#pyspark.SparkContext.broadcast" title="pyspark.SparkContext.broadcast"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast</span></code></a>(value)</p></td>
<td><p>Broadcast a read-only variable to the cluster, returning a <a class="reference internal" href="pyspark.Broadcast.html#pyspark.Broadcast" title="pyspark.Broadcast"><code class="xref py py-class docutils literal notranslate"><span class="pre">Broadcast</span></code></a> object for reading it in distributed functions.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.cancelAllJobs.html#pyspark.SparkContext.cancelAllJobs" title="pyspark.SparkContext.cancelAllJobs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cancelAllJobs</span></code></a>()</p></td>
<td><p>Cancel all jobs that have been scheduled or are running.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.cancelJobGroup.html#pyspark.SparkContext.cancelJobGroup" title="pyspark.SparkContext.cancelJobGroup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cancelJobGroup</span></code></a>(groupId)</p></td>
<td><p>Cancel active jobs for the specified group.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.dump_profiles.html#pyspark.SparkContext.dump_profiles" title="pyspark.SparkContext.dump_profiles"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dump_profiles</span></code></a>(path)</p></td>
<td><p>Dump the profile stats into directory <cite>path</cite></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.emptyRDD.html#pyspark.SparkContext.emptyRDD" title="pyspark.SparkContext.emptyRDD"><code class="xref py py-obj docutils literal notranslate"><span class="pre">emptyRDD</span></code></a>()</p></td>
<td><p>Create an <a class="reference internal" href="pyspark.RDD.html#pyspark.RDD" title="pyspark.RDD"><code class="xref py py-class docutils literal notranslate"><span class="pre">RDD</span></code></a> that has no partitions or elements.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.getCheckpointDir.html#pyspark.SparkContext.getCheckpointDir" title="pyspark.SparkContext.getCheckpointDir"><code class="xref py py-obj docutils literal notranslate"><span class="pre">getCheckpointDir</span></code></a>()</p></td>
<td><p>Return the directory where RDDs are checkpointed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.getConf.html#pyspark.SparkContext.getConf" title="pyspark.SparkContext.getConf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">getConf</span></code></a>()</p></td>
<td><p>Return a copy of this SparkContext’s configuration <a class="reference internal" href="pyspark.SparkConf.html#pyspark.SparkConf" title="pyspark.SparkConf"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkConf</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.getLocalProperty.html#pyspark.SparkContext.getLocalProperty" title="pyspark.SparkContext.getLocalProperty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">getLocalProperty</span></code></a>(key)</p></td>
<td><p>Get a local property set in this thread, or null if it is missing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.getOrCreate.html#pyspark.SparkContext.getOrCreate" title="pyspark.SparkContext.getOrCreate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">getOrCreate</span></code></a>([conf])</p></td>
<td><p>Get or instantiate a <a class="reference internal" href="#pyspark.SparkContext" title="pyspark.SparkContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code></a> and register it as a singleton object.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.hadoopFile.html#pyspark.SparkContext.hadoopFile" title="pyspark.SparkContext.hadoopFile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hadoopFile</span></code></a>(path, inputFormatClass, keyClass, …)</p></td>
<td><p>Read an ‘old’ Hadoop InputFormat with arbitrary key and value class from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.hadoopRDD.html#pyspark.SparkContext.hadoopRDD" title="pyspark.SparkContext.hadoopRDD"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hadoopRDD</span></code></a>(inputFormatClass, keyClass, valueClass)</p></td>
<td><p>Read an ‘old’ Hadoop InputFormat with arbitrary key and value class, from an arbitrary Hadoop configuration, which is passed in as a Python dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.newAPIHadoopFile.html#pyspark.SparkContext.newAPIHadoopFile" title="pyspark.SparkContext.newAPIHadoopFile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">newAPIHadoopFile</span></code></a>(path, inputFormatClass, …)</p></td>
<td><p>Read a ‘new API’ Hadoop InputFormat with arbitrary key and value class from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.newAPIHadoopRDD.html#pyspark.SparkContext.newAPIHadoopRDD" title="pyspark.SparkContext.newAPIHadoopRDD"><code class="xref py py-obj docutils literal notranslate"><span class="pre">newAPIHadoopRDD</span></code></a>(inputFormatClass, keyClass, …)</p></td>
<td><p>Read a ‘new API’ Hadoop InputFormat with arbitrary key and value class, from an arbitrary Hadoop configuration, which is passed in as a Python dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.parallelize.html#pyspark.SparkContext.parallelize" title="pyspark.SparkContext.parallelize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parallelize</span></code></a>(c[, numSlices])</p></td>
<td><p>Distribute a local Python collection to form an RDD.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.pickleFile.html#pyspark.SparkContext.pickleFile" title="pyspark.SparkContext.pickleFile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pickleFile</span></code></a>(name[, minPartitions])</p></td>
<td><p>Load an RDD previously saved using <a class="reference internal" href="pyspark.RDD.saveAsPickleFile.html#pyspark.RDD.saveAsPickleFile" title="pyspark.RDD.saveAsPickleFile"><code class="xref py py-meth docutils literal notranslate"><span class="pre">RDD.saveAsPickleFile()</span></code></a> method.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.range.html#pyspark.SparkContext.range" title="pyspark.SparkContext.range"><code class="xref py py-obj docutils literal notranslate"><span class="pre">range</span></code></a>(start[, end, step, numSlices])</p></td>
<td><p>Create a new RDD of int containing elements from <cite>start</cite> to <cite>end</cite> (exclusive), increased by <cite>step</cite> every element.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.runJob.html#pyspark.SparkContext.runJob" title="pyspark.SparkContext.runJob"><code class="xref py py-obj docutils literal notranslate"><span class="pre">runJob</span></code></a>(rdd, partitionFunc[, partitions, …])</p></td>
<td><p>Executes the given partitionFunc on the specified set of partitions, returning the result as an array of elements.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.sequenceFile.html#pyspark.SparkContext.sequenceFile" title="pyspark.SparkContext.sequenceFile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sequenceFile</span></code></a>(path[, keyClass, valueClass, …])</p></td>
<td><p>Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.setCheckpointDir.html#pyspark.SparkContext.setCheckpointDir" title="pyspark.SparkContext.setCheckpointDir"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setCheckpointDir</span></code></a>(dirName)</p></td>
<td><p>Set the directory under which RDDs are going to be checkpointed.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.setJobDescription.html#pyspark.SparkContext.setJobDescription" title="pyspark.SparkContext.setJobDescription"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setJobDescription</span></code></a>(value)</p></td>
<td><p>Set a human readable description of the current job.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.setJobGroup.html#pyspark.SparkContext.setJobGroup" title="pyspark.SparkContext.setJobGroup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setJobGroup</span></code></a>(groupId, description[, …])</p></td>
<td><p>Assigns a group ID to all the jobs started by this thread until the group ID is set to a different value or cleared.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.setLocalProperty.html#pyspark.SparkContext.setLocalProperty" title="pyspark.SparkContext.setLocalProperty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setLocalProperty</span></code></a>(key, value)</p></td>
<td><p>Set a local property that affects jobs submitted from this thread, such as the Spark fair scheduler pool.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.setLogLevel.html#pyspark.SparkContext.setLogLevel" title="pyspark.SparkContext.setLogLevel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setLogLevel</span></code></a>(logLevel)</p></td>
<td><p>Control our logLevel.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.setSystemProperty.html#pyspark.SparkContext.setSystemProperty" title="pyspark.SparkContext.setSystemProperty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setSystemProperty</span></code></a>(key, value)</p></td>
<td><p>Set a Java system property, such as <cite>spark.executor.memory</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.show_profiles.html#pyspark.SparkContext.show_profiles" title="pyspark.SparkContext.show_profiles"><code class="xref py py-obj docutils literal notranslate"><span class="pre">show_profiles</span></code></a>()</p></td>
<td><p>Print the profile stats to stdout</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.sparkUser.html#pyspark.SparkContext.sparkUser" title="pyspark.SparkContext.sparkUser"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparkUser</span></code></a>()</p></td>
<td><p>Get SPARK_USER for user who is running SparkContext.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.statusTracker.html#pyspark.SparkContext.statusTracker" title="pyspark.SparkContext.statusTracker"><code class="xref py py-obj docutils literal notranslate"><span class="pre">statusTracker</span></code></a>()</p></td>
<td><p>Return <code class="xref py py-class docutils literal notranslate"><span class="pre">StatusTracker</span></code> object</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.stop.html#pyspark.SparkContext.stop" title="pyspark.SparkContext.stop"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stop</span></code></a>()</p></td>
<td><p>Shut down the <a class="reference internal" href="#pyspark.SparkContext" title="pyspark.SparkContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.textFile.html#pyspark.SparkContext.textFile" title="pyspark.SparkContext.textFile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">textFile</span></code></a>(name[, minPartitions, use_unicode])</p></td>
<td><p>Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.union.html#pyspark.SparkContext.union" title="pyspark.SparkContext.union"><code class="xref py py-obj docutils literal notranslate"><span class="pre">union</span></code></a>(rdds)</p></td>
<td><p>Build the union of a list of RDDs.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.wholeTextFiles.html#pyspark.SparkContext.wholeTextFiles" title="pyspark.SparkContext.wholeTextFiles"><code class="xref py py-obj docutils literal notranslate"><span class="pre">wholeTextFiles</span></code></a>(path[, minPartitions, …])</p></td>
<td><p>Read a directory of text files from HDFS, a local file system (available on all nodes), or any  Hadoop-supported file system URI.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.PACKAGE_EXTENSIONS.html#pyspark.SparkContext.PACKAGE_EXTENSIONS" title="pyspark.SparkContext.PACKAGE_EXTENSIONS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PACKAGE_EXTENSIONS</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.applicationId.html#pyspark.SparkContext.applicationId" title="pyspark.SparkContext.applicationId"><code class="xref py py-obj docutils literal notranslate"><span class="pre">applicationId</span></code></a></p></td>
<td><p>A unique identifier for the Spark application.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.defaultMinPartitions.html#pyspark.SparkContext.defaultMinPartitions" title="pyspark.SparkContext.defaultMinPartitions"><code class="xref py py-obj docutils literal notranslate"><span class="pre">defaultMinPartitions</span></code></a></p></td>
<td><p>Default min number of partitions for Hadoop RDDs when not given by user</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.defaultParallelism.html#pyspark.SparkContext.defaultParallelism" title="pyspark.SparkContext.defaultParallelism"><code class="xref py py-obj docutils literal notranslate"><span class="pre">defaultParallelism</span></code></a></p></td>
<td><p>Default level of parallelism to use when not given by user (e.g.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.listArchives.html#pyspark.SparkContext.listArchives" title="pyspark.SparkContext.listArchives"><code class="xref py py-obj docutils literal notranslate"><span class="pre">listArchives</span></code></a></p></td>
<td><p>Returns a list of archive paths that are added to resources.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.listFiles.html#pyspark.SparkContext.listFiles" title="pyspark.SparkContext.listFiles"><code class="xref py py-obj docutils literal notranslate"><span class="pre">listFiles</span></code></a></p></td>
<td><p>Returns a list of file paths that are added to resources.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.resources.html#pyspark.SparkContext.resources" title="pyspark.SparkContext.resources"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resources</span></code></a></p></td>
<td><p>Return the resource information of this <a class="reference internal" href="#pyspark.SparkContext" title="pyspark.SparkContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.startTime.html#pyspark.SparkContext.startTime" title="pyspark.SparkContext.startTime"><code class="xref py py-obj docutils literal notranslate"><span class="pre">startTime</span></code></a></p></td>
<td><p>Return the epoch time when the <a class="reference internal" href="#pyspark.SparkContext" title="pyspark.SparkContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code></a> was started.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pyspark.SparkContext.uiWebUrl.html#pyspark.SparkContext.uiWebUrl" title="pyspark.SparkContext.uiWebUrl"><code class="xref py py-obj docutils literal notranslate"><span class="pre">uiWebUrl</span></code></a></p></td>
<td><p>Return the URL of the SparkUI instance started by this <a class="reference internal" href="#pyspark.SparkContext" title="pyspark.SparkContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparkContext</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pyspark.SparkContext.version.html#pyspark.SparkContext.version" title="pyspark.SparkContext.version"><code class="xref py py-obj docutils literal notranslate"><span class="pre">version</span></code></a></p></td>
<td><p>The version of Spark on which this application is running.</p></td>
</tr>
</tbody>
</table>
</dd></dl>

</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="../pyspark.html" title="previous page">Spark Core</a>
    <a class='right-next' id="next-link" href="pyspark.RDD.html" title="next page">pyspark.RDD</a>

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright .<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>