
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>pyspark.ml.torch.distributor &#8212; PySpark 3.4.1 documentation</title>
    
  <link rel="stylesheet" href="../../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/pyspark.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../../../index.html">
    
      <img src="../../../../_static/spark-logo-reverse.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../index.html">Overview</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../getting_started/index.html">Getting Started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../user_guide/index.html">User Guides</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../reference/index.html">API Reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../development/index.html">Development</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../migration_guide/index.html">Migration Guides</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for pyspark.ml.torch.distributor</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>

<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">textwrap</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Generator</span>

<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">cloudpickle</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.torch.log_communication</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
    <span class="n">get_driver_host</span><span class="p">,</span>
    <span class="n">LogStreamingClient</span><span class="p">,</span>
    <span class="n">LogStreamingServer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">pyspark.context</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.taskcontext</span> <span class="kn">import</span> <span class="n">BarrierTaskContext</span>


<span class="c1"># TODO(SPARK-41589): will move the functions and tests to an external file</span>
<span class="c1">#       once we are in agreement about which functions should be in utils.py</span>
<span class="k">def</span> <span class="nf">get_conf_boolean</span><span class="p">(</span><span class="n">sc</span><span class="p">:</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">default_value</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the conf &quot;key&quot; from the given spark context,</span>
<span class="sd">    or return the default value if the conf is not set.</span>
<span class="sd">    This expects the conf value to be a boolean or string;</span>
<span class="sd">    if the value is a string, this checks for all capitalization</span>
<span class="sd">    patterns of &quot;true&quot; and &quot;false&quot; to match Scala.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    sc : :class:`SparkContext`</span>
<span class="sd">        The :class:`SparkContext` for the distributor.</span>
<span class="sd">    key : str</span>
<span class="sd">        string for conf name</span>
<span class="sd">    default_value : str</span>
<span class="sd">        default value for the conf value for the given key</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    bool</span>
<span class="sd">        Returns the boolean value that corresponds to the conf</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    ValueError</span>
<span class="sd">        Thrown when the conf value is not a valid boolean</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">val</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">getConf</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">default_value</span><span class="p">)</span>
    <span class="n">lowercase_val</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">lowercase_val</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">lowercase_val</span> <span class="o">==</span> <span class="s2">&quot;false&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;The conf value for &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; was expected to be a boolean &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;value but found value of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;with value: </span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">get_logger</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">logging</span><span class="o">.</span><span class="n">Logger</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets a logger by name, or creates and configures it for the first time.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># If the logger is configured, skip the configure</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">logger</span><span class="o">.</span><span class="n">handlers</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">handlers</span><span class="p">:</span>
        <span class="n">handler</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">StreamHandler</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">addHandler</span><span class="p">(</span><span class="n">handler</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logger</span>


<span class="k">def</span> <span class="nf">get_gpus_owned</span><span class="p">(</span><span class="n">context</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">SparkContext</span><span class="p">,</span> <span class="n">BarrierTaskContext</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets the number of GPUs that Spark scheduled to the calling task.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    context : :class:`SparkContext` or :class:`BarrierTaskContext`</span>
<span class="sd">        The :class:`SparkContext` or :class:`BarrierTaskContext` that has GPUs available.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    list</span>
<span class="sd">        The correct mapping of addresses to workers.</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    ValueError</span>
<span class="sd">        Raised if the input addresses were not found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="o">=</span> <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span>
    <span class="n">pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">&quot;^[1-9][0-9]*|0$&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">SparkContext</span><span class="p">):</span>
        <span class="n">addresses</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">resources</span><span class="p">[</span><span class="s2">&quot;gpu&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">addresses</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">addresses</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">resources</span><span class="p">()[</span><span class="s2">&quot;gpu&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">addresses</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="n">pattern</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">address</span><span class="p">)</span> <span class="k">for</span> <span class="n">address</span> <span class="ow">in</span> <span class="n">addresses</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Found GPU addresses </span><span class="si">{</span><span class="n">addresses</span><span class="si">}</span><span class="s2"> which &quot;</span>
            <span class="s2">&quot;are not all in the correct format &quot;</span>
            <span class="s2">&quot;for CUDA_VISIBLE_DEVICES, which requires &quot;</span>
            <span class="s2">&quot;integers with no zero padding.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
        <span class="n">gpu_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">addresses</span><span class="p">))</span>
        <span class="n">gpu_list</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
        <span class="n">gpu_owned</span> <span class="o">=</span> <span class="p">[</span><span class="n">gpu_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">gpu_indices</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">gpu_owned</span>
    <span class="k">return</span> <span class="n">addresses</span>


<span class="k">class</span> <span class="nc">Distributor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The parent class for TorchDistributor. This class shouldn&#39;t be instantiated directly.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_processes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">local_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">use_gpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">get_logger</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">=</span> <span class="n">num_processes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_mode</span> <span class="o">=</span> <span class="n">local_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_gpu</span> <span class="o">=</span> <span class="n">use_gpu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">getActiveSession</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;An active SparkSession is required for the distributor.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_num_tasks</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ssl_conf</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_create_input_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">input_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">unneeded_param</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;spark&quot;</span><span class="p">,</span> <span class="s2">&quot;sc&quot;</span><span class="p">,</span> <span class="s2">&quot;ssl_conf&quot;</span><span class="p">,</span> <span class="s2">&quot;logger&quot;</span><span class="p">]:</span>
            <span class="k">del</span> <span class="n">input_params</span><span class="p">[</span><span class="n">unneeded_param</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">input_params</span>

    <span class="k">def</span> <span class="nf">_get_num_tasks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the number of Spark tasks to use for distributed training</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            The number of Spark tasks to use for distributed training</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        RuntimeError</span>
<span class="sd">            Raised when the SparkConf was misconfigured.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gpu</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_mode</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;spark.task.resource.gpu.amount&quot;</span>
                <span class="n">task_gpu_amount</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sc</span><span class="o">.</span><span class="n">getConf</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">task_gpu_amount</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; was unset, so gpu usage is unavailable.&quot;</span><span class="p">)</span>
                <span class="c1"># TODO(SPARK-41916): Address situation when spark.task.resource.gpu.amount &gt; 1</span>
                <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">/</span> <span class="n">task_gpu_amount</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;spark.driver.resource.gpu.amount&quot;</span>
                <span class="k">if</span> <span class="s2">&quot;gpu&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sc</span><span class="o">.</span><span class="n">resources</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;GPUs were unable to be found on the driver.&quot;</span><span class="p">)</span>
                <span class="n">num_available_gpus</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sc</span><span class="o">.</span><span class="n">getConf</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">num_available_gpus</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;GPU resources were not configured properly on the driver.&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">&gt;</span> <span class="n">num_available_gpus</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;&#39;num_processes&#39; cannot be set to a value greater than the number of &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;available GPUs on the driver, which is </span><span class="si">{</span><span class="n">num_available_gpus</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="s2">&quot;&#39;num_processes&#39; was reset to be equal to the number of available GPUs.&quot;</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">=</span> <span class="n">num_available_gpus</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span>

    <span class="k">def</span> <span class="nf">_validate_input_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;num_proccesses has to be a positive integer&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_encryption</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Checks to see if the user requires encrpytion of data.</span>
<span class="sd">        If required, throw an exception since we don&#39;t support that.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        RuntimeError</span>
<span class="sd">            Thrown when the user requires ssl encryption or when the user initializes</span>
<span class="sd">            the Distributor parent class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="s2">&quot;ssl_conf&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Distributor doesn&#39;t have this functionality. Use TorchDistributor instead.&quot;</span>
            <span class="p">)</span>
        <span class="n">is_ssl_enabled</span> <span class="o">=</span> <span class="n">get_conf_boolean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sc</span><span class="p">,</span> <span class="s2">&quot;spark.ssl.enabled&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span>
        <span class="n">ignore_ssl</span> <span class="o">=</span> <span class="n">get_conf_boolean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssl_conf</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="k">if</span> <span class="n">is_ssl_enabled</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="k">if</span> <span class="n">ignore_ssl</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                    This cluster has TLS encryption enabled;</span>
<span class="s2">                    however, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> does not</span>
<span class="s2">                    support data encryption in transit.</span>
<span class="s2">                    The Spark configuration</span>
<span class="s2">                    &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ssl_conf</span><span class="si">}</span><span class="s2">&#39; has been set to</span>
<span class="s2">                    &#39;true&#39; to override this</span>
<span class="s2">                    configuration and use </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> anyway. Please</span>
<span class="s2">                    note this will cause model</span>
<span class="s2">                    parameters and possibly training data to</span>
<span class="s2">                    be sent between nodes unencrypted.</span>
<span class="s2">                    &quot;&quot;&quot;</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="k">return</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                This cluster has TLS encryption enabled;</span>
<span class="s2">                however, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> does not support</span>
<span class="s2">                data encryption in transit. To override</span>
<span class="s2">                this configuration and use </span><span class="si">{</span><span class="n">name</span><span class="si">}</span>
<span class="s2">                anyway, you may set &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ssl_conf</span><span class="si">}</span><span class="s2">&#39;</span>
<span class="s2">                to &#39;true&#39; in the Spark configuration. Please note this</span>
<span class="s2">                will cause model parameters and possibly training</span>
<span class="s2">                data to be sent between nodes unencrypted.</span>
<span class="s2">                &quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>


<div class="viewcode-block" id="TorchDistributor"><a class="viewcode-back" href="../../../../reference/api/pyspark.ml.torch.distributor.TorchDistributor.html#pyspark.ml.torch.distributor.TorchDistributor">[docs]</a><span class="k">class</span> <span class="nc">TorchDistributor</span><span class="p">(</span><span class="n">Distributor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class to support distributed training on PyTorch and PyTorch Lightning using PySpark.</span>

<span class="sd">    .. versionadded:: 3.4.0</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    num_processes : int, optional</span>
<span class="sd">        An integer that determines how many different concurrent</span>
<span class="sd">        tasks are allowed. We expect spark.task.gpus = 1 for GPU-enabled training. Default</span>
<span class="sd">        should be 1; we don&#39;t want to invoke multiple cores/gpus without explicit mention.</span>
<span class="sd">    local_mode : bool, optional</span>
<span class="sd">        A boolean that determines whether we are using the driver</span>
<span class="sd">        node for training. Default should be false; we don&#39;t want to invoke executors without</span>
<span class="sd">        explicit mention.</span>
<span class="sd">    use_gpu : bool, optional</span>
<span class="sd">        A boolean that indicates whether or not we are doing training</span>
<span class="sd">        on the GPU. Note that there are differences in how GPU-enabled code looks like and</span>
<span class="sd">        how CPU-specific code looks like.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Run PyTorch Training locally on GPU (using a PyTorch native function)</span>

<span class="sd">    &gt;&gt;&gt; def train(learning_rate):</span>
<span class="sd">    ...     import torch.distributed</span>
<span class="sd">    ...     torch.distributed.init_process_group(backend=&quot;nccl&quot;)</span>
<span class="sd">    ...     # ...</span>
<span class="sd">    ...     torch.destroy_process_group()</span>
<span class="sd">    ...     return model # or anything else</span>
<span class="sd">    &gt;&gt;&gt; distributor = TorchDistributor(</span>
<span class="sd">    ...     num_processes=2,</span>
<span class="sd">    ...     local_mode=True,</span>
<span class="sd">    ...     use_gpu=True)</span>
<span class="sd">    &gt;&gt;&gt; model = distributor.run(train, 1e-3)</span>

<span class="sd">    Run PyTorch Training on GPU (using a file with PyTorch code)</span>

<span class="sd">    &gt;&gt;&gt; distributor = TorchDistributor(</span>
<span class="sd">    ...     num_processes=2,</span>
<span class="sd">    ...     local_mode=False,</span>
<span class="sd">    ...     use_gpu=True)</span>
<span class="sd">    &gt;&gt;&gt; distributor.run(&quot;/path/to/train.py&quot;, &quot;--learning-rate=1e-3&quot;)</span>

<span class="sd">    Run PyTorch Lightning Training on GPU</span>

<span class="sd">    &gt;&gt;&gt; num_proc = 2</span>
<span class="sd">    &gt;&gt;&gt; def train():</span>
<span class="sd">    ...     from pytorch_lightning import Trainer</span>
<span class="sd">    ...     # ...</span>
<span class="sd">    ...     # required to set devices = 1 and num_nodes = num_processes for multi node</span>
<span class="sd">    ...     # required to set devices = num_processes and num_nodes = 1 for single node multi GPU</span>
<span class="sd">    ...     trainer = Trainer(accelerator=&quot;gpu&quot;, devices=1, num_nodes=num_proc, strategy=&quot;ddp&quot;)</span>
<span class="sd">    ...     trainer.fit()</span>
<span class="sd">    ...     # ...</span>
<span class="sd">    ...     return trainer</span>
<span class="sd">    &gt;&gt;&gt; distributor = TorchDistributor(</span>
<span class="sd">    ...     num_processes=num_proc,</span>
<span class="sd">    ...     local_mode=True,</span>
<span class="sd">    ...     use_gpu=True)</span>
<span class="sd">    &gt;&gt;&gt; trainer = distributor.run(train)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_PICKLED_FUNC_FILE</span> <span class="o">=</span> <span class="s2">&quot;func.pickle&quot;</span>
    <span class="n">_TRAIN_FILE</span> <span class="o">=</span> <span class="s2">&quot;train.py&quot;</span>
    <span class="n">_PICKLED_OUTPUT_FILE</span> <span class="o">=</span> <span class="s2">&quot;output.pickle&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_processes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">local_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">use_gpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the distributor.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        num_processes : int, optional</span>
<span class="sd">            An integer that determines how many different concurrent</span>
<span class="sd">            tasks are allowed. We expect spark.task.gpus = 1 for GPU-enabled training. Default</span>
<span class="sd">            should be 1; we don&#39;t want to invoke multiple cores/gpus without explicit mention.</span>
<span class="sd">        local_mode : bool, optional</span>
<span class="sd">            A boolean that determines whether we are using the driver</span>
<span class="sd">            node for training. Default should be false; we don&#39;t want to invoke executors without</span>
<span class="sd">            explicit mention.</span>
<span class="sd">        use_gpu : bool, optional</span>
<span class="sd">            A boolean that indicates whether or not we are doing training</span>
<span class="sd">            on the GPU. Note that there are differences in how GPU-enabled code looks like and</span>
<span class="sd">            how CPU-specific code looks like.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If any of the parameters are incorrect.</span>
<span class="sd">        RuntimeError</span>
<span class="sd">            If an active SparkSession is unavailable.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_processes</span><span class="p">,</span> <span class="n">local_mode</span><span class="p">,</span> <span class="n">use_gpu</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ssl_conf</span> <span class="o">=</span> <span class="s2">&quot;pytorch.spark.distributor.ignoreSsl&quot;</span>  <span class="c1"># type: ignore</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_input_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_input_params</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_create_torchrun_command</span><span class="p">(</span>
        <span class="n">input_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">path_to_train_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">local_mode</span> <span class="o">=</span> <span class="n">input_params</span><span class="p">[</span><span class="s2">&quot;local_mode&quot;</span><span class="p">]</span>
        <span class="n">num_processes</span> <span class="o">=</span> <span class="n">input_params</span><span class="p">[</span><span class="s2">&quot;num_processes&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">local_mode</span><span class="p">:</span>
            <span class="n">torchrun_args</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;--standalone&quot;</span><span class="p">,</span> <span class="s2">&quot;--nnodes=1&quot;</span><span class="p">]</span>
            <span class="n">processes_per_node</span> <span class="o">=</span> <span class="n">num_processes</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">master_addr</span><span class="p">,</span> <span class="n">master_port</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">],</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">]</span>
            <span class="n">node_rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">]</span>
            <span class="n">torchrun_args</span> <span class="o">=</span> <span class="p">[</span>
                <span class="sa">f</span><span class="s2">&quot;--nnodes=</span><span class="si">{</span><span class="n">num_processes</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;--node_rank=</span><span class="si">{</span><span class="n">node_rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;--rdzv_endpoint=</span><span class="si">{</span><span class="n">master_addr</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">master_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="s2">&quot;--rdzv_id=0&quot;</span><span class="p">,</span>
            <span class="p">]</span>  <span class="c1"># TODO: setup random ID that is gleaned from env variables</span>
            <span class="n">processes_per_node</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">args_string</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">args</span><span class="p">))</span>  <span class="c1"># converting all args to strings</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span>
            <span class="s2">&quot;-m&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pyspark.ml.torch.torch_run_process_wrapper&quot;</span><span class="p">,</span>
            <span class="o">*</span><span class="n">torchrun_args</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--nproc_per_node=</span><span class="si">{</span><span class="n">processes_per_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">path_to_train_file</span><span class="p">,</span>
            <span class="o">*</span><span class="n">args_string</span><span class="p">,</span>
        <span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_execute_command</span><span class="p">(</span>
        <span class="n">cmd</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">_prctl</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">redirect_to_stdout</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">log_streaming_client</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogStreamingClient</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_TAIL_LINES_TO_KEEP</span> <span class="o">=</span> <span class="mi">100</span>

        <span class="n">task</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">Popen</span><span class="p">(</span>
            <span class="n">cmd</span><span class="p">,</span>
            <span class="n">stdout</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span>
            <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">STDOUT</span><span class="p">,</span>
            <span class="n">stdin</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span>
            <span class="n">env</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">task</span><span class="o">.</span><span class="n">stdin</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
        <span class="n">tail</span><span class="p">:</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">_TAIL_LINES_TO_KEEP</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">task</span><span class="o">.</span><span class="n">stdout</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">decoded</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
                <span class="n">tail</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoded</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">redirect_to_stdout</span><span class="p">:</span>
                    <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">decoded</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">log_streaming_client</span><span class="p">:</span>
                    <span class="n">log_streaming_client</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">decoded</span><span class="o">.</span><span class="n">rstrip</span><span class="p">())</span>
            <span class="n">task</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">task</span><span class="o">.</span><span class="n">poll</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">task</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>  <span class="c1"># SIGTERM</span>
                    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">task</span><span class="o">.</span><span class="n">poll</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">task</span><span class="o">.</span><span class="n">kill</span><span class="p">()</span>  <span class="c1"># SIGKILL</span>
                <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
                    <span class="k">pass</span>
        <span class="k">if</span> <span class="n">task</span><span class="o">.</span><span class="n">returncode</span> <span class="o">!=</span> <span class="n">os</span><span class="o">.</span><span class="n">EX_OK</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tail</span><span class="p">)</span> <span class="o">==</span> <span class="n">_TAIL_LINES_TO_KEEP</span><span class="p">:</span>
                <span class="n">last_n_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;last </span><span class="si">{</span><span class="n">_TAIL_LINES_TO_KEEP</span><span class="si">}</span><span class="s2"> lines of the task output are&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">last_n_msg</span> <span class="o">=</span> <span class="s2">&quot;task output is&quot;</span>
            <span class="n">task_output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tail</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Command </span><span class="si">{</span><span class="n">cmd</span><span class="si">}</span><span class="s2"> failed with return code </span><span class="si">{</span><span class="n">task</span><span class="o">.</span><span class="n">returncode</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">last_n_msg</span><span class="si">}</span><span class="s2"> included below: </span><span class="si">{</span><span class="n">task_output</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_run_local_training</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">framework_wrapper_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">train_object</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="o">=</span> <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span>
        <span class="n">cuda_state_was_set</span> <span class="o">=</span> <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span>
        <span class="n">old_cuda_visible_devices</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gpu</span><span class="p">:</span>
                <span class="n">gpus_owned</span> <span class="o">=</span> <span class="n">get_gpus_owned</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sc</span><span class="p">)</span>
                <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="nb">hash</span><span class="p">(</span><span class="n">train_object</span><span class="p">))</span>
                <span class="n">selected_gpus</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">gpus_owned</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="p">)]</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">selected_gpus</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Started local training with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="si">}</span><span class="s2"> processes&quot;</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">framework_wrapper_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_params</span><span class="p">,</span> <span class="n">train_object</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished local training with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="si">}</span><span class="s2"> processes&quot;</span><span class="p">)</span>

        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cuda_state_was_set</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_cuda_visible_devices</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
                    <span class="k">del</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_get_spark_task_function</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">framework_wrapper_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="n">train_object</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a spark task function that is used inside `mapPartitions`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        framework_wrapper_fn : Optional[Callable]</span>
<span class="sd">            The function that determines whether we are running training</span>
<span class="sd">            on a PyTorch file or a PyTorch function.</span>
<span class="sd">        train_object : Union[Callable, str]</span>
<span class="sd">            The actual train function/file.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Callable</span>
<span class="sd">            The wrapped function ready for use with `mapPartitions`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_processes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span>
        <span class="n">use_gpu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gpu</span>
        <span class="n">input_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_params</span>
        <span class="n">driver_address</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">driver_address</span>
        <span class="n">log_streaming_server_port</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_streaming_server_port</span>

        <span class="c1"># Spark task program</span>
        <span class="k">def</span> <span class="nf">wrapped_train_fn</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>  <span class="c1"># type: ignore[no-untyped-def]</span>
            <span class="kn">import</span> <span class="nn">os</span>
            <span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">BarrierTaskContext</span>

            <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="o">=</span> <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span>

            <span class="k">def</span> <span class="nf">get_free_port</span><span class="p">(</span><span class="n">address</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="s2">&quot;BarrierTaskContext&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
                <span class="n">port</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="kn">import</span> <span class="nn">socket</span>

                        <span class="n">sock</span> <span class="o">=</span> <span class="n">socket</span><span class="o">.</span><span class="n">socket</span><span class="p">()</span>
                        <span class="n">sock</span><span class="o">.</span><span class="n">bind</span><span class="p">((</span><span class="n">address</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
                        <span class="n">port</span> <span class="o">=</span> <span class="n">sock</span><span class="o">.</span><span class="n">getsockname</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="k">except</span> <span class="n">socket</span><span class="o">.</span><span class="n">error</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="n">available_port</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">allGather</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">port</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">available_port</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Failed to find free port for distributed training.&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">available_port</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">set_torch_config</span><span class="p">(</span><span class="n">context</span><span class="p">:</span> <span class="s2">&quot;BarrierTaskContext&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">addrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span><span class="o">.</span><span class="n">address</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">context</span><span class="o">.</span><span class="n">getTaskInfos</span><span class="p">()]</span>

                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">addrs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_free_port</span><span class="p">(</span><span class="n">addrs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">context</span><span class="p">))</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">num_processes</span><span class="p">)</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;NODE_RANK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">())</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">())</span>

            <span class="k">def</span> <span class="nf">set_gpus</span><span class="p">(</span><span class="n">context</span><span class="p">:</span> <span class="s2">&quot;BarrierTaskContext&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
                    <span class="k">return</span>

                <span class="n">gpus_owned</span> <span class="o">=</span> <span class="n">get_gpus_owned</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gpus_owned</span><span class="p">)</span>

            <span class="n">context</span> <span class="o">=</span> <span class="n">BarrierTaskContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span>
                <span class="n">set_gpus</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="n">set_torch_config</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>

            <span class="n">log_streaming_client</span> <span class="o">=</span> <span class="n">LogStreamingClient</span><span class="p">(</span><span class="n">driver_address</span><span class="p">,</span> <span class="n">log_streaming_server_port</span><span class="p">)</span>
            <span class="n">input_params</span><span class="p">[</span><span class="s2">&quot;log_streaming_client&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_streaming_client</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">framework_wrapper_fn</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">train_object</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">LogStreamingClient</span><span class="o">.</span><span class="n">_destroy</span><span class="p">()</span>
                <span class="k">except</span> <span class="ne">BaseException</span><span class="p">:</span>
                    <span class="k">pass</span>

            <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">wrapped_train_fn</span>

    <span class="k">def</span> <span class="nf">_run_distributed_training</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">framework_wrapper_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">train_object</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">framework_wrapper_fn</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Unknown combination of parameters&quot;</span><span class="p">)</span>

        <span class="n">log_streaming_server</span> <span class="o">=</span> <span class="n">LogStreamingServer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver_address</span> <span class="o">=</span> <span class="n">get_driver_host</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sc</span><span class="p">)</span>
        <span class="n">log_streaming_server</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">spark_host_address</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">driver_address</span><span class="p">)</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># wait for the server to start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_streaming_server_port</span> <span class="o">=</span> <span class="n">log_streaming_server</span><span class="o">.</span><span class="n">port</span>

        <span class="n">spark_task_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_spark_task_function</span><span class="p">(</span>
            <span class="n">framework_wrapper_fn</span><span class="p">,</span> <span class="n">train_object</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_encryption</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Started distributed training with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="si">}</span><span class="s2"> executor proceses&quot;</span>
        <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">)</span>
                <span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
                <span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">spark_task_function</span><span class="p">)</span>
                <span class="o">.</span><span class="n">collect</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">log_streaming_server</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Finished distributed training with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="si">}</span><span class="s2"> executor proceses&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_run_training_on_pytorch_file</span><span class="p">(</span>
        <span class="n">input_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">train_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">log_streaming_client</span> <span class="o">=</span> <span class="n">input_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;log_streaming_client&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">training_command</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_create_torchrun_command</span><span class="p">(</span>
            <span class="n">input_params</span><span class="p">,</span> <span class="n">train_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span>
        <span class="p">)</span>
        <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_execute_command</span><span class="p">(</span>
            <span class="n">training_command</span><span class="p">,</span> <span class="n">log_streaming_client</span><span class="o">=</span><span class="n">log_streaming_client</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">_setup_files</span><span class="p">(</span><span class="n">train_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="n">save_dir</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_create_save_dir</span><span class="p">()</span>
        <span class="n">pickle_file_path</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_save_pickled_function</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">train_fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="n">output_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_PICKLED_OUTPUT_FILE</span><span class="p">)</span>
        <span class="n">train_file_path</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_create_torchrun_train_file</span><span class="p">(</span>
            <span class="n">save_dir</span><span class="p">,</span> <span class="n">pickle_file_path</span><span class="p">,</span> <span class="n">output_file_path</span>
        <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">train_file_path</span><span class="p">,</span> <span class="n">output_file_path</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_cleanup_files</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_run_training_on_pytorch_function</span><span class="p">(</span>
        <span class="n">input_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">train_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_setup_files</span><span class="p">(</span><span class="n">train_fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="k">as</span> <span class="p">(</span><span class="n">train_file_path</span><span class="p">,</span> <span class="n">output_file_path</span><span class="p">):</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># type: ignore</span>
            <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_run_training_on_pytorch_file</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">train_file_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_file_path</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;TorchDistributor failed during training. &quot;</span>
                    <span class="s2">&quot;View stdout logs for detailed error message.&quot;</span>
                <span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_get_pickled_output</span><span class="p">(</span><span class="n">output_file_path</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;TorchDistributor failed due to a pickling error. &quot;</span>
                    <span class="s2">&quot;View stdout logs for detailed error message.&quot;</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_create_save_dir</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="c1"># TODO: need to do this in a safe way to avoid issues during concurrent runs</span>
        <span class="k">return</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_cleanup_files</span><span class="p">(</span><span class="n">save_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">ignore_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_save_pickled_function</span><span class="p">(</span><span class="n">save_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">train_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">saved_pickle_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_PICKLED_FUNC_FILE</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">saved_pickle_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">cloudpickle</span><span class="o">.</span><span class="n">dump</span><span class="p">((</span><span class="n">train_fn</span><span class="p">,</span> <span class="n">args</span><span class="p">),</span> <span class="n">f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">saved_pickle_path</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_create_torchrun_train_file</span><span class="p">(</span>
        <span class="n">save_dir_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">pickle_file_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">output_file_path</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">code</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                    import cloudpickle</span>
<span class="s2">                    import os</span>

<span class="s2">                    if __name__ == &quot;__main__&quot;:</span>
<span class="s2">                        with open(&quot;</span><span class="si">{</span><span class="n">pickle_file_path</span><span class="si">}</span><span class="s2">&quot;, &quot;rb&quot;) as f:</span>
<span class="s2">                            train_fn, args = cloudpickle.load(f)</span>
<span class="s2">                        output = train_fn(*args)</span>
<span class="s2">                        with open(&quot;</span><span class="si">{</span><span class="n">output_file_path</span><span class="si">}</span><span class="s2">&quot;, &quot;wb&quot;) as f:</span>
<span class="s2">                            cloudpickle.dump(output, f)</span>
<span class="s2">                    &quot;&quot;&quot;</span>
        <span class="p">)</span>
        <span class="n">saved_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir_path</span><span class="p">,</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_TRAIN_FILE</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">saved_file_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">code</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">saved_file_path</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_pickled_output</span><span class="p">(</span><span class="n">output_file_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_file_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">cloudpickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

<div class="viewcode-block" id="TorchDistributor.run"><a class="viewcode-back" href="../../../../reference/api/pyspark.ml.torch.distributor.TorchDistributor.html#pyspark.ml.torch.distributor.TorchDistributor.run">[docs]</a>    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_object</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs distributed training.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        train_object : callable object or str</span>
<span class="sd">            Either a PyTorch function, PyTorch Lightning function, or the path to a python file</span>
<span class="sd">            that launches distributed training.</span>
<span class="sd">        args :</span>
<span class="sd">            If train_object is a python function and not a path to a python file, args need</span>
<span class="sd">            to be the input parameters to that function. It would look like</span>

<span class="sd">            &gt;&gt;&gt; model = distributor.run(train, 1e-3, 64)</span>

<span class="sd">            where train is a function and 1e-3 is a regular numeric input to the function.</span>

<span class="sd">            If train_object is a python file, then args would be the command-line arguments for</span>
<span class="sd">            that python file which are all in the form of strings. An example would be</span>

<span class="sd">            &gt;&gt;&gt; distributor.run(&quot;/path/to/train.py&quot;, &quot;--learning-rate=1e-3&quot;, &quot;--batch-size=64&quot;)</span>

<span class="sd">            where since the input is a path, all of the parameters are strings that can be</span>
<span class="sd">            handled by argparse in that python file.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">            Returns the output of train_object called with args if the train_object is a</span>
<span class="sd">            Callable with an expected output. Returns None if train_object is a file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_object</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">framework_wrapper_fn</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_run_training_on_pytorch_file</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">framework_wrapper_fn</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_run_training_on_pytorch_function</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_mode</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_local_training</span><span class="p">(</span><span class="n">framework_wrapper_fn</span><span class="p">,</span> <span class="n">train_object</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_distributed_training</span><span class="p">(</span><span class="n">framework_wrapper_fn</span><span class="p">,</span> <span class="n">train_object</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span></div></div>
</pre></div>

              </div>
              
              
              <div class='prev-next-bottom'>
                

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright .<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>