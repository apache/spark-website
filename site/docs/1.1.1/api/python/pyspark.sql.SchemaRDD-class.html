<?xml version="1.0" encoding="ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
          "DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <title>pyspark.sql.SchemaRDD</title>
  <link rel="stylesheet" href="epydoc.css" type="text/css" />
  <script type="text/javascript" src="epydoc.js"></script>
</head>

<body bgcolor="white" text="black" link="blue" vlink="#204080"
      alink="#204080">
<!-- ==================== NAVIGATION BAR ==================== -->
<table class="navbar" border="0" width="100%" cellpadding="0"
       bgcolor="#a0c0ff" cellspacing="0">
  <tr valign="middle">
  <!-- Home link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="pyspark-module.html">Home</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Tree link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="module-tree.html">Trees</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Index link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="identifier-index.html">Indices</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Help link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="help.html">Help</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Project homepage -->
      <th class="navbar" align="right" width="100%">
        <table border="0" cellpadding="0" cellspacing="0">
          <tr><th class="navbar" align="center"
            ><a class="navbar" target="_top" href="http://spark.apache.org">Spark 1.1.0 Python API Docs</a></th>
          </tr></table></th>
  </tr>
</table>
<table width="100%" cellpadding="0" cellspacing="0">
  <tr valign="top">
    <td width="100%">
      <span class="breadcrumbs">
        <a href="pyspark-module.html">Package&nbsp;pyspark</a> ::
        <a href="pyspark.sql-module.html">Module&nbsp;sql</a> ::
        Class&nbsp;SchemaRDD
      </span>
    </td>
    <td>
      <table cellpadding="0" cellspacing="0">
        <!-- hide/show private -->
        <tr><td align="right"><span class="options"
            >[<a href="frames.html" target="_top">frames</a
            >]&nbsp;|&nbsp;<a href="pyspark.sql.SchemaRDD-class.html"
            target="_top">no&nbsp;frames</a>]</span></td></tr>
      </table>
    </td>
  </tr>
</table>
<!-- ==================== CLASS DESCRIPTION ==================== -->
<h1 class="epydoc">Class SchemaRDD</h1><p class="nomargin-top"><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD">source&nbsp;code</a></span></p>
<pre class="base-tree">
object --+    
         |    
   <a href="pyspark.rdd.RDD-class.html">rdd.RDD</a> --+
             |
            <strong class="uidshort">SchemaRDD</strong>
</pre>

<hr />
<p>An RDD of <a href="pyspark.sql.Row-class.html" class="link">Row</a> 
  objects that has an associated schema.</p>
  <p>The underlying JVM object is a SchemaRDD, not a PythonRDD, so we can 
  utilize the relational query api exposed by SparkSQL.</p>
  <p>For normal <a href="pyspark.rdd.RDD-class.html" 
  class="link">pyspark.rdd.RDD</a> operations (map, count, etc.) the <a 
  href="pyspark.sql.SchemaRDD-class.html" class="link">SchemaRDD</a> is not
  operated on directly, as it's underlying implementation is an RDD 
  composed of Java objects. Instead it is converted to a PythonRDD in the 
  JVM, on which Python operations can be done.</p>
  <p>This class receives raw tuples from Java but assigns a class to it in 
  all its data-collection methods (mapPartitionsWithIndex, collect, take, 
  etc) so that PySpark sees them as Row objects with named fields.</p>

<!-- ==================== INSTANCE METHODS ==================== -->
<a name="section-InstanceMethods"></a>
<table class="summary" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr bgcolor="#70b0f0" class="table-header">
  <td align="left" colspan="2" class="table-header">
    <span class="table-header">Instance Methods</span></td>
</tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#__init__" class="summary-sig-name">__init__</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">jschema_rdd</span>,
        <span class="summary-sig-arg">sql_ctx</span>)</span><br />
      x.__init__(...) initializes x; see help(type(x)) for signature</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.__init__">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#saveAsParquetFile" class="summary-sig-name">saveAsParquetFile</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">path</span>)</span><br />
      Save the contents as a Parquet file, preserving the schema.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.saveAsParquetFile">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#registerTempTable" class="summary-sig-name">registerTempTable</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">name</span>)</span><br />
      Registers this RDD as a temporary table using the given name.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.registerTempTable">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a name="registerAsTable"></a><span class="summary-sig-name">registerAsTable</span>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">name</span>)</span></td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.registerAsTable">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#insertInto" class="summary-sig-name">insertInto</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">tableName</span>,
        <span class="summary-sig-arg">overwrite</span>=<span class="summary-sig-default">False</span>)</span><br />
      Inserts the contents of this SchemaRDD into the specified table.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.insertInto">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a name="saveAsTable"></a><span class="summary-sig-name">saveAsTable</span>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">tableName</span>)</span><br />
      Creates a new table with the contents of this SchemaRDD.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.saveAsTable">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a name="schema"></a><span class="summary-sig-name">schema</span>(<span class="summary-sig-arg">self</span>)</span><br />
      Returns the schema of this SchemaRDD (represented by a <a 
      href="pyspark.sql.StructType-class.html" 
      class="link">StructType</a>).</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.schema">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a name="schemaString"></a><span class="summary-sig-name">schemaString</span>(<span class="summary-sig-arg">self</span>)</span><br />
      Returns the output schema in the tree format.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.schemaString">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a name="printSchema"></a><span class="summary-sig-name">printSchema</span>(<span class="summary-sig-arg">self</span>)</span><br />
      Prints out the schema in the tree format.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.printSchema">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#count" class="summary-sig-name">count</a>(<span class="summary-sig-arg">self</span>)</span><br />
      Return the number of elements in this RDD.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.count">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#collect" class="summary-sig-name">collect</a>(<span class="summary-sig-arg">self</span>)</span><br />
      Return a list that contains all of the rows in this RDD.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.collect">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#mapPartitionsWithIndex" class="summary-sig-name">mapPartitionsWithIndex</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">f</span>,
        <span class="summary-sig-arg">preservesPartitioning</span>=<span class="summary-sig-default">False</span>)</span><br />
      Return a new RDD by applying a function to each partition of this 
      RDD, while tracking the index of the original partition.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.mapPartitionsWithIndex">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#cache" class="summary-sig-name">cache</a>(<span class="summary-sig-arg">self</span>)</span><br />
      Persist this RDD with the default storage level 
      (<code>MEMORY_ONLY_SER</code>).</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.cache">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#persist" class="summary-sig-name">persist</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">storageLevel</span>)</span><br />
      Set this RDD's storage level to persist its values across operations 
      after the first time it is computed.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.persist">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#unpersist" class="summary-sig-name">unpersist</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">blocking</span>=<span class="summary-sig-default">True</span>)</span><br />
      Mark the RDD as non-persistent, and remove all blocks for it from 
      memory and disk.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.unpersist">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#checkpoint" class="summary-sig-name">checkpoint</a>(<span class="summary-sig-arg">self</span>)</span><br />
      Mark this RDD for checkpointing.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.checkpoint">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#isCheckpointed" class="summary-sig-name">isCheckpointed</a>(<span class="summary-sig-arg">self</span>)</span><br />
      Return whether this RDD has been checkpointed or not</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.isCheckpointed">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#getCheckpointFile" class="summary-sig-name">getCheckpointFile</a>(<span class="summary-sig-arg">self</span>)</span><br />
      Gets the name of the file to which this RDD was checkpointed</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.getCheckpointFile">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#coalesce" class="summary-sig-name">coalesce</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">numPartitions</span>,
        <span class="summary-sig-arg">shuffle</span>=<span class="summary-sig-default">False</span>)</span><br />
      Return a new RDD that is reduced into `numPartitions` partitions.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.coalesce">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#distinct" class="summary-sig-name">distinct</a>(<span class="summary-sig-arg">self</span>)</span><br />
      Return a new RDD containing the distinct elements in this RDD.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.distinct">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#intersection" class="summary-sig-name">intersection</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">other</span>)</span><br />
      Return the intersection of this RDD and another one.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.intersection">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#repartition" class="summary-sig-name">repartition</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">numPartitions</span>)</span><br />
      Return a new RDD that has exactly numPartitions partitions.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.repartition">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SchemaRDD-class.html#subtract" class="summary-sig-name">subtract</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">other</span>,
        <span class="summary-sig-arg">numPartitions</span>=<span class="summary-sig-default">None</span>)</span><br />
      Return each value in <code>self</code> that is not contained in 
      <code>other</code>.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.subtract">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
  <tr>
    <td colspan="2" class="summary">
    <p class="indent-wrapped-lines"><b>Inherited from <code><a href="pyspark.rdd.RDD-class.html">rdd.RDD</a></code></b>:
      <code><a href="pyspark.rdd.RDD-class.html#__add__">__add__</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#__repr__">__repr__</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#aggregate">aggregate</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#aggregateByKey">aggregateByKey</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#cartesian">cartesian</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#cogroup">cogroup</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#collectAsMap">collectAsMap</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#combineByKey">combineByKey</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#context">context</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#countByKey">countByKey</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#countByValue">countByValue</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#filter">filter</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#first">first</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#flatMap">flatMap</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#flatMapValues">flatMapValues</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#fold">fold</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#foldByKey">foldByKey</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#foreach">foreach</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#foreachPartition">foreachPartition</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#getNumPartitions">getNumPartitions</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#getStorageLevel">getStorageLevel</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#glom">glom</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#groupBy">groupBy</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#groupByKey">groupByKey</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#groupWith">groupWith</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#histogram">histogram</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#id">id</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#join">join</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#keyBy">keyBy</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#keys">keys</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#leftOuterJoin">leftOuterJoin</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#map">map</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#mapPartitions">mapPartitions</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#mapPartitionsWithSplit">mapPartitionsWithSplit</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#mapValues">mapValues</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#max">max</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#mean">mean</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#min">min</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#name">name</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#partitionBy">partitionBy</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#pipe">pipe</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#reduce">reduce</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#reduceByKey">reduceByKey</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#reduceByKeyLocally">reduceByKeyLocally</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#rightOuterJoin">rightOuterJoin</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#sample">sample</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#sampleByKey">sampleByKey</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#sampleStdev">sampleStdev</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#sampleVariance">sampleVariance</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#saveAsHadoopDataset">saveAsHadoopDataset</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#saveAsHadoopFile">saveAsHadoopFile</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#saveAsNewAPIHadoopDataset">saveAsNewAPIHadoopDataset</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#saveAsNewAPIHadoopFile">saveAsNewAPIHadoopFile</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#saveAsPickleFile">saveAsPickleFile</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#saveAsSequenceFile">saveAsSequenceFile</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#saveAsTextFile">saveAsTextFile</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#setName">setName</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#sortBy">sortBy</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#sortByKey">sortByKey</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#stats">stats</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#stdev">stdev</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#subtractByKey">subtractByKey</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#sum">sum</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#take">take</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#takeOrdered">takeOrdered</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#takeSample">takeSample</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#toDebugString">toDebugString</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#top">top</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#union">union</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#values">values</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#variance">variance</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#zip">zip</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#zipWithIndex">zipWithIndex</a></code>,
      <code><a href="pyspark.rdd.RDD-class.html#zipWithUniqueId">zipWithUniqueId</a></code>
      </p>
    <p class="indent-wrapped-lines"><b>Inherited from <code>object</code></b>:
      <code>__delattr__</code>,
      <code>__format__</code>,
      <code>__getattribute__</code>,
      <code>__hash__</code>,
      <code>__new__</code>,
      <code>__reduce__</code>,
      <code>__reduce_ex__</code>,
      <code>__setattr__</code>,
      <code>__sizeof__</code>,
      <code>__str__</code>,
      <code>__subclasshook__</code>
      </p>
    </td>
  </tr>
</table>
<!-- ==================== PROPERTIES ==================== -->
<a name="section-Properties"></a>
<table class="summary" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr bgcolor="#70b0f0" class="table-header">
  <td align="left" colspan="2" class="table-header">
    <span class="table-header">Properties</span></td>
</tr>
  <tr>
    <td colspan="2" class="summary">
    <p class="indent-wrapped-lines"><b>Inherited from <code>object</code></b>:
      <code>__class__</code>
      </p>
    </td>
  </tr>
</table>
<!-- ==================== METHOD DETAILS ==================== -->
<a name="section-MethodDetails"></a>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr bgcolor="#70b0f0" class="table-header">
  <td align="left" colspan="2" class="table-header">
    <span class="table-header">Method Details</span></td>
</tr>
</table>
<a name="__init__"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">__init__</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">jschema_rdd</span>,
        <span class="sig-arg">sql_ctx</span>)</span>
    <br /><em class="fname">(Constructor)</em>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.__init__">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>x.__init__(...) initializes x; see help(type(x)) for signature</p>
  <dl class="fields">
    <dt>Overrides:
        object.__init__
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="saveAsParquetFile"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">saveAsParquetFile</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">path</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.saveAsParquetFile">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Save the contents as a Parquet file, preserving the schema.</p>
  <p>Files that are written out using this method can be read back in as a 
  SchemaRDD using the <a 
  href="pyspark.sql.SQLContext-class.html#parquetFile" 
  class="link">SQLContext.parquetFile</a> method.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">import</span> tempfile, shutil
<span class="py-prompt">&gt;&gt;&gt; </span>parquetFile = tempfile.mkdtemp()
<span class="py-prompt">&gt;&gt;&gt; </span>shutil.rmtree(parquetFile)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(rdd)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd.saveAsParquetFile(parquetFile)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd2 = sqlCtx.parquetFile(parquetFile)
<span class="py-prompt">&gt;&gt;&gt; </span>sorted(srdd2.collect()) == sorted(srdd.collect())
<span class="py-output">True</span></pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="registerTempTable"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">registerTempTable</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">name</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.registerTempTable">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Registers this RDD as a temporary table using the given name.</p>
  <p>The lifetime of this temporary table is tied to the <a 
  href="pyspark.sql.SQLContext-class.html" class="link">SQLContext</a> that
  was used to create this SchemaRDD.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(rdd)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd.registerTempTable(<span class="py-string">&quot;test&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd2 = sqlCtx.sql(<span class="py-string">&quot;select * from test&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>sorted(srdd.collect()) == sorted(srdd2.collect())
<span class="py-output">True</span></pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="insertInto"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">insertInto</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">tableName</span>,
        <span class="sig-arg">overwrite</span>=<span class="sig-default">False</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.insertInto">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Inserts the contents of this SchemaRDD into the specified table.</p>
  <p>Optionally overwriting any existing data.</p>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="count"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">count</span>(<span class="sig-arg">self</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.count">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Return the number of elements in this RDD.</p>
  <p>Unlike the base RDD implementation of count, this implementation 
  leverages the query optimizer to compute the count on the SchemaRDD, 
  which supports features such as filter pushdown.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(rdd)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd.count()
<span class="py-output">3L</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>srdd.count() == srdd.map(<span class="py-keyword">lambda</span> x: x).count()
<span class="py-output">True</span></pre>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#count">rdd.RDD.count</a>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="collect"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">collect</span>(<span class="sig-arg">self</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.collect">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Return a list that contains all of the rows in this RDD.</p>
  <p>Each object in the list is on Row, the fields can be accessed as 
  attributes.</p>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#collect">rdd.RDD.collect</a>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="mapPartitionsWithIndex"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">mapPartitionsWithIndex</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">f</span>,
        <span class="sig-arg">preservesPartitioning</span>=<span class="sig-default">False</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.mapPartitionsWithIndex">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Return a new RDD by applying a function to each partition of this RDD,
  while tracking the index of the original partition.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>rdd = sc.parallelize([1, 2, 3, 4], 4)
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">def</span> <span class="py-defname">f</span>(splitIndex, iterator): yield splitIndex
<span class="py-prompt">&gt;&gt;&gt; </span>rdd.mapPartitionsWithIndex(f).sum()
<span class="py-output">6</span></pre>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#mapPartitionsWithIndex">rdd.RDD.mapPartitionsWithIndex</a>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="cache"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">cache</span>(<span class="sig-arg">self</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.cache">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Persist this RDD with the default storage level 
  (<code>MEMORY_ONLY_SER</code>).</p>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#cache">rdd.RDD.cache</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="persist"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">persist</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">storageLevel</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.persist">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Set this RDD's storage level to persist its values across operations 
  after the first time it is computed. This can only be used to assign a 
  new storage level if the RDD does not have a storage level set yet.</p>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#persist">rdd.RDD.persist</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="unpersist"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">unpersist</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">blocking</span>=<span class="sig-default">True</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.unpersist">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Mark the RDD as non-persistent, and remove all blocks for it from 
  memory and disk.</p>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#unpersist">rdd.RDD.unpersist</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="checkpoint"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">checkpoint</span>(<span class="sig-arg">self</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.checkpoint">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Mark this RDD for checkpointing. It will be saved to a file inside the
  checkpoint directory set with <code 
  class="link">SparkContext.setCheckpointDir()</code> and all references to
  its parent RDDs will be removed. This function must be called before any 
  job has been executed on this RDD. It is strongly recommended that this 
  RDD is persisted in memory, otherwise saving it on a file will require 
  recomputation.</p>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#checkpoint">rdd.RDD.checkpoint</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="isCheckpointed"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">isCheckpointed</span>(<span class="sig-arg">self</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.isCheckpointed">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Return whether this RDD has been checkpointed or not</p>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#isCheckpointed">rdd.RDD.isCheckpointed</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="getCheckpointFile"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">getCheckpointFile</span>(<span class="sig-arg">self</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.getCheckpointFile">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Gets the name of the file to which this RDD was checkpointed</p>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#getCheckpointFile">rdd.RDD.getCheckpointFile</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="coalesce"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">coalesce</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">numPartitions</span>,
        <span class="sig-arg">shuffle</span>=<span class="sig-default">False</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.coalesce">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Return a new RDD that is reduced into `numPartitions` partitions.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()
<span class="py-output">[[1], [2, 3], [4, 5]]</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()
<span class="py-output">[[1, 2, 3, 4, 5]]</span></pre>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#coalesce">rdd.RDD.coalesce</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="distinct"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">distinct</span>(<span class="sig-arg">self</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.distinct">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Return a new RDD containing the distinct elements in this RDD.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())
<span class="py-output">[1, 2, 3]</span></pre>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#distinct">rdd.RDD.distinct</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="intersection"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">intersection</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">other</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.intersection">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Return the intersection of this RDD and another one. The output will 
  not contain any duplicate elements, even if the input RDDs did.</p>
  <p>Note that this method performs a shuffle internally.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
<span class="py-prompt">&gt;&gt;&gt; </span>rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
<span class="py-prompt">&gt;&gt;&gt; </span>rdd1.intersection(rdd2).collect()
<span class="py-output">[1, 2, 3]</span></pre>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#intersection">rdd.RDD.intersection</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="repartition"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">repartition</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">numPartitions</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.repartition">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Return a new RDD that has exactly numPartitions partitions.</p>
  <p>Can increase or decrease the level of parallelism in this RDD. 
  Internally, this uses a shuffle to redistribute data. If you are 
  decreasing the number of partitions in this RDD, consider using 
  `coalesce`, which can avoid performing a shuffle.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>rdd = sc.parallelize([1,2,3,4,5,6,7], 4)
<span class="py-prompt">&gt;&gt;&gt; </span>sorted(rdd.glom().collect())
<span class="py-output">[[1], [2, 3], [4, 5], [6, 7]]</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>len(rdd.repartition(2).glom().collect())
<span class="py-output">2</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>len(rdd.repartition(10).glom().collect())
<span class="py-output">10</span></pre>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#repartition">rdd.RDD.repartition</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<a name="subtract"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">subtract</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">other</span>,
        <span class="sig-arg">numPartitions</span>=<span class="sig-default">None</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SchemaRDD.subtract">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Return each value in <code>self</code> that is not contained in 
  <code>other</code>.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>x = sc.parallelize([(<span class="py-string">&quot;a&quot;</span>, 1), (<span class="py-string">&quot;b&quot;</span>, 4), (<span class="py-string">&quot;b&quot;</span>, 5), (<span class="py-string">&quot;a&quot;</span>, 3)])
<span class="py-prompt">&gt;&gt;&gt; </span>y = sc.parallelize([(<span class="py-string">&quot;a&quot;</span>, 3), (<span class="py-string">&quot;c&quot;</span>, None)])
<span class="py-prompt">&gt;&gt;&gt; </span>sorted(x.subtract(y).collect())
<span class="py-output">[('a', 1), ('b', 4), ('b', 5)]</span></pre>
  <dl class="fields">
    <dt>Overrides:
        <a href="pyspark.rdd.RDD-class.html#subtract">rdd.RDD.subtract</a>
        <dd><em class="note">(inherited documentation)</em></dd>
    </dt>
  </dl>
</td></tr></table>
</div>
<br />
<!-- ==================== NAVIGATION BAR ==================== -->
<table class="navbar" border="0" width="100%" cellpadding="0"
       bgcolor="#a0c0ff" cellspacing="0">
  <tr valign="middle">
  <!-- Home link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="pyspark-module.html">Home</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Tree link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="module-tree.html">Trees</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Index link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="identifier-index.html">Indices</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Help link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="help.html">Help</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Project homepage -->
      <th class="navbar" align="right" width="100%">
        <table border="0" cellpadding="0" cellspacing="0">
          <tr><th class="navbar" align="center"
            ><a class="navbar" target="_top" href="http://spark.apache.org">Spark 1.1.0 Python API Docs</a></th>
          </tr></table></th>
  </tr>
</table>
<table border="0" cellpadding="0" cellspacing="0" width="100%%">
  <tr>
    <td align="left" class="footer">
    Generated by Epydoc 3.0.1 on Mon Nov 24 15:21:12 2014
    </td>
    <td align="right" class="footer">
      <a target="mainFrame" href="http://epydoc.sourceforge.net"
        >http://epydoc.sourceforge.net</a>
    </td>
  </tr>
</table>

<script type="text/javascript">
  <!--
  // Private objects are initially displayed (because if
  // javascript is turned off then we want them to be
  // visible); but by default, we want to hide them.  So hide
  // them unless we have a cookie that says to show them.
  checkCookie();
  // -->
</script>
</body>
</html>
