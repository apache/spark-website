<?xml version="1.0" encoding="ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
          "DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <title>pyspark.sql.SQLContext</title>
  <link rel="stylesheet" href="epydoc.css" type="text/css" />
  <script type="text/javascript" src="epydoc.js"></script>
</head>

<body bgcolor="white" text="black" link="blue" vlink="#204080"
      alink="#204080">
<!-- ==================== NAVIGATION BAR ==================== -->
<table class="navbar" border="0" width="100%" cellpadding="0"
       bgcolor="#a0c0ff" cellspacing="0">
  <tr valign="middle">
  <!-- Home link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="pyspark-module.html">Home</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Tree link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="module-tree.html">Trees</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Index link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="identifier-index.html">Indices</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Help link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="help.html">Help</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Project homepage -->
      <th class="navbar" align="right" width="100%">
        <table border="0" cellpadding="0" cellspacing="0">
          <tr><th class="navbar" align="center"
            ><a class="navbar" target="_top" href="http://spark.apache.org">Spark 1.1.0 Python API Docs</a></th>
          </tr></table></th>
  </tr>
</table>
<table width="100%" cellpadding="0" cellspacing="0">
  <tr valign="top">
    <td width="100%">
      <span class="breadcrumbs">
        <a href="pyspark-module.html">Package&nbsp;pyspark</a> ::
        <a href="pyspark.sql-module.html">Module&nbsp;sql</a> ::
        Class&nbsp;SQLContext
      </span>
    </td>
    <td>
      <table cellpadding="0" cellspacing="0">
        <!-- hide/show private -->
        <tr><td align="right"><span class="options"
            >[<a href="frames.html" target="_top">frames</a
            >]&nbsp;|&nbsp;<a href="pyspark.sql.SQLContext-class.html"
            target="_top">no&nbsp;frames</a>]</span></td></tr>
      </table>
    </td>
  </tr>
</table>
<!-- ==================== CLASS DESCRIPTION ==================== -->
<h1 class="epydoc">Class SQLContext</h1><p class="nomargin-top"><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext">source&nbsp;code</a></span></p>
<p>Main entry point for SparkSQL functionality.</p>
  <p>A SQLContext can be used create <a 
  href="pyspark.sql.SchemaRDD-class.html" class="link">SchemaRDD</a>s, 
  register <a href="pyspark.sql.SchemaRDD-class.html" 
  class="link">SchemaRDD</a>s as tables, execute SQL over tables, cache 
  tables, and read parquet files.</p>

<!-- ==================== INSTANCE METHODS ==================== -->
<a name="section-InstanceMethods"></a>
<table class="summary" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr bgcolor="#70b0f0" class="table-header">
  <td align="left" colspan="2" class="table-header">
    <span class="table-header">Instance Methods</span></td>
</tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SQLContext-class.html#__init__" class="summary-sig-name">__init__</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">sparkContext</span>,
        <span class="summary-sig-arg">sqlContext</span>=<span class="summary-sig-default">None</span>)</span><br />
      Create a new SQLContext.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.__init__">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SQLContext-class.html#registerFunction" class="summary-sig-name">registerFunction</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">name</span>,
        <span class="summary-sig-arg">f</span>,
        <span class="summary-sig-arg">returnType</span>=<span class="summary-sig-default">StringType()</span>)</span><br />
      Registers a lambda function as a UDF so it can be used in SQL 
      statements.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.registerFunction">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SQLContext-class.html#inferSchema" class="summary-sig-name">inferSchema</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">rdd</span>)</span><br />
      Infer and apply a schema to an RDD of <a 
      href="pyspark.sql.Row-class.html" class="link">Row</a>s.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.inferSchema">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SQLContext-class.html#applySchema" class="summary-sig-name">applySchema</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">rdd</span>,
        <span class="summary-sig-arg">schema</span>)</span><br />
      Applies the given schema to the given RDD of <code 
      class="link">tuple</code> or <code class="link">list</code>s.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.applySchema">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SQLContext-class.html#registerRDDAsTable" class="summary-sig-name">registerRDDAsTable</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">rdd</span>,
        <span class="summary-sig-arg">tableName</span>)</span><br />
      Registers the given RDD as a temporary table in the catalog.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.registerRDDAsTable">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SQLContext-class.html#parquetFile" class="summary-sig-name">parquetFile</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">path</span>)</span><br />
      Loads a Parquet file, returning the result as a <a 
      href="pyspark.sql.SchemaRDD-class.html" class="link">SchemaRDD</a>.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.parquetFile">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SQLContext-class.html#jsonFile" class="summary-sig-name">jsonFile</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">path</span>,
        <span class="summary-sig-arg">schema</span>=<span class="summary-sig-default">None</span>)</span><br />
      Loads a text file storing one JSON object per line as a <a 
      href="pyspark.sql.SchemaRDD-class.html" class="link">SchemaRDD</a>.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.jsonFile">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SQLContext-class.html#jsonRDD" class="summary-sig-name">jsonRDD</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">rdd</span>,
        <span class="summary-sig-arg">schema</span>=<span class="summary-sig-default">None</span>)</span><br />
      Loads an RDD storing one JSON object per string as a <a 
      href="pyspark.sql.SchemaRDD-class.html" class="link">SchemaRDD</a>.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.jsonRDD">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SQLContext-class.html#sql" class="summary-sig-name">sql</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">sqlQuery</span>)</span><br />
      Return a <a href="pyspark.sql.SchemaRDD-class.html" 
      class="link">SchemaRDD</a> representing the result of the given 
      query.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.sql">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a href="pyspark.sql.SQLContext-class.html#table" class="summary-sig-name">table</a>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">tableName</span>)</span><br />
      Returns the specified table as a <a 
      href="pyspark.sql.SchemaRDD-class.html" class="link">SchemaRDD</a>.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.table">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a name="cacheTable"></a><span class="summary-sig-name">cacheTable</span>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">tableName</span>)</span><br />
      Caches the specified table in-memory.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.cacheTable">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
<tr>
    <td width="15%" align="right" valign="top" class="summary">
      <span class="summary-type">&nbsp;</span>
    </td><td class="summary">
      <table width="100%" cellpadding="0" cellspacing="0" border="0">
        <tr>
          <td><span class="summary-sig"><a name="uncacheTable"></a><span class="summary-sig-name">uncacheTable</span>(<span class="summary-sig-arg">self</span>,
        <span class="summary-sig-arg">tableName</span>)</span><br />
      Removes the specified table from the in-memory cache.</td>
          <td align="right" valign="top">
            <span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.uncacheTable">source&nbsp;code</a></span>
            
          </td>
        </tr>
      </table>
      
    </td>
  </tr>
</table>
<!-- ==================== METHOD DETAILS ==================== -->
<a name="section-MethodDetails"></a>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr bgcolor="#70b0f0" class="table-header">
  <td align="left" colspan="2" class="table-header">
    <span class="table-header">Method Details</span></td>
</tr>
</table>
<a name="__init__"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">__init__</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">sparkContext</span>,
        <span class="sig-arg">sqlContext</span>=<span class="sig-default">None</span>)</span>
    <br /><em class="fname">(Constructor)</em>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.__init__">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Create a new SQLContext.</p>
  <dl class="fields">
    <dt>Parameters:</dt>
    <dd><ul class="nomargin-top">
        <li><strong class="pname"><code>sparkContext</code></strong> - The SparkContext to wrap.</li>
        <li><strong class="pname"><code>sqlContext</code></strong> - An optional JVM Scala SQLContext. If set, we do not instatiate a 
          new SQLContext in the JVM, instead we make all calls to this 
          object.
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(rdd)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.inferSchema(srdd) <span class="py-comment"># doctest: +IGNORE_EXCEPTION_DETAIL</span>
<span class="py-except">Traceback (most recent call last):</span>
<span class="py-except">    ...</span>
<span class="py-except">TypeError:...</span></pre>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>bad_rdd = sc.parallelize([1,2,3])
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.inferSchema(bad_rdd) <span class="py-comment"># doctest: +IGNORE_EXCEPTION_DETAIL</span>
<span class="py-except">Traceback (most recent call last):</span>
<span class="py-except">    ...</span>
<span class="py-except">ValueError:...</span></pre>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> datetime <span class="py-keyword">import</span> datetime
<span class="py-prompt">&gt;&gt;&gt; </span>allTypes = sc.parallelize([Row(i=1, s=<span class="py-string">&quot;string&quot;</span>, d=1.0, l=1L,
<span class="py-more">... </span>    b=True, list=[1, 2, 3], dict={<span class="py-string">&quot;s&quot;</span>: 0}, row=Row(a=1),
<span class="py-more">... </span>    time=datetime(2014, 8, 1, 14, 1, 5))])
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(allTypes)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd.registerTempTable(<span class="py-string">&quot;allTypes&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.sql(<span class="py-string">'select i+1, d+1, not b, list[1], dict[&quot;s&quot;], time, row.a '</span>
<span class="py-more">... </span>           <span class="py-string">'from allTypes where b and i &gt; 0'</span>).collect()
<span class="py-output">[Row(c0=2, c1=2.0, c2=False, c3=2, c4=0...8, 1, 14, 1, 5), a=1)]</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>srdd.map(<span class="py-keyword">lambda</span> x: (x.i, x.s, x.d, x.l, x.b, x.time,
<span class="py-more">... </span>                    x.row.a, x.list)).collect()
<span class="py-output">[(1, u'string', 1.0, 1, True, ...(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]</span></pre></li>
    </ul></dd>
  </dl>
</td></tr></table>
</div>
<a name="registerFunction"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">registerFunction</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">name</span>,
        <span class="sig-arg">f</span>,
        <span class="sig-arg">returnType</span>=<span class="sig-default">StringType()</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.registerFunction">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Registers a lambda function as a UDF so it can be used in SQL 
  statements.</p>
  <p>In addition to a name and the function itself, the return type can be 
  optionally specified. When the return type is not given it default to a 
  string and conversion will automatically be done.  For any other return 
  type, the produced object must match the specified type.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerFunction(<span class="py-string">&quot;stringLengthString&quot;</span>, <span class="py-keyword">lambda</span> x: len(x))
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.sql(<span class="py-string">&quot;SELECT stringLengthString('test')&quot;</span>).collect()
<span class="py-output">[Row(c0=u'4')]</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerFunction(<span class="py-string">&quot;stringLengthInt&quot;</span>, <span class="py-keyword">lambda</span> x: len(x), IntegerType())
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.sql(<span class="py-string">&quot;SELECT stringLengthInt('test')&quot;</span>).collect()
<span class="py-output">[Row(c0=4)]</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerFunction(<span class="py-string">&quot;twoArgs&quot;</span>, <span class="py-keyword">lambda</span> x, y: len(x) + y, IntegerType())
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.sql(<span class="py-string">&quot;SELECT twoArgs('test', 1)&quot;</span>).collect()
<span class="py-output">[Row(c0=5)]</span></pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="inferSchema"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">inferSchema</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">rdd</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.inferSchema">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Infer and apply a schema to an RDD of <a 
  href="pyspark.sql.Row-class.html" class="link">Row</a>s.</p>
  <p>We peek at the first row of the RDD to determine the fields' names and
  types. Nested collections are supported, which include array, dict, list,
  Row, tuple, namedtuple, or object.</p>
  <p>All the rows in `rdd` should have the same type with the first one, or
  it will cause runtime exceptions.</p>
  <p>Each row could be <a href="pyspark.sql.Row-class.html" 
  class="link">pyspark.sql.Row</a> object or namedtuple or objects, using 
  dict is deprecated.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>rdd = sc.parallelize(
<span class="py-more">... </span>    [Row(field1=1, field2=<span class="py-string">&quot;row1&quot;</span>),
<span class="py-more">... </span>     Row(field1=2, field2=<span class="py-string">&quot;row2&quot;</span>),
<span class="py-more">... </span>     Row(field1=3, field2=<span class="py-string">&quot;row3&quot;</span>)])
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(rdd)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd.collect()[0]
<span class="py-output">Row(field1=1, field2=u'row1')</span></pre>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>NestedRow = Row(<span class="py-string">&quot;f1&quot;</span>, <span class="py-string">&quot;f2&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>nestedRdd1 = sc.parallelize([
<span class="py-more">... </span>    NestedRow(array(<span class="py-string">'i'</span>, [1, 2]), {<span class="py-string">&quot;row1&quot;</span>: 1.0}),
<span class="py-more">... </span>    NestedRow(array(<span class="py-string">'i'</span>, [2, 3]), {<span class="py-string">&quot;row2&quot;</span>: 2.0})])
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(nestedRdd1)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd.collect()
<span class="py-output">[Row(f1=[1, 2], f2={u'row1': 1.0}), ..., f2={u'row2': 2.0})]</span></pre>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>nestedRdd2 = sc.parallelize([
<span class="py-more">... </span>    NestedRow([[1, 2], [2, 3]], [1, 2]),
<span class="py-more">... </span>    NestedRow([[2, 3], [3, 4]], [2, 3])])
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(nestedRdd2)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd.collect()
<span class="py-output">[Row(f1=[[1, 2], [2, 3]], f2=[1, 2]), ..., f2=[2, 3])]</span></pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="applySchema"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">applySchema</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">rdd</span>,
        <span class="sig-arg">schema</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.applySchema">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Applies the given schema to the given RDD of <code 
  class="link">tuple</code> or <code class="link">list</code>s.</p>
  <p>These tuples or lists can contain complex nested structures like 
  lists, maps or nested rows.</p>
  <p>The schema should be a StructType.</p>
  <p>It is important that the schema matches the types of the objects in 
  each row or exceptions could be thrown at runtime.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>rdd2 = sc.parallelize([(1, <span class="py-string">&quot;row1&quot;</span>), (2, <span class="py-string">&quot;row2&quot;</span>), (3, <span class="py-string">&quot;row3&quot;</span>)])
<span class="py-prompt">&gt;&gt;&gt; </span>schema = StructType([StructField(<span class="py-string">&quot;field1&quot;</span>, IntegerType(), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;field2&quot;</span>, StringType(), False)])
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.applySchema(rdd2, schema)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerRDDAsTable(srdd, <span class="py-string">&quot;table1&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd2 = sqlCtx.sql(<span class="py-string">&quot;SELECT * from table1&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd2.collect()
<span class="py-output">[Row(field1=1, field2=u'row1'),..., Row(field1=3, field2=u'row3')]</span></pre>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> datetime <span class="py-keyword">import</span> datetime
<span class="py-prompt">&gt;&gt;&gt; </span>rdd = sc.parallelize([(127, -128L, -32768, 32767, 2147483647L, 1.0,
<span class="py-more">... </span>    datetime(2010, 1, 1, 1, 1, 1),
<span class="py-more">... </span>    {<span class="py-string">&quot;a&quot;</span>: 1}, (2,), [1, 2, 3], None)])
<span class="py-prompt">&gt;&gt;&gt; </span>schema = StructType([
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;byte1&quot;</span>, ByteType(), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;byte2&quot;</span>, ByteType(), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;short1&quot;</span>, ShortType(), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;short2&quot;</span>, ShortType(), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;int&quot;</span>, IntegerType(), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;float&quot;</span>, FloatType(), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;time&quot;</span>, TimestampType(), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;map&quot;</span>,
<span class="py-more">... </span>        MapType(StringType(), IntegerType(), False), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;struct&quot;</span>,
<span class="py-more">... </span>        StructType([StructField(<span class="py-string">&quot;b&quot;</span>, ShortType(), False)]), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;list&quot;</span>, ArrayType(ByteType(), False), False),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;null&quot;</span>, DoubleType(), True)])
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.applySchema(rdd, schema)
<span class="py-prompt">&gt;&gt;&gt; </span>results = srdd.map(
<span class="py-more">... </span>    <span class="py-keyword">lambda</span> x: (x.byte1, x.byte2, x.short1, x.short2, x.int, x.float, x.time,
<span class="py-more">... </span>        x.map[<span class="py-string">&quot;a&quot;</span>], x.struct.b, x.list, x.null))
<span class="py-prompt">&gt;&gt;&gt; </span>results.collect()[0]
<span class="py-output">(127, -128, -32768, 32767, 2147483647, 1.0, ...(2010, 1, 1, 1, 1, 1), 1, 2, [1, 2, 3], None)</span></pre>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>srdd.registerTempTable(<span class="py-string">&quot;table2&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.sql(
<span class="py-more">... </span>  <span class="py-string">&quot;SELECT byte1 - 1 AS byte1, byte2 + 1 AS byte2, &quot;</span> +
<span class="py-more">... </span>    <span class="py-string">&quot;short1 + 1 AS short1, short2 - 1 AS short2, int - 1 AS int, &quot;</span> +
<span class="py-more">... </span>    <span class="py-string">&quot;float + 1.5 as float FROM table2&quot;</span>).collect()
<span class="py-output">[Row(byte1=126, byte2=-127, short1=-32767, short2=32766, int=2147483646, float=2.5)]</span></pre>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>rdd = sc.parallelize([(127, -32768, 1.0,
<span class="py-more">... </span>    datetime(2010, 1, 1, 1, 1, 1),
<span class="py-more">... </span>    {<span class="py-string">&quot;a&quot;</span>: 1}, (2,), [1, 2, 3])])
<span class="py-prompt">&gt;&gt;&gt; </span>abstract = <span class="py-string">&quot;byte short float time map{} struct(b) list[]&quot;</span>
<span class="py-prompt">&gt;&gt;&gt; </span>schema = _parse_schema_abstract(abstract)
<span class="py-prompt">&gt;&gt;&gt; </span>typedSchema = _infer_schema_type(rdd.first(), schema)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.applySchema(rdd, typedSchema)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd.collect()
<span class="py-output">[Row(byte=127, short=-32768, float=1.0, time=..., list=[1, 2, 3])]</span></pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="registerRDDAsTable"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">registerRDDAsTable</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">rdd</span>,
        <span class="sig-arg">tableName</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.registerRDDAsTable">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Registers the given RDD as a temporary table in the catalog.</p>
  <p>Temporary tables exist only during the lifetime of this instance of 
  SQLContext.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(rdd)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerRDDAsTable(srdd, <span class="py-string">&quot;table1&quot;</span>)</pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="parquetFile"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">parquetFile</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">path</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.parquetFile">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Loads a Parquet file, returning the result as a <a 
  href="pyspark.sql.SchemaRDD-class.html" class="link">SchemaRDD</a>.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">import</span> tempfile, shutil
<span class="py-prompt">&gt;&gt;&gt; </span>parquetFile = tempfile.mkdtemp()
<span class="py-prompt">&gt;&gt;&gt; </span>shutil.rmtree(parquetFile)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(rdd)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd.saveAsParquetFile(parquetFile)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd2 = sqlCtx.parquetFile(parquetFile)
<span class="py-prompt">&gt;&gt;&gt; </span>sorted(srdd.collect()) == sorted(srdd2.collect())
<span class="py-output">True</span></pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="jsonFile"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">jsonFile</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">path</span>,
        <span class="sig-arg">schema</span>=<span class="sig-default">None</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.jsonFile">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Loads a text file storing one JSON object per line as a <a 
  href="pyspark.sql.SchemaRDD-class.html" class="link">SchemaRDD</a>.</p>
  <p>If the schema is provided, applies the given schema to this JSON 
  dataset.</p>
  <p>Otherwise, it goes through the entire dataset once to determine the 
  schema.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">import</span> tempfile, shutil
<span class="py-prompt">&gt;&gt;&gt; </span>jsonFile = tempfile.mkdtemp()
<span class="py-prompt">&gt;&gt;&gt; </span>shutil.rmtree(jsonFile)
<span class="py-prompt">&gt;&gt;&gt; </span>ofn = open(jsonFile, <span class="py-string">'w'</span>)
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">for</span> json <span class="py-keyword">in</span> jsonStrings:
<span class="py-more">... </span>  <span class="py-keyword">print</span>&gt;&gt;ofn, json
<span class="py-prompt">&gt;&gt;&gt; </span>ofn.close()
<span class="py-prompt">&gt;&gt;&gt; </span>srdd1 = sqlCtx.jsonFile(jsonFile)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerRDDAsTable(srdd1, <span class="py-string">&quot;table1&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd2 = sqlCtx.sql(
<span class="py-more">... </span>  <span class="py-string">&quot;SELECT field1 AS f1, field2 as f2, field3 as f3, &quot;</span>
<span class="py-more">... </span>  <span class="py-string">&quot;field6 as f4 from table1&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">for</span> r <span class="py-keyword">in</span> srdd2.collect():
<span class="py-more">... </span>    <span class="py-keyword">print</span> r
<span class="py-output">Row(f1=1, f2=u'row1', f3=Row(field4=11, field5=None), f4=None)</span>
<span class="py-output">Row(f1=2, f2=None, f3=Row(field4=22,..., f4=[Row(field7=u'row2')])</span>
<span class="py-output">Row(f1=None, f2=u'row3', f3=Row(field4=33, field5=[]), f4=None)</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>srdd3 = sqlCtx.jsonFile(jsonFile, srdd1.schema())
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerRDDAsTable(srdd3, <span class="py-string">&quot;table2&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd4 = sqlCtx.sql(
<span class="py-more">... </span>  <span class="py-string">&quot;SELECT field1 AS f1, field2 as f2, field3 as f3, &quot;</span>
<span class="py-more">... </span>  <span class="py-string">&quot;field6 as f4 from table2&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">for</span> r <span class="py-keyword">in</span> srdd4.collect():
<span class="py-more">... </span>   <span class="py-keyword">print</span> r
<span class="py-output">Row(f1=1, f2=u'row1', f3=Row(field4=11, field5=None), f4=None)</span>
<span class="py-output">Row(f1=2, f2=None, f3=Row(field4=22,..., f4=[Row(field7=u'row2')])</span>
<span class="py-output">Row(f1=None, f2=u'row3', f3=Row(field4=33, field5=[]), f4=None)</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>schema = StructType([
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;field2&quot;</span>, StringType(), True),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;field3&quot;</span>,
<span class="py-more">... </span>        StructType([
<span class="py-more">... </span>            StructField(<span class="py-string">&quot;field5&quot;</span>,
<span class="py-more">... </span>                ArrayType(IntegerType(), False), True)]), False)])
<span class="py-prompt">&gt;&gt;&gt; </span>srdd5 = sqlCtx.jsonFile(jsonFile, schema)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerRDDAsTable(srdd5, <span class="py-string">&quot;table3&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd6 = sqlCtx.sql(
<span class="py-more">... </span>  <span class="py-string">&quot;SELECT field2 AS f1, field3.field5 as f2, &quot;</span>
<span class="py-more">... </span>  <span class="py-string">&quot;field3.field5[0] as f3 from table3&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd6.collect()
<span class="py-output">[Row(f1=u'row1', f2=None, f3=None)...Row(f1=u'row3', f2=[], f3=None)]</span></pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="jsonRDD"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">jsonRDD</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">rdd</span>,
        <span class="sig-arg">schema</span>=<span class="sig-default">None</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.jsonRDD">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Loads an RDD storing one JSON object per string as a <a 
  href="pyspark.sql.SchemaRDD-class.html" class="link">SchemaRDD</a>.</p>
  <p>If the schema is provided, applies the given schema to this JSON 
  dataset.</p>
  <p>Otherwise, it goes through the entire dataset once to determine the 
  schema.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>srdd1 = sqlCtx.jsonRDD(json)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerRDDAsTable(srdd1, <span class="py-string">&quot;table1&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd2 = sqlCtx.sql(
<span class="py-more">... </span>  <span class="py-string">&quot;SELECT field1 AS f1, field2 as f2, field3 as f3, &quot;</span>
<span class="py-more">... </span>  <span class="py-string">&quot;field6 as f4 from table1&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">for</span> r <span class="py-keyword">in</span> srdd2.collect():
<span class="py-more">... </span>    <span class="py-keyword">print</span> r
<span class="py-output">Row(f1=1, f2=u'row1', f3=Row(field4=11, field5=None), f4=None)</span>
<span class="py-output">Row(f1=2, f2=None, f3=Row(field4=22..., f4=[Row(field7=u'row2')])</span>
<span class="py-output">Row(f1=None, f2=u'row3', f3=Row(field4=33, field5=[]), f4=None)</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>srdd3 = sqlCtx.jsonRDD(json, srdd1.schema())
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerRDDAsTable(srdd3, <span class="py-string">&quot;table2&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd4 = sqlCtx.sql(
<span class="py-more">... </span>  <span class="py-string">&quot;SELECT field1 AS f1, field2 as f2, field3 as f3, &quot;</span>
<span class="py-more">... </span>  <span class="py-string">&quot;field6 as f4 from table2&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">for</span> r <span class="py-keyword">in</span> srdd4.collect():
<span class="py-more">... </span>    <span class="py-keyword">print</span> r
<span class="py-output">Row(f1=1, f2=u'row1', f3=Row(field4=11, field5=None), f4=None)</span>
<span class="py-output">Row(f1=2, f2=None, f3=Row(field4=22..., f4=[Row(field7=u'row2')])</span>
<span class="py-output">Row(f1=None, f2=u'row3', f3=Row(field4=33, field5=[]), f4=None)</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>schema = StructType([
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;field2&quot;</span>, StringType(), True),
<span class="py-more">... </span>    StructField(<span class="py-string">&quot;field3&quot;</span>,
<span class="py-more">... </span>        StructType([
<span class="py-more">... </span>            StructField(<span class="py-string">&quot;field5&quot;</span>,
<span class="py-more">... </span>                ArrayType(IntegerType(), False), True)]), False)])
<span class="py-prompt">&gt;&gt;&gt; </span>srdd5 = sqlCtx.jsonRDD(json, schema)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerRDDAsTable(srdd5, <span class="py-string">&quot;table3&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd6 = sqlCtx.sql(
<span class="py-more">... </span>  <span class="py-string">&quot;SELECT field2 AS f1, field3.field5 as f2, &quot;</span>
<span class="py-more">... </span>  <span class="py-string">&quot;field3.field5[0] as f3 from table3&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd6.collect()
<span class="py-output">[Row(f1=u'row1', f2=None,...Row(f1=u'row3', f2=[], f3=None)]</span></pre>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.jsonRDD(sc.parallelize([<span class="py-string">'{}'</span>,
<span class="py-more">... </span>        <span class="py-string">'{&quot;key0&quot;: {&quot;key1&quot;: &quot;value1&quot;}}'</span>])).collect()
<span class="py-output">[Row(key0=None), Row(key0=Row(key1=u'value1'))]</span>
<span class="py-output"></span><span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.jsonRDD(sc.parallelize([<span class="py-string">'{&quot;key0&quot;: null}'</span>,
<span class="py-more">... </span>        <span class="py-string">'{&quot;key0&quot;: {&quot;key1&quot;: &quot;value1&quot;}}'</span>])).collect()
<span class="py-output">[Row(key0=None), Row(key0=Row(key1=u'value1'))]</span></pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="sql"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">sql</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">sqlQuery</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.sql">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Return a <a href="pyspark.sql.SchemaRDD-class.html" 
  class="link">SchemaRDD</a> representing the result of the given 
  query.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(rdd)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerRDDAsTable(srdd, <span class="py-string">&quot;table1&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd2 = sqlCtx.sql(<span class="py-string">&quot;SELECT field1 AS f1, field2 as f2 from table1&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd2.collect()
<span class="py-output">[Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]</span></pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<a name="table"></a>
<div>
<table class="details" border="1" cellpadding="3"
       cellspacing="0" width="100%" bgcolor="white">
<tr><td>
  <table width="100%" cellpadding="0" cellspacing="0" border="0">
  <tr valign="top"><td>
  <h3 class="epydoc"><span class="sig"><span class="sig-name">table</span>(<span class="sig-arg">self</span>,
        <span class="sig-arg">tableName</span>)</span>
  </h3>
  </td><td align="right" valign="top"
    ><span class="codelink"><a href="pyspark.sql-pysrc.html#SQLContext.table">source&nbsp;code</a></span>&nbsp;
    </td>
  </tr></table>
  
  <p>Returns the specified table as a <a 
  href="pyspark.sql.SchemaRDD-class.html" class="link">SchemaRDD</a>.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>srdd = sqlCtx.inferSchema(rdd)
<span class="py-prompt">&gt;&gt;&gt; </span>sqlCtx.registerRDDAsTable(srdd, <span class="py-string">&quot;table1&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>srdd2 = sqlCtx.table(<span class="py-string">&quot;table1&quot;</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>sorted(srdd.collect()) == sorted(srdd2.collect())
<span class="py-output">True</span></pre>
  <dl class="fields">
  </dl>
</td></tr></table>
</div>
<br />
<!-- ==================== NAVIGATION BAR ==================== -->
<table class="navbar" border="0" width="100%" cellpadding="0"
       bgcolor="#a0c0ff" cellspacing="0">
  <tr valign="middle">
  <!-- Home link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="pyspark-module.html">Home</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Tree link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="module-tree.html">Trees</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Index link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="identifier-index.html">Indices</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Help link -->
      <th>&nbsp;&nbsp;&nbsp;<a
        href="help.html">Help</a>&nbsp;&nbsp;&nbsp;</th>

  <!-- Project homepage -->
      <th class="navbar" align="right" width="100%">
        <table border="0" cellpadding="0" cellspacing="0">
          <tr><th class="navbar" align="center"
            ><a class="navbar" target="_top" href="http://spark.apache.org">Spark 1.1.0 Python API Docs</a></th>
          </tr></table></th>
  </tr>
</table>
<table border="0" cellpadding="0" cellspacing="0" width="100%%">
  <tr>
    <td align="left" class="footer">
    Generated by Epydoc 3.0.1 on Mon Nov 24 15:21:12 2014
    </td>
    <td align="right" class="footer">
      <a target="mainFrame" href="http://epydoc.sourceforge.net"
        >http://epydoc.sourceforge.net</a>
    </td>
  </tr>
</table>

<script type="text/javascript">
  <!--
  // Private objects are initially displayed (because if
  // javascript is turned off then we want them to be
  // visible); but by default, we want to hide them.  So hide
  // them unless we have a cookie that says to show them.
  checkCookie();
  // -->
</script>
</body>
</html>
