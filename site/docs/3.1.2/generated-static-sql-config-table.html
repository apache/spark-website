
<table class="table">
<tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr>

<tr>
    <td><code>spark.sql.cache.serializer</code></td>
    <td>org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer</td>
    <td><p>The name of a class that implements org.apache.spark.sql.columnar.CachedBatchSerializer. It will be used to translate SQL data into a format that can more efficiently be cached. The underlying API is subject to change so use with caution. Multiple classes cannot be specified. The class must have a no-arg constructor.</p></td>
    <td>3.1.0</td>
</tr>

<tr>
    <td><code>spark.sql.event.truncate.length</code></td>
    <td>2147483647</td>
    <td><p>Threshold of SQL length beyond which it will be truncated before adding to event. Defaults to no truncation. If set to 0, callsite will be logged instead.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.extensions</code></td>
    <td>(none)</td>
    <td><p>A comma-separated list of classes that implement Function1[SparkSessionExtensions, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor. For the case of function name conflicts, the last registered function name is used.</p></td>
    <td>2.2.0</td>
</tr>

<tr>
    <td><code>spark.sql.hive.metastore.barrierPrefixes</code></td>
    <td></td>
    <td><p>A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>).</p></td>
    <td>1.4.0</td>
</tr>

               <tr>
                   <td><code>spark.sql.hive.metastore.jars</code></td>
                   <td>builtin</td>
                   <td><p>Location of the jars that should be used to instantiate the HiveMetastoreClient.
This property can be one of four options:
1. "builtin"
  Use Hive 2.3.7, which is bundled with the Spark assembly when
  <code>-Phive</code> is enabled. When this option is chosen,
  <code>spark.sql.hive.metastore.version</code> must be either
  <code>2.3.7</code> or not defined.
2. "maven"
  Use Hive jars of specified version downloaded from Maven repositories.
3. "path"
  Use Hive jars configured by <code>spark.sql.hive.metastore.jars.path</code>
  in comma separated format. Support both local or remote paths.The provided jars
  should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>., public=true, version=1.4.0).
4. A classpath in the standard format for both Hive and Hadoop. The provided jars
  should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>., public=true, version=1.4.0).</p></td>
                   <td>1.4.0</td>
               </tr>

               <tr>
                   <td><code>spark.sql.hive.metastore.jars.path</code></td>
                   <td></td>
                   <td><p>Comma-separated paths of the jars that used to instantiate the HiveMetastoreClient.
This configuration is useful only when <code>{ConfigEntry(key=spark.sql.hive.metastore.jars, defaultValue=builtin, doc=
Location of the jars that should be used to instantiate the HiveMetastoreClient.
This property can be one of four options:
1. "builtin"
  Use Hive 2.3.7, which is bundled with the Spark assembly when
  &lt;code&gt;-Phive&lt;/code&gt; is enabled. When this option is chosen,
  &lt;code&gt;spark.sql.hive.metastore.version&lt;/code&gt; must be either
  &lt;code&gt;2.3.7&lt;/code&gt; or not defined.
2. "maven"
  Use Hive jars of specified version downloaded from Maven repositories.
3. "path"
  Use Hive jars configured by</code>spark.sql.hive.metastore.jars.path<code>in comma separated format. Support both local or remote paths.The provided jars
  should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are &lt;code&gt;0.12.0&lt;/code&gt; through &lt;code&gt;2.3.7&lt;/code&gt; and &lt;code&gt;3.0.0&lt;/code&gt; through &lt;code&gt;3.1.2&lt;/code&gt;., public=true, version=1.4.0).
4. A classpath in the standard format for both Hive and Hadoop. The provided jars
  should be the same version as ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version of the Hive metastore. Available options are &lt;code&gt;0.12.0&lt;/code&gt; through &lt;code&gt;2.3.7&lt;/code&gt; and &lt;code&gt;3.0.0&lt;/code&gt; through &lt;code&gt;3.1.2&lt;/code&gt;., public=true, version=1.4.0).
     , public=true, version=1.4.0).key}</code> is set as <code>path</code>.
The paths can be any of the following format:
1. file://path/to/jar/foo.jar
2. hdfs://nameservice/path/to/jar/foo.jar
3. /path/to/jar/ (path without URI scheme follow conf <code>fs.defaultFS</code>'s URI schema)
4. [http/https/ftp]://path/to/jar/foo.jar
Note that 1, 2, and 3 support wildcard. For example:
1. file://path/to/jar/<em>,file://path2/to/jar/</em>/<em>.jar
2. hdfs://nameservice/path/to/jar/</em>,hdfs://nameservice2/path/to/jar/<em>/</em>.jar</p></td>
                   <td>3.1.0</td>
               </tr>

<tr>
    <td><code>spark.sql.hive.metastore.sharedPrefixes</code></td>
    <td>com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc</td>
    <td><p>A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.</p></td>
    <td>1.4.0</td>
</tr>

<tr>
    <td><code>spark.sql.hive.metastore.version</code></td>
    <td>2.3.7</td>
    <td><p>Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>.</p></td>
    <td>1.4.0</td>
</tr>

<tr>
    <td><code>spark.sql.hive.thriftServer.singleSession</code></td>
    <td>false</td>
    <td><p>When set to true, Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.</p></td>
    <td>1.6.0</td>
</tr>

<tr>
    <td><code>spark.sql.legacy.sessionInitWithConfigDefaults</code></td>
    <td>false</td>
    <td><p>Flag to revert to legacy behavior where a cloned SparkSession receives SparkConf defaults, dropping any overrides in its parent SparkSession.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.metadataCacheTTLSeconds</code></td>
    <td>-1000ms</td>
    <td><p>Time-to-live (TTL) value for the metadata caches: partition file metadata cache and session catalog cache. This configuration only has an effect when this value having a positive value (&gt; 0). It also requires setting 'spark.sql.catalogImplementation' to <code>hive</code>, setting 'spark.sql.hive.filesourcePartitionFileCacheSize' &gt; 0 and setting 'spark.sql.hive.manageFilesourcePartitions' to <code>true</code> to be applied to the partition file metadata cache.</p></td>
    <td>3.1.0</td>
</tr>

<tr>
    <td><code>spark.sql.queryExecutionListeners</code></td>
    <td>(none)</td>
    <td><p>List of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</p></td>
    <td>2.3.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.streamingQueryListeners</code></td>
    <td>(none)</td>
    <td><p>List of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</p></td>
    <td>2.4.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.ui.enabled</code></td>
    <td>true</td>
    <td><p>Whether to run the Structured Streaming Web UI for the Spark application when the Spark Web UI is enabled.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.ui.retainedProgressUpdates</code></td>
    <td>100</td>
    <td><p>The number of progress updates to retain for a streaming query for Structured Streaming UI.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.ui.retainedQueries</code></td>
    <td>100</td>
    <td><p>The number of inactive queries to retain for Structured Streaming UI.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.ui.retainedExecutions</code></td>
    <td>1000</td>
    <td><p>Number of executions to retain in the Spark UI.</p></td>
    <td>1.5.0</td>
</tr>

<tr>
    <td><code>spark.sql.warehouse.dir</code></td>
    <td>(value of <code>$PWD/spark-warehouse</code>)</td>
    <td><p>The default location for managed databases and tables.</p></td>
    <td>2.0.0</td>
</tr>
</table>
