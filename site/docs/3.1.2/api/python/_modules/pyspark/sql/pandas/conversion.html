
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>pyspark.sql.pandas.conversion &#8212; PySpark 3.1.2 documentation</title>
    
  <link rel="stylesheet" href="../../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/pyspark.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../../../index.html">
    
      <img src="../../../../_static/spark-logo-reverse.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../getting_started/index.html">Getting Started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../user_guide/index.html">User Guide</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../reference/index.html">API Reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../development/index.html">Development</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../migration_guide/index.html">Migration Guide</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for pyspark.sql.pandas.conversion</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="kn">import</span> <span class="n">_load_from_socket</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.pandas.serializers</span> <span class="kn">import</span> <span class="n">ArrowCollectSerializer</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegralType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">ByteType</span><span class="p">,</span> <span class="n">ShortType</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="n">LongType</span><span class="p">,</span> <span class="n">FloatType</span><span class="p">,</span> \
    <span class="n">DoubleType</span><span class="p">,</span> <span class="n">BooleanType</span><span class="p">,</span> <span class="n">MapType</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">,</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">DataType</span>
<span class="kn">from</span> <span class="nn">pyspark.traceback_utils</span> <span class="kn">import</span> <span class="n">SCCallSiteSync</span>


<span class="k">class</span> <span class="nc">PandasConversionMixin</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Min-in for the conversion from Spark to pandas. Currently, only :class:`DataFrame`</span>
<span class="sd">    can use this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">toPandas</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.</span>

<span class="sd">        This is only available if Pandas is installed and available.</span>

<span class="sd">        .. versionadded:: 1.3.0</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This method should only be used if the resulting Pandas&#39;s :class:`DataFrame` is</span>
<span class="sd">        expected to be small, as all the data is loaded into the driver&#39;s memory.</span>

<span class="sd">        Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; df.toPandas()  # doctest: +SKIP</span>
<span class="sd">           age   name</span>
<span class="sd">        0    2  Alice</span>
<span class="sd">        1    5    Bob</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pandas_version</span>
        <span class="n">require_minimum_pandas_version</span><span class="p">()</span>

        <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
        <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

        <span class="n">timezone</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sql_ctx</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">sessionLocalTimeZone</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sql_ctx</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">arrowPySparkEnabled</span><span class="p">():</span>
            <span class="n">use_arrow</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">to_arrow_schema</span>
                <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pyarrow_version</span>

                <span class="n">require_minimum_pyarrow_version</span><span class="p">()</span>
                <span class="n">to_arrow_schema</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sql_ctx</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">arrowPySparkFallbackEnabled</span><span class="p">():</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;toPandas attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true; however, &quot;</span>
                        <span class="s2">&quot;failed by the reason below:</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="s2">&quot;Attempting non-optimization as &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; is set to &quot;</span>
                        <span class="s2">&quot;true.&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="n">use_arrow</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;toPandas attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has &quot;</span>
                        <span class="s2">&quot;reached the error below and will not continue because automatic fallback &quot;</span>
                        <span class="s2">&quot;with &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; has been set to &quot;</span>
                        <span class="s2">&quot;false.</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="k">raise</span>

            <span class="c1"># Try to use Arrow optimization when the schema is supported and the required version</span>
            <span class="c1"># of PyArrow is found, if &#39;spark.sql.execution.arrow.pyspark.enabled&#39; is enabled.</span>
            <span class="k">if</span> <span class="n">use_arrow</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">_check_series_localize_timestamps</span><span class="p">,</span> \
                        <span class="n">_convert_map_items_to_dict</span>
                    <span class="kn">import</span> <span class="nn">pyarrow</span>
                    <span class="c1"># Rename columns to avoid duplicated column names.</span>
                    <span class="n">tmp_column_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;col_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">))]</span>
                    <span class="n">batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="o">*</span><span class="n">tmp_column_names</span><span class="p">)</span><span class="o">.</span><span class="n">_collect_as_arrow</span><span class="p">()</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">table</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">Table</span><span class="o">.</span><span class="n">from_batches</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
                        <span class="c1"># Pandas DataFrame created from PyArrow uses datetime64[ns] for date type</span>
                        <span class="c1"># values, but we should use datetime.date to match the behavior with when</span>
                        <span class="c1"># Arrow optimization is disabled.</span>
                        <span class="n">pdf</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">(</span><span class="n">date_as_object</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                        <span class="c1"># Rename back to the original column names.</span>
                        <span class="n">pdf</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">columns</span>
                        <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
                            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">):</span>
                                <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> \
                                    <span class="n">_check_series_localize_timestamps</span><span class="p">(</span><span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">],</span> <span class="n">timezone</span><span class="p">)</span>
                            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">MapType</span><span class="p">):</span>
                                <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> \
                                    <span class="n">_convert_map_items_to_dict</span><span class="p">(</span><span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">])</span>
                        <span class="k">return</span> <span class="n">pdf</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">([],</span> <span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="c1"># We might have to allow fallback here as well but multiple Spark jobs can</span>
                    <span class="c1"># be executed. So, simply fail in this case for now.</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;toPandas attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has &quot;</span>
                        <span class="s2">&quot;reached the error below and can not continue. Note that &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; does not have an &quot;</span>
                        <span class="s2">&quot;effect on failures in the middle of &quot;</span>
                        <span class="s2">&quot;computation.</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="k">raise</span>

        <span class="c1"># Below is toPandas without Arrow optimization.</span>
        <span class="n">pdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">collect</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">column_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">fieldIdx</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">):</span>
            <span class="c1"># For duplicate column name, we use `iloc` to access it.</span>
            <span class="k">if</span> <span class="n">column_counter</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">pandas_col</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">fieldIdx</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pandas_col</span> <span class="o">=</span> <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>

            <span class="n">pandas_type</span> <span class="o">=</span> <span class="n">PandasConversionMixin</span><span class="o">.</span><span class="n">_to_corrected_pandas_type</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">)</span>
            <span class="c1"># SPARK-21766: if an integer field is nullable and has null values, it can be</span>
            <span class="c1"># inferred by pandas as float column. Once we convert the column with NaN back</span>
            <span class="c1"># to integer type e.g., np.int16, we will hit exception. So we use the inferred</span>
            <span class="c1"># float type, not the corrected type from the schema in this case.</span>
            <span class="k">if</span> <span class="n">pandas_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> \
                <span class="ow">not</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">IntegralType</span><span class="p">)</span> <span class="ow">and</span> <span class="n">field</span><span class="o">.</span><span class="n">nullable</span> <span class="ow">and</span>
                    <span class="n">pandas_col</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">()):</span>
                <span class="n">dtype</span><span class="p">[</span><span class="n">fieldIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">pandas_type</span>
            <span class="c1"># Ensure we fall back to nullable numpy types, even when whole column is null:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">IntegralType</span><span class="p">)</span> <span class="ow">and</span> <span class="n">pandas_col</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">dtype</span><span class="p">[</span><span class="n">fieldIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">BooleanType</span><span class="p">)</span> <span class="ow">and</span> <span class="n">pandas_col</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">dtype</span><span class="p">[</span><span class="n">fieldIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">object</span>

        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
            <span class="n">column_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>

            <span class="c1"># For duplicate column name, we use `iloc` to access it.</span>
            <span class="k">if</span> <span class="n">column_counter</span><span class="p">[</span><span class="n">column_name</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">series</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">index</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">series</span> <span class="o">=</span> <span class="n">pdf</span><span class="p">[</span><span class="n">column_name</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">series</span> <span class="o">=</span> <span class="n">series</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="c1"># `insert` API makes copy of data, we only do it for Series of duplicate column names.</span>
            <span class="c1"># `pdf.iloc[:, index] = pdf.iloc[:, index]...` doesn&#39;t always work because `iloc` could</span>
            <span class="c1"># return a view or a copy depending by context.</span>
            <span class="k">if</span> <span class="n">column_counter</span><span class="p">[</span><span class="n">column_name</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">column_name</span><span class="p">,</span> <span class="n">series</span><span class="p">,</span> <span class="n">allow_duplicates</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">df</span><span class="p">[</span><span class="n">column_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">series</span>

        <span class="n">pdf</span> <span class="o">=</span> <span class="n">df</span>

        <span class="k">if</span> <span class="n">timezone</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pdf</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">_check_series_convert_timestamps_local_tz</span>
            <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
                <span class="c1"># TODO: handle nested timestamps, such as ArrayType(TimestampType())?</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">):</span>
                    <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> \
                        <span class="n">_check_series_convert_timestamps_local_tz</span><span class="p">(</span><span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">],</span> <span class="n">timezone</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pdf</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_to_corrected_pandas_type</span><span class="p">(</span><span class="n">dt</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        When converting Spark SQL records to Pandas :class:`DataFrame`, the inferred data type</span>
<span class="sd">        may be wrong. This method gets the corrected data type for Pandas if that type may be</span>
<span class="sd">        inferred incorrectly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">ByteType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">int8</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">ShortType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">int16</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">IntegerType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">LongType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">FloatType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">DoubleType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">BooleanType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">bool</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="o">==</span> <span class="n">TimestampType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">datetime64</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_collect_as_arrow</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns all records as a list of ArrowRecordBatches, pyarrow must be installed</span>
<span class="sd">        and available on driver and worker Python environments.</span>
<span class="sd">        This is an experimental feature.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">SCCallSiteSync</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sc</span><span class="p">):</span>
            <span class="n">port</span><span class="p">,</span> <span class="n">auth_secret</span><span class="p">,</span> <span class="n">jsocket_auth_server</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jdf</span><span class="o">.</span><span class="n">collectAsArrowToPython</span><span class="p">()</span>

        <span class="c1"># Collect list of un-ordered batches where last element is a list of correct order indices</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">_load_from_socket</span><span class="p">((</span><span class="n">port</span><span class="p">,</span> <span class="n">auth_secret</span><span class="p">),</span> <span class="n">ArrowCollectSerializer</span><span class="p">()))</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># Join serving thread and raise any exceptions from collectAsArrowToPython</span>
            <span class="n">jsocket_auth_server</span><span class="o">.</span><span class="n">getResult</span><span class="p">()</span>

        <span class="c1"># Separate RecordBatches from batch order indices in results</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="n">results</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">batch_order</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Re-order the batch list using the correct order</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">batches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">batch_order</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">SparkConversionMixin</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Min-in for the conversion from pandas to Spark. Currently, only :class:`SparkSession`</span>
<span class="sd">    can use this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">createDataFrame</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">samplingRatio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verifySchema</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pandas_version</span>
        <span class="n">require_minimum_pandas_version</span><span class="p">()</span>

        <span class="n">timezone</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">sessionLocalTimeZone</span><span class="p">()</span>

        <span class="c1"># If no schema supplied by user then get the names of columns only</span>
        <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">schema</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span>
                      <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">)</span>
                      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">arrowPySparkEnabled</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_from_pandas_with_arrow</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">timezone</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">arrowPySparkFallbackEnabled</span><span class="p">():</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;createDataFrame attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true; however, &quot;</span>
                        <span class="s2">&quot;failed by the reason below:</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="s2">&quot;Attempting non-optimization as &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; is set to &quot;</span>
                        <span class="s2">&quot;true.&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;createDataFrame attempted Arrow optimization because &quot;</span>
                        <span class="s2">&quot;&#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true, but has &quot;</span>
                        <span class="s2">&quot;reached the error below and will not continue because automatic &quot;</span>
                        <span class="s2">&quot;fallback with &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; &quot;</span>
                        <span class="s2">&quot;has been set to false.</span><span class="se">\n</span><span class="s2">  </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="k">raise</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_from_pandas</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">timezone</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_dataframe</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">samplingRatio</span><span class="p">,</span> <span class="n">verifySchema</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert_from_pandas</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pdf</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">timezone</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Convert a pandas.DataFrame to list of records that can be used to make a DataFrame</span>

<span class="sd">         Returns</span>
<span class="sd">         -------</span>
<span class="sd">         list</span>
<span class="sd">             list of records</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">timezone</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">_check_series_convert_timestamps_tz_local</span>
            <span class="n">copied</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">StructType</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">schema</span><span class="p">:</span>
                    <span class="c1"># TODO: handle nested timestamps, such as ArrayType(TimestampType())?</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">):</span>
                        <span class="n">s</span> <span class="o">=</span> <span class="n">_check_series_convert_timestamps_tz_local</span><span class="p">(</span><span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">],</span> <span class="n">timezone</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]:</span>
                            <span class="k">if</span> <span class="ow">not</span> <span class="n">copied</span><span class="p">:</span>
                                <span class="c1"># Copy once if the series is modified to prevent the original</span>
                                <span class="c1"># Pandas DataFrame from being updated</span>
                                <span class="n">pdf</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                                <span class="n">copied</span> <span class="o">=</span> <span class="kc">True</span>
                            <span class="n">pdf</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">series</span> <span class="ow">in</span> <span class="n">pdf</span><span class="o">.</span><span class="n">iteritems</span><span class="p">():</span>
                    <span class="n">s</span> <span class="o">=</span> <span class="n">_check_series_convert_timestamps_tz_local</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">timezone</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">series</span><span class="p">:</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">copied</span><span class="p">:</span>
                            <span class="c1"># Copy once if the series is modified to prevent the original</span>
                            <span class="c1"># Pandas DataFrame from being updated</span>
                            <span class="n">pdf</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                            <span class="n">copied</span> <span class="o">=</span> <span class="kc">True</span>
                        <span class="n">pdf</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

        <span class="c1"># Convert pandas.DataFrame to list of numpy records</span>
        <span class="n">np_records</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">to_records</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Check if any columns need to be fixed for Spark to infer properly</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">np_records</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">record_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_numpy_record_dtype</span><span class="p">(</span><span class="n">np_records</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">record_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">record_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">np_records</span><span class="p">]</span>

        <span class="c1"># Convert list of numpy records to python lists</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">np_records</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_get_numpy_record_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rec</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Used when converting a pandas.DataFrame to Spark using to_records(), this will correct</span>
<span class="sd">        the dtypes of fields in a record so they can be properly loaded into Spark.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        rec : numpy.record</span>
<span class="sd">            a numpy record to check field dtypes</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        numpy.dtype</span>
<span class="sd">            corrected dtype for a numpy.record or None if no correction needed</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
        <span class="n">cur_dtypes</span> <span class="o">=</span> <span class="n">rec</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">col_names</span> <span class="o">=</span> <span class="n">cur_dtypes</span><span class="o">.</span><span class="n">names</span>
        <span class="n">record_type_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">has_rec_fix</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cur_dtypes</span><span class="p">)):</span>
            <span class="n">curr_type</span> <span class="o">=</span> <span class="n">cur_dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># If type is a datetime64 timestamp, convert to microseconds</span>
            <span class="c1"># NOTE: if dtype is datetime[ns] then np.record.tolist() will output values as longs,</span>
            <span class="c1"># conversion from [us] or lower will lead to py datetime objects, see SPARK-22417</span>
            <span class="k">if</span> <span class="n">curr_type</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;datetime64[ns]&#39;</span><span class="p">):</span>
                <span class="n">curr_type</span> <span class="o">=</span> <span class="s1">&#39;datetime64[us]&#39;</span>
                <span class="n">has_rec_fix</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">record_type_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">str</span><span class="p">(</span><span class="n">col_names</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">curr_type</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">record_type_list</span><span class="p">)</span> <span class="k">if</span> <span class="n">has_rec_fix</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_create_from_pandas_with_arrow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pdf</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">timezone</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a DataFrame from a given pandas.DataFrame by slicing it into partitions, converting</span>
<span class="sd">        to Arrow data, then sending to the JVM to parallelize. If a schema is passed in, the</span>
<span class="sd">        data types will be used to coerce the data in Pandas to Arrow conversion.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.serializers</span> <span class="kn">import</span> <span class="n">ArrowStreamPandasSerializer</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">TimestampType</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.types</span> <span class="kn">import</span> <span class="n">from_arrow_type</span><span class="p">,</span> <span class="n">to_arrow_type</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pandas_version</span><span class="p">,</span> \
            <span class="n">require_minimum_pyarrow_version</span>

        <span class="n">require_minimum_pandas_version</span><span class="p">()</span>
        <span class="n">require_minimum_pyarrow_version</span><span class="p">()</span>

        <span class="kn">from</span> <span class="nn">pandas.api.types</span> <span class="kn">import</span> <span class="n">is_datetime64_dtype</span><span class="p">,</span> <span class="n">is_datetime64tz_dtype</span>
        <span class="kn">import</span> <span class="nn">pyarrow</span> <span class="k">as</span> <span class="nn">pa</span>

        <span class="c1"># Create the Spark schema from list of names passed in with Arrow types</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">arrow_schema</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">Schema</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="n">preserve_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">struct</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">arrow_schema</span><span class="p">):</span>
                <span class="n">struct</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">from_arrow_type</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">type</span><span class="p">),</span> <span class="n">nullable</span><span class="o">=</span><span class="n">field</span><span class="o">.</span><span class="n">nullable</span><span class="p">)</span>
            <span class="n">schema</span> <span class="o">=</span> <span class="n">struct</span>

        <span class="c1"># Determine arrow types to coerce data when creating batches</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">StructType</span><span class="p">):</span>
            <span class="n">arrow_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">to_arrow_type</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">dataType</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">schema</span><span class="o">.</span><span class="n">fields</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">DataType</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Single data type </span><span class="si">%s</span><span class="s2"> is not supported with Arrow&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">schema</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Any timestamps must be coerced to be compatible with Spark</span>
            <span class="n">arrow_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">to_arrow_type</span><span class="p">(</span><span class="n">TimestampType</span><span class="p">())</span>
                           <span class="k">if</span> <span class="n">is_datetime64_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_datetime64tz_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
                           <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">pdf</span><span class="o">.</span><span class="n">dtypes</span><span class="p">]</span>

        <span class="c1"># Slice the DataFrame to be batched</span>
        <span class="n">step</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">defaultParallelism</span><span class="p">)</span>  <span class="c1"># round int up</span>
        <span class="n">pdf_slices</span> <span class="o">=</span> <span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">start</span> <span class="o">+</span> <span class="n">step</span><span class="p">]</span> <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pdf</span><span class="p">),</span> <span class="n">step</span><span class="p">))</span>

        <span class="c1"># Create list of Arrow (columns, type) for serializer dump_stream</span>
        <span class="n">arrow_data</span> <span class="o">=</span> <span class="p">[[(</span><span class="n">c</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">c</span><span class="p">),</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pdf_slice</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(),</span> <span class="n">arrow_types</span><span class="p">)]</span>
                      <span class="k">for</span> <span class="n">pdf_slice</span> <span class="ow">in</span> <span class="n">pdf_slices</span><span class="p">]</span>

        <span class="n">jsqlContext</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="o">.</span><span class="n">_jsqlContext</span>

        <span class="n">safecheck</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">arrowSafeTypeConversion</span><span class="p">()</span>
        <span class="n">col_by_name</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># col by name only applies to StructType columns, can&#39;t happen here</span>
        <span class="n">ser</span> <span class="o">=</span> <span class="n">ArrowStreamPandasSerializer</span><span class="p">(</span><span class="n">timezone</span><span class="p">,</span> <span class="n">safecheck</span><span class="p">,</span> <span class="n">col_by_name</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">reader_func</span><span class="p">(</span><span class="n">temp_filename</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">PythonSQLUtils</span><span class="o">.</span><span class="n">readArrowStreamFromFile</span><span class="p">(</span><span class="n">jsqlContext</span><span class="p">,</span> <span class="n">temp_filename</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">create_RDD_server</span><span class="p">():</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">ArrowRDDServer</span><span class="p">(</span><span class="n">jsqlContext</span><span class="p">)</span>

        <span class="c1"># Create Spark DataFrame from Arrow stream file, using one batch per partition</span>
        <span class="n">jrdd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_serialize_to_jvm</span><span class="p">(</span><span class="n">arrow_data</span><span class="p">,</span> <span class="n">ser</span><span class="p">,</span> <span class="n">reader_func</span><span class="p">,</span> <span class="n">create_RDD_server</span><span class="p">)</span>
        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">PythonSQLUtils</span><span class="o">.</span><span class="n">toDataFrame</span><span class="p">(</span><span class="n">jrdd</span><span class="p">,</span> <span class="n">schema</span><span class="o">.</span><span class="n">json</span><span class="p">(),</span> <span class="n">jsqlContext</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrapped</span><span class="p">)</span>
        <span class="n">df</span><span class="o">.</span><span class="n">_schema</span> <span class="o">=</span> <span class="n">schema</span>
        <span class="k">return</span> <span class="n">df</span>


<span class="k">def</span> <span class="nf">_test</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
    <span class="kn">import</span> <span class="nn">pyspark.sql.pandas.conversion</span>
    <span class="n">globs</span> <span class="o">=</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">pandas</span><span class="o">.</span><span class="n">conversion</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span>\
        <span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[4]&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;sql.pandas.conversion tests&quot;</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="n">globs</span><span class="p">[</span><span class="s1">&#39;spark&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spark</span>
    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span>
        <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">pandas</span><span class="o">.</span><span class="n">conversion</span><span class="p">,</span> <span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
        <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">NORMALIZE_WHITESPACE</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">REPORT_NDIFF</span><span class="p">)</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

              </div>
              
              
              <div class='prev-next-bottom'>
                

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright .<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>