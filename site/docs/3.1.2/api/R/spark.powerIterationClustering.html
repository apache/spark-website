<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: PowerIterationClustering</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.3/styles/github.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.3/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.3/languages/r.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</head><body>

<table width="100%" summary="page for spark.assignClusters {SparkR}"><tr><td>spark.assignClusters {SparkR}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>PowerIterationClustering</h2>

<h3>Description</h3>

<p>A scalable graph clustering algorithm. Users can call <code>spark.assignClusters</code> to
return a cluster assignment for each input vertex.
Run the PIC algorithm and returns a cluster assignment for each input vertex.
</p>


<h3>Usage</h3>

<pre>
spark.assignClusters(data, ...)

## S4 method for signature 'SparkDataFrame'
spark.assignClusters(
  data,
  k = 2L,
  initMode = c("random", "degree"),
  maxIter = 20L,
  sourceCol = "src",
  destinationCol = "dst",
  weightCol = NULL
)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>data</code></td>
<td>
<p>a SparkDataFrame.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>additional argument(s) passed to the method.</p>
</td></tr>
<tr valign="top"><td><code>k</code></td>
<td>
<p>the number of clusters to create.</p>
</td></tr>
<tr valign="top"><td><code>initMode</code></td>
<td>
<p>the initialization algorithm; &quot;random&quot; or &quot;degree&quot;</p>
</td></tr>
<tr valign="top"><td><code>maxIter</code></td>
<td>
<p>the maximum number of iterations.</p>
</td></tr>
<tr valign="top"><td><code>sourceCol</code></td>
<td>
<p>the name of the input column for source vertex IDs.</p>
</td></tr>
<tr valign="top"><td><code>destinationCol</code></td>
<td>
<p>the name of the input column for destination vertex IDs</p>
</td></tr>
<tr valign="top"><td><code>weightCol</code></td>
<td>
<p>weight column name. If this is not set or <code>NULL</code>,
we treat all instance weights as 1.0.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset that contains columns of vertex id and the corresponding cluster for the id.
The schema of it will be: <code>id: integer</code>, <code>cluster: integer</code>
</p>


<h3>Note</h3>

<p>spark.assignClusters(SparkDataFrame) since 3.0.0
</p>


<h3>Examples</h3>

<pre><code class="r">## Not run: 
##D df &lt;- createDataFrame(list(list(0L, 1L, 1.0), list(0L, 2L, 1.0),
##D                            list(1L, 2L, 1.0), list(3L, 4L, 1.0),
##D                            list(4L, 0L, 0.1)),
##D                       schema = c(&quot;src&quot;, &quot;dst&quot;, &quot;weight&quot;))
##D clusters &lt;- spark.assignClusters(df, initMode = &quot;degree&quot;, weightCol = &quot;weight&quot;)
##D showDF(clusters)
## End(Not run)
</code></pre>


<hr /><div style="text-align: center;">[Package <em>SparkR</em> version 3.1.2 <a href="00Index.html">Index</a>]</div>
</body></html>
