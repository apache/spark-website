
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>pyspark.ml.torch.distributor &#8212; PySpark 3.5.0 documentation</title>
    
    <link href="../../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" href="../../../../_static/styles/pydata-sphinx-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/pyspark.css" />
    
    <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="canonical" href="https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/torch/distributor.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../../index.html">
  <img src="../../../../_static/spark-logo-reverse.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../index.html">
  Overview
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../getting_started/index.html">
  Getting Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../user_guide/index.html">
  User Guides
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../reference/index.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../development/index.html">
  Development
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../migration_guide/index.html">
  Migration Guides
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for pyspark.ml.torch.distributor</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">textwrap</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Generator</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">cloudpickle</span>
<span class="kn">from</span> <span class="nn">pyspark.resource.information</span> <span class="kn">import</span> <span class="n">ResourceInformation</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.taskcontext</span> <span class="kn">import</span> <span class="n">BarrierTaskContext</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.torch.log_communication</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
    <span class="n">LogStreamingClient</span><span class="p">,</span>
    <span class="n">LogStreamingServer</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_resources</span><span class="p">(</span><span class="n">session</span><span class="p">:</span> <span class="n">SparkSession</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ResourceInformation</span><span class="p">]:</span>
    <span class="n">resources</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ResourceInformation</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">resources</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">resources</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="n">resources</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">_resources</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">return</span> <span class="n">resources</span>


<span class="k">def</span> <span class="nf">_get_conf</span><span class="p">(</span><span class="n">spark</span><span class="p">:</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">default_value</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the conf &quot;key&quot; from the given spark session,</span>
<span class="sd">    or return the default value if the conf is not set.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    spark : :class:`SparkSession`</span>
<span class="sd">        The :class:`SparkSession` for the distributor.</span>
<span class="sd">    key : str</span>
<span class="sd">        string for conf name</span>
<span class="sd">    default_value : str</span>
<span class="sd">        default value for the conf value for the given key</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    str</span>
<span class="sd">        Returns the string value that corresponds to the conf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">default_value</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">value</span>


<span class="c1"># TODO(SPARK-41589): will move the functions and tests to an external file</span>
<span class="c1">#       once we are in agreement about which functions should be in utils.py</span>
<span class="k">def</span> <span class="nf">_get_conf_boolean</span><span class="p">(</span><span class="n">spark</span><span class="p">:</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">default_value</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">_get_conf</span><span class="p">(</span><span class="n">spark</span><span class="o">=</span><span class="n">spark</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="n">default_value</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">value</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;true&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">value</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span>


<span class="k">def</span> <span class="nf">_get_logger</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">logging</span><span class="o">.</span><span class="n">Logger</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets a logger by name, or creates and configures it for the first time.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># If the logger is configured, skip the configure</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">logger</span><span class="o">.</span><span class="n">handlers</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">handlers</span><span class="p">:</span>
        <span class="n">handler</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">StreamHandler</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">addHandler</span><span class="p">(</span><span class="n">handler</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logger</span>


<span class="k">def</span> <span class="nf">_get_gpus_owned</span><span class="p">(</span><span class="n">context</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">SparkSession</span><span class="p">,</span> <span class="n">BarrierTaskContext</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets the number of GPUs that Spark scheduled to the calling task.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    context : :class:`SparkSession` or :class:`BarrierTaskContext`</span>
<span class="sd">        The :class:`SparkSession` or :class:`BarrierTaskContext` that has GPUs available.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    list</span>
<span class="sd">        The correct mapping of addresses to workers.</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    ValueError</span>
<span class="sd">        Raised if the input addresses were not found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="o">=</span> <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span>
    <span class="n">pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">&quot;^[1-9][0-9]*|0$&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">BarrierTaskContext</span><span class="p">):</span>
        <span class="n">addresses</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">resources</span><span class="p">()[</span><span class="s2">&quot;gpu&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">addresses</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">addresses</span> <span class="o">=</span> <span class="n">_get_resources</span><span class="p">(</span><span class="n">context</span><span class="p">)[</span><span class="s2">&quot;gpu&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">addresses</span>

    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="n">pattern</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">address</span><span class="p">)</span> <span class="k">for</span> <span class="n">address</span> <span class="ow">in</span> <span class="n">addresses</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Found GPU addresses </span><span class="si">{</span><span class="n">addresses</span><span class="si">}</span><span class="s2"> which &quot;</span>
            <span class="s2">&quot;are not all in the correct format &quot;</span>
            <span class="s2">&quot;for CUDA_VISIBLE_DEVICES, which requires &quot;</span>
            <span class="s2">&quot;integers with no zero padding.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
        <span class="n">gpu_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">addresses</span><span class="p">))</span>
        <span class="n">gpu_list</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
        <span class="n">gpu_owned</span> <span class="o">=</span> <span class="p">[</span><span class="n">gpu_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">gpu_indices</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">gpu_owned</span>
    <span class="k">return</span> <span class="n">addresses</span>


<span class="n">SPARK_PARTITION_ARROW_DATA_FILE</span> <span class="o">=</span> <span class="s2">&quot;SPARK_PARTITION_ARROW_DATA_FILE&quot;</span>
<span class="n">SPARK_DATAFRAME_SCHEMA_FILE</span> <span class="o">=</span> <span class="s2">&quot;SPARK_DATAFRAME_SCHEMA_FILE&quot;</span>


<span class="k">class</span> <span class="nc">Distributor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The parent class for TorchDistributor. This class shouldn&#39;t be instantiated directly.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_processes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">local_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">use_gpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">ssl_conf</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.utils</span> <span class="kn">import</span> <span class="n">is_remote</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_remote</span> <span class="o">=</span> <span class="n">is_remote</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">active</span><span class="p">()</span>

        <span class="c1"># indicate whether the server side is local mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_spark_local_master</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Refer to &#39;org.apache.spark.util.Utils#isLocalMaster&#39;</span>
        <span class="n">master</span> <span class="o">=</span> <span class="n">_get_conf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">,</span> <span class="s2">&quot;spark.master&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">master</span> <span class="o">==</span> <span class="s2">&quot;local&quot;</span> <span class="ow">or</span> <span class="n">master</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;local[&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_spark_local_master</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">_get_logger</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">=</span> <span class="n">num_processes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_mode</span> <span class="o">=</span> <span class="n">local_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_gpu</span> <span class="o">=</span> <span class="n">use_gpu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_num_tasks</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ssl_conf</span> <span class="o">=</span> <span class="n">ssl_conf</span>

    <span class="k">def</span> <span class="nf">_create_input_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">input_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">unneeded_param</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;spark&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ssl_conf&quot;</span><span class="p">,</span>
            <span class="s2">&quot;logger&quot;</span><span class="p">,</span>
            <span class="s2">&quot;is_remote&quot;</span><span class="p">,</span>
            <span class="s2">&quot;is_spark_local_master&quot;</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">del</span> <span class="n">input_params</span><span class="p">[</span><span class="n">unneeded_param</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">input_params</span>

    <span class="k">def</span> <span class="nf">_get_num_tasks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the number of Spark tasks to use for distributed training</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            The number of Spark tasks to use for distributed training</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        RuntimeError</span>
<span class="sd">            Raised when the SparkConf was misconfigured.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gpu</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_mode</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;spark.task.resource.gpu.amount&quot;</span>
                <span class="n">task_gpu_amount</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">_get_conf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">task_gpu_amount</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; was unset, so gpu usage is unavailable.&quot;</span><span class="p">)</span>
                <span class="c1"># TODO(SPARK-41916): Address situation when spark.task.resource.gpu.amount &gt; 1</span>
                <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">/</span> <span class="n">task_gpu_amount</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;spark.driver.resource.gpu.amount&quot;</span>
                <span class="k">if</span> <span class="s2">&quot;gpu&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_get_resources</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;GPUs were unable to be found on the driver.&quot;</span><span class="p">)</span>
                <span class="n">num_available_gpus</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">_get_conf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">num_available_gpus</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;GPU resources were not configured properly on the driver.&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">&gt;</span> <span class="n">num_available_gpus</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;&#39;num_processes&#39; cannot be set to a value greater than the number of &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;available GPUs on the driver, which is </span><span class="si">{</span><span class="n">num_available_gpus</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="s2">&quot;&#39;num_processes&#39; was reset to be equal to the number of available GPUs.&quot;</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">=</span> <span class="n">num_available_gpus</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span>

    <span class="k">def</span> <span class="nf">_validate_input_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;num_proccesses has to be a positive integer&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_encryption</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Checks to see if the user requires encrpytion of data.</span>
<span class="sd">        If required, throw an exception since we don&#39;t support that.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        RuntimeError</span>
<span class="sd">            Thrown when the user requires ssl encryption or when the user initializes</span>
<span class="sd">            the Distributor parent class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;ssl_conf&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Distributor doesn&#39;t have this functionality. Use TorchDistributor instead.&quot;</span>
            <span class="p">)</span>
        <span class="n">is_ssl_enabled</span> <span class="o">=</span> <span class="n">_get_conf_boolean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">,</span> <span class="s2">&quot;spark.ssl.enabled&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span>
        <span class="n">ignore_ssl</span> <span class="o">=</span> <span class="n">_get_conf_boolean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssl_conf</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="k">if</span> <span class="n">is_ssl_enabled</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="k">if</span> <span class="n">ignore_ssl</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                    This cluster has TLS encryption enabled;</span>
<span class="s2">                    however, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> does not</span>
<span class="s2">                    support data encryption in transit.</span>
<span class="s2">                    The Spark configuration</span>
<span class="s2">                    &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ssl_conf</span><span class="si">}</span><span class="s2">&#39; has been set to</span>
<span class="s2">                    &#39;true&#39; to override this</span>
<span class="s2">                    configuration and use </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> anyway. Please</span>
<span class="s2">                    note this will cause model</span>
<span class="s2">                    parameters and possibly training data to</span>
<span class="s2">                    be sent between nodes unencrypted.</span>
<span class="s2">                    &quot;&quot;&quot;</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="k">return</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                This cluster has TLS encryption enabled;</span>
<span class="s2">                however, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> does not support</span>
<span class="s2">                data encryption in transit. To override</span>
<span class="s2">                this configuration and use </span><span class="si">{</span><span class="n">name</span><span class="si">}</span>
<span class="s2">                anyway, you may set &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ssl_conf</span><span class="si">}</span><span class="s2">&#39;</span>
<span class="s2">                to &#39;true&#39; in the Spark configuration. Please note this</span>
<span class="s2">                will cause model parameters and possibly training</span>
<span class="s2">                data to be sent between nodes unencrypted.</span>
<span class="s2">                &quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>


<div class="viewcode-block" id="TorchDistributor"><a class="viewcode-back" href="../../../../reference/api/pyspark.ml.torch.distributor.TorchDistributor.html#pyspark.ml.torch.distributor.TorchDistributor">[docs]</a><span class="k">class</span> <span class="nc">TorchDistributor</span><span class="p">(</span><span class="n">Distributor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class to support distributed training on PyTorch and PyTorch Lightning using PySpark.</span>

<span class="sd">    .. versionadded:: 3.4.0</span>

<span class="sd">    .. versionchanged:: 3.5.0</span>
<span class="sd">        Supports Spark Connect.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    num_processes : int, optional</span>
<span class="sd">        An integer that determines how many different concurrent</span>
<span class="sd">        tasks are allowed. We expect spark.task.gpus = 1 for GPU-enabled training. Default</span>
<span class="sd">        should be 1; we don&#39;t want to invoke multiple cores/gpus without explicit mention.</span>
<span class="sd">    local_mode : bool, optional</span>
<span class="sd">        A boolean that determines whether we are using the driver</span>
<span class="sd">        node for training. Default should be false; we don&#39;t want to invoke executors without</span>
<span class="sd">        explicit mention.</span>
<span class="sd">    use_gpu : bool, optional</span>
<span class="sd">        A boolean that indicates whether or not we are doing training</span>
<span class="sd">        on the GPU. Note that there are differences in how GPU-enabled code looks like and</span>
<span class="sd">        how CPU-specific code looks like.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Run PyTorch Training locally on GPU (using a PyTorch native function)</span>

<span class="sd">    &gt;&gt;&gt; def train(learning_rate):</span>
<span class="sd">    ...     import torch.distributed</span>
<span class="sd">    ...     torch.distributed.init_process_group(backend=&quot;nccl&quot;)</span>
<span class="sd">    ...     # ...</span>
<span class="sd">    ...     torch.destroy_process_group()</span>
<span class="sd">    ...     return model # or anything else</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; distributor = TorchDistributor(</span>
<span class="sd">    ...     num_processes=2,</span>
<span class="sd">    ...     local_mode=True,</span>
<span class="sd">    ...     use_gpu=True)</span>
<span class="sd">    &gt;&gt;&gt; model = distributor.run(train, 1e-3)</span>

<span class="sd">    Run PyTorch Training on GPU (using a file with PyTorch code)</span>

<span class="sd">    &gt;&gt;&gt; distributor = TorchDistributor(</span>
<span class="sd">    ...     num_processes=2,</span>
<span class="sd">    ...     local_mode=False,</span>
<span class="sd">    ...     use_gpu=True)</span>
<span class="sd">    &gt;&gt;&gt; distributor.run(&quot;/path/to/train.py&quot;, &quot;--learning-rate=1e-3&quot;)</span>

<span class="sd">    Run PyTorch Lightning Training on GPU</span>

<span class="sd">    &gt;&gt;&gt; num_proc = 2</span>
<span class="sd">    &gt;&gt;&gt; def train():</span>
<span class="sd">    ...     from pytorch_lightning import Trainer</span>
<span class="sd">    ...     # ...</span>
<span class="sd">    ...     # required to set devices = 1 and num_nodes = num_processes for multi node</span>
<span class="sd">    ...     # required to set devices = num_processes and num_nodes = 1 for single node multi GPU</span>
<span class="sd">    ...     trainer = Trainer(accelerator=&quot;gpu&quot;, devices=1, num_nodes=num_proc, strategy=&quot;ddp&quot;)</span>
<span class="sd">    ...     trainer.fit()</span>
<span class="sd">    ...     # ...</span>
<span class="sd">    ...     return trainer</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; distributor = TorchDistributor(</span>
<span class="sd">    ...     num_processes=num_proc,</span>
<span class="sd">    ...     local_mode=True,</span>
<span class="sd">    ...     use_gpu=True)</span>
<span class="sd">    &gt;&gt;&gt; trainer = distributor.run(train)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_PICKLED_FUNC_FILE</span> <span class="o">=</span> <span class="s2">&quot;func.pickle&quot;</span>
    <span class="n">_TRAIN_FILE</span> <span class="o">=</span> <span class="s2">&quot;train.py&quot;</span>
    <span class="n">_PICKLED_OUTPUT_FILE</span> <span class="o">=</span> <span class="s2">&quot;output.pickle&quot;</span>
    <span class="n">_TORCH_SSL_CONF</span> <span class="o">=</span> <span class="s2">&quot;pytorch.spark.distributor.ignoreSsl&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_processes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">local_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">use_gpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">_ssl_conf</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">_TORCH_SSL_CONF</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the distributor.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        num_processes : int, optional</span>
<span class="sd">            An integer that determines how many different concurrent</span>
<span class="sd">            tasks are allowed. We expect spark.task.gpus = 1 for GPU-enabled training. Default</span>
<span class="sd">            should be 1; we don&#39;t want to invoke multiple cores/gpus without explicit mention.</span>
<span class="sd">        local_mode : bool, optional</span>
<span class="sd">            A boolean that determines whether we are using the driver</span>
<span class="sd">            node for training. Default should be false; we don&#39;t want to invoke executors without</span>
<span class="sd">            explicit mention.</span>
<span class="sd">        use_gpu : bool, optional</span>
<span class="sd">            A boolean that indicates whether or not we are doing training</span>
<span class="sd">            on the GPU. Note that there are differences in how GPU-enabled code looks like and</span>
<span class="sd">            how CPU-specific code looks like.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If any of the parameters are incorrect.</span>
<span class="sd">        RuntimeError</span>
<span class="sd">            If an active SparkSession is unavailable.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_processes</span><span class="p">,</span> <span class="n">local_mode</span><span class="p">,</span> <span class="n">use_gpu</span><span class="p">,</span> <span class="n">ssl_conf</span><span class="o">=</span><span class="n">_ssl_conf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_input_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_input_params</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_torchrun_args</span><span class="p">(</span><span class="n">local_mode</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">num_processes</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given the mode and the number of processes, create the arguments to be given to for torch</span>

<span class="sd">        Parameters</span>
<span class="sd">        ---------</span>
<span class="sd">        local_mode: bool</span>
<span class="sd">            Whether or not we are running training locally or in a distributed fashion</span>

<span class="sd">        num_processes: int</span>
<span class="sd">            The number of processes that we are going to use</span>

<span class="sd">        Returns</span>
<span class="sd">        ------</span>
<span class="sd">        Tuple[List[Any], int]</span>
<span class="sd">            A tuple containing a list of arguments to pass as pytorch args,</span>
<span class="sd">            as well as the number of processes per node</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">local_mode</span><span class="p">:</span>
            <span class="n">torchrun_args</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;--standalone&quot;</span><span class="p">,</span> <span class="s2">&quot;--nnodes=1&quot;</span><span class="p">]</span>
            <span class="n">processes_per_node</span> <span class="o">=</span> <span class="n">num_processes</span>
            <span class="k">return</span> <span class="n">torchrun_args</span><span class="p">,</span> <span class="n">processes_per_node</span>

        <span class="n">master_addr</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">]</span>
        <span class="n">master_port</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">]</span>
        <span class="n">node_rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">]</span>
        <span class="n">torchrun_args</span> <span class="o">=</span> <span class="p">[</span>
            <span class="sa">f</span><span class="s2">&quot;--nnodes=</span><span class="si">{</span><span class="n">num_processes</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--node_rank=</span><span class="si">{</span><span class="n">node_rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--rdzv_endpoint=</span><span class="si">{</span><span class="n">master_addr</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">master_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="s2">&quot;--rdzv_id=0&quot;</span><span class="p">,</span>  <span class="c1"># TODO: setup random ID that is gleaned from env variables</span>
        <span class="p">]</span>
        <span class="n">processes_per_node</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">torchrun_args</span><span class="p">,</span> <span class="n">processes_per_node</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_create_torchrun_command</span><span class="p">(</span>
        <span class="n">input_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">path_to_train_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">local_mode</span> <span class="o">=</span> <span class="n">input_params</span><span class="p">[</span><span class="s2">&quot;local_mode&quot;</span><span class="p">]</span>
        <span class="n">num_processes</span> <span class="o">=</span> <span class="n">input_params</span><span class="p">[</span><span class="s2">&quot;num_processes&quot;</span><span class="p">]</span>

        <span class="n">torchrun_args</span><span class="p">,</span> <span class="n">processes_per_node</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_get_torchrun_args</span><span class="p">(</span>
            <span class="n">local_mode</span><span class="o">=</span><span class="n">local_mode</span><span class="p">,</span> <span class="n">num_processes</span><span class="o">=</span><span class="n">num_processes</span>
        <span class="p">)</span>
        <span class="n">args_string</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">args</span><span class="p">))</span>  <span class="c1"># converting all args to strings</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span>
            <span class="s2">&quot;-m&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pyspark.ml.torch.torch_run_process_wrapper&quot;</span><span class="p">,</span>
            <span class="o">*</span><span class="n">torchrun_args</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--nproc_per_node=</span><span class="si">{</span><span class="n">processes_per_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">path_to_train_file</span><span class="p">,</span>
            <span class="o">*</span><span class="n">args_string</span><span class="p">,</span>
        <span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_execute_command</span><span class="p">(</span>
        <span class="n">cmd</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">_prctl</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">redirect_to_stdout</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">log_streaming_client</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogStreamingClient</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_TAIL_LINES_TO_KEEP</span> <span class="o">=</span> <span class="mi">100</span>

        <span class="n">task</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">Popen</span><span class="p">(</span>
            <span class="n">cmd</span><span class="p">,</span>
            <span class="n">stdout</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span>
            <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">STDOUT</span><span class="p">,</span>
            <span class="n">stdin</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span>
            <span class="n">env</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">task</span><span class="o">.</span><span class="n">stdin</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
        <span class="n">tail</span><span class="p">:</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">_TAIL_LINES_TO_KEEP</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">task</span><span class="o">.</span><span class="n">stdout</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">decoded</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
                <span class="n">tail</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoded</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">redirect_to_stdout</span><span class="p">:</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">log_streaming_client</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="n">log_streaming_client</span><span class="o">.</span><span class="n">failed</span>
                        <span class="ow">and</span> <span class="p">(</span>
                            <span class="n">log_streaming_client</span><span class="o">.</span><span class="n">sock</span><span class="o">.</span><span class="n">getsockname</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                            <span class="o">==</span> <span class="n">log_streaming_client</span><span class="o">.</span><span class="n">sock</span><span class="o">.</span><span class="n">getpeername</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">):</span>
                        <span class="c1"># If log_streaming_client and log_stream_server are in the same</span>
                        <span class="c1"># node (typical case is spark local mode),</span>
                        <span class="c1"># server side will redirect the log to STDOUT,</span>
                        <span class="c1"># to avoid STDOUT outputs duplication, skip redirecting</span>
                        <span class="c1"># logs to STDOUT in client side.</span>
                        <span class="k">pass</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">decoded</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">log_streaming_client</span><span class="p">:</span>
                    <span class="n">log_streaming_client</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">decoded</span><span class="o">.</span><span class="n">rstrip</span><span class="p">())</span>
            <span class="n">task</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">task</span><span class="o">.</span><span class="n">poll</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">task</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>  <span class="c1"># SIGTERM</span>
                    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">task</span><span class="o">.</span><span class="n">poll</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">task</span><span class="o">.</span><span class="n">kill</span><span class="p">()</span>  <span class="c1"># SIGKILL</span>
                <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
                    <span class="k">pass</span>
        <span class="k">if</span> <span class="n">task</span><span class="o">.</span><span class="n">returncode</span> <span class="o">!=</span> <span class="n">os</span><span class="o">.</span><span class="n">EX_OK</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tail</span><span class="p">)</span> <span class="o">==</span> <span class="n">_TAIL_LINES_TO_KEEP</span><span class="p">:</span>
                <span class="n">last_n_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;last </span><span class="si">{</span><span class="n">_TAIL_LINES_TO_KEEP</span><span class="si">}</span><span class="s2"> lines of the task output are&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">last_n_msg</span> <span class="o">=</span> <span class="s2">&quot;task output is&quot;</span>
            <span class="n">task_output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tail</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Command </span><span class="si">{</span><span class="n">cmd</span><span class="si">}</span><span class="s2"> failed with return code </span><span class="si">{</span><span class="n">task</span><span class="o">.</span><span class="n">returncode</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">last_n_msg</span><span class="si">}</span><span class="s2"> included below: </span><span class="si">{</span><span class="n">task_output</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_output_from_framework_wrapper</span><span class="p">(</span>
        <span class="n">framework_wrapper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="n">input_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
        <span class="n">train_object</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">run_pytorch_file_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function is meant to get the output from framework wrapper function by passing in the</span>
<span class="sd">        correct arguments, depending on the type of train_object.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        framework_wrapper: Optional[Callable]</span>
<span class="sd">            Function pointer that will be invoked. Can either be the function that runs distributed</span>
<span class="sd">            training on files if train_object is a string. Otherwise, it will be the function that</span>
<span class="sd">            runs distributed training for functions if the train_object is a Callable</span>
<span class="sd">        input_params: Dict</span>
<span class="sd">            A dictionary that maps parameter to arguments for the command to be created.</span>
<span class="sd">        train_object: Union[Callable, str]</span>
<span class="sd">            This input comes from the user. If the user inputs a string, then this means</span>
<span class="sd">            it&#39;s a filepath. Otherwise, if the input is a function, then this means that</span>
<span class="sd">            the user wants to run this function in a distributed manner.</span>
<span class="sd">        run_pytorch_file_fn: Optional[Callable]</span>
<span class="sd">            The function that will be used to run distributed training of a file;</span>
<span class="sd">            mainly used for the distributed training using a function.</span>
<span class="sd">        *args: Any</span>
<span class="sd">            Extra arguments to be used by framework wrapper.</span>
<span class="sd">        **kwargs: Any</span>
<span class="sd">            Extra keyword args to be used. Not currently supported but kept for</span>
<span class="sd">            future improvement.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Optional[Any]</span>
<span class="sd">            Returns the result of the framework_wrapper</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">framework_wrapper</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;`framework_wrapper` is not set. ...&quot;</span><span class="p">)</span>
        <span class="c1"># The object to train is a file path, so framework_wrapper is some</span>
        <span class="c1"># run_training_on_pytorch_file function.</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">train_object</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">framework_wrapper</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">train_object</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># We are doing training with a function, will call run_training_on_pytorch_function</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">run_pytorch_file_fn</span><span class="p">:</span>
                <span class="n">run_pytorch_file_fn</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_run_training_on_pytorch_file</span>
            <span class="k">return</span> <span class="n">framework_wrapper</span><span class="p">(</span>
                <span class="n">input_params</span><span class="p">,</span> <span class="n">train_object</span><span class="p">,</span> <span class="n">run_pytorch_file_fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_run_local_training</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">framework_wrapper_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">train_object</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">run_pytorch_file_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="o">=</span> <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span>
        <span class="n">cuda_state_was_set</span> <span class="o">=</span> <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span>
        <span class="n">old_cuda_visible_devices</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Only replace the GPUs with &#39;SparkContext.resources&#39; in legacy mode.</span>
            <span class="c1"># In connect mode, this replacement is skipped since only GPUs on the client side</span>
            <span class="c1"># can be used.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gpu</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_remote</span><span class="p">:</span>
                <span class="n">gpus_owned</span> <span class="o">=</span> <span class="n">_get_gpus_owned</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">)</span>
                <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="nb">hash</span><span class="p">(</span><span class="n">train_object</span><span class="p">))</span>
                <span class="n">selected_gpus</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">gpus_owned</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="p">)]</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">selected_gpus</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Started local training with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="si">}</span><span class="s2"> processes&quot;</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_get_output_from_framework_wrapper</span><span class="p">(</span>
                <span class="n">framework_wrapper_fn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_params</span><span class="p">,</span>
                <span class="n">train_object</span><span class="p">,</span>
                <span class="n">run_pytorch_file_fn</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished local training with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="si">}</span><span class="s2"> processes&quot;</span><span class="p">)</span>

        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cuda_state_was_set</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_cuda_visible_devices</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
                    <span class="k">del</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_get_spark_task_function</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">framework_wrapper_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="n">train_object</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">run_pytorch_file_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="n">input_dataframe</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;DataFrame&quot;</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a spark task function that is used inside `mapPartitions`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        framework_wrapper_fn : Optional[Callable]</span>
<span class="sd">            The function that determines whether we are running training</span>
<span class="sd">            on a PyTorch file or a PyTorch function.</span>
<span class="sd">        train_object : Union[Callable, str]</span>
<span class="sd">            The actual train function/file.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Callable</span>
<span class="sd">            The wrapped function ready for use with `mapPartitions`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_processes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span>
        <span class="n">use_gpu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gpu</span>
        <span class="n">input_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_params</span>
        <span class="n">driver_address</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">driver_address</span>
        <span class="n">log_streaming_server_port</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_streaming_server_port</span>
        <span class="n">is_spark_local_master</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_spark_local_master</span>
        <span class="n">driver_owned_gpus</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">is_spark_local_master</span> <span class="ow">and</span> <span class="n">use_gpu</span><span class="p">:</span>
            <span class="n">driver_owned_gpus</span> <span class="o">=</span> <span class="n">_get_gpus_owned</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_dataframe</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">schema_json</span> <span class="o">=</span> <span class="n">input_dataframe</span><span class="o">.</span><span class="n">schema</span><span class="o">.</span><span class="n">jsonValue</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">schema_json</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Spark task program</span>
        <span class="k">def</span> <span class="nf">wrapped_train_fn</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>  <span class="c1"># type: ignore[no-untyped-def]</span>
            <span class="kn">import</span> <span class="nn">os</span>
            <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
            <span class="kn">import</span> <span class="nn">pyarrow</span>
            <span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">BarrierTaskContext</span>

            <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="o">=</span> <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span>

            <span class="k">def</span> <span class="nf">get_free_port</span><span class="p">(</span><span class="n">address</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="s2">&quot;BarrierTaskContext&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
                <span class="n">port</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="kn">import</span> <span class="nn">socket</span>

                        <span class="n">sock</span> <span class="o">=</span> <span class="n">socket</span><span class="o">.</span><span class="n">socket</span><span class="p">()</span>
                        <span class="n">sock</span><span class="o">.</span><span class="n">bind</span><span class="p">((</span><span class="n">address</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
                        <span class="n">port</span> <span class="o">=</span> <span class="n">sock</span><span class="o">.</span><span class="n">getsockname</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="k">except</span> <span class="n">socket</span><span class="o">.</span><span class="n">error</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="n">available_port</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">allGather</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">port</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">available_port</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Failed to find free port for distributed training.&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">available_port</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">set_torch_config</span><span class="p">(</span><span class="n">context</span><span class="p">:</span> <span class="s2">&quot;BarrierTaskContext&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">addrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span><span class="o">.</span><span class="n">address</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">context</span><span class="o">.</span><span class="n">getTaskInfos</span><span class="p">()]</span>

                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">addrs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_free_port</span><span class="p">(</span><span class="n">addrs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">context</span><span class="p">))</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">num_processes</span><span class="p">)</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;NODE_RANK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">())</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">())</span>

                <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">num_processes</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;TorchDistributor._train_on_dataframe requires setting num_processes &quot;</span>
                        <span class="s2">&quot;equal to input spark dataframe partition number.&quot;</span>
                    <span class="p">)</span>

            <span class="k">if</span> <span class="n">is_spark_local_master</span><span class="p">:</span>
                <span class="c1"># distributed training on a local mode spark cluster</span>
                <span class="k">def</span> <span class="nf">set_gpus</span><span class="p">(</span><span class="n">context</span><span class="p">:</span> <span class="s2">&quot;BarrierTaskContext&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
                        <span class="k">return</span>

                    <span class="n">gpu_owned</span> <span class="o">=</span> <span class="n">driver_owned_gpus</span><span class="p">[</span><span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">()]</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_owned</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">set_gpus</span><span class="p">(</span><span class="n">context</span><span class="p">:</span> <span class="s2">&quot;BarrierTaskContext&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">CUDA_VISIBLE_DEVICES</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
                        <span class="k">return</span>

                    <span class="n">gpus_owned</span> <span class="o">=</span> <span class="n">_get_gpus_owned</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gpus_owned</span><span class="p">)</span>

            <span class="n">context</span> <span class="o">=</span> <span class="n">BarrierTaskContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span>
                <span class="n">set_gpus</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="n">set_torch_config</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>

            <span class="n">log_streaming_client</span> <span class="o">=</span> <span class="n">LogStreamingClient</span><span class="p">(</span><span class="n">driver_address</span><span class="p">,</span> <span class="n">log_streaming_server_port</span><span class="p">)</span>
            <span class="n">input_params</span><span class="p">[</span><span class="s2">&quot;log_streaming_client&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_streaming_client</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_setup_spark_partition_data</span><span class="p">(</span><span class="n">iterator</span><span class="p">,</span> <span class="n">schema_json</span><span class="p">):</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_get_output_from_framework_wrapper</span><span class="p">(</span>
                        <span class="n">framework_wrapper_fn</span><span class="p">,</span>
                        <span class="n">input_params</span><span class="p">,</span>
                        <span class="n">train_object</span><span class="p">,</span>
                        <span class="n">run_pytorch_file_fn</span><span class="p">,</span>
                        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">LogStreamingClient</span><span class="o">.</span><span class="n">_destroy</span><span class="p">()</span>
                <span class="k">except</span> <span class="ne">BaseException</span><span class="p">:</span>
                    <span class="k">pass</span>

            <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">output_bytes</span> <span class="o">=</span> <span class="n">cloudpickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
                <span class="n">output_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_bytes</span><span class="p">)</span>

                <span class="c1"># In Spark Connect, DataFrame.collect stacks rows to size</span>
                <span class="c1"># &#39;spark.connect.grpc.arrow.maxBatchSize&#39; (default 4MiB),</span>
                <span class="c1"># here use 4KiB for each chunk, which mean each arrow batch</span>
                <span class="c1"># may contain about 1000 chunks.</span>
                <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">4096</span>
                <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">while</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">:</span>
                    <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_bytes</span><span class="p">[</span><span class="n">index</span> <span class="p">:</span> <span class="n">index</span> <span class="o">+</span> <span class="n">chunk_size</span><span class="p">])</span>
                    <span class="n">index</span> <span class="o">+=</span> <span class="n">chunk_size</span>

                <span class="k">yield</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">RecordBatch</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;chunk&quot;</span><span class="p">:</span> <span class="n">chunks</span><span class="p">}))</span>

        <span class="k">return</span> <span class="n">wrapped_train_fn</span>

    <span class="k">def</span> <span class="nf">_run_distributed_training</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">framework_wrapper_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">train_object</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">run_pytorch_file_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="n">spark_dataframe</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;DataFrame&quot;</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">framework_wrapper_fn</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Unknown combination of parameters&quot;</span><span class="p">)</span>

        <span class="n">log_streaming_server</span> <span class="o">=</span> <span class="n">LogStreamingServer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver_address</span> <span class="o">=</span> <span class="n">_get_conf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">,</span> <span class="s2">&quot;spark.driver.host&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">driver_address</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">log_streaming_server</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">spark_host_address</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">driver_address</span><span class="p">)</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># wait for the server to start</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_streaming_server_port</span> <span class="o">=</span> <span class="n">log_streaming_server</span><span class="o">.</span><span class="n">port</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># If starting log streaming server failed, we don&#39;t need to break</span>
            <span class="c1"># the distributor training but emit a warning instead.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_streaming_server_port</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Start torch distributor log streaming server failed, &quot;</span>
                <span class="s2">&quot;You cannot receive logs sent from distributor workers, &quot;</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;error: </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">spark_task_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_spark_task_function</span><span class="p">(</span>
                <span class="n">framework_wrapper_fn</span><span class="p">,</span>
                <span class="n">train_object</span><span class="p">,</span>
                <span class="n">run_pytorch_file_fn</span><span class="p">,</span>
                <span class="n">spark_dataframe</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_encryption</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Started distributed training with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="si">}</span><span class="s2"> executor processes&quot;</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">spark_dataframe</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">input_df</span> <span class="o">=</span> <span class="n">spark_dataframe</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span>
                    <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">numPartitions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span>
                <span class="p">)</span>
            <span class="n">rows</span> <span class="o">=</span> <span class="n">input_df</span><span class="o">.</span><span class="n">mapInArrow</span><span class="p">(</span>
                <span class="n">func</span><span class="o">=</span><span class="n">spark_task_function</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="s2">&quot;chunk binary&quot;</span><span class="p">,</span> <span class="n">barrier</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
            <span class="n">output_bytes</span> <span class="o">=</span> <span class="sa">b</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">row</span><span class="o">.</span><span class="n">chunk</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">])</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">cloudpickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">output_bytes</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">log_streaming_server</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Finished distributed training with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="si">}</span><span class="s2"> executor processes&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_run_training_on_pytorch_file</span><span class="p">(</span>
        <span class="n">input_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">train_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Running pytorch file does not support key-word type arguments.&quot;</span><span class="p">)</span>
        <span class="n">log_streaming_client</span> <span class="o">=</span> <span class="n">input_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;log_streaming_client&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">training_command</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_create_torchrun_command</span><span class="p">(</span>
            <span class="n">input_params</span><span class="p">,</span> <span class="n">train_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span>
        <span class="p">)</span>
        <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_execute_command</span><span class="p">(</span>
            <span class="n">training_command</span><span class="p">,</span> <span class="n">log_streaming_client</span><span class="o">=</span><span class="n">log_streaming_client</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">_setup_files</span><span class="p">(</span>
        <span class="n">train_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="n">save_dir</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_create_save_dir</span><span class="p">()</span>
        <span class="n">pickle_file_path</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_save_pickled_function</span><span class="p">(</span>
            <span class="n">save_dir</span><span class="p">,</span> <span class="n">train_fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
        <span class="n">output_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_PICKLED_OUTPUT_FILE</span><span class="p">)</span>
        <span class="n">train_file_path</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_create_torchrun_train_file</span><span class="p">(</span>
            <span class="n">save_dir</span><span class="p">,</span> <span class="n">pickle_file_path</span><span class="p">,</span> <span class="n">output_file_path</span>
        <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">train_file_path</span><span class="p">,</span> <span class="n">output_file_path</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_cleanup_files</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">_setup_spark_partition_data</span><span class="p">(</span>
        <span class="n">partition_data_iterator</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">input_schema_json</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.serializers</span> <span class="kn">import</span> <span class="n">ArrowStreamSerializer</span>
        <span class="kn">from</span> <span class="nn">pyspark.files</span> <span class="kn">import</span> <span class="n">SparkFiles</span>
        <span class="kn">import</span> <span class="nn">json</span>

        <span class="k">if</span> <span class="n">input_schema_json</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">yield</span>
            <span class="k">return</span>

        <span class="c1"># We need to temporarily write partition data into a temp dir,</span>
        <span class="c1"># partition data might be huge, so we need to write it under</span>
        <span class="c1"># configured `SPARK_LOCAL_DIRS`.</span>
        <span class="n">save_dir</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_create_save_dir</span><span class="p">(</span><span class="n">root_dir</span><span class="o">=</span><span class="n">SparkFiles</span><span class="o">.</span><span class="n">getRootDirectory</span><span class="p">())</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">serializer</span> <span class="o">=</span> <span class="n">ArrowStreamSerializer</span><span class="p">()</span>
            <span class="n">arrow_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s2">&quot;data.arrow&quot;</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">arrow_file_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">serializer</span><span class="o">.</span><span class="n">dump_stream</span><span class="p">(</span><span class="n">partition_data_iterator</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">tell</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># Nothing is written to file, this partition is empty</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Empty Spark partition is not allowed in &quot;</span>
                        <span class="s2">&quot;TorchDistributor.train_on_dataframe.&quot;</span>
                    <span class="p">)</span>

            <span class="n">schema_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s2">&quot;schema.json&quot;</span><span class="p">)</span>
            <span class="n">schema_json_string</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">input_schema_json</span><span class="p">)</span>

            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">schema_file_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">schema_json_string</span><span class="p">)</span>

            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">SPARK_PARTITION_ARROW_DATA_FILE</span><span class="p">]</span> <span class="o">=</span> <span class="n">arrow_file_path</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">SPARK_DATAFRAME_SCHEMA_FILE</span><span class="p">]</span> <span class="o">=</span> <span class="n">schema_file_path</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">SPARK_PARTITION_ARROW_DATA_FILE</span><span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">SPARK_DATAFRAME_SCHEMA_FILE</span><span class="p">)</span>
            <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_cleanup_files</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_run_training_on_pytorch_function</span><span class="p">(</span>
        <span class="n">input_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">train_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">run_pytorch_file_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">run_pytorch_file_fn</span><span class="p">:</span>
            <span class="n">run_pytorch_file_fn</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_run_training_on_pytorch_file</span>

        <span class="k">with</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_setup_files</span><span class="p">(</span><span class="n">train_fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">as</span> <span class="p">(</span>
            <span class="n">train_file_path</span><span class="p">,</span>
            <span class="n">output_file_path</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">run_pytorch_file_fn</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">train_file_path</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_file_path</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;TorchDistributor failed during training.&quot;</span>
                    <span class="s2">&quot;View stdout logs for detailed error message.&quot;</span>
                <span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_get_pickled_output</span><span class="p">(</span><span class="n">output_file_path</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;TorchDistributor failed due to a pickling error. &quot;</span>
                    <span class="s2">&quot;View stdout logs for detailed error message.&quot;</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_create_save_dir</span><span class="p">(</span><span class="n">root_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="c1"># TODO: need to do this in a safe way to avoid issues during concurrent runs</span>
        <span class="k">return</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(</span><span class="nb">dir</span><span class="o">=</span><span class="n">root_dir</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_cleanup_files</span><span class="p">(</span><span class="n">save_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">ignore_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_save_pickled_function</span><span class="p">(</span>
        <span class="n">save_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">train_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">saved_pickle_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_PICKLED_FUNC_FILE</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">saved_pickle_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">cloudpickle</span><span class="o">.</span><span class="n">dump</span><span class="p">((</span><span class="n">train_fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">),</span> <span class="n">f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">saved_pickle_path</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_create_torchrun_train_file</span><span class="p">(</span>
        <span class="n">save_dir_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">pickle_file_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">output_file_path</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">code</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                    from pyspark import cloudpickle</span>
<span class="s2">                    import os</span>

<span class="s2">                    if __name__ == &quot;__main__&quot;:</span>
<span class="s2">                        with open(&quot;</span><span class="si">{</span><span class="n">pickle_file_path</span><span class="si">}</span><span class="s2">&quot;, &quot;rb&quot;) as f:</span>
<span class="s2">                            train_fn, args, kwargs = cloudpickle.load(f)</span>
<span class="s2">                        output = train_fn(*args, **kwargs)</span>
<span class="s2">                        with open(&quot;</span><span class="si">{</span><span class="n">output_file_path</span><span class="si">}</span><span class="s2">&quot;, &quot;wb&quot;) as f:</span>
<span class="s2">                            cloudpickle.dump(output, f)</span>
<span class="s2">                    &quot;&quot;&quot;</span>
        <span class="p">)</span>
        <span class="n">saved_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir_path</span><span class="p">,</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_TRAIN_FILE</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">saved_file_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">code</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">saved_file_path</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_pickled_output</span><span class="p">(</span><span class="n">output_file_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_file_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">cloudpickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

<div class="viewcode-block" id="TorchDistributor.run"><a class="viewcode-back" href="../../../../reference/api/pyspark.ml.torch.distributor.TorchDistributor.html#pyspark.ml.torch.distributor.TorchDistributor.run">[docs]</a>    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_object</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs distributed training.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        train_object : callable object or str</span>
<span class="sd">            Either a PyTorch function, PyTorch Lightning function, or the path to a python file</span>
<span class="sd">            that launches distributed training.</span>
<span class="sd">        args :</span>
<span class="sd">            If train_object is a python function and not a path to a python file, args need</span>
<span class="sd">            to be the input parameters to that function. It would look like</span>

<span class="sd">            &gt;&gt;&gt; model = distributor.run(train, 1e-3, 64)</span>

<span class="sd">            where train is a function and 1e-3 and 64 are regular numeric inputs to the function.</span>

<span class="sd">            If train_object is a python file, then args would be the command-line arguments for</span>
<span class="sd">            that python file which are all in the form of strings. An example would be</span>

<span class="sd">            &gt;&gt;&gt; distributor.run(&quot;/path/to/train.py&quot;, &quot;--learning-rate=1e-3&quot;, &quot;--batch-size=64&quot;)</span>

<span class="sd">            where since the input is a path, all of the parameters are strings that can be</span>
<span class="sd">            handled by argparse in that python file.</span>
<span class="sd">        kwargs :</span>
<span class="sd">            If train_object is a python function and not a path to a python file, kwargs need</span>
<span class="sd">            to be the key-word input parameters to that function. It would look like</span>

<span class="sd">            &gt;&gt;&gt; model = distributor.run(train, tol=1e-3, max_iter=64)</span>

<span class="sd">            where train is a function of 2 arguments `tol` and `max_iter`.</span>

<span class="sd">            If train_object is a python file, then you should not set kwargs arguments.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">            Returns the output of train_object called with args inside spark rank 0 task if the</span>
<span class="sd">            train_object is a Callable with an expected output. Returns None if train_object is</span>
<span class="sd">            a file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run</span><span class="p">(</span>
            <span class="n">train_object</span><span class="p">,</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_run_training_on_pytorch_file</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_run</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">train_object</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">run_pytorch_file_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_object</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">framework_wrapper_fn</span> <span class="o">=</span> <span class="n">run_pytorch_file_fn</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">framework_wrapper_fn</span> <span class="o">=</span> <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_run_training_on_pytorch_function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_mode</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_local_training</span><span class="p">(</span>
                <span class="n">framework_wrapper_fn</span><span class="p">,</span> <span class="n">train_object</span><span class="p">,</span> <span class="n">run_pytorch_file_fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_distributed_training</span><span class="p">(</span>
                <span class="n">framework_wrapper_fn</span><span class="p">,</span> <span class="n">train_object</span><span class="p">,</span> <span class="n">run_pytorch_file_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_train_on_dataframe</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">train_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">spark_dataframe</span><span class="p">:</span> <span class="s2">&quot;DataFrame&quot;</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs distributed training using provided Spark DataFrame as input data.</span>
<span class="sd">        You should ensure the input Spark DataFrame have evenly distributed partitions,</span>
<span class="sd">        and this method starts a barrier Spark job that each Spark task in the job</span>
<span class="sd">        process one partition of the input Spark DataFrame.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        train_function :</span>
<span class="sd">            Either a PyTorch function, PyTorch Lightning function that launches distributed</span>
<span class="sd">            training. Note that inside the function, you can call</span>
<span class="sd">            `pyspark.ml.torch.distributor.get_spark_partition_data_loader` API to get a torch</span>
<span class="sd">            data loader, the data loader loads data from the corresponding partition of the</span>
<span class="sd">            input Spark DataFrame.</span>
<span class="sd">        spark_dataframe :</span>
<span class="sd">            An input Spark DataFrame that can be used in PyTorch `train_function` function.</span>
<span class="sd">            See `train_function` argument doc for details.</span>
<span class="sd">        args :</span>
<span class="sd">            `args` need to be the input parameters to `train_function` function. It would look like</span>

<span class="sd">            &gt;&gt;&gt; model = distributor.run(train, 1e-3, 64)</span>

<span class="sd">            where train is a function and 1e-3 and 64 are regular numeric inputs to the function.</span>
<span class="sd">        kwargs :</span>
<span class="sd">            `kwargs` need to be the key-word input parameters to `train_function` function.</span>
<span class="sd">            It would look like</span>

<span class="sd">            &gt;&gt;&gt; model = distributor.run(train, tol=1e-3, max_iter=64)</span>

<span class="sd">            where train is a function of 2 arguments `tol` and `max_iter`.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">            Returns the output of `train_function` called with args inside Spark rank 0 task.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_mode</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;TorchDistributor.train_on_dataframe requires setting &quot;</span>
                <span class="s2">&quot;TorchDistributor.local_mode to False.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_distributed_training</span><span class="p">(</span>
            <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_run_training_on_pytorch_function</span><span class="p">,</span>
            <span class="n">train_function</span><span class="p">,</span>
            <span class="n">TorchDistributor</span><span class="o">.</span><span class="n">_run_training_on_pytorch_file</span><span class="p">,</span>
            <span class="n">spark_dataframe</span><span class="p">,</span>
            <span class="o">*</span><span class="n">args</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_get_spark_partition_data_loader</span><span class="p">(</span>
    <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">prefetch_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function must be called inside the `train_function` where `train_function`</span>
<span class="sd">    is the input argument of `TorchDistributor.train_on_dataframe`.</span>
<span class="sd">    The function returns a pytorch data loader that loads data from</span>
<span class="sd">    the corresponding spark partition data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    num_samples :</span>
<span class="sd">        Number of samples to generate per epoch. If `num_samples` is less than the number of</span>
<span class="sd">        rows in the spark partition, it generate the first `num_samples` rows of</span>
<span class="sd">        the spark partition, if `num_samples` is greater than the number of</span>
<span class="sd">        rows in the spark partition, then after the iterator loaded all rows from the partition,</span>
<span class="sd">        it wraps round back to the first row.</span>
<span class="sd">    batch_size:</span>
<span class="sd">        How many samples per batch to load.</span>
<span class="sd">    num_workers:</span>
<span class="sd">        How many subprocesses to use for data loading.</span>
<span class="sd">        0 means that the data will be loaded in the main process.</span>
<span class="sd">    prefetch_factor:</span>
<span class="sd">        Number of batches loaded in advance by each worker</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span>
    <span class="kn">from</span> <span class="nn">pyspark.ml.torch.data</span> <span class="kn">import</span> <span class="n">_SparkPartitionTorchDataset</span>
    <span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

    <span class="n">arrow_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">SPARK_PARTITION_ARROW_DATA_FILE</span><span class="p">]</span>
    <span class="n">schema_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">SPARK_DATAFRAME_SCHEMA_FILE</span><span class="p">]</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">schema_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="o">.</span><span class="n">fromJson</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">))</span>

    <span class="n">dataset</span> <span class="o">=</span> <span class="n">_SparkPartitionTorchDataset</span><span class="p">(</span><span class="n">arrow_file</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">prefetch_factor</span><span class="o">=</span><span class="n">prefetch_factor</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># if num_workers is zero, we cannot set `prefetch_factor` otherwise</span>
        <span class="c1"># torch will raise error.</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
</pre></div>

              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright .<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>