
<table class="table">
<tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr>

<tr>
    <td><code>spark.sql.cache.serializer</code></td>
    <td>org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer</td>
    <td><p>The name of a class that implements org.apache.spark.sql.columnar.CachedBatchSerializer. It will be used to translate SQL data into a format that can more efficiently be cached. The underlying API is subject to change so use with caution. Multiple classes cannot be specified. The class must have a no-arg constructor.</p></td>
    <td>3.1.0</td>
</tr>

<tr>
    <td><code>spark.sql.catalog.spark_catalog.defaultDatabase</code></td>
    <td>default</td>
    <td><p>The default database for session catalog.</p></td>
    <td>3.4.0</td>
</tr>

<tr>
    <td><code>spark.sql.event.truncate.length</code></td>
    <td>2147483647</td>
    <td><p>Threshold of SQL length beyond which it will be truncated before adding to event. Defaults to no truncation. If set to 0, callsite will be logged instead.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.extensions</code></td>
    <td>(none)</td>
    <td><p>A comma-separated list of classes that implement Function1[SparkSessionExtensions, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor. For the case of function name conflicts, the last registered function name is used.</p></td>
    <td>2.2.0</td>
</tr>

<tr>
    <td><code>spark.sql.hive.metastore.barrierPrefixes</code></td>
    <td></td>
    <td><p>A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>).</p></td>
    <td>1.4.0</td>
</tr>

               <tr>
                   <td><code>spark.sql.hive.metastore.jars</code></td>
                   <td>builtin</td>
                   <td><p>Location of the jars that should be used to instantiate the HiveMetastoreClient.
This property can be one of four options:
1. "builtin"
  Use Hive 2.3.9, which is bundled with the Spark assembly when
  <code>-Phive</code> is enabled. When this option is chosen,
  <code>spark.sql.hive.metastore.version</code> must be either
  <code>2.3.9</code> or not defined.
2. "maven"
  Use Hive jars of specified version downloaded from Maven repositories.
3. "path"
  Use Hive jars configured by <code>spark.sql.hive.metastore.jars.path</code>
  in comma separated format. Support both local or remote paths.The provided jars
  should be the same version as <code>spark.sql.hive.metastore.version</code>.
4. A classpath in the standard format for both Hive and Hadoop. The provided jars
  should be the same version as <code>spark.sql.hive.metastore.version</code>.</p></td>
                   <td>1.4.0</td>
               </tr>

               <tr>
                   <td><code>spark.sql.hive.metastore.jars.path</code></td>
                   <td></td>
                   <td><p>Comma-separated paths of the jars that used to instantiate the HiveMetastoreClient.
This configuration is useful only when <code>spark.sql.hive.metastore.jars</code> is set as <code>path</code>.
The paths can be any of the following format:
1. file://path/to/jar/foo.jar
2. hdfs://nameservice/path/to/jar/foo.jar
3. /path/to/jar/ (path without URI scheme follow conf <code>fs.defaultFS</code>'s URI schema)
4. [http/https/ftp]://path/to/jar/foo.jar
Note that 1, 2, and 3 support wildcard. For example:
1. file://path/to/jar/<em>,file://path2/to/jar/</em>/<em>.jar
2. hdfs://nameservice/path/to/jar/</em>,hdfs://nameservice2/path/to/jar/<em>/</em>.jar</p></td>
                   <td>3.1.0</td>
               </tr>

<tr>
    <td><code>spark.sql.hive.metastore.sharedPrefixes</code></td>
    <td>com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc</td>
    <td><p>A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.</p></td>
    <td>1.4.0</td>
</tr>

<tr>
    <td><code>spark.sql.hive.metastore.version</code></td>
    <td>2.3.9</td>
    <td><p>Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.9</code> and <code>3.0.0</code> through <code>3.1.3</code>.</p></td>
    <td>1.4.0</td>
</tr>

<tr>
    <td><code>spark.sql.hive.thriftServer.singleSession</code></td>
    <td>false</td>
    <td><p>When set to true, Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.</p></td>
    <td>1.6.0</td>
</tr>

<tr>
    <td><code>spark.sql.hive.version</code></td>
    <td>2.3.9</td>
    <td><p>The compiled, a.k.a, builtin Hive version of the Spark distribution bundled with. Note that, this a read-only conf and only used to report the built-in hive version. If you want a different metastore client for Spark to call, please refer to spark.sql.hive.metastore.version.</p></td>
    <td>1.1.1</td>
</tr>

<tr>
    <td><code>spark.sql.metadataCacheTTLSeconds</code></td>
    <td>-1000ms</td>
    <td><p>Time-to-live (TTL) value for the metadata caches: partition file metadata cache and session catalog cache. This configuration only has an effect when this value having a positive value (&gt; 0). It also requires setting 'spark.sql.catalogImplementation' to <code>hive</code>, setting 'spark.sql.hive.filesourcePartitionFileCacheSize' &gt; 0 and setting 'spark.sql.hive.manageFilesourcePartitions' to <code>true</code> to be applied to the partition file metadata cache.</p></td>
    <td>3.1.0</td>
</tr>

<tr>
    <td><code>spark.sql.queryExecutionListeners</code></td>
    <td>(none)</td>
    <td><p>List of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</p></td>
    <td>2.3.0</td>
</tr>

<tr>
    <td><code>spark.sql.sources.disabledJdbcConnProviderList</code></td>
    <td></td>
    <td><p>Configures a list of JDBC connection providers, which are disabled. The list contains the name of the JDBC connection providers separated by comma.</p></td>
    <td>3.1.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.streamingQueryListeners</code></td>
    <td>(none)</td>
    <td><p>List of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</p></td>
    <td>2.4.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.ui.enabled</code></td>
    <td>true</td>
    <td><p>Whether to run the Structured Streaming Web UI for the Spark application when the Spark Web UI is enabled.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.ui.retainedProgressUpdates</code></td>
    <td>100</td>
    <td><p>The number of progress updates to retain for a streaming query for Structured Streaming UI.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.ui.retainedQueries</code></td>
    <td>100</td>
    <td><p>The number of inactive queries to retain for Structured Streaming UI.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.ui.retainedExecutions</code></td>
    <td>1000</td>
    <td><p>Number of executions to retain in the Spark UI.</p></td>
    <td>1.5.0</td>
</tr>

<tr>
    <td><code>spark.sql.warehouse.dir</code></td>
    <td>(value of <code>$PWD/spark-warehouse</code>)</td>
    <td><p>The default location for managed databases and tables.</p></td>
    <td>2.0.0</td>
</tr>
</table>
