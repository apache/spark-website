
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>PySpark Usage Guide for Pandas with Apache Arrow - Spark 3.0.0-preview2 Documentation</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html">
                      <img src="img/spark-logo-hd.png" style="height:50px;"/></a><span class="version">3.0.0-preview2</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">Overview</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Programming Guides<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">Quick Start</a></li>
                                <li><a href="rdd-programming-guide.html">RDDs, Accumulators, Broadcasts Vars</a></li>
                                <li><a href="sql-programming-guide.html">SQL, DataFrames, and Datasets</a></li>
                                <li><a href="structured-streaming-programming-guide.html">Structured Streaming</a></li>
                                <li><a href="streaming-programming-guide.html">Spark Streaming (DStreams)</a></li>
                                <li><a href="ml-guide.html">MLlib (Machine Learning)</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX (Graph Processing)</a></li>
                                <li><a href="sparkr.html">SparkR (R on Spark)</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API Docs<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">Scala</a></li>
                                <li><a href="api/java/index.html">Java</a></li>
                                <li><a href="api/python/index.html">Python</a></li>
                                <li><a href="api/R/index.html">R</a></li>
                                <li><a href="api/sql/index.html">SQL, Built-in Functions</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Deploying<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">Overview</a></li>
                                <li><a href="submitting-applications.html">Submitting Applications</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark Standalone</a></li>
                                <li><a href="running-on-mesos.html">Mesos</a></li>
                                <li><a href="running-on-yarn.html">YARN</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">More<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">Configuration</a></li>
                                <li><a href="monitoring.html">Monitoring</a></li>
                                <li><a href="tuning.html">Tuning Guide</a></li>
                                <li><a href="job-scheduling.html">Job Scheduling</a></li>
                                <li><a href="security.html">Security</a></li>
                                <li><a href="hardware-provisioning.html">Hardware Provisioning</a></li>
                                <li><a href="migration-guide.html">Migration Guide</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">Building Spark</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">Contributing to Spark</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">Third Party Projects</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v3.0.0-preview2</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                
                    <div class="left-menu-wrapper">
    <div class="left-menu">
        <h3><a href="sql-programming-guide.html">Spark SQL Guide</a></h3>
        
<ul>

    <li>
        <a href="sql-getting-started.html">
            
                Getting Started
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources.html">
            
                Data Sources
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-performance-tuning.html">
            
                Performance Tuning
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-distributed-sql-engine.html">
            
                Distributed SQL Engine
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html">
            
                <b>PySpark Usage Guide for Pandas with Apache Arrow</b>
            
        </a>
    </li>
    
    
        
<ul>

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html#apache-arrow-in-spark">
            
                Apache Arrow in Spark
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html#enabling-for-conversion-tofrom-pandas">
            
                Enabling for Conversion to/from Pandas
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html#pandas-udfs-aka-vectorized-udfs">
            
                Pandas UDFs (a.k.a. Vectorized UDFs)
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html#usage-notes">
            
                Usage Notes
            
        </a>
    </li>
    
    

</ul>

    

    <li>
        <a href="sql-migration-old.html">
            
                Migration Guide
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-ref.html">
            
                SQL Reference
            
        </a>
    </li>
    
    

</ul>

    </div>
</div>
                
                <input id="nav-trigger" class="nav-trigger" checked type="checkbox">
                <label for="nav-trigger"></label>
                <div class="content-with-sidebar" id="content">
                    
                        <h1 class="title">PySpark Usage Guide for Pandas with Apache Arrow</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#apache-arrow-in-pyspark" id="markdown-toc-apache-arrow-in-pyspark">Apache Arrow in PySpark</a>    <ul>
      <li><a href="#ensure-pyarrow-installed" id="markdown-toc-ensure-pyarrow-installed">Ensure PyArrow Installed</a></li>
    </ul>
  </li>
  <li><a href="#enabling-for-conversion-tofrom-pandas" id="markdown-toc-enabling-for-conversion-tofrom-pandas">Enabling for Conversion to/from Pandas</a></li>
  <li><a href="#pandas-udfs-aka-vectorized-udfs" id="markdown-toc-pandas-udfs-aka-vectorized-udfs">Pandas UDFs (a.k.a. Vectorized UDFs)</a>    <ul>
      <li><a href="#scalar" id="markdown-toc-scalar">Scalar</a></li>
    </ul>
  </li>
  <li><a href="#section" id="markdown-toc-section">1    4</a></li>
  <li><a href="#section-1" id="markdown-toc-section-1">2    9</a></li>
  <li><a href="#dtype-int64" id="markdown-toc-dtype-int64">dtype: int64</a></li>
  <li><a href="#multiplyfuncx-x" id="markdown-toc-multiplyfuncx-x">|multiply_func(x, x)|</a></li>
  <li><a href="#section-2" id="markdown-toc-section-2">+&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-+</a></li>
  <li><a href="#section-3" id="markdown-toc-section-3">|                  1|</a></li>
  <li><a href="#section-4" id="markdown-toc-section-4">|                  4|</a></li>
  <li><a href="#section-5" id="markdown-toc-section-5">|                  9|</a></li>
  <li><a href="#section-6" id="markdown-toc-section-6">+&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-+</a>    <ul>
      <li><a href="#scalar-iterator" id="markdown-toc-scalar-iterator">Scalar Iterator</a></li>
    </ul>
  </li>
  <li><a href="#the-input-to-the-underlying-function-is-an-iterator-of-pdseries" id="markdown-toc-the-input-to-the-underlying-function-is-an-iterator-of-pdseries">the input to the underlying function is an iterator of pd.Series.</a></li>
  <li><a href="#plusonex" id="markdown-toc-plusonex">|plus_one(x)|</a></li>
  <li><a href="#section-7" id="markdown-toc-section-7">+&#8212;&#8212;&#8212;&#8211;+</a></li>
  <li><a href="#section-8" id="markdown-toc-section-8">|          2|</a></li>
  <li><a href="#section-9" id="markdown-toc-section-9">|          3|</a></li>
  <li><a href="#section-10" id="markdown-toc-section-10">|          4|</a></li>
  <li><a href="#section-11" id="markdown-toc-section-11">+&#8212;&#8212;&#8212;&#8211;+</a></li>
  <li><a href="#the-input-to-the-underlying-function-is-an-iterator-of-pdseries-tuple" id="markdown-toc-the-input-to-the-underlying-function-is-an-iterator-of-pdseries-tuple">the input to the underlying function is an iterator of pd.Series tuple.</a></li>
  <li><a href="#multiplytwocolsx-x" id="markdown-toc-multiplytwocolsx-x">|multiply_two_cols(x, x)|</a></li>
  <li><a href="#section-12" id="markdown-toc-section-12">+&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;+</a></li>
  <li><a href="#section-13" id="markdown-toc-section-13">|                      1|</a></li>
  <li><a href="#section-14" id="markdown-toc-section-14">|                      4|</a></li>
  <li><a href="#section-15" id="markdown-toc-section-15">|                      9|</a></li>
  <li><a href="#section-16" id="markdown-toc-section-16">+&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;+</a></li>
  <li><a href="#the-input-to-the-underlying-function-is-an-iterator-of-pddataframe" id="markdown-toc-the-input-to-the-underlying-function-is-an-iterator-of-pddataframe">the input to the underlying function is an iterator of pd.DataFrame.</a></li>
  <li><a href="#y" id="markdown-toc-y">|  y|</a></li>
  <li><a href="#section-17" id="markdown-toc-section-17">+&#8212;+</a></li>
  <li><a href="#section-18" id="markdown-toc-section-18">|  1|</a></li>
  <li><a href="#section-19" id="markdown-toc-section-19">|  4|</a></li>
  <li><a href="#section-20" id="markdown-toc-section-20">|  9|</a></li>
  <li><a href="#section-21" id="markdown-toc-section-21">+&#8212;+</a></li>
  <li><a href="#wrap-your-code-with-tryfinally-or-use-context-managers-to-ensure" id="markdown-toc-wrap-your-code-with-tryfinally-or-use-context-managers-to-ensure">Wrap your code with try/finally or use context managers to ensure</a></li>
  <li><a href="#the-release-of-resources-at-the-end" id="markdown-toc-the-release-of-resources-at-the-end">the release of resources at the end.</a></li>
  <li><a href="#plusyx" id="markdown-toc-plusyx">|plus_y(x)|</a></li>
  <li><a href="#section-22" id="markdown-toc-section-22">+&#8212;&#8212;&#8212;+</a></li>
  <li><a href="#section-23" id="markdown-toc-section-23">|        2|</a></li>
  <li><a href="#section-24" id="markdown-toc-section-24">|        3|</a></li>
  <li><a href="#section-25" id="markdown-toc-section-25">|        4|</a></li>
  <li><a href="#section-26" id="markdown-toc-section-26">+&#8212;&#8212;&#8212;+</a>    <ul>
      <li><a href="#grouped-map" id="markdown-toc-grouped-map">Grouped Map</a></li>
    </ul>
  </li>
  <li><a href="#id---v" id="markdown-toc-id---v">| id|   v|</a></li>
  <li><a href="#section-27" id="markdown-toc-section-27">+&#8212;+&#8212;-+</a></li>
  <li><a href="#section-28" id="markdown-toc-section-28">|  1|-0.5|</a></li>
  <li><a href="#section-29" id="markdown-toc-section-29">|  1| 0.5|</a></li>
  <li><a href="#section-30" id="markdown-toc-section-30">|  2|-3.0|</a></li>
  <li><a href="#section-31" id="markdown-toc-section-31">|  2|-1.0|</a></li>
  <li><a href="#section-32" id="markdown-toc-section-32">|  2| 4.0|</a></li>
  <li><a href="#section-33" id="markdown-toc-section-33">+&#8212;+&#8212;-+</a>    <ul>
      <li><a href="#grouped-aggregate" id="markdown-toc-grouped-aggregate">Grouped Aggregate</a></li>
    </ul>
  </li>
  <li><a href="#idmeanudfv" id="markdown-toc-idmeanudfv">| id|mean_udf(v)|</a></li>
  <li><a href="#section-34" id="markdown-toc-section-34">+&#8212;+&#8212;&#8212;&#8212;&#8211;+</a></li>
  <li><a href="#section-35" id="markdown-toc-section-35">|  1|        1.5|</a></li>
  <li><a href="#section-36" id="markdown-toc-section-36">|  2|        6.0|</a></li>
  <li><a href="#section-37" id="markdown-toc-section-37">+&#8212;+&#8212;&#8212;&#8212;&#8211;+</a></li>
  <li><a href="#id---vmeanv" id="markdown-toc-id---vmeanv">| id|   v|mean_v|</a></li>
  <li><a href="#section-38" id="markdown-toc-section-38">+&#8212;+&#8212;-+&#8212;&#8212;+</a></li>
  <li><a href="#section-39" id="markdown-toc-section-39">|  1| 1.0|   1.5|</a></li>
  <li><a href="#section-40" id="markdown-toc-section-40">|  1| 2.0|   1.5|</a></li>
  <li><a href="#section-41" id="markdown-toc-section-41">|  2| 3.0|   6.0|</a></li>
  <li><a href="#section-42" id="markdown-toc-section-42">|  2| 5.0|   6.0|</a></li>
  <li><a href="#section-43" id="markdown-toc-section-43">|  2|10.0|   6.0|</a></li>
  <li><a href="#section-44" id="markdown-toc-section-44">+&#8212;+&#8212;-+&#8212;&#8212;+</a>    <ul>
      <li><a href="#map-iterator" id="markdown-toc-map-iterator">Map Iterator</a></li>
    </ul>
  </li>
  <li><a href="#idage" id="markdown-toc-idage">| id|age|</a></li>
  <li><a href="#section-45" id="markdown-toc-section-45">+&#8212;+&#8212;+</a></li>
  <li><a href="#section-46" id="markdown-toc-section-46">|  1| 21|</a></li>
  <li><a href="#section-47" id="markdown-toc-section-47">+&#8212;+&#8212;+</a>    <ul>
      <li><a href="#cogrouped-map" id="markdown-toc-cogrouped-map">Cogrouped Map</a></li>
    </ul>
  </li>
  <li><a href="#time-id-v1-v2" id="markdown-toc-time-id-v1-v2">|    time| id| v1| v2|</a></li>
  <li><a href="#section-48" id="markdown-toc-section-48">+&#8212;&#8212;&#8211;+&#8212;+&#8212;+&#8212;+</a></li>
  <li><a href="#x" id="markdown-toc-x">|20000101|  1|1.0|  x|</a></li>
  <li><a href="#x-1" id="markdown-toc-x-1">|20000102|  1|3.0|  x|</a></li>
  <li><a href="#y-1" id="markdown-toc-y-1">|20000101|  2|2.0|  y|</a></li>
  <li><a href="#y-2" id="markdown-toc-y-2">|20000102|  2|4.0|  y|</a></li>
  <li><a href="#section-49" id="markdown-toc-section-49">+&#8212;&#8212;&#8211;+&#8212;+&#8212;+&#8212;+</a>    <ul>
      <li><a href="#usage-notes" id="markdown-toc-usage-notes">Usage Notes</a>        <ul>
          <li><a href="#supported-sql-types" id="markdown-toc-supported-sql-types">Supported SQL Types</a></li>
          <li><a href="#setting-arrow-batch-size" id="markdown-toc-setting-arrow-batch-size">Setting Arrow Batch Size</a></li>
          <li><a href="#timestamp-with-time-zone-semantics" id="markdown-toc-timestamp-with-time-zone-semantics">Timestamp with Time Zone Semantics</a></li>
          <li><a href="#compatibiliy-setting-for-pyarrow--0150-and-spark-23x-24x" id="markdown-toc-compatibiliy-setting-for-pyarrow--0150-and-spark-23x-24x">Compatibiliy Setting for PyArrow &gt;= 0.15.0 and Spark 2.3.x, 2.4.x</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="apache-arrow-in-pyspark">Apache Arrow in PySpark</h2>

<p>Apache Arrow is an in-memory columnar data format that is used in Spark to efficiently transfer
data between JVM and Python processes. This currently is most beneficial to Python users that
work with Pandas/NumPy data. Its usage is not automatic and might require some minor
changes to configuration or code to take full advantage and ensure compatibility. This guide will
give a high-level description of how to use Arrow in Spark and highlight any differences when
working with Arrow-enabled data.</p>

<h3 id="ensure-pyarrow-installed">Ensure PyArrow Installed</h3>

<p>If you install PySpark using pip, then PyArrow can be brought in as an extra dependency of the
SQL module with the command <code class="highlighter-rouge">pip install pyspark[sql]</code>. Otherwise, you must ensure that PyArrow
is installed and available on all cluster nodes. The current supported version is 0.12.1.
You can install using pip or conda from the conda-forge channel. See PyArrow
<a href="https://arrow.apache.org/docs/python/install.html">installation</a> for details.</p>

<h2 id="enabling-for-conversion-tofrom-pandas">Enabling for Conversion to/from Pandas</h2>

<p>Arrow is available as an optimization when converting a Spark DataFrame to a Pandas DataFrame
using the call <code class="highlighter-rouge">toPandas()</code> and when creating a Spark DataFrame from a Pandas DataFrame with
<code class="highlighter-rouge">createDataFrame(pandas_df)</code>. To use Arrow when executing these calls, users need to first set
the Spark configuration <code class="highlighter-rouge">spark.sql.execution.arrow.pyspark.enabled</code> to <code class="highlighter-rouge">true</code>. This is disabled by default.</p>

<p>In addition, optimizations enabled by <code class="highlighter-rouge">spark.sql.execution.arrow.pyspark.enabled</code> could fallback automatically
to non-Arrow optimization implementation if an error occurs before the actual computation within Spark.
This can be controlled by <code class="highlighter-rouge">spark.sql.execution.arrow.pyspark.fallback.enabled</code>.</p>

<div class="codetabs">
<div data-lang="python">
    <p><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span></p>

    <p><span class="c1"># Enable Arrow-based columnar data transfers
</span><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">&#8220;spark.sql.execution.arrow.pyspark.enabled&#8221;</span><span class="p">,</span> <span class="s">&#8220;true&#8221;</span><span class="p">)</span></p>

    <p><span class="c1"># Generate a Pandas DataFrame
</span><span class="n">pdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span></p>

    <p><span class="c1"># Create a Spark DataFrame from a Pandas DataFrame using Arrow
</span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span></p>

    <p><span class="c1"># Convert the Spark DataFrame back to a Pandas DataFrame using Arrow
</span><span class="n">result_pdf</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&#8220;*&#8221;</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span></p>

    <div><small>Find full example code at "examples/src/main/python/sql/arrow.py" in the Spark repo.</small></div>
  </div>
</div>

<p>Using the above optimizations with Arrow will produce the same results as when Arrow is not
enabled. Note that even with Arrow, <code class="highlighter-rouge">toPandas()</code> results in the collection of all records in the
DataFrame to the driver program and should be done on a small subset of the data. Not all Spark
data types are currently supported and an error can be raised if a column has an unsupported type,
see <a href="#supported-sql-types">Supported SQL Types</a>. If an error occurs during <code class="highlighter-rouge">createDataFrame()</code>,
Spark will fall back to create the DataFrame without Arrow.</p>

<h2 id="pandas-udfs-aka-vectorized-udfs">Pandas UDFs (a.k.a. Vectorized UDFs)</h2>

<p>Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and
Pandas to work with the data. A Pandas UDF is defined using the keyword <code class="highlighter-rouge">pandas_udf</code> as a decorator
or to wrap the function, no additional configuration is required. Currently, there are two types of
Pandas UDF: Scalar and Grouped Map.</p>

<h3 id="scalar">Scalar</h3>

<p>Scalar Pandas UDFs are used for vectorizing scalar operations. They can be used with functions such
as <code class="highlighter-rouge">select</code> and <code class="highlighter-rouge">withColumn</code>. The Python function should take <code class="highlighter-rouge">pandas.Series</code> as inputs and return
a <code class="highlighter-rouge">pandas.Series</code> of the same length. Internally, Spark will execute a Pandas UDF by splitting
columns into batches and calling the function for each batch as a subset of the data, then
concatenating the results together.</p>

<p>The following example shows how to create a scalar Pandas UDF that computes the product of 2 columns.</p>

<div class="codetabs">
<div data-lang="python">
    <p><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span></p>

    <p><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">pandas_udf</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">LongType</span></p>

    <p><span class="c1"># Declare the function and create the UDF
</span><span class="k">def</span> <span class="nf">multiply_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span></p>

    <p><span class="n">multiply</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span><span class="n">multiply_func</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">LongType</span><span class="p">())</span></p>

    <p><span class="c1"># The function for a pandas_udf should be able to execute with local Pandas data
</span><span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">multiply_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
<span class="c1"># 0    1</span></p>
    <h1 id="section">1    4</h1>
    <h1 id="section-1">2    9</h1>
    <h1 id="dtype-int64">dtype: int64</h1>
    <p>&lt;/span&gt;
<span class="c1"># Create a Spark DataFrame, &#8216;spark&#8217; is an existing SparkSession
</span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">&#8220;x&#8221;</span><span class="p">]))</span></p>

    <p><span class="c1"># Execute function as a Spark vectorized UDF
</span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">multiply</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">&#8220;x&#8221;</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s">&#8220;x&#8221;</span><span class="p">)))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-+</span></p>
    <h1 id="multiplyfuncx-x">|multiply_func(x, x)|</h1>
    <h1 id="section-2">+&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-+</h1>
    <h1 id="section-3">|                  1|</h1>
    <h1 id="section-4">|                  4|</h1>
    <h1 id="section-5">|                  9|</h1>
    <h1 id="section-6">+&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-+</h1>
    <p>&lt;/span&gt;</p>
    <div><small>Find full example code at "examples/src/main/python/sql/arrow.py" in the Spark repo.</small></div>
  </div>
</div>

<h3 id="scalar-iterator">Scalar Iterator</h3>

<p>Scalar iterator (<code class="highlighter-rouge">SCALAR_ITER</code>) Pandas UDF is the same as scalar Pandas UDF above except that the
underlying Python function takes an iterator of batches as input instead of a single batch and,
instead of returning a single output batch, it yields output batches or returns an iterator of
output batches.
It is useful when the UDF execution requires initializing some states, e.g., loading an machine
learning model file to apply inference to every input batch.</p>

<p>The following example shows how to create scalar iterator Pandas UDFs:</p>

<div class="codetabs">
<div data-lang="python">
    <p><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span></p>

    <p><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">struct</span><span class="p">,</span> <span class="n">PandasUDFType</span></p>

    <p><span class="n">pdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">&#8220;x&#8221;</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span></p>

    <p><span class="c1"># When the UDF is called with a single column that is not StructType,</span></p>
    <h1 id="the-input-to-the-underlying-function-is-an-iterator-of-pdseries">the input to the underlying function is an iterator of pd.Series.</h1>
    <p>&lt;/span&gt;<span class="o">@</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s">&#8220;long&#8221;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">SCALAR_ITER</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">plus_one</span><span class="p">(</span><span class="n">batch_iter</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch_iter</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span></p>

    <p><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">plus_one</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">&#8220;x&#8221;</span><span class="p">)))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +&#8212;&#8212;&#8212;&#8211;+</span></p>
    <h1 id="plusonex">|plus_one(x)|</h1>
    <h1 id="section-7">+&#8212;&#8212;&#8212;&#8211;+</h1>
    <h1 id="section-8">|          2|</h1>
    <h1 id="section-9">|          3|</h1>
    <h1 id="section-10">|          4|</h1>
    <h1 id="section-11">+&#8212;&#8212;&#8212;&#8211;+</h1>
    <p>&lt;/span&gt;
<span class="c1"># When the UDF is called with more than one columns,</span></p>
    <h1 id="the-input-to-the-underlying-function-is-an-iterator-of-pdseries-tuple">the input to the underlying function is an iterator of pd.Series tuple.</h1>
    <p>&lt;/span&gt;<span class="o">@</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s">&#8220;long&#8221;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">SCALAR_ITER</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">multiply_two_cols</span><span class="p">(</span><span class="n">batch_iter</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch_iter</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span></p>

    <p><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">multiply_two_cols</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">&#8220;x&#8221;</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s">&#8220;x&#8221;</span><span class="p">)))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;+</span></p>
    <h1 id="multiplytwocolsx-x">|multiply_two_cols(x, x)|</h1>
    <h1 id="section-12">+&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;+</h1>
    <h1 id="section-13">|                      1|</h1>
    <h1 id="section-14">|                      4|</h1>
    <h1 id="section-15">|                      9|</h1>
    <h1 id="section-16">+&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;+</h1>
    <p>&lt;/span&gt;
<span class="c1"># When the UDF is called with a single column that is StructType,</span></p>
    <h1 id="the-input-to-the-underlying-function-is-an-iterator-of-pddataframe">the input to the underlying function is an iterator of pd.DataFrame.</h1>
    <p>&lt;/span&gt;<span class="o">@</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s">&#8220;long&#8221;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">SCALAR_ITER</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">multiply_two_nested_cols</span><span class="p">(</span><span class="n">pdf_iter</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">pdf</span> <span class="ow">in</span> <span class="n">pdf_iter</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">pdf</span><span class="p">[</span><span class="s">&#8220;a&#8221;</span><span class="p">]</span> <span class="o">*</span> <span class="n">pdf</span><span class="p">[</span><span class="s">&#8220;b&#8221;</span><span class="p">]</span></p>

    <p><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
    <span class="n">multiply_two_nested_cols</span><span class="p">(</span>
        <span class="n">struct</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">&#8220;x&#8221;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#8220;a&#8221;</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s">&#8220;x&#8221;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#8220;b&#8221;</span><span class="p">))</span>
    <span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#8220;y&#8221;</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +&#8212;+</span></p>
    <h1 id="y">|  y|</h1>
    <h1 id="section-17">+&#8212;+</h1>
    <h1 id="section-18">|  1|</h1>
    <h1 id="section-19">|  4|</h1>
    <h1 id="section-20">|  9|</h1>
    <h1 id="section-21">+&#8212;+</h1>
    <p>&lt;/span&gt;
<span class="c1"># In the UDF, you can initialize some states before processing batches.</span></p>
    <h1 id="wrap-your-code-with-tryfinally-or-use-context-managers-to-ensure">Wrap your code with try/finally or use context managers to ensure</h1>
    <h1 id="the-release-of-resources-at-the-end">the release of resources at the end.</h1>
    <p>&lt;/span&gt;<span class="n">y_bc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span></p>

    <p><span class="o">@</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s">&#8220;long&#8221;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">SCALAR_ITER</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">plus_y</span><span class="p">(</span><span class="n">batch_iter</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y_bc</span><span class="o">.</span><span class="n">value</span>  <span class="c1"># initialize states
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch_iter</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">pass</span>  <span class="c1"># release resources here, if any
</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">plus_y</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">&#8220;x&#8221;</span><span class="p">)))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +&#8212;&#8212;&#8212;+</span></p>
    <h1 id="plusyx">|plus_y(x)|</h1>
    <h1 id="section-22">+&#8212;&#8212;&#8212;+</h1>
    <h1 id="section-23">|        2|</h1>
    <h1 id="section-24">|        3|</h1>
    <h1 id="section-25">|        4|</h1>
    <h1 id="section-26">+&#8212;&#8212;&#8212;+</h1>
    <p>&lt;/span&gt;</p>
    <div><small>Find full example code at "examples/src/main/python/sql/arrow.py" in the Spark repo.</small></div>
  </div>
</div>

<h3 id="grouped-map">Grouped Map</h3>
<p>Grouped map Pandas UDFs are used with <code class="highlighter-rouge">groupBy().apply()</code> which implements the &#8220;split-apply-combine&#8221; pattern.
Split-apply-combine consists of three steps:</p>
<ul>
  <li>Split the data into groups by using <code class="highlighter-rouge">DataFrame.groupBy</code>.</li>
  <li>Apply a function on each group. The input and output of the function are both <code class="highlighter-rouge">pandas.DataFrame</code>. The
input data contains all the rows and columns for each group.</li>
  <li>Combine the results into a new <code class="highlighter-rouge">DataFrame</code>.</li>
</ul>

<p>To use <code class="highlighter-rouge">groupBy().apply()</code>, the user needs to define the following:</p>
<ul>
  <li>A Python function that defines the computation for each group.</li>
  <li>A <code class="highlighter-rouge">StructType</code> object or a string that defines the schema of the output <code class="highlighter-rouge">DataFrame</code>.</li>
</ul>

<p>The column labels of the returned <code class="highlighter-rouge">pandas.DataFrame</code> must either match the field names in the
defined output schema if specified as strings, or match the field data types by position if not
strings, e.g. integer indices. See <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame">pandas.DataFrame</a>
on how to label columns when constructing a <code class="highlighter-rouge">pandas.DataFrame</code>.</p>

<p>Note that all data for a group will be loaded into memory before the function is applied. This can
lead to out of memory exceptions, especially if the group sizes are skewed. The configuration for
<a href="#setting-arrow-batch-size">maxRecordsPerBatch</a> is not applied on groups and it is up to the user
to ensure that the grouped data will fit into the available memory.</p>

<p>The following example shows how to use <code class="highlighter-rouge">groupby().apply()</code> to subtract the mean from each value in the group.</p>

<div class="codetabs">
<div data-lang="python">
    <p><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span></p>

    <p><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)],</span>
    <span class="p">(</span><span class="s">&#8220;id&#8221;</span><span class="p">,</span> <span class="s">&#8220;v&#8221;</span><span class="p">))</span></p>

    <p><span class="o">@</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s">&#8220;id long, v double&#8221;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_MAP</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">subtract_mean</span><span class="p">(</span><span class="n">pdf</span><span class="p">):</span>
    <span class="c1"># pdf is a pandas.DataFrame
</span>    <span class="n">v</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">v</span>
    <span class="k">return</span> <span class="n">pdf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">v</span> <span class="o">-</span> <span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span></p>

    <p><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">&#8220;id&#8221;</span><span class="p">)</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">subtract_mean</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +&#8212;+&#8212;-+</span></p>
    <h1 id="id---v">| id|   v|</h1>
    <h1 id="section-27">+&#8212;+&#8212;-+</h1>
    <h1 id="section-28">|  1|-0.5|</h1>
    <h1 id="section-29">|  1| 0.5|</h1>
    <h1 id="section-30">|  2|-3.0|</h1>
    <h1 id="section-31">|  2|-1.0|</h1>
    <h1 id="section-32">|  2| 4.0|</h1>
    <h1 id="section-33">+&#8212;+&#8212;-+</h1>
    <p>&lt;/span&gt;</p>
    <div><small>Find full example code at "examples/src/main/python/sql/arrow.py" in the Spark repo.</small></div>
  </div>
</div>

<p>For detailed usage, please see <a href="api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"><code class="highlighter-rouge">pyspark.sql.functions.pandas_udf</code></a> and
<a href="api/python/pyspark.sql.html#pyspark.sql.GroupedData.apply"><code class="highlighter-rouge">pyspark.sql.GroupedData.apply</code></a>.</p>

<h3 id="grouped-aggregate">Grouped Aggregate</h3>

<p>Grouped aggregate Pandas UDFs are similar to Spark aggregate functions. Grouped aggregate Pandas UDFs are used with <code class="highlighter-rouge">groupBy().agg()</code> and
<a href="api/python/pyspark.sql.html#pyspark.sql.Window"><code class="highlighter-rouge">pyspark.sql.Window</code></a>. It defines an aggregation from one or more <code class="highlighter-rouge">pandas.Series</code>
to a scalar value, where each <code class="highlighter-rouge">pandas.Series</code> represents a column within the group or window.</p>

<p>Note that this type of UDF does not support partial aggregation and all data for a group or window will be loaded into memory. Also,
only unbounded window is supported with Grouped aggregate Pandas UDFs currently.</p>

<p>The following example shows how to use this type of UDF to compute mean with groupBy and window operations:</p>

<div class="codetabs">
<div data-lang="python">
    <p><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Window</span></p>

    <p><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)],</span>
    <span class="p">(</span><span class="s">&#8220;id&#8221;</span><span class="p">,</span> <span class="s">&#8220;v&#8221;</span><span class="p">))</span></p>

    <p><span class="o">@</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s">&#8220;double&#8221;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_AGG</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">mean_udf</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></p>

    <p><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">&#8220;id&#8221;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">mean_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#8216;v&#8217;</span><span class="p">]))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +&#8212;+&#8212;&#8212;&#8212;&#8211;+</span></p>
    <h1 id="idmeanudfv">| id|mean_udf(v)|</h1>
    <h1 id="section-34">+&#8212;+&#8212;&#8212;&#8212;&#8211;+</h1>
    <h1 id="section-35">|  1|        1.5|</h1>
    <h1 id="section-36">|  2|        6.0|</h1>
    <h1 id="section-37">+&#8212;+&#8212;&#8212;&#8212;&#8211;+</h1>
    <p>&lt;/span&gt;
<span class="n">w</span> <span class="o">=</span> <span class="n">Window</span> \
    <span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s">&#8216;id&#8217;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">rowsBetween</span><span class="p">(</span><span class="n">Window</span><span class="o">.</span><span class="n">unboundedPreceding</span><span class="p">,</span> <span class="n">Window</span><span class="o">.</span><span class="n">unboundedFollowing</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">&#8216;mean_v&#8217;</span><span class="p">,</span> <span class="n">mean_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#8216;v&#8217;</span><span class="p">])</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">w</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +&#8212;+&#8212;-+&#8212;&#8212;+</span></p>
    <h1 id="id---vmeanv">| id|   v|mean_v|</h1>
    <h1 id="section-38">+&#8212;+&#8212;-+&#8212;&#8212;+</h1>
    <h1 id="section-39">|  1| 1.0|   1.5|</h1>
    <h1 id="section-40">|  1| 2.0|   1.5|</h1>
    <h1 id="section-41">|  2| 3.0|   6.0|</h1>
    <h1 id="section-42">|  2| 5.0|   6.0|</h1>
    <h1 id="section-43">|  2|10.0|   6.0|</h1>
    <h1 id="section-44">+&#8212;+&#8212;-+&#8212;&#8212;+</h1>
    <p>&lt;/span&gt;</p>
    <div><small>Find full example code at "examples/src/main/python/sql/arrow.py" in the Spark repo.</small></div>
  </div>
</div>

<p>For detailed usage, please see <a href="api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"><code class="highlighter-rouge">pyspark.sql.functions.pandas_udf</code></a></p>

<h3 id="map-iterator">Map Iterator</h3>

<p>Map iterator Pandas UDFs are used to transform data with an iterator of batches. Map iterator
Pandas UDFs can be used with 
<a href="api/python/pyspark.sql.html#pyspark.sql.DataFrame.mapInPandas"><code class="highlighter-rouge">pyspark.sql.DataFrame.mapInPandas</code></a>.
It defines a map function that transforms an iterator of <code class="highlighter-rouge">pandas.DataFrame</code> to another.</p>

<p>It can return the output of arbitrary length in contrast to the scalar Pandas UDF. It maps an iterator of <code class="highlighter-rouge">pandas.DataFrame</code>s,
that represents the current <code class="highlighter-rouge">DataFrame</code>, using the map iterator UDF and returns the result as a <code class="highlighter-rouge">DataFrame</code>.</p>

<p>The following example shows how to create map iterator Pandas UDFs:</p>

<div class="codetabs">
<div data-lang="python">
    <p><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span></p>

    <p><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span></p>

    <p><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">30</span><span class="p">)],</span> <span class="p">(</span><span class="s">&#8220;id&#8221;</span><span class="p">,</span> <span class="s">&#8220;age&#8221;</span><span class="p">))</span></p>

    <p><span class="o">@</span><span class="n">pandas_udf</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">schema</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">MAP_ITER</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">filter_func</span><span class="p">(</span><span class="n">batch_iter</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">pdf</span> <span class="ow">in</span> <span class="n">batch_iter</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">pdf</span><span class="p">[</span><span class="n">pdf</span><span class="o">.</span><span class="nb">id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span></p>

    <p><span class="n">df</span><span class="o">.</span><span class="n">mapInPandas</span><span class="p">(</span><span class="n">filter_func</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +&#8212;+&#8212;+</span></p>
    <h1 id="idage">| id|age|</h1>
    <h1 id="section-45">+&#8212;+&#8212;+</h1>
    <h1 id="section-46">|  1| 21|</h1>
    <h1 id="section-47">+&#8212;+&#8212;+</h1>
    <p>&lt;/span&gt;</p>
    <div><small>Find full example code at "examples/src/main/python/sql/arrow.py" in the Spark repo.</small></div>
  </div>
</div>

<p>For detailed usage, please see <a href="api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"><code class="highlighter-rouge">pyspark.sql.functions.pandas_udf</code></a> and
<a href="api/python/pyspark.sql.html#pyspark.sql.DataFrame.mapInPandas"><code class="highlighter-rouge">pyspark.sql.DataFrame.mapsInPandas</code></a>.</p>

<h3 id="cogrouped-map">Cogrouped Map</h3>

<p>Cogrouped map Pandas UDFs allow two DataFrames to be cogrouped by a common key and then a python function applied to
each cogroup.  They are used with <code class="highlighter-rouge">groupBy().cogroup().apply()</code> which consists of the following steps:</p>

<ul>
  <li>Shuffle the data such that the groups of each dataframe which share a key are cogrouped together.</li>
  <li>Apply a function to each cogroup.  The input of the function is two <code class="highlighter-rouge">pandas.DataFrame</code> (with an optional Tuple
representing the key).  The output of the function is a <code class="highlighter-rouge">pandas.DataFrame</code>.</li>
  <li>Combine the pandas.DataFrames from all groups into a new <code class="highlighter-rouge">DataFrame</code>.</li>
</ul>

<p>To use <code class="highlighter-rouge">groupBy().cogroup().apply()</code>, the user needs to define the following:</p>
<ul>
  <li>A Python function that defines the computation for each cogroup.</li>
  <li>A <code class="highlighter-rouge">StructType</code> object or a string that defines the schema of the output <code class="highlighter-rouge">DataFrame</code>.</li>
</ul>

<p>The column labels of the returned <code class="highlighter-rouge">pandas.DataFrame</code> must either match the field names in the
defined output schema if specified as strings, or match the field data types by position if not
strings, e.g. integer indices. See <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame">pandas.DataFrame</a>
on how to label columns when constructing a <code class="highlighter-rouge">pandas.DataFrame</code>.</p>

<p>Note that all data for a cogroup will be loaded into memory before the function is applied. This can lead to out of
memory exceptions, especially if the group sizes are skewed. The configuration for <a href="#setting-arrow-batch-size">maxRecordsPerBatch</a>
is not applied and it is up to the user to ensure that the cogrouped data will fit into the available memory.</p>

<p>The following example shows how to use <code class="highlighter-rouge">groupby().cogroup().apply()</code> to perform an asof join between two datasets.</p>

<div class="codetabs">
<div data-lang="python">
    <p><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span></p>

    <p><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span></p>

    <p><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="p">[(</span><span class="mi">20000101</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">20000101</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">20000102</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">20000102</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)],</span>
    <span class="p">(</span><span class="s">&#8220;time&#8221;</span><span class="p">,</span> <span class="s">&#8220;id&#8221;</span><span class="p">,</span> <span class="s">&#8220;v1&#8221;</span><span class="p">))</span></p>

    <p><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="p">[(</span><span class="mi">20000101</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s">&#8220;x&#8221;</span><span class="p">),</span> <span class="p">(</span><span class="mi">20000101</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s">&#8220;y&#8221;</span><span class="p">)],</span>
    <span class="p">(</span><span class="s">&#8220;time&#8221;</span><span class="p">,</span> <span class="s">&#8220;id&#8221;</span><span class="p">,</span> <span class="s">&#8220;v2&#8221;</span><span class="p">))</span></p>

    <p><span class="o">@</span><span class="n">pandas_udf</span><span class="p">(</span><span class="s">&#8220;time int, id int, v1 double, v2 string&#8221;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">COGROUPED_MAP</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">asof_join</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge_asof</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">&#8220;time&#8221;</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s">&#8220;id&#8221;</span><span class="p">)</span></p>

    <p><span class="n">df1</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">&#8220;id&#8221;</span><span class="p">)</span><span class="o">.</span><span class="n">cogroup</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">&#8220;id&#8221;</span><span class="p">))</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">asof_join</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +&#8212;&#8212;&#8211;+&#8212;+&#8212;+&#8212;+</span></p>
    <h1 id="time-id-v1-v2">|    time| id| v1| v2|</h1>
    <h1 id="section-48">+&#8212;&#8212;&#8211;+&#8212;+&#8212;+&#8212;+</h1>
    <h1 id="x">|20000101|  1|1.0|  x|</h1>
    <h1 id="x-1">|20000102|  1|3.0|  x|</h1>
    <h1 id="y-1">|20000101|  2|2.0|  y|</h1>
    <h1 id="y-2">|20000102|  2|4.0|  y|</h1>
    <h1 id="section-49">+&#8212;&#8212;&#8211;+&#8212;+&#8212;+&#8212;+</h1>
    <p>&lt;/span&gt;</p>
    <div><small>Find full example code at "examples/src/main/python/sql/arrow.py" in the Spark repo.</small></div>
  </div>
</div>

<p>For detailed usage, please see <a href="api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"><code class="highlighter-rouge">pyspark.sql.functions.pandas_udf</code></a> and
<a href="api/python/pyspark.sql.html#pyspark.sql.CoGroupedData.apply"><code class="highlighter-rouge">pyspark.sql.CoGroupedData.apply</code></a>.</p>

<h2 id="usage-notes">Usage Notes</h2>

<h3 id="supported-sql-types">Supported SQL Types</h3>

<p>Currently, all Spark SQL data types are supported by Arrow-based conversion except <code class="highlighter-rouge">MapType</code>,
<code class="highlighter-rouge">ArrayType</code> of <code class="highlighter-rouge">TimestampType</code>, and nested <code class="highlighter-rouge">StructType</code>.</p>

<h3 id="setting-arrow-batch-size">Setting Arrow Batch Size</h3>

<p>Data partitions in Spark are converted into Arrow record batches, which can temporarily lead to
high memory usage in the JVM. To avoid possible out of memory exceptions, the size of the Arrow
record batches can be adjusted by setting the conf &#8220;spark.sql.execution.arrow.maxRecordsPerBatch&#8221;
to an integer that will determine the maximum number of rows for each batch. The default value is
10,000 records per batch. If the number of columns is large, the value should be adjusted
accordingly. Using this limit, each data partition will be made into 1 or more record batches for
processing.</p>

<h3 id="timestamp-with-time-zone-semantics">Timestamp with Time Zone Semantics</h3>

<p>Spark internally stores timestamps as UTC values, and timestamp data that is brought in without
a specified time zone is converted as local time to UTC with microsecond resolution. When timestamp
data is exported or displayed in Spark, the session time zone is used to localize the timestamp
values. The session time zone is set with the configuration &#8216;spark.sql.session.timeZone&#8217; and will
default to the JVM system local time zone if not set. Pandas uses a <code class="highlighter-rouge">datetime64</code> type with nanosecond
resolution, <code class="highlighter-rouge">datetime64[ns]</code>, with optional time zone on a per-column basis.</p>

<p>When timestamp data is transferred from Spark to Pandas it will be converted to nanoseconds
and each column will be converted to the Spark session time zone then localized to that time
zone, which removes the time zone and displays values as local time. This will occur
when calling <code class="highlighter-rouge">toPandas()</code> or <code class="highlighter-rouge">pandas_udf</code> with timestamp columns.</p>

<p>When timestamp data is transferred from Pandas to Spark, it will be converted to UTC microseconds. This
occurs when calling <code class="highlighter-rouge">createDataFrame</code> with a Pandas DataFrame or when returning a timestamp from a
<code class="highlighter-rouge">pandas_udf</code>. These conversions are done automatically to ensure Spark will have data in the
expected format, so it is not necessary to do any of these conversions yourself. Any nanosecond
values will be truncated.</p>

<p>Note that a standard UDF (non-Pandas) will load timestamp data as Python datetime objects, which is
different than a Pandas timestamp. It is recommended to use Pandas time series functionality when
working with timestamps in <code class="highlighter-rouge">pandas_udf</code>s to get the best performance, see
<a href="https://pandas.pydata.org/pandas-docs/stable/timeseries.html">here</a> for details.</p>

<h3 id="compatibiliy-setting-for-pyarrow--0150-and-spark-23x-24x">Compatibiliy Setting for PyArrow &gt;= 0.15.0 and Spark 2.3.x, 2.4.x</h3>

<p>Since Arrow 0.15.0, a change in the binary IPC format requires an environment variable to be
compatible with previous versions of Arrow &lt;= 0.14.1. This is only necessary to do for PySpark
users with versions 2.3.x and 2.4.x that have manually upgraded PyArrow to 0.15.0. The following
can be added to <code class="highlighter-rouge">conf/spark-env.sh</code> to use the legacy Arrow IPC format:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ARROW_PRE_0_15_IPC_FORMAT=1
</code></pre></div></div>

<p>This will instruct PyArrow &gt;= 0.15.0 to use the legacy IPC format with the older Arrow Java that
is in Spark 2.3.x and 2.4.x. Not setting this environment variable will lead to a similar error as
described in <a href="https://issues.apache.org/jira/browse/SPARK-29367">SPARK-29367</a> when running
<code class="highlighter-rouge">pandas_udf</code>s or <code class="highlighter-rouge">toPandas()</code> with Arrow enabled. More information about the Arrow IPC change can
be read on the Arrow 0.15.0 release <a href="http://arrow.apache.org/blog/2019/10/06/0.15.0-release/#columnar-streaming-protocol-change-since-0140">blog</a>.</p>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-3.4.1.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    </body>
</html>
