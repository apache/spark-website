
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>pyspark.sql.pandas.functions &#8212; PySpark 3.3.1 documentation</title>
    
  <link rel="stylesheet" href="../../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/pyspark.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../../../index.html">
    
      <img src="../../../../_static/spark-logo-reverse.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../getting_started/index.html">Getting Started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../user_guide/index.html">User Guide</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../reference/index.html">API Reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../development/index.html">Development</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../migration_guide/index.html">Migration Guide</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for pyspark.sql.pandas.functions</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>

<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">inspect</span> <span class="kn">import</span> <span class="n">getfullargspec</span><span class="p">,</span> <span class="n">signature</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">get_type_hints</span>

<span class="kn">from</span> <span class="nn">pyspark.rdd</span> <span class="kn">import</span> <span class="n">PythonEvalType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.pandas.typehints</span> <span class="kn">import</span> <span class="n">infer_eval_type</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.pandas.utils</span> <span class="kn">import</span> <span class="n">require_minimum_pandas_version</span><span class="p">,</span> <span class="n">require_minimum_pyarrow_version</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">DataType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.udf</span> <span class="kn">import</span> <span class="n">_create_udf</span>


<span class="k">class</span> <span class="nc">PandasUDFType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Pandas UDF Types. See :meth:`pyspark.sql.functions.pandas_udf`.&quot;&quot;&quot;</span>

    <span class="n">SCALAR</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span>

    <span class="n">SCALAR_ITER</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_ITER_UDF</span>

    <span class="n">GROUPED_MAP</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span>

    <span class="n">GROUPED_AGG</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_AGG_PANDAS_UDF</span>


<div class="viewcode-block" id="pandas_udf"><a class="viewcode-back" href="../../../../reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf">[docs]</a><span class="k">def</span> <span class="nf">pandas_udf</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">functionType</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a pandas user defined function (a.k.a. vectorized user defined function).</span>

<span class="sd">    Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer</span>
<span class="sd">    data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF</span>
<span class="sd">    is defined using the `pandas_udf` as a decorator or to wrap the function, and no</span>
<span class="sd">    additional configuration is required. A Pandas UDF behaves as a regular PySpark function</span>
<span class="sd">    API in general.</span>

<span class="sd">    .. versionadded:: 2.3.0</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    f : function, optional</span>
<span class="sd">        user-defined function. A python function if used as a standalone function</span>
<span class="sd">    returnType : :class:`pyspark.sql.types.DataType` or str, optional</span>
<span class="sd">        the return type of the user-defined function. The value can be either a</span>
<span class="sd">        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>
<span class="sd">    functionType : int, optional</span>
<span class="sd">        an enum value in :class:`pyspark.sql.functions.PandasUDFType`.</span>
<span class="sd">        Default: SCALAR. This parameter exists for compatibility.</span>
<span class="sd">        Using Python type hints is encouraged.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    In order to use this API, customarily the below are imported:</span>

<span class="sd">    &gt;&gt;&gt; import pandas as pd</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf</span>

<span class="sd">    From Spark 3.0 with Python 3.6+, `Python type hints &lt;https://www.python.org/dev/peps/pep-0484&gt;`_</span>
<span class="sd">    detect the function types as below:</span>

<span class="sd">    &gt;&gt;&gt; @pandas_udf(IntegerType())</span>
<span class="sd">    ... def slen(s: pd.Series) -&gt; pd.Series:</span>
<span class="sd">    ...     return s.str.len()</span>

<span class="sd">    Prior to Spark 3.0, the pandas UDF used `functionType` to decide the execution type as below:</span>

<span class="sd">    &gt;&gt;&gt; from pyspark.sql.functions import PandasUDFType</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.sql.types import IntegerType</span>
<span class="sd">    &gt;&gt;&gt; @pandas_udf(IntegerType(), PandasUDFType.SCALAR)</span>
<span class="sd">    ... def slen(s):</span>
<span class="sd">    ...     return s.str.len()</span>

<span class="sd">    It is preferred to specify type hints for the pandas UDF instead of specifying pandas UDF</span>
<span class="sd">    type via `functionType` which will be deprecated in the future releases.</span>

<span class="sd">    Note that the type hint should use `pandas.Series` in all cases but there is one variant</span>
<span class="sd">    that `pandas.DataFrame` should be used for its input or output type hint instead when the input</span>
<span class="sd">    or output column is of :class:`pyspark.sql.types.StructType`. The following example shows</span>
<span class="sd">    a Pandas UDF which takes long column, string column and struct column, and outputs a struct</span>
<span class="sd">    column. It requires the function to specify the type hints of `pandas.Series` and</span>
<span class="sd">    `pandas.DataFrame` as below:</span>

<span class="sd">    &gt;&gt;&gt; @pandas_udf(&quot;col1 string, col2 long&quot;)</span>
<span class="sd">    &gt;&gt;&gt; def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -&gt; pd.DataFrame:</span>
<span class="sd">    ...     s3[&#39;col2&#39;] = s1 + s2.str.len()</span>
<span class="sd">    ...     return s3</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; # Create a Spark DataFrame that has three columns including a struct column.</span>
<span class="sd">    ... df = spark.createDataFrame(</span>
<span class="sd">    ...     [[1, &quot;a string&quot;, (&quot;a nested string&quot;,)]],</span>
<span class="sd">    ...     &quot;long_col long, string_col string, struct_col struct&lt;col1:string&gt;&quot;)</span>
<span class="sd">    &gt;&gt;&gt; df.printSchema()</span>
<span class="sd">    root</span>
<span class="sd">    |-- long_column: long (nullable = true)</span>
<span class="sd">    |-- string_column: string (nullable = true)</span>
<span class="sd">    |-- struct_column: struct (nullable = true)</span>
<span class="sd">    |    |-- col1: string (nullable = true)</span>
<span class="sd">    &gt;&gt;&gt; df.select(func(&quot;long_col&quot;, &quot;string_col&quot;, &quot;struct_col&quot;)).printSchema()</span>
<span class="sd">    |-- func(long_col, string_col, struct_col): struct (nullable = true)</span>
<span class="sd">    |    |-- col1: string (nullable = true)</span>
<span class="sd">    |    |-- col2: long (nullable = true)</span>

<span class="sd">    In the following sections, it describes the combinations of the supported type hints. For</span>
<span class="sd">    simplicity, `pandas.DataFrame` variant is omitted.</span>

<span class="sd">    * Series to Series</span>
<span class="sd">        `pandas.Series`, ... -&gt; `pandas.Series`</span>

<span class="sd">        The function takes one or more `pandas.Series` and outputs one `pandas.Series`.</span>
<span class="sd">        The output of the function should always be of the same length as the input.</span>

<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;string&quot;)</span>
<span class="sd">        ... def to_upper(s: pd.Series) -&gt; pd.Series:</span>
<span class="sd">        ...     return s.str.upper()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([(&quot;John Doe&quot;,)], (&quot;name&quot;,))</span>
<span class="sd">        &gt;&gt;&gt; df.select(to_upper(&quot;name&quot;)).show()</span>
<span class="sd">        +--------------+</span>
<span class="sd">        |to_upper(name)|</span>
<span class="sd">        +--------------+</span>
<span class="sd">        |      JOHN DOE|</span>
<span class="sd">        +--------------+</span>

<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;first string, last string&quot;)</span>
<span class="sd">        ... def split_expand(s: pd.Series) -&gt; pd.DataFrame:</span>
<span class="sd">        ...     return s.str.split(expand=True)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame([(&quot;John Doe&quot;,)], (&quot;name&quot;,))</span>
<span class="sd">        &gt;&gt;&gt; df.select(split_expand(&quot;name&quot;)).show()</span>
<span class="sd">        +------------------+</span>
<span class="sd">        |split_expand(name)|</span>
<span class="sd">        +------------------+</span>
<span class="sd">        |       [John, Doe]|</span>
<span class="sd">        +------------------+</span>

<span class="sd">        .. note:: The length of the input is not that of the whole input column, but is the</span>
<span class="sd">            length of an internal batch used for each call to the function.</span>

<span class="sd">    * Iterator of Series to Iterator of Series</span>
<span class="sd">        `Iterator[pandas.Series]` -&gt; `Iterator[pandas.Series]`</span>

<span class="sd">        The function takes an iterator of `pandas.Series` and outputs an iterator of</span>
<span class="sd">        `pandas.Series`. In this case, the created pandas UDF instance requires one input</span>
<span class="sd">        column when this is called as a PySpark column. The length of the entire output from</span>
<span class="sd">        the function should be the same length of the entire input; therefore, it can</span>
<span class="sd">        prefetch the data from the input iterator as long as the lengths are the same.</span>

<span class="sd">        It is also useful when the UDF execution</span>
<span class="sd">        requires initializing some states although internally it works identically as</span>
<span class="sd">        Series to Series case. The pseudocode below illustrates the example.</span>

<span class="sd">        .. highlight:: python</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            @pandas_udf(&quot;long&quot;)</span>
<span class="sd">            def calculate(iterator: Iterator[pd.Series]) -&gt; Iterator[pd.Series]:</span>
<span class="sd">                # Do some expensive initialization with a state</span>
<span class="sd">                state = very_expensive_initialization()</span>
<span class="sd">                for x in iterator:</span>
<span class="sd">                    # Use that state for whole iterator.</span>
<span class="sd">                    yield calculate_with_state(x, state)</span>

<span class="sd">            df.select(calculate(&quot;value&quot;)).show()</span>

<span class="sd">        &gt;&gt;&gt; from typing import Iterator</span>
<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;long&quot;)</span>
<span class="sd">        ... def plus_one(iterator: Iterator[pd.Series]) -&gt; Iterator[pd.Series]:</span>
<span class="sd">        ...     for s in iterator:</span>
<span class="sd">        ...         yield s + 1</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[&quot;v&quot;]))</span>
<span class="sd">        &gt;&gt;&gt; df.select(plus_one(df.v)).show()</span>
<span class="sd">        +-----------+</span>
<span class="sd">        |plus_one(v)|</span>
<span class="sd">        +-----------+</span>
<span class="sd">        |          2|</span>
<span class="sd">        |          3|</span>
<span class="sd">        |          4|</span>
<span class="sd">        +-----------+</span>

<span class="sd">        .. note:: The length of each series is the length of a batch internally used.</span>

<span class="sd">    * Iterator of Multiple Series to Iterator of Series</span>
<span class="sd">        `Iterator[Tuple[pandas.Series, ...]]` -&gt; `Iterator[pandas.Series]`</span>

<span class="sd">        The function takes an iterator of a tuple of multiple `pandas.Series` and outputs an</span>
<span class="sd">        iterator of `pandas.Series`. In this case, the created pandas UDF instance requires</span>
<span class="sd">        input columns as many as the series when this is called as a PySpark column.</span>
<span class="sd">        Otherwise, it has the same characteristics and restrictions as Iterator of Series</span>
<span class="sd">        to Iterator of Series case.</span>

<span class="sd">        &gt;&gt;&gt; from typing import Iterator, Tuple</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.functions import struct, col</span>
<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;long&quot;)</span>
<span class="sd">        ... def multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -&gt; Iterator[pd.Series]:</span>
<span class="sd">        ...     for s1, df in iterator:</span>
<span class="sd">        ...         yield s1 * df.v</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[&quot;v&quot;]))</span>
<span class="sd">        &gt;&gt;&gt; df.withColumn(&#39;output&#39;, multiply(col(&quot;v&quot;), struct(col(&quot;v&quot;)))).show()</span>
<span class="sd">        +---+------+</span>
<span class="sd">        |  v|output|</span>
<span class="sd">        +---+------+</span>
<span class="sd">        |  1|     1|</span>
<span class="sd">        |  2|     4|</span>
<span class="sd">        |  3|     9|</span>
<span class="sd">        +---+------+</span>

<span class="sd">        .. note:: The length of each series is the length of a batch internally used.</span>

<span class="sd">    * Series to Scalar</span>
<span class="sd">        `pandas.Series`, ... -&gt; `Any`</span>

<span class="sd">        The function takes `pandas.Series` and returns a scalar value. The `returnType`</span>
<span class="sd">        should be a primitive data type, and the returned scalar can be either a python primitive</span>
<span class="sd">        type, e.g., int or float or a numpy data type, e.g., numpy.int64 or numpy.float64.</span>
<span class="sd">        `Any` should ideally be a specific scalar type accordingly.</span>

<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;double&quot;)</span>
<span class="sd">        ... def mean_udf(v: pd.Series) -&gt; float:</span>
<span class="sd">        ...     return v.mean()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (&quot;id&quot;, &quot;v&quot;))</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&quot;id&quot;).agg(mean_udf(df[&#39;v&#39;])).show()</span>
<span class="sd">        +---+-----------+</span>
<span class="sd">        | id|mean_udf(v)|</span>
<span class="sd">        +---+-----------+</span>
<span class="sd">        |  1|        1.5|</span>
<span class="sd">        |  2|        6.0|</span>
<span class="sd">        +---+-----------+</span>

<span class="sd">        This UDF can also be used as window functions as below:</span>

<span class="sd">        &gt;&gt;&gt; from pyspark.sql import Window</span>
<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;double&quot;)</span>
<span class="sd">        ... def mean_udf(v: pd.Series) -&gt; float:</span>
<span class="sd">        ...     return v.mean()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (&quot;id&quot;, &quot;v&quot;))</span>
<span class="sd">        &gt;&gt;&gt; w = Window.partitionBy(&#39;id&#39;).orderBy(&#39;v&#39;).rowsBetween(-1, 0)</span>
<span class="sd">        &gt;&gt;&gt; df.withColumn(&#39;mean_v&#39;, mean_udf(&quot;v&quot;).over(w)).show()</span>
<span class="sd">        +---+----+------+</span>
<span class="sd">        | id|   v|mean_v|</span>
<span class="sd">        +---+----+------+</span>
<span class="sd">        |  1| 1.0|   1.0|</span>
<span class="sd">        |  1| 2.0|   1.5|</span>
<span class="sd">        |  2| 3.0|   3.0|</span>
<span class="sd">        |  2| 5.0|   4.0|</span>
<span class="sd">        |  2|10.0|   7.5|</span>
<span class="sd">        +---+----+------+</span>

<span class="sd">        .. note:: For performance reasons, the input series to window functions are not copied.</span>
<span class="sd">            Therefore, mutating the input series is not allowed and will cause incorrect results.</span>
<span class="sd">            For the same reason, users should also not rely on the index of the input series.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The user-defined functions do not support conditional expressions or short circuiting</span>
<span class="sd">    in boolean expressions and it ends up with being executed all internally. If the functions</span>
<span class="sd">    can fail on special rows, the workaround is to incorporate the condition into the functions.</span>

<span class="sd">    The user-defined functions do not take keyword arguments on the calling side.</span>

<span class="sd">    The data type of returned `pandas.Series` from the user-defined functions should be</span>
<span class="sd">    matched with defined `returnType` (see :meth:`types.to_arrow_type` and</span>
<span class="sd">    :meth:`types.from_arrow_type`). When there is mismatch between them, Spark might do</span>
<span class="sd">    conversion on returned data. The conversion is not guaranteed to be correct and results</span>
<span class="sd">    should be checked for accuracy by users.</span>

<span class="sd">    Currently,</span>
<span class="sd">    :class:`pyspark.sql.types.ArrayType` of :class:`pyspark.sql.types.TimestampType` and</span>
<span class="sd">    nested :class:`pyspark.sql.types.StructType`</span>
<span class="sd">    are currently not supported as output types.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    pyspark.sql.GroupedData.agg</span>
<span class="sd">    pyspark.sql.DataFrame.mapInPandas</span>
<span class="sd">    pyspark.sql.GroupedData.applyInPandas</span>
<span class="sd">    pyspark.sql.PandasCogroupedOps.applyInPandas</span>
<span class="sd">    pyspark.sql.UDFRegistration.register</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># The following table shows most of Pandas data and SQL type conversions in Pandas UDFs that</span>
    <span class="c1"># are not yet visible to the user. Some of behaviors are buggy and might be changed in the near</span>
    <span class="c1"># future. The table might have to be eventually documented externally.</span>
    <span class="c1"># Please see SPARK-28132&#39;s PR to see the codes in order to generate the table below.</span>
    <span class="c1">#</span>
    <span class="c1"># +-----------------------------+----------------------+------------------+------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+------------------+--------------+--------------+--------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+--------------+-----------------+------------------+---------------+--------------------------------+  # noqa</span>
    <span class="c1"># |SQL Type \ Pandas Value(Type)|None(object(NoneType))|        True(bool)|           1(int8)|          1(int16)|            1(int32)|            1(int64)|          1(uint8)|         1(uint16)|         1(uint32)|         1(uint64)|  1.0(float16)|  1.0(float32)|  1.0(float64)|1970-01-01 00:00:00(datetime64[ns])|1970-01-01 00:00:00-05:00(datetime64[ns, US/Eastern])|a(object(string))|  1(object(Decimal))|[1 2 3](object(array[int32]))| 1.0(float128)|(1+0j)(complex64)|(1+0j)(complex128)|    A(category)|1 days 00:00:00(timedelta64[ns])|  # noqa</span>
    <span class="c1"># +-----------------------------+----------------------+------------------+------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+------------------+--------------+--------------+--------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+--------------+-----------------+------------------+---------------+--------------------------------+  # noqa</span>
    <span class="c1"># |                      boolean|                  None|              True|              True|              True|                True|                True|              True|              True|              True|              True|          True|          True|          True|                                  X|                                                    X|                X|                   X|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |                      tinyint|                  None|                 1|                 1|                 1|                   1|                   1|                 1|                 1|                 1|                 1|             1|             1|             1|                                  X|                                                    X|                X|                   1|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |                     smallint|                  None|                 1|                 1|                 1|                   1|                   1|                 1|                 1|                 1|                 1|             1|             1|             1|                                  X|                                                    X|                X|                   1|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |                          int|                  None|                 1|                 1|                 1|                   1|                   1|                 1|                 1|                 1|                 1|             1|             1|             1|                                  X|                                                    X|                X|                   1|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |                       bigint|                  None|                 1|                 1|                 1|                   1|                   1|                 1|                 1|                 1|                 1|             1|             1|             1|                                  0|                                       18000000000000|                X|                   1|                            X|             X|                X|                 X|              X|                  86400000000000|  # noqa</span>
    <span class="c1"># |                        float|                  None|               1.0|               1.0|               1.0|                 1.0|                 1.0|               1.0|               1.0|               1.0|               1.0|           1.0|           1.0|           1.0|                                  X|                                                    X|                X|                   X|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |                       double|                  None|               1.0|               1.0|               1.0|                 1.0|                 1.0|               1.0|               1.0|               1.0|               1.0|           1.0|           1.0|           1.0|                                  X|                                                    X|                X|                   X|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |                         date|                  None|                 X|                 X|                 X|datetime.date(197...|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|               datetime.date(197...|                                 datetime.date(197...|                X|datetime.date(197...|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |                    timestamp|                  None|                 X|                 X|                 X|                   X|datetime.datetime...|                 X|                 X|                 X|                 X|             X|             X|             X|               datetime.datetime...|                                 datetime.datetime...|                X|datetime.datetime...|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |                       string|                  None|                 X|                 X|                 X|                   X|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|                                  X|                                                    X|              &#39;a&#39;|                   X|                            X|             X|                X|                 X|            &#39;A&#39;|                               X|  # noqa</span>
    <span class="c1"># |                decimal(10,0)|                  None|                 X|                 X|                 X|                   X|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|                                  X|                                                    X|                X|        Decimal(&#39;1&#39;)|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |                   array&lt;int&gt;|                  None|                 X|                 X|                 X|                   X|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|                                  X|                                                    X|                X|                   X|                    [1, 2, 3]|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |              map&lt;string,int&gt;|                  None|                 X|                 X|                 X|                   X|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|                                  X|                                                    X|                X|                   X|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |               struct&lt;_1:int&gt;|                     X|                 X|                 X|                 X|                   X|                   X|                 X|                 X|                 X|                 X|             X|             X|             X|                                  X|                                                    X|                X|                   X|                            X|             X|                X|                 X|              X|                               X|  # noqa</span>
    <span class="c1"># |                       binary|                  None|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|  bytearray(b&#39;\x01&#39;)|  bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;\x01&#39;)|bytearray(b&#39;&#39;)|bytearray(b&#39;&#39;)|bytearray(b&#39;&#39;)|                     bytearray(b&#39;&#39;)|                                       bytearray(b&#39;&#39;)|  bytearray(b&#39;a&#39;)|                   X|                            X|bytearray(b&#39;&#39;)|   bytearray(b&#39;&#39;)|    bytearray(b&#39;&#39;)|bytearray(b&#39;A&#39;)|                  bytearray(b&#39;&#39;)|  # noqa</span>
    <span class="c1"># +-----------------------------+----------------------+------------------+------------------+------------------+--------------------+--------------------+------------------+------------------+------------------+------------------+--------------+--------------+--------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+--------------+-----------------+------------------+---------------+--------------------------------+  # noqa</span>
    <span class="c1">#</span>
    <span class="c1"># Note: DDL formatted string is used for &#39;SQL Type&#39; for simplicity. This string can be</span>
    <span class="c1">#       used in `returnType`.</span>
    <span class="c1"># Note: The values inside of the table are generated by `repr`.</span>
    <span class="c1"># Note: Python 3.9.5, Pandas 1.4.0 and PyArrow 6.0.1 are used.</span>
    <span class="c1"># Note: Timezone is KST.</span>
    <span class="c1"># Note: &#39;X&#39; means it throws an exception during the conversion.</span>
    <span class="n">require_minimum_pandas_version</span><span class="p">()</span>
    <span class="n">require_minimum_pyarrow_version</span><span class="p">()</span>

    <span class="c1"># decorator @pandas_udf(returnType, functionType)</span>
    <span class="n">is_decorator</span> <span class="o">=</span> <span class="n">f</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">DataType</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">is_decorator</span><span class="p">:</span>
        <span class="c1"># If DataType has been passed as a positional argument</span>
        <span class="c1"># for decorator use it as a returnType</span>
        <span class="n">return_type</span> <span class="o">=</span> <span class="n">f</span> <span class="ow">or</span> <span class="n">returnType</span>

        <span class="k">if</span> <span class="n">functionType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># @pandas_udf(dataType, functionType=functionType)</span>
            <span class="c1"># @pandas_udf(returnType=dataType, functionType=functionType)</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">functionType</span>
        <span class="k">elif</span> <span class="n">returnType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">returnType</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="c1"># @pandas_udf(dataType, functionType)</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">returnType</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># @pandas_udf(dataType) or @pandas_udf(returnType=dataType)</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">return_type</span> <span class="o">=</span> <span class="n">returnType</span>

        <span class="k">if</span> <span class="n">functionType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="n">functionType</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_type</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">return_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid return type: returnType can not be None&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">eval_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_ITER_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_AGG_PANDAS_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_MAP_PANDAS_ITER_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_MAP_ARROW_ITER_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_COGROUPED_MAP_PANDAS_UDF</span><span class="p">,</span>
        <span class="kc">None</span><span class="p">,</span>
    <span class="p">]:</span>  <span class="c1"># None means it should infer the type from type hints.</span>

        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function type: &quot;</span> <span class="s2">&quot;functionType must be one the values from PandasUDFType&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">is_decorator</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_create_pandas_udf</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">return_type</span><span class="p">,</span> <span class="n">evalType</span><span class="o">=</span><span class="n">eval_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_create_pandas_udf</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">return_type</span><span class="p">,</span> <span class="n">evalType</span><span class="o">=</span><span class="n">eval_type</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_create_pandas_udf</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">returnType</span><span class="p">,</span> <span class="n">evalType</span><span class="p">):</span>
    <span class="n">argspec</span> <span class="o">=</span> <span class="n">getfullargspec</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="c1"># pandas UDF by type hints.</span>
    <span class="k">if</span> <span class="n">evalType</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_ITER_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_AGG_PANDAS_UDF</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for &quot;</span>
            <span class="s2">&quot;pandas UDF instead of specifying pandas UDF type which will be deprecated &quot;</span>
            <span class="s2">&quot;in the future releases. See SPARK-28264 for more details.&quot;</span><span class="p">,</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">evalType</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_MAP_PANDAS_ITER_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_MAP_ARROW_ITER_UDF</span><span class="p">,</span>
        <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_COGROUPED_MAP_PANDAS_UDF</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="c1"># In case of &#39;SQL_GROUPED_MAP_PANDAS_UDF&#39;, deprecation warning is being triggered</span>
        <span class="c1"># at `apply` instead.</span>
        <span class="c1"># In case of &#39;SQL_MAP_PANDAS_ITER_UDF&#39;, &#39;SQL_MAP_ARROW_ITER_UDF&#39; and</span>
        <span class="c1"># &#39;SQL_COGROUPED_MAP_PANDAS_UDF&#39;, the evaluation type will always be set.</span>
        <span class="k">pass</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">argspec</span><span class="o">.</span><span class="n">annotations</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">type_hints</span> <span class="o">=</span> <span class="n">get_type_hints</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
            <span class="n">type_hints</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">evalType</span> <span class="o">=</span> <span class="n">infer_eval_type</span><span class="p">(</span><span class="n">signature</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="n">type_hints</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">evalType</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">evalType</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Set default is scalar UDF.</span>
        <span class="n">evalType</span> <span class="o">=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="p">(</span>
            <span class="n">evalType</span> <span class="o">==</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_UDF</span>
            <span class="ow">or</span> <span class="n">evalType</span> <span class="o">==</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_SCALAR_PANDAS_ITER_UDF</span>
        <span class="p">)</span>
        <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">argspec</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="ow">and</span> <span class="n">argspec</span><span class="o">.</span><span class="n">varargs</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function: 0-arg pandas_udfs are not supported. &quot;</span>
            <span class="s2">&quot;Instead, create a 1-arg pandas_udf and ignore the arg in your function.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">evalType</span> <span class="o">==</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">argspec</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function: pandas_udf with function type GROUPED_MAP or &quot;</span>
            <span class="s2">&quot;the function in groupby.applyInPandas &quot;</span>
            <span class="s2">&quot;must take either one argument (data) or two arguments (key, data).&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">evalType</span> <span class="o">==</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_COGROUPED_MAP_PANDAS_UDF</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">argspec</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function: the function in cogroup.applyInPandas &quot;</span>
            <span class="s2">&quot;must take either two arguments (left, right) &quot;</span>
            <span class="s2">&quot;or three arguments (key, left, right).&quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">_create_udf</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">returnType</span><span class="p">,</span> <span class="n">evalType</span><span class="p">)</span>
</pre></div>

              </div>
              
              
              <div class='prev-next-bottom'>
                

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright .<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>