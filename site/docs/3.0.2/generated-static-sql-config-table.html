
<table class="table">
<tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr>

<tr>
    <td><code>spark.sql.event.truncate.length</code></td>
    <td>2147483647</td>
    <td><p>Threshold of SQL length beyond which it will be truncated before adding to event. Defaults to no truncation. If set to 0, callsite will be logged instead.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.extensions</code></td>
    <td>(none)</td>
    <td><p>A comma-separated list of classes that implement Function1[SparkSessionExtensions, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor. For the case of function name conflicts, the last registered function name is used.</p></td>
    <td>2.2.0</td>
</tr>

<tr>
    <td><code>spark.sql.hive.metastore.barrierPrefixes</code></td>
    <td></td>
    <td><p>A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>).</p></td>
    <td>1.4.0</td>
</tr>

               <tr>
                   <td><code>spark.sql.hive.metastore.jars</code></td>
                   <td>builtin</td>
                   <td><p>Location of the jars that should be used to instantiate the HiveMetastoreClient.
This property can be one of three options: "
1. "builtin"
  Use Hive 2.3.7, which is bundled with the Spark assembly when
  <code>-Phive</code> is enabled. When this option is chosen,
  <code>spark.sql.hive.metastore.version</code> must be either
  <code>2.3.7</code> or not defined.
2. "maven"
  Use Hive jars of specified version downloaded from Maven repositories.
3. A classpath in the standard format for both Hive and Hadoop.</p></td>
                   <td>1.4.0</td>
               </tr>

<tr>
    <td><code>spark.sql.hive.metastore.sharedPrefixes</code></td>
    <td>com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc</td>
    <td><p>A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.</p></td>
    <td>1.4.0</td>
</tr>

<tr>
    <td><code>spark.sql.hive.metastore.version</code></td>
    <td>2.3.7</td>
    <td><p>Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>2.3.7</code> and <code>3.0.0</code> through <code>3.1.2</code>.</p></td>
    <td>1.4.0</td>
</tr>

<tr>
    <td><code>spark.sql.hive.thriftServer.singleSession</code></td>
    <td>false</td>
    <td><p>When set to true, Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.</p></td>
    <td>1.6.0</td>
</tr>

<tr>
    <td><code>spark.sql.legacy.sessionInitWithConfigDefaults</code></td>
    <td>false</td>
    <td><p>Flag to revert to legacy behavior where a cloned SparkSession receives SparkConf defaults, dropping any overrides in its parent SparkSession.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.queryExecutionListeners</code></td>
    <td>(none)</td>
    <td><p>List of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</p></td>
    <td>2.3.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.streamingQueryListeners</code></td>
    <td>(none)</td>
    <td><p>List of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</p></td>
    <td>2.4.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.ui.enabled</code></td>
    <td>true</td>
    <td><p>Whether to run the Structured Streaming Web UI for the Spark application when the Spark Web UI is enabled.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.ui.retainedProgressUpdates</code></td>
    <td>100</td>
    <td><p>The number of progress updates to retain for a streaming query for Structured Streaming UI.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.streaming.ui.retainedQueries</code></td>
    <td>100</td>
    <td><p>The number of inactive queries to retain for Structured Streaming UI.</p></td>
    <td>3.0.0</td>
</tr>

<tr>
    <td><code>spark.sql.ui.retainedExecutions</code></td>
    <td>1000</td>
    <td><p>Number of executions to retain in the Spark UI.</p></td>
    <td>1.5.0</td>
</tr>

<tr>
    <td><code>spark.sql.warehouse.dir</code></td>
    <td>(value of <code>$PWD/spark-warehouse</code>)</td>
    <td><p>The default location for managed databases and tables.</p></td>
    <td>2.0.0</td>
</tr>
</table>
