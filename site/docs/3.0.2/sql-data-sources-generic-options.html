
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Generic File Source Options - Spark 3.0.2 Documentation</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html">
                      <img src="img/spark-logo-hd.png" style="height:50px;"/></a><span class="version">3.0.2</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">Overview</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Programming Guides<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">Quick Start</a></li>
                                <li><a href="rdd-programming-guide.html">RDDs, Accumulators, Broadcasts Vars</a></li>
                                <li><a href="sql-programming-guide.html">SQL, DataFrames, and Datasets</a></li>
                                <li><a href="structured-streaming-programming-guide.html">Structured Streaming</a></li>
                                <li><a href="streaming-programming-guide.html">Spark Streaming (DStreams)</a></li>
                                <li><a href="ml-guide.html">MLlib (Machine Learning)</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX (Graph Processing)</a></li>
                                <li><a href="sparkr.html">SparkR (R on Spark)</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API Docs<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/org/apache/spark/index.html">Scala</a></li>
                                <li><a href="api/java/index.html">Java</a></li>
                                <li><a href="api/python/index.html">Python</a></li>
                                <li><a href="api/R/index.html">R</a></li>
                                <li><a href="api/sql/index.html">SQL, Built-in Functions</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Deploying<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">Overview</a></li>
                                <li><a href="submitting-applications.html">Submitting Applications</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark Standalone</a></li>
                                <li><a href="running-on-mesos.html">Mesos</a></li>
                                <li><a href="running-on-yarn.html">YARN</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">More<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">Configuration</a></li>
                                <li><a href="monitoring.html">Monitoring</a></li>
                                <li><a href="tuning.html">Tuning Guide</a></li>
                                <li><a href="job-scheduling.html">Job Scheduling</a></li>
                                <li><a href="security.html">Security</a></li>
                                <li><a href="hardware-provisioning.html">Hardware Provisioning</a></li>
                                <li><a href="migration-guide.html">Migration Guide</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">Building Spark</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">Contributing to Spark</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">Third Party Projects</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v3.0.2</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                
                    <div class="left-menu-wrapper">
    <div class="left-menu">
        <h3><a href="sql-programming-guide.html">Spark SQL Guide</a></h3>
        
<ul>

    <li>
        <a href="sql-getting-started.html">
            
                Getting Started
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources.html">
            
                Data Sources
            
        </a>
    </li>
    
    
        
<ul>

    <li>
        <a href="sql-data-sources-load-save-functions.html">
            
                Generic Load/Save Functions
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-generic-options.html">
            
                <b>Generic File Source Options</b>
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-parquet.html">
            
                Parquet Files
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-orc.html">
            
                ORC Files
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-json.html">
            
                JSON Files
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-hive-tables.html">
            
                Hive Tables
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-jdbc.html">
            
                JDBC To Other Databases
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-avro.html">
            
                Avro Files
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-binaryFile.html">
            
                Whole Binary Files
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-troubleshooting.html">
            
                Troubleshooting
            
        </a>
    </li>
    
    

</ul>

    

    <li>
        <a href="sql-performance-tuning.html">
            
                Performance Tuning
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-distributed-sql-engine.html">
            
                Distributed SQL Engine
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html">
            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-migration-old.html">
            
                Migration Guide
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-ref.html">
            
                SQL Reference
            
        </a>
    </li>
    
    

</ul>

    </div>
</div>
                
                <input id="nav-trigger" class="nav-trigger" checked type="checkbox">
                <label for="nav-trigger"></label>
                <div class="content-with-sidebar" id="content">
                    
                        <h1 class="title">Generic File Source Options</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#ignore-corrupt-files" id="markdown-toc-ignore-corrupt-files">Ignore Corrupt Files</a></li>
  <li><a href="#ignore-missing-files" id="markdown-toc-ignore-missing-files">Ignore Missing Files</a></li>
  <li><a href="#path-global-filter" id="markdown-toc-path-global-filter">Path Global Filter</a></li>
  <li><a href="#recursive-file-lookup" id="markdown-toc-recursive-file-lookup">Recursive File Lookup</a></li>
</ul>

<p>These generic options/configurations are effective only when using file-based sources: parquet, orc, avro, json, csv, text.</p>

<p>Please note that the hierarchy of directories used in examples below are:</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text">dir1/
 ├── dir2/
 │    └── file2.parquet (schema: &lt;file: string&gt;, content: "file2.parquet")
 └── file1.parquet (schema: &lt;file, string&gt;, content: "file1.parquet")
 └── file3.json (schema: &lt;file, string&gt;, content: "{'file':'corrupt.json'}")</code></pre></figure>

<h3 id="ignore-corrupt-files">Ignore Corrupt Files</h3>

<p>Spark allows you to use <code class="highlighter-rouge">spark.sql.files.ignoreCorruptFiles</code> to ignore corrupt files while reading data
from files. When set to true, the Spark jobs will continue to run when encountering corrupted files and
the contents that have been read will still be returned.</p>

<p>To ignore corrupt files while reading data files, you can use:</p>

<div class="codetabs">
<div data-lang="scala">
    <div class="highlight"><pre class="codehilite"><code><span class="c1">// enable ignore corrupt files</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"set spark.sql.files.ignoreCorruptFiles=true"</span><span class="o">)</span>
<span class="c1">// dir1/file3.json is corrupt from parquet's view</span>
<span class="k">val</span> <span class="nv">testCorruptDF</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">parquet</span><span class="o">(</span>
  <span class="s">"examples/src/main/resources/dir1/"</span><span class="o">,</span>
  <span class="s">"examples/src/main/resources/dir1/dir2/"</span><span class="o">)</span>
<span class="nv">testCorruptDF</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |         file|</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |file1.parquet|</span>
<span class="c1">// |file2.parquet|</span>
<span class="c1">// +-------------+</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala" in the Spark repo.</small></div>
  </div>

<div data-lang="java">
    <div class="highlight"><pre class="codehilite"><code><span class="c1">// enable ignore corrupt files</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">"set spark.sql.files.ignoreCorruptFiles=true"</span><span class="o">);</span>
<span class="c1">// dir1/file3.json is corrupt from parquet's view</span>
<span class="nc">Dataset</span><span class="o">&lt;</span><span class="nc">Row</span><span class="o">&gt;</span> <span class="n">testCorruptDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">parquet</span><span class="o">(</span>
        <span class="s">"examples/src/main/resources/dir1/"</span><span class="o">,</span>
        <span class="s">"examples/src/main/resources/dir1/dir2/"</span><span class="o">);</span>
<span class="n">testCorruptDF</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |         file|</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |file1.parquet|</span>
<span class="c1">// |file2.parquet|</span>
<span class="c1">// +-------------+</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java" in the Spark repo.</small></div>
  </div>

<div data-lang="python">
    <div class="highlight"><pre class="codehilite"><code><span class="c1"># enable ignore corrupt files
</span><span class="n">spark</span><span class="p">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"set spark.sql.files.ignoreCorruptFiles=true"</span><span class="p">)</span>
<span class="c1"># dir1/file3.json is corrupt from parquet's view
</span><span class="n">test_corrupt_df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">"examples/src/main/resources/dir1/"</span><span class="p">,</span>
                                     <span class="s">"examples/src/main/resources/dir1/dir2/"</span><span class="p">)</span>
<span class="n">test_corrupt_df</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +-------------+
# |         file|
# +-------------+
# |file1.parquet|
# |file2.parquet|
# +-------------+</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/python/sql/datasource.py" in the Spark repo.</small></div>
  </div>

<div data-lang="r">
    <div class="highlight"><pre class="codehilite"><code><span class="c1"># enable ignore corrupt files</span><span class="w">
</span><span class="n">sql</span><span class="p">(</span><span class="s2">"set spark.sql.files.ignoreCorruptFiles=true"</span><span class="p">)</span><span class="w">
</span><span class="c1"># dir1/file3.json is corrupt from parquet's view</span><span class="w">
</span><span class="n">testCorruptDF</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.parquet</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s2">"examples/src/main/resources/dir1/"</span><span class="p">,</span><span class="w"> </span><span class="s2">"examples/src/main/resources/dir1/dir2/"</span><span class="p">))</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">testCorruptDF</span><span class="p">)</span><span class="w">
</span><span class="c1">#            file</span><span class="w">
</span><span class="c1"># 1 file1.parquet</span><span class="w">
</span><span class="c1"># 2 file2.parquet</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/r/RSparkSQLExample.R" in the Spark repo.</small></div>
  </div>
</div>

<h3 id="ignore-missing-files">Ignore Missing Files</h3>

<p>Spark allows you to use <code class="highlighter-rouge">spark.sql.files.ignoreMissingFiles</code> to ignore missing files while reading data
from files. Here, missing file really means the deleted file under directory after you construct the
<code class="highlighter-rouge">DataFrame</code>. When set to true, the Spark jobs will continue to run when encountering missing files and
the contents that have been read will still be returned.</p>

<h3 id="path-global-filter">Path Global Filter</h3>

<p><code class="highlighter-rouge">pathGlobFilter</code> is used to only include files with file names matching the pattern.
The syntax follows <code>org.apache.hadoop.fs.GlobFilter</code>.
It does not change the behavior of partition discovery.</p>

<p>To load files with paths matching a given glob pattern while keeping the behavior of partition discovery,
you can use:</p>

<div class="codetabs">
<div data-lang="scala">
    <div class="highlight"><pre class="codehilite"><code><span class="k">val</span> <span class="nv">testGlobFilterDF</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"parquet"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"pathGlobFilter"</span><span class="o">,</span> <span class="s">"*.parquet"</span><span class="o">)</span> <span class="c1">// json file should be filtered out</span>
  <span class="o">.</span><span class="py">load</span><span class="o">(</span><span class="s">"examples/src/main/resources/dir1"</span><span class="o">)</span>
<span class="nv">testGlobFilterDF</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |         file|</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |file1.parquet|</span>
<span class="c1">// +-------------+</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala" in the Spark repo.</small></div>
  </div>

<div data-lang="java">
    <div class="highlight"><pre class="codehilite"><code><span class="nc">Dataset</span><span class="o">&lt;</span><span class="nc">Row</span><span class="o">&gt;</span> <span class="n">testGlobFilterDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">format</span><span class="o">(</span><span class="s">"parquet"</span><span class="o">)</span>
        <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">"pathGlobFilter"</span><span class="o">,</span> <span class="s">"*.parquet"</span><span class="o">)</span> <span class="c1">// json file should be filtered out</span>
        <span class="o">.</span><span class="na">load</span><span class="o">(</span><span class="s">"examples/src/main/resources/dir1"</span><span class="o">);</span>
<span class="n">testGlobFilterDF</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |         file|</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |file1.parquet|</span>
<span class="c1">// +-------------+</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java" in the Spark repo.</small></div>
  </div>

<div data-lang="python">
    <div class="highlight"><pre class="codehilite"><code><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"examples/src/main/resources/dir1"</span><span class="p">,</span>
                     <span class="nb">format</span><span class="o">=</span><span class="s">"parquet"</span><span class="p">,</span> <span class="n">pathGlobFilter</span><span class="o">=</span><span class="s">"*.parquet"</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +-------------+
# |         file|
# +-------------+
# |file1.parquet|
# +-------------+</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/python/sql/datasource.py" in the Spark repo.</small></div>
  </div>

<div data-lang="r">
    <div class="highlight"><pre class="codehilite"><code><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.df</span><span class="p">(</span><span class="s2">"examples/src/main/resources/dir1"</span><span class="p">,</span><span class="w"> </span><span class="s2">"parquet"</span><span class="p">,</span><span class="w"> </span><span class="n">pathGlobFilter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"*.parquet"</span><span class="p">)</span><span class="w">
</span><span class="c1">#            file</span><span class="w">
</span><span class="c1"># 1 file1.parquet</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/r/RSparkSQLExample.R" in the Spark repo.</small></div>
  </div>
</div>

<h3 id="recursive-file-lookup">Recursive File Lookup</h3>
<p><code class="highlighter-rouge">recursiveFileLookup</code> is used to recursively load files and it disables partition inferring. Its default value is <code class="highlighter-rouge">false</code>.
If data source explicitly specifies the <code class="highlighter-rouge">partitionSpec</code> when <code class="highlighter-rouge">recursiveFileLookup</code> is true, exception will be thrown.</p>

<p>To load all files recursively, you can use:</p>

<div class="codetabs">
<div data-lang="scala">
    <div class="highlight"><pre class="codehilite"><code><span class="k">val</span> <span class="nv">recursiveLoadedDF</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"parquet"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"recursiveFileLookup"</span><span class="o">,</span> <span class="s">"true"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">load</span><span class="o">(</span><span class="s">"examples/src/main/resources/dir1"</span><span class="o">)</span>
<span class="nv">recursiveLoadedDF</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |         file|</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |file1.parquet|</span>
<span class="c1">// |file2.parquet|</span>
<span class="c1">// +-------------+</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala" in the Spark repo.</small></div>
  </div>

<div data-lang="java">
    <div class="highlight"><pre class="codehilite"><code><span class="nc">Dataset</span><span class="o">&lt;</span><span class="nc">Row</span><span class="o">&gt;</span> <span class="n">recursiveLoadedDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">format</span><span class="o">(</span><span class="s">"parquet"</span><span class="o">)</span>
        <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">"recursiveFileLookup"</span><span class="o">,</span> <span class="s">"true"</span><span class="o">)</span>
        <span class="o">.</span><span class="na">load</span><span class="o">(</span><span class="s">"examples/src/main/resources/dir1"</span><span class="o">);</span>
<span class="n">recursiveLoadedDF</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |         file|</span>
<span class="c1">// +-------------+</span>
<span class="c1">// |file1.parquet|</span>
<span class="c1">// |file2.parquet|</span>
<span class="c1">// +-------------+</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java" in the Spark repo.</small></div>
  </div>

<div data-lang="python">
    <div class="highlight"><pre class="codehilite"><code><span class="n">recursive_loaded_df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"parquet"</span><span class="p">)</span>\
    <span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"recursiveFileLookup"</span><span class="p">,</span> <span class="s">"true"</span><span class="p">)</span>\
    <span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"examples/src/main/resources/dir1"</span><span class="p">)</span>
<span class="n">recursive_loaded_df</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +-------------+
# |         file|
# +-------------+
# |file1.parquet|
# |file2.parquet|
# +-------------+</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/python/sql/datasource.py" in the Spark repo.</small></div>
  </div>

<div data-lang="r">
    <div class="highlight"><pre class="codehilite"><code><span class="n">recursiveLoadedDF</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.df</span><span class="p">(</span><span class="s2">"examples/src/main/resources/dir1"</span><span class="p">,</span><span class="w"> </span><span class="s2">"parquet"</span><span class="p">,</span><span class="w"> </span><span class="n">recursiveFileLookup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"true"</span><span class="p">)</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">recursiveLoadedDF</span><span class="p">)</span><span class="w">
</span><span class="c1">#            file</span><span class="w">
</span><span class="c1"># 1 file1.parquet</span><span class="w">
</span><span class="c1"># 2 file2.parquet</span></code></pre></div>
    <div><small>Find full example code at "examples/src/main/r/RSparkSQLExample.R" in the Spark repo.</small></div>
  </div>
</div>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-3.4.1.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    </body>
</html>
