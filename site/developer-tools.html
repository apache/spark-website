<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
     Useful Developer Tools | Apache Spark
    
  </title>

  

  

  <!-- Bootstrap core CSS -->
  <link href="/css/cerulean.min.css" rel="stylesheet">
  <link href="/css/custom.css" rel="stylesheet">

  <!-- Code highlighter CSS -->
  <link href="/css/pygments-default.css" rel="stylesheet">

  <script type="text/javascript">
  <!-- Google Analytics initialization -->
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-32518208-2']);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

  <!-- Adds slight delay to links to allow async reporting -->
  function trackOutboundLink(link, category, action) {
    try {
      _gaq.push(['_trackEvent', category , action]);
    } catch(err){}

    setTimeout(function() {
      document.location.href = link.href;
    }, 100);
  }
  </script>

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
  <![endif]-->
</head>

<body>

<script src="https://code.jquery.com/jquery.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script>
<script src="/js/lang-tabs.js"></script>
<script src="/js/downloads.js"></script>

<div class="container" style="max-width: 1200px;">

<div class="masthead">
  
    <p class="lead">
      <a href="/">
      <img src="/images/spark-logo-trademark.png"
        style="height:100px; width:auto; vertical-align: bottom; margin-top: 20px;"></a><span class="tagline">
          Lightning-fast cluster computing
      </span>
    </p>
  
</div>

<nav class="navbar navbar-default" role="navigation">
  <!-- Brand and toggle get grouped for better mobile display -->
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse"
            data-target="#navbar-collapse-1">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
  </div>

  <!-- Collect the nav links, forms, and other content for toggling -->
  <div class="collapse navbar-collapse" id="navbar-collapse-1">
    <ul class="nav navbar-nav">
      <li><a href="/downloads.html">Download</a></li>
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">
          Libraries <b class="caret"></b>
        </a>
        <ul class="dropdown-menu">
          <li><a href="/sql/">SQL and DataFrames</a></li>
          <li><a href="/streaming/">Spark Streaming</a></li>
          <li><a href="/mllib/">MLlib (machine learning)</a></li>
          <li><a href="/graphx/">GraphX (graph)</a></li>
          <li class="divider"></li>
          <li><a href="/third-party-projects.html">Third-Party Projects</a></li>
        </ul>
      </li>
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">
          Documentation <b class="caret"></b>
        </a>
        <ul class="dropdown-menu">
          <li><a href="/docs/latest/">Latest Release (Spark 2.2.0)</a></li>
          <li><a href="/documentation.html">Older Versions and Other Resources</a></li>
          <li><a href="/faq.html">Frequently Asked Questions</a></li>
        </ul>
      </li>
      <li><a href="/examples.html">Examples</a></li>
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">
          Community <b class="caret"></b>
        </a>
        <ul class="dropdown-menu">
          <li><a href="/community.html">Mailing Lists &amp; Resources</a></li>
          <li><a href="/contributing.html">Contributing to Spark</a></li>
          <li><a href="/improvement-proposals.html">Improvement Proposals (SPIP)</a></li>
          <li><a href="https://issues.apache.org/jira/browse/SPARK">Issue Tracker</a></li>
          <li><a href="/powered-by.html">Powered By</a></li>
          <li><a href="/committers.html">Project Committers</a></li>
        </ul>
      </li>
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">
           Developers <b class="caret"></b>
        </a>
        <ul class="dropdown-menu">
          <li><a href="/developer-tools.html">Useful Developer Tools</a></li>
          <li><a href="/versioning-policy.html">Versioning Policy</a></li>
          <li><a href="/release-process.html">Release Process</a></li>
          <li><a href="/security.html">Security</a></li>
        </ul>
      </li>
    </ul>
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="https://www.apache.org/" class="dropdown-toggle" data-toggle="dropdown">
          Apache Software Foundation <b class="caret"></b></a>
        <ul class="dropdown-menu">
          <li><a href="https://www.apache.org/">Apache Homepage</a></li>
          <li><a href="https://www.apache.org/licenses/">License</a></li>
          <li><a href="https://www.apache.org/foundation/sponsorship.html">Sponsorship</a></li>
          <li><a href="https://www.apache.org/foundation/thanks.html">Thanks</a></li>
          <li><a href="https://www.apache.org/security/">Security</a></li>
        </ul>
      </li>
    </ul>
  </div>
  <!-- /.navbar-collapse -->
</nav>


<div class="row">
  <div class="col-md-3 col-md-push-9">
    <div class="news" style="margin-bottom: 20px;">
      <h5>Latest News</h5>
      <ul class="list-unstyled">
        
          <li><a href="/news/spark-summit-eu-2017-agenda-posted.html">Spark Summit Europe (October 24-26th, 2017, Dublin, Ireland) agenda posted</a>
          <span class="small">(Aug 28, 2017)</span></li>
        
          <li><a href="/news/spark-2-2-0-released.html">Spark 2.2.0 released</a>
          <span class="small">(Jul 11, 2017)</span></li>
        
          <li><a href="/news/spark-2-1-1-released.html">Spark 2.1.1 released</a>
          <span class="small">(May 02, 2017)</span></li>
        
          <li><a href="/news/spark-summit-june-2017-agenda-posted.html">Spark Summit (June 5-7th, 2017, San Francisco) agenda posted</a>
          <span class="small">(Mar 31, 2017)</span></li>
        
      </ul>
      <p class="small" style="text-align: right;"><a href="/news/index.html">Archive</a></p>
    </div>
    <div class="hidden-xs hidden-sm">
      <a href="/downloads.html" class="btn btn-success btn-lg btn-block" style="margin-bottom: 30px;">
        Download Spark
      </a>
      <p style="font-size: 16px; font-weight: 500; color: #555;">
        Built-in Libraries:
      </p>
      <ul class="list-none">
        <li><a href="/sql/">SQL and DataFrames</a></li>
        <li><a href="/streaming/">Spark Streaming</a></li>
        <li><a href="/mllib/">MLlib (machine learning)</a></li>
        <li><a href="/graphx/">GraphX (graph)</a></li>
      </ul>
      <a href="/third-party-projects.html">Third-Party Projects</a>
    </div>
  </div>

  <div class="col-md-9 col-md-pull-3">
    <h2>Useful Developer Tools</h2>

<h3 id="reducing-build-times">Reducing Build Times</h3>

<h4>SBT: Avoiding Re-Creating the Assembly JAR</h4>

<p>Spark&#8217;s default build strategy is to assemble a jar including all of its dependencies. This can 
be cumbersome when doing iterative development. When developing locally, it is possible to create 
an assembly jar including all of Spark&#8217;s dependencies and then re-package only Spark itself 
when making changes.</p>

<pre><code>$ build/sbt clean package
$ ./bin/spark-shell
$ export SPARK_PREPEND_CLASSES=true
$ ./bin/spark-shell # Now it's using compiled classes
# ... do some local development ... #
$ build/sbt compile
# ... do some local development ... #
$ build/sbt compile
$ unset SPARK_PREPEND_CLASSES
$ ./bin/spark-shell
 
# You can also use ~ to let sbt do incremental builds on file changes without running a new sbt session every time
$ build/sbt ~compile
</code></pre>

<h4>Maven: Speeding up Compilation with Zinc</h4>

<p><a href="https://github.com/typesafehub/zinc">Zinc</a> is a long-running server version of SBT&#8217;s incremental
compiler. When run locally as a background process, it speeds up builds of Scala-based projects
like Spark. Developers who regularly recompile Spark with Maven will be the most interested in
Zinc. The project site gives instructions for building and running <code>zinc</code>; OS X users can
install it using <code>brew install zinc</code>.</p>

<p>If using the <code>build/mvn</code> package <code>zinc</code> will automatically be downloaded and leveraged for all
builds. This process will auto-start after the first time <code>build/mvn</code> is called and bind to port
3030 unless the <code>ZINC_PORT</code> environment variable is set. The <code>zinc</code> process can subsequently be
shut down at any time by running <code>build/zinc-&lt;version&gt;/bin/zinc -shutdown</code> and will automatically
restart whenever <code>build/mvn</code> is called.</p>

<h3>Building submodules individually</h3>

<p>For instance, you can build the Spark Core module using:</p>

<pre><code>$ # sbt
$ build/sbt
&gt; project core
&gt; package

$ # or you can build the spark-core module with sbt directly using:
$ build/sbt core/package

$ # Maven
$ build/mvn package -DskipTests -pl core
</code></pre>

<p><a name="individual-tests"></a></p>
<h3 id="running-individual-tests">Running Individual Tests</h3>

<p>When developing locally, it&#8217;s often convenient to run a single test or a few tests, rather than running the entire test suite.</p>

<h4>Testing with SBT</h4>

<p>The fastest way to run individual tests is to use the <code>sbt</code> console. It&#8217;s fastest to keep a <code>sbt</code> console open, and use it to re-run tests as necessary.  For example, to run all of the tests in a particular project, e.g., <code>core</code>:</p>

<pre><code>$ build/sbt
&gt; project core
&gt; test
</code></pre>

<p>You can run a single test suite using the <code>testOnly</code> command.  For example, to run the DAGSchedulerSuite:</p>

<pre><code>&gt; testOnly org.apache.spark.scheduler.DAGSchedulerSuite
</code></pre>

<p>The <code>testOnly</code> command accepts wildcards; e.g., you can also run the <code>DAGSchedulerSuite</code> with:</p>

<pre><code>&gt; testOnly *DAGSchedulerSuite
</code></pre>

<p>Or you could run all of the tests in the scheduler package:</p>

<pre><code>&gt; testOnly org.apache.spark.scheduler.*
</code></pre>

<p>If you&#8217;d like to run just a single test in the <code>DAGSchedulerSuite</code>, e.g., a test that includes &#8220;SPARK-12345&#8221; in the name, you run the following command in the sbt console:</p>

<pre><code>&gt; testOnly *DAGSchedulerSuite -- -z "SPARK-12345"
</code></pre>

<p>If you&#8217;d prefer, you can run all of these commands on the command line (but this will be slower than running tests using an open cosole).  To do this, you need to surround <code>testOnly</code> and the following arguments in quotes:</p>

<pre><code>$ build/sbt "core/testOnly *DAGSchedulerSuite -- -z SPARK-12345"
</code></pre>

<p>For more about how to run individual tests with sbt, see the <a href="http://www.scala-sbt.org/0.13/docs/Testing.html">sbt documentation</a>.</p>

<h4>Testing with Maven</h4>

<p>With Maven, you can use the <code>-DwildcardSuites</code> flag to run individual Scala tests:</p>

<pre><code>build/mvn -Dtest=none -DwildcardSuites=org.apache.spark.scheduler.DAGSchedulerSuite test
</code></pre>

<p>You need <code>-Dtest=none</code> to avoid running the Java tests.  For more information about the ScalaTest Maven Plugin, refer to the <a href="http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin">ScalaTest documentation</a>.</p>

<p>To run individual Java tests, you can use the <code>-Dtest</code> flag:</p>

<pre><code>build/mvn test -DwildcardSuites=none -Dtest=org.apache.spark.streaming.JavaAPISuite test
</code></pre>

<h3>ScalaTest Issues</h3>

<p>If the following error occurs when running ScalaTest</p>

<pre><code>An internal error occurred during: "Launching XYZSuite.scala".
java.lang.NullPointerException
</code></pre>
<p>It is due to an incorrect Scala library in the classpath. To fix it:</p>

<ul>
  <li>Right click on project</li>
  <li>Select <code>Build Path | Configure Build Path</code></li>
  <li><code>Add Library | Scala Library</code></li>
  <li>Remove <code>scala-library-2.10.4.jar - lib_managed\jars</code></li>
</ul>

<p>In the event of &#8220;Could not find resource path for Web UI: org/apache/spark/ui/static&#8221;, 
it&#8217;s due to a classpath issue (some classes were probably not compiled). To fix this, it 
sufficient to run a test from the command line:</p>

<pre><code>build/sbt "test-only org.apache.spark.rdd.SortingSuite"
</code></pre>

<h3>Running Different Test Permutations on Jenkins</h3>

<p>When running tests for a pull request on Jenkins, you can add special phrases to the title of 
your pull request to change testing behavior. This includes:</p>

<ul>
  <li><code>[test-maven]</code> - signals to test the pull request using maven</li>
  <li><code>[test-hadoop2.7]</code> - signals to test using Spark&#8217;s Hadoop 2.7 profile</li>
</ul>

<h3>Binary compatibility</h3>

<p>To ensure binary compatibility, Spark uses <a href="https://github.com/typesafehub/migration-manager">MiMa</a>.</p>

<h4>Ensuring binary compatibility</h4>

<p>When working on an issue, it&#8217;s always a good idea to check that your changes do
not introduce binary incompatibilities before opening a pull request.</p>

<p>You can do so by running the following command:</p>

<pre><code>$ dev/mima
</code></pre>

<p>A binary incompatibility reported by MiMa might look like the following:</p>

<pre><code>[error] method this(org.apache.spark.sql.Dataset)Unit in class org.apache.spark.SomeClass does not have a correspondent in current version
[error] filter with: ProblemFilters.exclude[DirectMissingMethodProblem]("org.apache.spark.SomeClass.this")
</code></pre>

<p>If you open a pull request containing binary incompatibilities anyway, Jenkins
will remind you by failing the test build with the following message:</p>

<pre><code>Test build #xx has finished for PR yy at commit ffffff.

  This patch fails MiMa tests.
  This patch merges cleanly.
  This patch adds no public classes.
</code></pre>

<h4>Solving a binary incompatibility</h4>

<p>If you believe that your binary incompatibilies are justified or that MiMa
reported false positives (e.g. the reported binary incompatibilities are about a
non-user facing API), you can filter them out by adding an exclusion in
<a href="https://github.com/apache/spark/blob/master/project/MimaExcludes.scala">project/MimaExcludes.scala</a>
containing what was suggested by the MiMa report and a comment containing the
JIRA number of the issue you&#8217;re working on as well as its title.</p>

<p>For the problem described above, we might add the following:</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// [SPARK-zz][CORE] Fix an issue</span>
<span class="nc">ProblemFilters</span><span class="o">.</span><span class="n">exclude</span><span class="o">[</span><span class="kt">DirectMissingMethodProblem</span><span class="o">](</span><span class="s">&quot;org.apache.spark.SomeClass.this&quot;</span><span class="o">)</span></code></pre></figure>

<p>Otherwise, you will have to resolve those incompatibilies before opening or
updating your pull request. Usually, the problems reported by MiMa are
self-explanatory and revolve around missing members (methods or fields) that
you will have to add back in order to maintain binary compatibility.</p>

<h3>Checking Out Pull Requests</h3>

<p>Git provides a mechanism for fetching remote pull requests into your own local repository. 
This is useful when reviewing code or testing patches locally. If you haven&#8217;t yet cloned the 
Spark Git repository, use the following command:</p>

<pre><code>$ git clone https://github.com/apache/spark.git
$ cd spark
</code></pre>

<p>To enable this feature you&#8217;ll need to configure the git remote repository to fetch pull request 
data. Do this by modifying the <code>.git/config</code> file inside of your Spark directory. The remote may 
not be named &#8220;origin&#8221; if you&#8217;ve named it something else:</p>

<pre><code>[remote "origin"]
  url = git@github.com:apache/spark.git
  fetch = +refs/heads/*:refs/remotes/origin/*
  fetch = +refs/pull/*/head:refs/remotes/origin/pr/*   # Add this line
</code></pre>

<p>Once you&#8217;ve done this you can fetch remote pull requests</p>

<pre><code># Fetch remote pull requests
$ git fetch origin
# Checkout a remote pull request
$ git checkout origin/pr/112
# Create a local branch from a remote pull request
$ git checkout origin/pr/112 -b new-branch
</code></pre>

<h3>Generating Dependency Graphs</h3>

<pre><code>$ # sbt
$ build/sbt dependency-tree
 
$ # Maven
$ build/mvn -DskipTests install
$ build/mvn dependency:tree
</code></pre>

<h3>Organizing Imports</h3>

<p>You can use a <a href="https://plugins.jetbrains.com/plugin/7350">IntelliJ Imports Organizer</a> 
from Aaron Davidson to help you organize the imports in 
your code.  It can be configured to match the import ordering from the style guide.</p>

<h3>IDE Setup</h3>

<h4>IntelliJ</h4>

<p>While many of the Spark developers use SBT or Maven on the command line, the most common IDE we 
use is IntelliJ IDEA. You can get the community edition for free (Apache committers can get 
free IntelliJ Ultimate Edition licenses) and install the JetBrains Scala plugin from <code>Preferences &gt; Plugins</code>.</p>

<p>To create a Spark project for IntelliJ:</p>

<ul>
  <li>Download IntelliJ and install the 
<a href="https://confluence.jetbrains.com/display/SCA/Scala+Plugin+for+IntelliJ+IDEA">Scala plug-in for IntelliJ</a>.</li>
  <li>Go to <code>File -&gt; Import Project</code>, locate the spark source directory, and select &#8220;Maven Project&#8221;.</li>
  <li>In the Import wizard, it&#8217;s fine to leave settings at their default. However it is usually useful 
to enable &#8220;Import Maven projects automatically&#8221;, since changes to the project structure will 
automatically update the IntelliJ project.</li>
  <li>As documented in <a href="https://spark.apache.org/docs/latest/building-spark.html">Building Spark</a>, 
some build configurations require specific profiles to be 
enabled. The same profiles that are enabled with <code>-P[profile name]</code> above may be enabled on the 
Profiles screen in the Import wizard. For example, if developing for Hadoop 2.7 with YARN support, 
enable profiles <code>yarn</code> and <code>hadoop-2.7</code>. These selections can be changed later by accessing the 
&#8220;Maven Projects&#8221; tool window from the View menu, and expanding the Profiles section.</li>
</ul>

<p>Other tips:</p>

<ul>
  <li>&#8220;Rebuild Project&#8221; can fail the first time the project is compiled, because generate source files 
are not automatically generated. Try clicking the &#8220;Generate Sources and Update Folders For All 
Projects&#8221; button in the &#8220;Maven Projects&#8221; tool window to manually generate these sources.</li>
  <li>Some of the modules have pluggable source directories based on Maven profiles (i.e. to support 
both Scala 2.11 and 2.10 or to allow cross building against different versions of Hive). In some 
cases IntelliJ&#8217;s does not correctly detect use of the maven-build-plugin to add source directories. 
In these cases, you may need to add source locations explicitly to compile the entire project. If 
so, open the &#8220;Project Settings&#8221; and select &#8220;Modules&#8221;. Based on your selected Maven profiles, you 
may need to add source folders to the following modules:
    <ul>
      <li>spark-hive: add v0.13.1/src/main/scala</li>
      <li>spark-streaming-flume-sink: add target\scala-2.11\src_managed\main\compiled_avro</li>
      <li>spark-catalyst: add target\scala-2.11\src_managed\main</li>
    </ul>
  </li>
  <li>Compilation may fail with an error like &#8220;scalac: bad option: 
-P:/home/jakub/.m2/repository/org/scalamacros/paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar&#8221;. 
If so, go to Preferences &gt; Build, Execution, Deployment &gt; Scala Compiler and clear the &#8220;Additional 
compiler options&#8221; field.  It will work then although the option will come back when the project 
reimports.  If you try to build any of the projects using quasiquotes (eg., sql) then you will 
need to make that jar a compiler plugin (just below &#8220;Additional compiler options&#8221;). 
Otherwise you will see errors like:
    <pre><code>/Users/irashid/github/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala
Error:(147, 9) value q is not a member of StringContext
 Note: implicit class Evaluate2 is not applicable here because it comes after the application point and it lacks an explicit result type
      q"""
      ^ 
</code></pre>
  </li>
</ul>

<h4>Eclipse</h4>

<p>Eclipse can be used to develop and test Spark. The following configuration is known to work:</p>

<ul>
  <li>Eclipse Juno</li>
  <li><a href="http://scala-ide.org/">Scala IDE 4.0</a></li>
  <li>Scala Test</li>
</ul>

<p>The easiest way is to download the Scala IDE bundle from the Scala IDE download page. It comes 
pre-installed with ScalaTest. Alternatively, use the Scala IDE update site or Eclipse Marketplace.</p>

<p>SBT can create Eclipse <code>.project</code> and <code>.classpath</code> files. To create these files for each Spark sub 
project, use this command:</p>

<pre><code>sbt/sbt eclipse
</code></pre>

<p>To import a specific project, e.g. spark-core, select <code>File | Import | Existing Projects</code> into 
Workspace. Do not select &#8220;Copy projects into workspace&#8221;.</p>

<p>If you want to develop on Scala 2.10 you need to configure a Scala installation for the 
exact Scala version thatâ€™s used to compile Spark. 
 Since Scala IDE bundles the latest versions (2.10.5 and 2.11.8 at this point), you need to add one 
in <code>Eclipse Preferences -&gt; Scala -&gt; Installations</code> by pointing to the <code>lib/</code> directory of your 
Scala 2.10.5 distribution. Once this is done, select all Spark projects and right-click, 
choose <code>Scala -&gt; Set Scala Installation</code> and point to the 2.10.5 installation. 
This should clear all errors about invalid cross-compiled libraries. A clean build should succeed now.</p>

<p>ScalaTest can execute unit tests by right clicking a source file and selecting <code>Run As | Scala Test</code>.</p>

<p>If Java memory errors occur, it might be necessary to increase the settings in <code>eclipse.ini</code> 
in the Eclipse install directory. Increase the following setting as needed:</p>

<pre><code>--launcher.XXMaxPermSize
256M
</code></pre>

<p><a name="nightly-builds"></a></p>
<h3>Nightly Builds</h3>

<p>Packages are built regularly off of Spark&#8217;s master branch and release branches. These provide 
Spark developers access to the bleeding-edge of Spark master or the most recent fixes not yet 
incorporated into a maintenance release. These should only be used by Spark developers, as they 
may have bugs and have not undergone the same level of testing as releases. Spark nightly packages 
are available at:</p>

<ul>
  <li>Latest master build: <a href="https://people.apache.org/~pwendell/spark-nightly/spark-master-bin/latest">https://people.apache.org/~pwendell/spark-nightly/spark-master-bin/latest</a></li>
  <li>All nightly builds: <a href="https://people.apache.org/~pwendell/spark-nightly/">https://people.apache.org/~pwendell/spark-nightly/</a></li>
</ul>

<p>Spark also publishes SNAPSHOT releases of its Maven artifacts for both master and maintenance 
branches on a nightly basis. To link to a SNAPSHOT you need to add the ASF snapshot 
repository to your build. Note that SNAPSHOT artifacts are ephemeral and may change or
be removed. To use these you must add the ASF snapshot repository at 
&lt;a href=&#8221;https://repository.apache.org/snapshots/<a>.</a></p>

<pre><code>groupId: org.apache.spark
artifactId: spark-core_2.10
version: 1.5.0-SNAPSHOT
</code></pre>

<p><a name="profiling"></a></p>
<h3>Profiling Spark Applications Using YourKit</h3>

<p>Here are instructions on profiling Spark applications using YourKit Java Profiler.</p>

<h4>On Spark EC2 images</h4>

<ul>
  <li>After logging into the master node, download the YourKit Java Profiler for Linux from the 
<a href="https://www.yourkit.com/download/index.jsp">YourKit downloads page</a>. 
This file is pretty big (~100 MB) and YourKit downloads site is somewhat slow, so you may 
consider mirroring this file or including it on a custom AMI.</li>
  <li>Untar this file somewhere (in <code>/root</code> in our case): <code>tar xvjf yjp-12.0.5-linux.tar.bz2</code></li>
  <li>Copy the expanded YourKit files to each node using copy-dir: <code>~/spark-ec2/copy-dir /root/yjp-12.0.5</code></li>
  <li>Configure the Spark JVMs to use the YourKit profiling agent by editing <code>~/spark/conf/spark-env.sh</code> 
and adding the lines
    <pre><code>SPARK_DAEMON_JAVA_OPTS+=" -agentpath:/root/yjp-12.0.5/bin/linux-x86-64/libyjpagent.so=sampling"
export SPARK_DAEMON_JAVA_OPTS
SPARK_JAVA_OPTS+=" -agentpath:/root/yjp-12.0.5/bin/linux-x86-64/libyjpagent.so=sampling"
export SPARK_JAVA_OPTS
</code></pre>
  </li>
  <li>Copy the updated configuration to each node: <code>~/spark-ec2/copy-dir ~/spark/conf/spark-env.sh</code></li>
  <li>Restart your Spark cluster: <code>~/spark/bin/stop-all.sh</code> and <code>~/spark/bin/start-all.sh</code></li>
  <li>By default, the YourKit profiler agents use ports 10001-10010. To connect the YourKit desktop 
application to the remote profiler agents, you&#8217;ll have to open these ports in the cluster&#8217;s EC2 
security groups. To do this, sign into the AWS Management Console. Go to the EC2 section and 
select <code>Security Groups</code> from the <code>Network &amp; Security</code> section on the left side of the page. 
Find the security groups corresponding to your cluster; if you launched a cluster named <code>test_cluster</code>, 
then you will want to modify the settings for the <code>test_cluster-slaves</code> and <code>test_cluster-master</code> 
security groups. For each group, select it from the list, click the <code>Inbound</code> tab, and create a 
new <code>Custom TCP Rule</code> opening the port range <code>10001-10010</code>. Finally, click <code>Apply Rule Changes</code>. 
Make sure to do this for both security groups.
Note: by default, <code>spark-ec2</code> re-uses security groups: if you stop this cluster and launch another 
cluster with the same name, your security group settings will be re-used.</li>
  <li>Launch the YourKit profiler on your desktop.</li>
  <li>Select &#8220;Connect to remote application&#8230;&#8221; from the welcome screen and enter the the address of your Spark master or worker machine, e.g. <code>ec2--.compute-1.amazonaws.com</code></li>
  <li>YourKit should now be connected to the remote profiling agent. It may take a few moments for profiling information to appear.</li>
</ul>

<p>Please see the full YourKit documentation for the full list of profiler agent
<a href="http://www.yourkit.com/docs/80/help/startup_options.jsp">startup options</a>.</p>

<h4>In Spark unit tests</h4>

<p>When running Spark tests through SBT, add <code>javaOptions in Test += "-agentpath:/path/to/yjp"</code>
to <code>SparkBuild.scala</code> to launch the tests with the YourKit profiler agent enabled.<br />
The platform-specific paths to the profiler agents are listed in the 
<a href="http://www.yourkit.com/docs/80/help/agent.jsp">YourKit documentation</a>.</p>

  </div>
</div>



<footer class="small">
  <hr>
  Apache Spark, Spark, Apache, and the Spark logo are <a href="/trademarks.html">trademarks</a> of
  <a href="https://www.apache.org">The Apache Software Foundation</a>.
</footer>

</div>

</body>
</html>
